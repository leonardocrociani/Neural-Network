{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 76. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 1.0434\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 121. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9160\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 105. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9791\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 104. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9587\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 169. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9601\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 193. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9606\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 193. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9988\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 193. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9494\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 222. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9728\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 137. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8481\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 169. Restoring best model parameters.\n",
      "Early stopping triggered at epoch 111. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9699\n",
      "\n",
      "Fold 2/5Fold 2 Evaluation Metric: 0.8993\n",
      "\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 111. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8273\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 111. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8114\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 236. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9438\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 73. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.2321\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 67. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.1861\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 102. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.0296\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 191. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8362\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 127. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.0025\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 71. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.1452\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 105. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0580\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 71. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.1103\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 191. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8524\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 231. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8168\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 101. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0407\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 231. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.9052\n",
      "Early stopping triggered at epoch 231. Restoring best model parameters.\n",
      "\n",
      "Fold 3/5\n",
      "Fold 2 Evaluation Metric: 0.8857\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 286. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.7858\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 232. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8412\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 89. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 1.0310\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.9898\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.9898\n",
      "Early stopping triggered at epoch 137. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.1294\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 90. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 1.0442\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 1.0221\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 1.0221\n",
      "Early stopping triggered at epoch 115. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 1.0130\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 1.0363\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 1.0363\n",
      "Early stopping triggered at epoch 89. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 1.0524\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.9974\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.9974\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 137. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.1415\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 137. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.1512\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 149. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.2266\n",
      "\n",
      "Fold 4/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 137. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.1617\n",
      "\n",
      "Fold 4/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 149. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.2177\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 137. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.2089\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 182. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9974\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 121. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9264\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 182. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0118\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 117. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9293\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 135. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9411\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 188. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0253\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 236. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9524\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 188. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0377\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 193. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0801\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 155. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.1184\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 193. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0389\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 111. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8262\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 109. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8386\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 111. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.9084\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 67. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.1985\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 73. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.2427\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 242. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.9253\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.9697\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.9697\n",
      "Early stopping triggered at epoch 102. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.0431\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 242. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.9320\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.9815\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.9815\n",
      "Early stopping triggered at epoch 231. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8591\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 71. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.1233\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 242. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.9474\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.9780\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.9780\n",
      "Early stopping triggered at epoch 71. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.1606\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 242. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.9757\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 1.0341\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 1.0341\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 242. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.9878\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 1.0397\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 1.0397\n",
      "Early stopping triggered at epoch 242. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.9542\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.9800\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.9800\n",
      "Early stopping triggered at epoch 118. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0269\n",
      "\n",
      "Fold 5/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 242. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.9529\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.9972\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.9972\n",
      "Early stopping triggered at epoch 90. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 1.0582\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 1.0265\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 1.0265\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 137. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.2155\n",
      "\n",
      "Fold 4/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 107. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 1.0065\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.9956\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.9956\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 135. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9348\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 115. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 1.0187\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 1.0275\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 1.0275\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 76. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 1.0434\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 104. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9587\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 76. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 1.0434\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 169. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9601\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 104. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9587\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 169. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9601\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 111. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8936\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 137. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8481\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 236. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9438\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 111. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8114\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 236. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9438\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 193. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0517\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 111. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8114\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 73. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.2409\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 137. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8481\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 169. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9699\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 67. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.1861\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 127. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.0025\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 191. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8362\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 67. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.1861\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 191. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8362\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 101. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0535\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 127. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.0025\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 71. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.1103\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 71. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.1103\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 101. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0407\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 232. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8412\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 137. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.1294\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 191. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8524\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 89. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 1.0592\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 1.0364\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 1.0364\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 218. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.9828\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 1.0123\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 1.0123\n",
      "Early stopping triggered at epoch 137. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.1294\n",
      "\n",
      "Fold 4/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 90. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 1.0442\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 1.0221\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 1.0221\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 89. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 1.0524\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.9974\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.9974\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 101. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0407\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 90. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 1.0442\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 1.0221\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 1.0221\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 89. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 1.0524\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.9974\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.9974\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 137. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.1415\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 137. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.2089\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 121. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9264\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 232. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8412\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 182. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9974\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 135. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9348\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 182. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9974\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 236. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9524\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 169. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9699\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 121. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9264\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 111. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8262\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 236. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9524\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 182. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0118\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 193. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0389\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 111. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8936\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 137. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.2089\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 67. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.1985\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 73. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.2409\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 111. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8262\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 71. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.1233\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 191. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8524\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 231. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8591\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 242. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.9253\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.9697\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.9697\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 67. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.1985\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 242. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.9253\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.9697\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.9697\n",
      "Early stopping triggered at epoch 90. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 1.0582\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 1.0265\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 1.0265\n",
      "Early stopping triggered at epoch 101. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0535\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 137. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.1415\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 231. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8591\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 71. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.1233\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 193. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0389\n",
      "Early stopping triggered at epoch 242. Restoring best model parameters.\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 Evaluation Metric: 0.9320\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.9815\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.9815\n",
      "Early stopping triggered at epoch 89. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 1.0592\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 1.0364\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 1.0364\n",
      "Early stopping triggered at epoch 242. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.9529\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.9972\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.9972\n",
      "Early stopping triggered at epoch 135. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.9348\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 137. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.2155\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 90. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 1.0582\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 1.0265\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 1.0265\n",
      "Early stopping triggered at epoch 137. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.2155\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 182. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0118\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 111. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8936\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 73. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 1.2409\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 193. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0517\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 242. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.9529\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.9972\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.9972\n",
      "Early stopping triggered at epoch 193. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0517\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 101. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 1.0535\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 242. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.9320\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.9815\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.9815\n",
      "Early stopping triggered at epoch 89. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 1.0592\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 1.0364\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 1.0364\n",
      "Early stopping triggered at epoch 218. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.9828\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 1.0123\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 1.0123\n",
      "Early stopping triggered at epoch 218. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.9828\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 1.0123\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 1.0123\n",
      "\n",
      "Grid Search Results:\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.9780\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 1.0341\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.9697\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.9972\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.9898\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 1.0363\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 1.0221\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.9974\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.9800\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 1.0397\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.9815\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 1.0123\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.9956\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 1.0275\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 1.0265\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 1.0364\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.9697\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.9972\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.9697\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.9972\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 1.0221\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.9974\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 1.0221\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.9974\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.9815\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 1.0123\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.9815\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 1.0123\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 1.0265\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 1.0364\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 1.0265\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 1.0364\n",
      "\n",
      "Best Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Best Average Metric: 1.0397\n",
      "\n",
      "Final Grid Search Best Result:\n",
      "{'params': {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, 'average_metric': np.float64(1.0396927494224422)}\n",
      "Epoch    0, Training Loss: 12.5791, Validation Loss: 11.0229, Learning Rate: 0.100000\n",
      "Epoch    1, Training Loss: 12.4045, Validation Loss: 10.8848, Learning Rate: 0.099900\n",
      "Epoch    2, Training Loss: 12.2466, Validation Loss: 10.7596, Learning Rate: 0.099800\n",
      "Epoch    3, Training Loss: 12.1063, Validation Loss: 10.6471, Learning Rate: 0.099700\n",
      "Epoch    4, Training Loss: 11.9743, Validation Loss: 10.5400, Learning Rate: 0.099600\n",
      "Epoch    5, Training Loss: 11.8506, Validation Loss: 10.4397, Learning Rate: 0.099500\n",
      "Epoch    6, Training Loss: 11.7154, Validation Loss: 10.3315, Learning Rate: 0.099400\n",
      "Epoch    7, Training Loss: 11.5929, Validation Loss: 10.2321, Learning Rate: 0.099300\n",
      "Epoch    8, Training Loss: 11.4721, Validation Loss: 10.1369, Learning Rate: 0.099200\n",
      "Epoch    9, Training Loss: 11.3384, Validation Loss: 10.0339, Learning Rate: 0.099100\n",
      "Epoch   10, Training Loss: 11.2035, Validation Loss: 9.9259, Learning Rate: 0.099000\n",
      "Epoch   11, Training Loss: 11.0743, Validation Loss: 9.8227, Learning Rate: 0.098900\n",
      "Epoch   12, Training Loss: 10.9391, Validation Loss: 9.7129, Learning Rate: 0.098800\n",
      "Epoch   13, Training Loss: 10.8059, Validation Loss: 9.6058, Learning Rate: 0.098700\n",
      "Epoch   14, Training Loss: 10.6685, Validation Loss: 9.4941, Learning Rate: 0.098600\n",
      "Epoch   15, Training Loss: 10.5289, Validation Loss: 9.3818, Learning Rate: 0.098500\n",
      "Epoch   16, Training Loss: 10.3855, Validation Loss: 9.2645, Learning Rate: 0.098400\n",
      "Epoch   17, Training Loss: 10.2440, Validation Loss: 9.1495, Learning Rate: 0.098300\n",
      "Epoch   18, Training Loss: 10.0824, Validation Loss: 9.0179, Learning Rate: 0.098200\n",
      "Epoch   19, Training Loss: 9.9026, Validation Loss: 8.8706, Learning Rate: 0.098100\n",
      "Epoch   20, Training Loss: 9.7267, Validation Loss: 8.7289, Learning Rate: 0.098000\n",
      "Epoch   21, Training Loss: 9.5574, Validation Loss: 8.5907, Learning Rate: 0.097900\n",
      "Epoch   22, Training Loss: 9.3634, Validation Loss: 8.4309, Learning Rate: 0.097800\n",
      "Epoch   23, Training Loss: 9.1624, Validation Loss: 8.2658, Learning Rate: 0.097700\n",
      "Epoch   24, Training Loss: 8.8958, Validation Loss: 8.0428, Learning Rate: 0.097600\n",
      "Epoch   25, Training Loss: 8.6353, Validation Loss: 7.8250, Learning Rate: 0.097500\n",
      "Epoch   26, Training Loss: 8.3522, Validation Loss: 7.5872, Learning Rate: 0.097400\n",
      "Epoch   27, Training Loss: 8.0372, Validation Loss: 7.3253, Learning Rate: 0.097300\n",
      "Epoch   28, Training Loss: 7.7108, Validation Loss: 7.0513, Learning Rate: 0.097200\n",
      "Epoch   29, Training Loss: 7.3267, Validation Loss: 6.7284, Learning Rate: 0.097100\n",
      "Epoch   30, Training Loss: 6.9293, Validation Loss: 6.3920, Learning Rate: 0.097000\n",
      "Epoch   31, Training Loss: 6.5173, Validation Loss: 6.0415, Learning Rate: 0.096900\n",
      "Epoch   32, Training Loss: 6.0960, Validation Loss: 5.6825, Learning Rate: 0.096800\n",
      "Epoch   33, Training Loss: 5.6351, Validation Loss: 5.2958, Learning Rate: 0.096700\n",
      "Epoch   34, Training Loss: 5.1870, Validation Loss: 4.9172, Learning Rate: 0.096600\n",
      "Epoch   35, Training Loss: 4.7983, Validation Loss: 4.5907, Learning Rate: 0.096500\n",
      "Epoch   36, Training Loss: 4.4324, Validation Loss: 4.2788, Learning Rate: 0.096400\n",
      "Epoch   37, Training Loss: 4.0966, Validation Loss: 4.0035, Learning Rate: 0.096300\n",
      "Epoch   38, Training Loss: 3.8045, Validation Loss: 3.7552, Learning Rate: 0.096200\n",
      "Epoch   39, Training Loss: 3.6415, Validation Loss: 3.6057, Learning Rate: 0.096100\n",
      "Epoch   40, Training Loss: 3.5281, Validation Loss: 3.5105, Learning Rate: 0.096000\n",
      "Epoch   41, Training Loss: 3.4656, Validation Loss: 3.4477, Learning Rate: 0.095900\n",
      "Epoch   42, Training Loss: 3.4248, Validation Loss: 3.4098, Learning Rate: 0.095800\n",
      "Epoch   43, Training Loss: 3.3862, Validation Loss: 3.3686, Learning Rate: 0.095700\n",
      "Epoch   44, Training Loss: 3.3670, Validation Loss: 3.3654, Learning Rate: 0.095600\n",
      "Epoch   45, Training Loss: 3.3631, Validation Loss: 3.3412, Learning Rate: 0.095500\n",
      "Epoch   46, Training Loss: 3.3390, Validation Loss: 3.3480, Learning Rate: 0.095400\n",
      "Epoch   47, Training Loss: 3.3349, Validation Loss: 3.3507, Learning Rate: 0.095300\n",
      "Epoch   48, Training Loss: 3.3145, Validation Loss: 3.3197, Learning Rate: 0.095200\n",
      "Epoch   49, Training Loss: 3.3060, Validation Loss: 3.3160, Learning Rate: 0.095100\n",
      "Epoch   50, Training Loss: 3.3055, Validation Loss: 3.3176, Learning Rate: 0.095000\n",
      "Epoch   51, Training Loss: 3.2915, Validation Loss: 3.2997, Learning Rate: 0.094900\n",
      "Epoch   52, Training Loss: 3.2768, Validation Loss: 3.2638, Learning Rate: 0.094800\n",
      "Epoch   53, Training Loss: 3.2688, Validation Loss: 3.2563, Learning Rate: 0.094700\n",
      "Epoch   54, Training Loss: 3.2626, Validation Loss: 3.2485, Learning Rate: 0.094600\n",
      "Epoch   55, Training Loss: 3.2560, Validation Loss: 3.2453, Learning Rate: 0.094500\n",
      "Epoch   56, Training Loss: 3.2528, Validation Loss: 3.2426, Learning Rate: 0.094400\n",
      "Epoch   57, Training Loss: 3.2410, Validation Loss: 3.2359, Learning Rate: 0.094300\n",
      "Epoch   58, Training Loss: 3.2331, Validation Loss: 3.2250, Learning Rate: 0.094200\n",
      "Epoch   59, Training Loss: 3.2221, Validation Loss: 3.2013, Learning Rate: 0.094100\n",
      "Epoch   60, Training Loss: 3.2149, Validation Loss: 3.1967, Learning Rate: 0.094000\n",
      "Epoch   61, Training Loss: 3.2069, Validation Loss: 3.1930, Learning Rate: 0.093900\n",
      "Epoch   62, Training Loss: 3.2000, Validation Loss: 3.1864, Learning Rate: 0.093800\n",
      "Epoch   63, Training Loss: 3.1966, Validation Loss: 3.1869, Learning Rate: 0.093700\n",
      "Epoch   64, Training Loss: 3.1835, Validation Loss: 3.1602, Learning Rate: 0.093600\n",
      "Epoch   65, Training Loss: 3.1758, Validation Loss: 3.1505, Learning Rate: 0.093500\n",
      "Epoch   66, Training Loss: 3.1701, Validation Loss: 3.1502, Learning Rate: 0.093400\n",
      "Epoch   67, Training Loss: 3.1632, Validation Loss: 3.1373, Learning Rate: 0.093300\n",
      "Epoch   68, Training Loss: 3.1573, Validation Loss: 3.1321, Learning Rate: 0.093200\n",
      "Epoch   69, Training Loss: 3.1489, Validation Loss: 3.1346, Learning Rate: 0.093100\n",
      "Epoch   70, Training Loss: 3.1425, Validation Loss: 3.1287, Learning Rate: 0.093000\n",
      "Epoch   71, Training Loss: 3.1332, Validation Loss: 3.1065, Learning Rate: 0.092900\n",
      "Epoch   72, Training Loss: 3.1295, Validation Loss: 3.0919, Learning Rate: 0.092800\n",
      "Epoch   73, Training Loss: 3.1193, Validation Loss: 3.0860, Learning Rate: 0.092700\n",
      "Epoch   74, Training Loss: 3.1140, Validation Loss: 3.0729, Learning Rate: 0.092600\n",
      "Epoch   75, Training Loss: 3.1047, Validation Loss: 3.0671, Learning Rate: 0.092500\n",
      "Epoch   76, Training Loss: 3.1013, Validation Loss: 3.0596, Learning Rate: 0.092400\n",
      "Epoch   77, Training Loss: 3.0918, Validation Loss: 3.0513, Learning Rate: 0.092300\n",
      "Epoch   78, Training Loss: 3.0857, Validation Loss: 3.0439, Learning Rate: 0.092200\n",
      "Epoch   79, Training Loss: 3.0819, Validation Loss: 3.0257, Learning Rate: 0.092100\n",
      "Epoch   80, Training Loss: 3.0771, Validation Loss: 3.0144, Learning Rate: 0.092000\n",
      "Epoch   81, Training Loss: 3.0653, Validation Loss: 3.0208, Learning Rate: 0.091900\n",
      "Epoch   82, Training Loss: 3.0572, Validation Loss: 3.0079, Learning Rate: 0.091800\n",
      "Epoch   83, Training Loss: 3.0546, Validation Loss: 2.9915, Learning Rate: 0.091700\n",
      "Epoch   84, Training Loss: 3.0513, Validation Loss: 2.9816, Learning Rate: 0.091600\n",
      "Epoch   85, Training Loss: 3.0349, Validation Loss: 2.9797, Learning Rate: 0.091500\n",
      "Epoch   86, Training Loss: 3.0303, Validation Loss: 2.9702, Learning Rate: 0.091400\n",
      "Epoch   87, Training Loss: 3.0230, Validation Loss: 2.9693, Learning Rate: 0.091300\n",
      "Epoch   88, Training Loss: 3.0153, Validation Loss: 2.9524, Learning Rate: 0.091200\n",
      "Epoch   89, Training Loss: 3.0094, Validation Loss: 2.9463, Learning Rate: 0.091100\n",
      "Epoch   90, Training Loss: 3.0055, Validation Loss: 2.9520, Learning Rate: 0.091000\n",
      "Epoch   91, Training Loss: 2.9973, Validation Loss: 2.9387, Learning Rate: 0.090900\n",
      "Epoch   92, Training Loss: 2.9883, Validation Loss: 2.9186, Learning Rate: 0.090800\n",
      "Epoch   93, Training Loss: 2.9830, Validation Loss: 2.9118, Learning Rate: 0.090700\n",
      "Epoch   94, Training Loss: 2.9734, Validation Loss: 2.9000, Learning Rate: 0.090600\n",
      "Epoch   95, Training Loss: 2.9641, Validation Loss: 2.8990, Learning Rate: 0.090500\n",
      "Epoch   96, Training Loss: 2.9649, Validation Loss: 2.9083, Learning Rate: 0.090400\n",
      "Epoch   97, Training Loss: 2.9495, Validation Loss: 2.8852, Learning Rate: 0.090300\n",
      "Epoch   98, Training Loss: 2.9436, Validation Loss: 2.8873, Learning Rate: 0.090200\n",
      "Epoch   99, Training Loss: 2.9375, Validation Loss: 2.8803, Learning Rate: 0.090100\n",
      "Epoch  100, Training Loss: 2.9311, Validation Loss: 2.8702, Learning Rate: 0.090000\n",
      "Epoch  101, Training Loss: 2.9212, Validation Loss: 2.8596, Learning Rate: 0.089900\n",
      "Epoch  102, Training Loss: 2.9175, Validation Loss: 2.8539, Learning Rate: 0.089800\n",
      "Epoch  103, Training Loss: 2.9138, Validation Loss: 2.8403, Learning Rate: 0.089700\n",
      "Epoch  104, Training Loss: 2.9008, Validation Loss: 2.8368, Learning Rate: 0.089600\n",
      "Epoch  105, Training Loss: 2.8967, Validation Loss: 2.8220, Learning Rate: 0.089500\n",
      "Epoch  106, Training Loss: 2.8914, Validation Loss: 2.8119, Learning Rate: 0.089400\n",
      "Epoch  107, Training Loss: 2.8862, Validation Loss: 2.8012, Learning Rate: 0.089300\n",
      "Epoch  108, Training Loss: 2.8732, Validation Loss: 2.8005, Learning Rate: 0.089200\n",
      "Epoch  109, Training Loss: 2.8653, Validation Loss: 2.7831, Learning Rate: 0.089100\n",
      "Epoch  110, Training Loss: 2.8664, Validation Loss: 2.7749, Learning Rate: 0.089000\n",
      "Epoch  111, Training Loss: 2.8536, Validation Loss: 2.7700, Learning Rate: 0.088900\n",
      "Epoch  112, Training Loss: 2.8459, Validation Loss: 2.7700, Learning Rate: 0.088800\n",
      "Epoch  113, Training Loss: 2.8379, Validation Loss: 2.7635, Learning Rate: 0.088700\n",
      "Epoch  114, Training Loss: 2.8340, Validation Loss: 2.7584, Learning Rate: 0.088600\n",
      "Epoch  115, Training Loss: 2.8245, Validation Loss: 2.7440, Learning Rate: 0.088500\n",
      "Epoch  116, Training Loss: 2.8218, Validation Loss: 2.7485, Learning Rate: 0.088400\n",
      "Epoch  117, Training Loss: 2.8158, Validation Loss: 2.7424, Learning Rate: 0.088300\n",
      "Epoch  118, Training Loss: 2.8279, Validation Loss: 2.7563, Learning Rate: 0.088200\n",
      "Epoch  119, Training Loss: 2.8066, Validation Loss: 2.7313, Learning Rate: 0.088100\n",
      "Epoch  120, Training Loss: 2.7897, Validation Loss: 2.7068, Learning Rate: 0.088000\n",
      "Epoch  121, Training Loss: 2.7867, Validation Loss: 2.7047, Learning Rate: 0.087900\n",
      "Epoch  122, Training Loss: 2.7791, Validation Loss: 2.6919, Learning Rate: 0.087800\n",
      "Epoch  123, Training Loss: 2.7734, Validation Loss: 2.6683, Learning Rate: 0.087700\n",
      "Epoch  124, Training Loss: 2.7700, Validation Loss: 2.6607, Learning Rate: 0.087600\n",
      "Epoch  125, Training Loss: 2.7544, Validation Loss: 2.6625, Learning Rate: 0.087500\n",
      "Epoch  126, Training Loss: 2.7494, Validation Loss: 2.6457, Learning Rate: 0.087400\n",
      "Epoch  127, Training Loss: 2.7387, Validation Loss: 2.6396, Learning Rate: 0.087300\n",
      "Epoch  128, Training Loss: 2.7313, Validation Loss: 2.6417, Learning Rate: 0.087200\n",
      "Epoch  129, Training Loss: 2.7329, Validation Loss: 2.6466, Learning Rate: 0.087100\n",
      "Epoch  130, Training Loss: 2.7248, Validation Loss: 2.6322, Learning Rate: 0.087000\n",
      "Epoch  131, Training Loss: 2.7088, Validation Loss: 2.6141, Learning Rate: 0.086900\n",
      "Epoch  132, Training Loss: 2.7019, Validation Loss: 2.6089, Learning Rate: 0.086800\n",
      "Epoch  133, Training Loss: 2.6937, Validation Loss: 2.5956, Learning Rate: 0.086700\n",
      "Epoch  134, Training Loss: 2.6883, Validation Loss: 2.5863, Learning Rate: 0.086600\n",
      "Epoch  135, Training Loss: 2.6863, Validation Loss: 2.5698, Learning Rate: 0.086500\n",
      "Epoch  136, Training Loss: 2.6767, Validation Loss: 2.5672, Learning Rate: 0.086400\n",
      "Epoch  137, Training Loss: 2.6729, Validation Loss: 2.5713, Learning Rate: 0.086300\n",
      "Epoch  138, Training Loss: 2.6651, Validation Loss: 2.5586, Learning Rate: 0.086200\n",
      "Epoch  139, Training Loss: 2.6543, Validation Loss: 2.5401, Learning Rate: 0.086100\n",
      "Epoch  140, Training Loss: 2.6459, Validation Loss: 2.5328, Learning Rate: 0.086000\n",
      "Epoch  141, Training Loss: 2.6366, Validation Loss: 2.5352, Learning Rate: 0.085900\n",
      "Epoch  142, Training Loss: 2.6307, Validation Loss: 2.5257, Learning Rate: 0.085800\n",
      "Epoch  143, Training Loss: 2.6374, Validation Loss: 2.5376, Learning Rate: 0.085700\n",
      "Epoch  144, Training Loss: 2.6144, Validation Loss: 2.5079, Learning Rate: 0.085600\n",
      "Epoch  145, Training Loss: 2.6082, Validation Loss: 2.5043, Learning Rate: 0.085500\n",
      "Epoch  146, Training Loss: 2.6003, Validation Loss: 2.4930, Learning Rate: 0.085400\n",
      "Epoch  147, Training Loss: 2.5938, Validation Loss: 2.4826, Learning Rate: 0.085300\n",
      "Epoch  148, Training Loss: 2.5921, Validation Loss: 2.4882, Learning Rate: 0.085200\n",
      "Epoch  149, Training Loss: 2.5808, Validation Loss: 2.4720, Learning Rate: 0.085100\n",
      "Epoch  150, Training Loss: 2.5740, Validation Loss: 2.4673, Learning Rate: 0.085000\n",
      "Epoch  151, Training Loss: 2.5665, Validation Loss: 2.4603, Learning Rate: 0.084900\n",
      "Epoch  152, Training Loss: 2.5608, Validation Loss: 2.4537, Learning Rate: 0.084800\n",
      "Epoch  153, Training Loss: 2.5522, Validation Loss: 2.4435, Learning Rate: 0.084700\n",
      "Epoch  154, Training Loss: 2.5469, Validation Loss: 2.4357, Learning Rate: 0.084600\n",
      "Epoch  155, Training Loss: 2.5396, Validation Loss: 2.4267, Learning Rate: 0.084500\n",
      "Epoch  156, Training Loss: 2.5351, Validation Loss: 2.4261, Learning Rate: 0.084400\n",
      "Epoch  157, Training Loss: 2.5290, Validation Loss: 2.4130, Learning Rate: 0.084300\n",
      "Epoch  158, Training Loss: 2.5207, Validation Loss: 2.4092, Learning Rate: 0.084200\n",
      "Epoch  159, Training Loss: 2.5133, Validation Loss: 2.4000, Learning Rate: 0.084100\n",
      "Epoch  160, Training Loss: 2.5058, Validation Loss: 2.3930, Learning Rate: 0.084000\n",
      "Epoch  161, Training Loss: 2.4999, Validation Loss: 2.3868, Learning Rate: 0.083900\n",
      "Epoch  162, Training Loss: 2.4922, Validation Loss: 2.3787, Learning Rate: 0.083800\n",
      "Epoch  163, Training Loss: 2.4871, Validation Loss: 2.3699, Learning Rate: 0.083700\n",
      "Epoch  164, Training Loss: 2.4789, Validation Loss: 2.3649, Learning Rate: 0.083600\n",
      "Epoch  165, Training Loss: 2.4749, Validation Loss: 2.3504, Learning Rate: 0.083500\n",
      "Epoch  166, Training Loss: 2.4687, Validation Loss: 2.3536, Learning Rate: 0.083400\n",
      "Epoch  167, Training Loss: 2.4596, Validation Loss: 2.3383, Learning Rate: 0.083300\n",
      "Epoch  168, Training Loss: 2.4564, Validation Loss: 2.3379, Learning Rate: 0.083200\n",
      "Epoch  169, Training Loss: 2.4470, Validation Loss: 2.3221, Learning Rate: 0.083100\n",
      "Epoch  170, Training Loss: 2.4412, Validation Loss: 2.3171, Learning Rate: 0.083000\n",
      "Epoch  171, Training Loss: 2.4342, Validation Loss: 2.3174, Learning Rate: 0.082900\n",
      "Epoch  172, Training Loss: 2.4291, Validation Loss: 2.3138, Learning Rate: 0.082800\n",
      "Epoch  173, Training Loss: 2.4211, Validation Loss: 2.2982, Learning Rate: 0.082700\n",
      "Epoch  174, Training Loss: 2.4188, Validation Loss: 2.2866, Learning Rate: 0.082600\n",
      "Epoch  175, Training Loss: 2.4087, Validation Loss: 2.2854, Learning Rate: 0.082500\n",
      "Epoch  176, Training Loss: 2.4013, Validation Loss: 2.2785, Learning Rate: 0.082400\n",
      "Epoch  177, Training Loss: 2.4023, Validation Loss: 2.2724, Learning Rate: 0.082300\n",
      "Epoch  178, Training Loss: 2.3895, Validation Loss: 2.2637, Learning Rate: 0.082200\n",
      "Epoch  179, Training Loss: 2.3826, Validation Loss: 2.2549, Learning Rate: 0.082100\n",
      "Epoch  180, Training Loss: 2.3798, Validation Loss: 2.2502, Learning Rate: 0.082000\n",
      "Epoch  181, Training Loss: 2.3754, Validation Loss: 2.2441, Learning Rate: 0.081900\n",
      "Epoch  182, Training Loss: 2.3653, Validation Loss: 2.2438, Learning Rate: 0.081800\n",
      "Epoch  183, Training Loss: 2.3560, Validation Loss: 2.2331, Learning Rate: 0.081700\n",
      "Epoch  184, Training Loss: 2.3538, Validation Loss: 2.2230, Learning Rate: 0.081600\n",
      "Epoch  185, Training Loss: 2.3446, Validation Loss: 2.2161, Learning Rate: 0.081500\n",
      "Epoch  186, Training Loss: 2.3386, Validation Loss: 2.2145, Learning Rate: 0.081400\n",
      "Epoch  187, Training Loss: 2.3349, Validation Loss: 2.2057, Learning Rate: 0.081300\n",
      "Epoch  188, Training Loss: 2.3275, Validation Loss: 2.2042, Learning Rate: 0.081200\n",
      "Epoch  189, Training Loss: 2.3219, Validation Loss: 2.1979, Learning Rate: 0.081100\n",
      "Epoch  190, Training Loss: 2.3334, Validation Loss: 2.2087, Learning Rate: 0.081000\n",
      "Epoch  191, Training Loss: 2.3150, Validation Loss: 2.1920, Learning Rate: 0.080900\n",
      "Epoch  192, Training Loss: 2.3097, Validation Loss: 2.1829, Learning Rate: 0.080800\n",
      "Epoch  193, Training Loss: 2.3019, Validation Loss: 2.1765, Learning Rate: 0.080700\n",
      "Epoch  194, Training Loss: 2.2928, Validation Loss: 2.1641, Learning Rate: 0.080600\n",
      "Epoch  195, Training Loss: 2.2869, Validation Loss: 2.1624, Learning Rate: 0.080500\n",
      "Epoch  196, Training Loss: 2.2844, Validation Loss: 2.1545, Learning Rate: 0.080400\n",
      "Epoch  197, Training Loss: 2.2769, Validation Loss: 2.1491, Learning Rate: 0.080300\n",
      "Epoch  198, Training Loss: 2.2701, Validation Loss: 2.1440, Learning Rate: 0.080200\n",
      "Epoch  199, Training Loss: 2.2613, Validation Loss: 2.1364, Learning Rate: 0.080100\n",
      "Epoch  200, Training Loss: 2.2565, Validation Loss: 2.1318, Learning Rate: 0.080000\n",
      "Epoch  201, Training Loss: 2.2543, Validation Loss: 2.1283, Learning Rate: 0.079900\n",
      "Epoch  202, Training Loss: 2.2465, Validation Loss: 2.1185, Learning Rate: 0.079800\n",
      "Epoch  203, Training Loss: 2.2411, Validation Loss: 2.1116, Learning Rate: 0.079700\n",
      "Epoch  204, Training Loss: 2.2377, Validation Loss: 2.1113, Learning Rate: 0.079600\n",
      "Epoch  205, Training Loss: 2.2274, Validation Loss: 2.0977, Learning Rate: 0.079500\n",
      "Epoch  206, Training Loss: 2.2273, Validation Loss: 2.0907, Learning Rate: 0.079400\n",
      "Epoch  207, Training Loss: 2.2204, Validation Loss: 2.0873, Learning Rate: 0.079300\n",
      "Epoch  208, Training Loss: 2.2066, Validation Loss: 2.0835, Learning Rate: 0.079200\n",
      "Epoch  209, Training Loss: 2.2008, Validation Loss: 2.0797, Learning Rate: 0.079100\n",
      "Epoch  210, Training Loss: 2.1990, Validation Loss: 2.0778, Learning Rate: 0.079000\n",
      "Epoch  211, Training Loss: 2.2029, Validation Loss: 2.0768, Learning Rate: 0.078900\n",
      "Epoch  212, Training Loss: 2.1835, Validation Loss: 2.0627, Learning Rate: 0.078800\n",
      "Epoch  213, Training Loss: 2.1787, Validation Loss: 2.0577, Learning Rate: 0.078700\n",
      "Epoch  214, Training Loss: 2.1698, Validation Loss: 2.0508, Learning Rate: 0.078600\n",
      "Epoch  215, Training Loss: 2.1640, Validation Loss: 2.0464, Learning Rate: 0.078500\n",
      "Epoch  216, Training Loss: 2.1589, Validation Loss: 2.0406, Learning Rate: 0.078400\n",
      "Epoch  217, Training Loss: 2.1695, Validation Loss: 2.0457, Learning Rate: 0.078300\n",
      "Epoch  218, Training Loss: 2.1520, Validation Loss: 2.0344, Learning Rate: 0.078200\n",
      "Epoch  219, Training Loss: 2.1408, Validation Loss: 2.0277, Learning Rate: 0.078100\n",
      "Epoch  220, Training Loss: 2.1309, Validation Loss: 2.0137, Learning Rate: 0.078000\n",
      "Epoch  221, Training Loss: 2.1239, Validation Loss: 2.0082, Learning Rate: 0.077900\n",
      "Epoch  222, Training Loss: 2.1204, Validation Loss: 2.0050, Learning Rate: 0.077800\n",
      "Epoch  223, Training Loss: 2.1148, Validation Loss: 1.9959, Learning Rate: 0.077700\n",
      "Epoch  224, Training Loss: 2.1189, Validation Loss: 2.0009, Learning Rate: 0.077600\n",
      "Epoch  225, Training Loss: 2.1022, Validation Loss: 1.9914, Learning Rate: 0.077500\n",
      "Epoch  226, Training Loss: 2.0971, Validation Loss: 1.9857, Learning Rate: 0.077400\n",
      "Epoch  227, Training Loss: 2.0895, Validation Loss: 1.9788, Learning Rate: 0.077300\n",
      "Epoch  228, Training Loss: 2.0841, Validation Loss: 1.9760, Learning Rate: 0.077200\n",
      "Epoch  229, Training Loss: 2.0789, Validation Loss: 1.9693, Learning Rate: 0.077100\n",
      "Epoch  230, Training Loss: 2.0701, Validation Loss: 1.9619, Learning Rate: 0.077000\n",
      "Epoch  231, Training Loss: 2.0677, Validation Loss: 1.9543, Learning Rate: 0.076900\n",
      "Epoch  232, Training Loss: 2.0787, Validation Loss: 1.9636, Learning Rate: 0.076800\n",
      "Epoch  233, Training Loss: 2.0571, Validation Loss: 1.9439, Learning Rate: 0.076700\n",
      "Epoch  234, Training Loss: 2.0486, Validation Loss: 1.9405, Learning Rate: 0.076600\n",
      "Epoch  235, Training Loss: 2.0396, Validation Loss: 1.9335, Learning Rate: 0.076500\n",
      "Epoch  236, Training Loss: 2.0343, Validation Loss: 1.9299, Learning Rate: 0.076400\n",
      "Epoch  237, Training Loss: 2.0264, Validation Loss: 1.9233, Learning Rate: 0.076300\n",
      "Epoch  238, Training Loss: 2.0218, Validation Loss: 1.9196, Learning Rate: 0.076200\n",
      "Epoch  239, Training Loss: 2.0184, Validation Loss: 1.9133, Learning Rate: 0.076100\n",
      "Epoch  240, Training Loss: 2.0072, Validation Loss: 1.9083, Learning Rate: 0.076000\n",
      "Epoch  241, Training Loss: 2.0017, Validation Loss: 1.9021, Learning Rate: 0.075900\n",
      "Epoch  242, Training Loss: 1.9938, Validation Loss: 1.8948, Learning Rate: 0.075800\n",
      "Epoch  243, Training Loss: 1.9911, Validation Loss: 1.8881, Learning Rate: 0.075700\n",
      "Epoch  244, Training Loss: 1.9843, Validation Loss: 1.8920, Learning Rate: 0.075600\n",
      "Epoch  245, Training Loss: 1.9754, Validation Loss: 1.8794, Learning Rate: 0.075500\n",
      "Epoch  246, Training Loss: 1.9734, Validation Loss: 1.8712, Learning Rate: 0.075400\n",
      "Epoch  247, Training Loss: 1.9642, Validation Loss: 1.8649, Learning Rate: 0.075300\n",
      "Epoch  248, Training Loss: 1.9589, Validation Loss: 1.8659, Learning Rate: 0.075200\n",
      "Epoch  249, Training Loss: 1.9474, Validation Loss: 1.8583, Learning Rate: 0.075100\n",
      "Epoch  250, Training Loss: 1.9491, Validation Loss: 1.8637, Learning Rate: 0.075000\n",
      "Epoch  251, Training Loss: 1.9386, Validation Loss: 1.8453, Learning Rate: 0.074900\n",
      "Epoch  252, Training Loss: 1.9275, Validation Loss: 1.8394, Learning Rate: 0.074800\n",
      "Epoch  253, Training Loss: 1.9219, Validation Loss: 1.8361, Learning Rate: 0.074700\n",
      "Epoch  254, Training Loss: 1.9133, Validation Loss: 1.8238, Learning Rate: 0.074600\n",
      "Epoch  255, Training Loss: 1.9061, Validation Loss: 1.8196, Learning Rate: 0.074500\n",
      "Epoch  256, Training Loss: 1.8989, Validation Loss: 1.8154, Learning Rate: 0.074400\n",
      "Epoch  257, Training Loss: 1.9020, Validation Loss: 1.8209, Learning Rate: 0.074300\n",
      "Epoch  258, Training Loss: 1.8887, Validation Loss: 1.8066, Learning Rate: 0.074200\n",
      "Epoch  259, Training Loss: 1.8836, Validation Loss: 1.7990, Learning Rate: 0.074100\n",
      "Epoch  260, Training Loss: 1.8728, Validation Loss: 1.7926, Learning Rate: 0.074000\n",
      "Epoch  261, Training Loss: 1.8822, Validation Loss: 1.8053, Learning Rate: 0.073900\n",
      "Epoch  262, Training Loss: 1.8690, Validation Loss: 1.7978, Learning Rate: 0.073800\n",
      "Epoch  263, Training Loss: 1.8549, Validation Loss: 1.7739, Learning Rate: 0.073700\n",
      "Epoch  264, Training Loss: 1.8468, Validation Loss: 1.7667, Learning Rate: 0.073600\n",
      "Epoch  265, Training Loss: 1.8410, Validation Loss: 1.7645, Learning Rate: 0.073500\n",
      "Epoch  266, Training Loss: 1.8377, Validation Loss: 1.7560, Learning Rate: 0.073400\n",
      "Epoch  267, Training Loss: 1.8269, Validation Loss: 1.7528, Learning Rate: 0.073300\n",
      "Epoch  268, Training Loss: 1.8219, Validation Loss: 1.7471, Learning Rate: 0.073200\n",
      "Epoch  269, Training Loss: 1.8155, Validation Loss: 1.7414, Learning Rate: 0.073100\n",
      "Epoch  270, Training Loss: 1.8078, Validation Loss: 1.7376, Learning Rate: 0.073000\n",
      "Epoch  271, Training Loss: 1.8158, Validation Loss: 1.7522, Learning Rate: 0.072900\n",
      "Epoch  272, Training Loss: 1.7964, Validation Loss: 1.7332, Learning Rate: 0.072800\n",
      "Epoch  273, Training Loss: 1.7867, Validation Loss: 1.7211, Learning Rate: 0.072700\n",
      "Epoch  274, Training Loss: 1.7862, Validation Loss: 1.7210, Learning Rate: 0.072600\n",
      "Epoch  275, Training Loss: 1.7787, Validation Loss: 1.7105, Learning Rate: 0.072500\n",
      "Epoch  276, Training Loss: 1.7725, Validation Loss: 1.7037, Learning Rate: 0.072400\n",
      "Epoch  277, Training Loss: 1.7958, Validation Loss: 1.7079, Learning Rate: 0.072300\n",
      "Epoch  278, Training Loss: 1.7578, Validation Loss: 1.6961, Learning Rate: 0.072200\n",
      "Epoch  279, Training Loss: 1.7565, Validation Loss: 1.6871, Learning Rate: 0.072100\n",
      "Epoch  280, Training Loss: 1.7422, Validation Loss: 1.6787, Learning Rate: 0.072000\n",
      "Epoch  281, Training Loss: 1.7415, Validation Loss: 1.6860, Learning Rate: 0.071900\n",
      "Epoch  282, Training Loss: 1.7288, Validation Loss: 1.6669, Learning Rate: 0.071800\n",
      "Epoch  283, Training Loss: 1.7310, Validation Loss: 1.6596, Learning Rate: 0.071700\n",
      "Epoch  284, Training Loss: 1.7113, Validation Loss: 1.6556, Learning Rate: 0.071600\n",
      "Epoch  285, Training Loss: 1.7084, Validation Loss: 1.6496, Learning Rate: 0.071500\n",
      "Epoch  286, Training Loss: 1.6994, Validation Loss: 1.6439, Learning Rate: 0.071400\n",
      "Epoch  287, Training Loss: 1.6947, Validation Loss: 1.6391, Learning Rate: 0.071300\n",
      "Epoch  288, Training Loss: 1.6914, Validation Loss: 1.6412, Learning Rate: 0.071200\n",
      "Epoch  289, Training Loss: 1.6809, Validation Loss: 1.6331, Learning Rate: 0.071100\n",
      "Epoch  290, Training Loss: 1.6750, Validation Loss: 1.6272, Learning Rate: 0.071000\n",
      "Epoch  291, Training Loss: 1.6776, Validation Loss: 1.6122, Learning Rate: 0.070900\n",
      "Epoch  292, Training Loss: 1.6612, Validation Loss: 1.6119, Learning Rate: 0.070800\n",
      "Epoch  293, Training Loss: 1.6528, Validation Loss: 1.5997, Learning Rate: 0.070700\n",
      "Epoch  294, Training Loss: 1.6482, Validation Loss: 1.5964, Learning Rate: 0.070600\n",
      "Epoch  295, Training Loss: 1.6427, Validation Loss: 1.5951, Learning Rate: 0.070500\n",
      "Epoch  296, Training Loss: 1.6402, Validation Loss: 1.5974, Learning Rate: 0.070400\n",
      "Epoch  297, Training Loss: 1.6265, Validation Loss: 1.5848, Learning Rate: 0.070300\n",
      "Epoch  298, Training Loss: 1.6193, Validation Loss: 1.5758, Learning Rate: 0.070200\n",
      "Epoch  299, Training Loss: 1.6144, Validation Loss: 1.5752, Learning Rate: 0.070100\n",
      "Epoch  300, Training Loss: 1.6128, Validation Loss: 1.5750, Learning Rate: 0.070000\n",
      "Epoch  301, Training Loss: 1.6002, Validation Loss: 1.5514, Learning Rate: 0.069900\n",
      "Epoch  302, Training Loss: 1.6099, Validation Loss: 1.5450, Learning Rate: 0.069800\n",
      "Epoch  303, Training Loss: 1.5919, Validation Loss: 1.5402, Learning Rate: 0.069700\n",
      "Epoch  304, Training Loss: 1.5824, Validation Loss: 1.5438, Learning Rate: 0.069600\n",
      "Epoch  305, Training Loss: 1.5766, Validation Loss: 1.5323, Learning Rate: 0.069500\n",
      "Epoch  306, Training Loss: 1.5703, Validation Loss: 1.5318, Learning Rate: 0.069400\n",
      "Epoch  307, Training Loss: 1.5675, Validation Loss: 1.5324, Learning Rate: 0.069300\n",
      "Epoch  308, Training Loss: 1.5601, Validation Loss: 1.5182, Learning Rate: 0.069200\n",
      "Epoch  309, Training Loss: 1.5522, Validation Loss: 1.5120, Learning Rate: 0.069100\n",
      "Epoch  310, Training Loss: 1.5476, Validation Loss: 1.5044, Learning Rate: 0.069000\n",
      "Epoch  311, Training Loss: 1.5418, Validation Loss: 1.4960, Learning Rate: 0.068900\n",
      "Epoch  312, Training Loss: 1.5342, Validation Loss: 1.4935, Learning Rate: 0.068800\n",
      "Epoch  313, Training Loss: 1.5298, Validation Loss: 1.4948, Learning Rate: 0.068700\n",
      "Epoch  314, Training Loss: 1.5218, Validation Loss: 1.4832, Learning Rate: 0.068600\n",
      "Epoch  315, Training Loss: 1.5217, Validation Loss: 1.4919, Learning Rate: 0.068500\n",
      "Epoch  316, Training Loss: 1.5131, Validation Loss: 1.4832, Learning Rate: 0.068400\n",
      "Epoch  317, Training Loss: 1.5084, Validation Loss: 1.4676, Learning Rate: 0.068300\n",
      "Epoch  318, Training Loss: 1.5049, Validation Loss: 1.4644, Learning Rate: 0.068200\n",
      "Epoch  319, Training Loss: 1.4972, Validation Loss: 1.4674, Learning Rate: 0.068100\n",
      "Epoch  320, Training Loss: 1.4955, Validation Loss: 1.4553, Learning Rate: 0.068000\n",
      "Epoch  321, Training Loss: 1.4887, Validation Loss: 1.4635, Learning Rate: 0.067900\n",
      "Epoch  322, Training Loss: 1.4839, Validation Loss: 1.4472, Learning Rate: 0.067800\n",
      "Epoch  323, Training Loss: 1.4791, Validation Loss: 1.4507, Learning Rate: 0.067700\n",
      "Epoch  324, Training Loss: 1.4741, Validation Loss: 1.4435, Learning Rate: 0.067600\n",
      "Epoch  325, Training Loss: 1.4658, Validation Loss: 1.4376, Learning Rate: 0.067500\n",
      "Epoch  326, Training Loss: 1.4656, Validation Loss: 1.4201, Learning Rate: 0.067400\n",
      "Epoch  327, Training Loss: 1.4580, Validation Loss: 1.4320, Learning Rate: 0.067300\n",
      "Epoch  328, Training Loss: 1.4509, Validation Loss: 1.4147, Learning Rate: 0.067200\n",
      "Epoch  329, Training Loss: 1.4519, Validation Loss: 1.4275, Learning Rate: 0.067100\n",
      "Epoch  330, Training Loss: 1.4444, Validation Loss: 1.4197, Learning Rate: 0.067000\n",
      "Epoch  331, Training Loss: 1.4357, Validation Loss: 1.4065, Learning Rate: 0.066900\n",
      "Epoch  332, Training Loss: 1.4363, Validation Loss: 1.4117, Learning Rate: 0.066800\n",
      "Epoch  333, Training Loss: 1.4362, Validation Loss: 1.4133, Learning Rate: 0.066700\n",
      "Epoch  334, Training Loss: 1.4246, Validation Loss: 1.4010, Learning Rate: 0.066600\n",
      "Epoch  335, Training Loss: 1.4196, Validation Loss: 1.3933, Learning Rate: 0.066500\n",
      "Epoch  336, Training Loss: 1.4211, Validation Loss: 1.3937, Learning Rate: 0.066400\n",
      "Epoch  337, Training Loss: 1.4146, Validation Loss: 1.3792, Learning Rate: 0.066300\n",
      "Epoch  338, Training Loss: 1.4079, Validation Loss: 1.3783, Learning Rate: 0.066200\n",
      "Epoch  339, Training Loss: 1.4067, Validation Loss: 1.3846, Learning Rate: 0.066100\n",
      "Epoch  340, Training Loss: 1.4088, Validation Loss: 1.3699, Learning Rate: 0.066000\n",
      "Epoch  341, Training Loss: 1.3987, Validation Loss: 1.3726, Learning Rate: 0.065900\n",
      "Epoch  342, Training Loss: 1.3978, Validation Loss: 1.3659, Learning Rate: 0.065800\n",
      "Epoch  343, Training Loss: 1.3961, Validation Loss: 1.3683, Learning Rate: 0.065700\n",
      "Epoch  344, Training Loss: 1.3927, Validation Loss: 1.3689, Learning Rate: 0.065600\n",
      "Epoch  345, Training Loss: 1.3891, Validation Loss: 1.3647, Learning Rate: 0.065500\n",
      "Epoch  346, Training Loss: 1.4062, Validation Loss: 1.3890, Learning Rate: 0.065400\n",
      "Epoch  347, Training Loss: 1.4020, Validation Loss: 1.3855, Learning Rate: 0.065300\n",
      "Epoch  348, Training Loss: 1.3798, Validation Loss: 1.3539, Learning Rate: 0.065200\n",
      "Epoch  349, Training Loss: 1.3794, Validation Loss: 1.3559, Learning Rate: 0.065100\n",
      "Epoch  350, Training Loss: 1.3755, Validation Loss: 1.3450, Learning Rate: 0.065000\n",
      "Epoch  351, Training Loss: 1.3760, Validation Loss: 1.3520, Learning Rate: 0.064900\n",
      "Epoch  352, Training Loss: 1.3734, Validation Loss: 1.3456, Learning Rate: 0.064800\n",
      "Epoch  353, Training Loss: 1.3673, Validation Loss: 1.3398, Learning Rate: 0.064700\n",
      "Epoch  354, Training Loss: 1.3668, Validation Loss: 1.3350, Learning Rate: 0.064600\n",
      "Epoch  355, Training Loss: 1.3736, Validation Loss: 1.3552, Learning Rate: 0.064500\n",
      "Epoch  356, Training Loss: 1.3656, Validation Loss: 1.3471, Learning Rate: 0.064400\n",
      "Epoch  357, Training Loss: 1.3622, Validation Loss: 1.3273, Learning Rate: 0.064300\n",
      "Epoch  358, Training Loss: 1.3576, Validation Loss: 1.3316, Learning Rate: 0.064200\n",
      "Epoch  359, Training Loss: 1.3744, Validation Loss: 1.3229, Learning Rate: 0.064100\n",
      "Epoch  360, Training Loss: 1.3551, Validation Loss: 1.3254, Learning Rate: 0.064000\n",
      "Epoch  361, Training Loss: 1.3550, Validation Loss: 1.3193, Learning Rate: 0.063900\n",
      "Epoch  362, Training Loss: 1.3545, Validation Loss: 1.3274, Learning Rate: 0.063800\n",
      "Epoch  363, Training Loss: 1.3513, Validation Loss: 1.3272, Learning Rate: 0.063700\n",
      "Epoch  364, Training Loss: 1.3575, Validation Loss: 1.3278, Learning Rate: 0.063600\n",
      "Epoch  365, Training Loss: 1.3510, Validation Loss: 1.3266, Learning Rate: 0.063500\n",
      "Epoch  366, Training Loss: 1.3411, Validation Loss: 1.3174, Learning Rate: 0.063400\n",
      "Epoch  367, Training Loss: 1.3449, Validation Loss: 1.3210, Learning Rate: 0.063300\n",
      "Epoch  368, Training Loss: 1.3406, Validation Loss: 1.3059, Learning Rate: 0.063200\n",
      "Epoch  369, Training Loss: 1.3361, Validation Loss: 1.3155, Learning Rate: 0.063100\n",
      "Epoch  370, Training Loss: 1.3407, Validation Loss: 1.3084, Learning Rate: 0.063000\n",
      "Epoch  371, Training Loss: 1.3361, Validation Loss: 1.3042, Learning Rate: 0.062900\n",
      "Epoch  372, Training Loss: 1.3321, Validation Loss: 1.3073, Learning Rate: 0.062800\n",
      "Epoch  373, Training Loss: 1.3298, Validation Loss: 1.3091, Learning Rate: 0.062700\n",
      "Epoch  374, Training Loss: 1.3282, Validation Loss: 1.3008, Learning Rate: 0.062600\n",
      "Epoch  375, Training Loss: 1.3300, Validation Loss: 1.3001, Learning Rate: 0.062500\n",
      "Epoch  376, Training Loss: 1.3305, Validation Loss: 1.3100, Learning Rate: 0.062400\n",
      "Epoch  377, Training Loss: 1.3269, Validation Loss: 1.3000, Learning Rate: 0.062300\n",
      "Epoch  378, Training Loss: 1.3275, Validation Loss: 1.2996, Learning Rate: 0.062200\n",
      "Epoch  379, Training Loss: 1.3233, Validation Loss: 1.3012, Learning Rate: 0.062100\n",
      "Epoch  380, Training Loss: 1.3260, Validation Loss: 1.2961, Learning Rate: 0.062000\n",
      "Epoch  381, Training Loss: 1.3342, Validation Loss: 1.2956, Learning Rate: 0.061900\n",
      "Epoch  382, Training Loss: 1.3222, Validation Loss: 1.2983, Learning Rate: 0.061800\n",
      "Epoch  383, Training Loss: 1.3186, Validation Loss: 1.2932, Learning Rate: 0.061700\n",
      "Epoch  384, Training Loss: 1.3174, Validation Loss: 1.3004, Learning Rate: 0.061600\n",
      "Epoch  385, Training Loss: 1.3202, Validation Loss: 1.2989, Learning Rate: 0.061500\n",
      "Epoch  386, Training Loss: 1.3171, Validation Loss: 1.2972, Learning Rate: 0.061400\n",
      "Epoch  387, Training Loss: 1.3192, Validation Loss: 1.2940, Learning Rate: 0.061300\n",
      "Epoch  388, Training Loss: 1.3132, Validation Loss: 1.2927, Learning Rate: 0.061200\n",
      "Epoch  389, Training Loss: 1.3112, Validation Loss: 1.2871, Learning Rate: 0.061100\n",
      "Epoch  390, Training Loss: 1.3113, Validation Loss: 1.2831, Learning Rate: 0.061000\n",
      "Epoch  391, Training Loss: 1.3120, Validation Loss: 1.2864, Learning Rate: 0.060900\n",
      "Epoch  392, Training Loss: 1.3121, Validation Loss: 1.2860, Learning Rate: 0.060800\n",
      "Epoch  393, Training Loss: 1.3106, Validation Loss: 1.2907, Learning Rate: 0.060700\n",
      "Epoch  394, Training Loss: 1.3087, Validation Loss: 1.2882, Learning Rate: 0.060600\n",
      "Epoch  395, Training Loss: 1.3108, Validation Loss: 1.2928, Learning Rate: 0.060500\n",
      "Epoch  396, Training Loss: 1.3066, Validation Loss: 1.2911, Learning Rate: 0.060400\n",
      "Epoch  397, Training Loss: 1.3086, Validation Loss: 1.2949, Learning Rate: 0.060300\n",
      "Epoch  398, Training Loss: 1.3047, Validation Loss: 1.2822, Learning Rate: 0.060200\n",
      "Epoch  399, Training Loss: 1.3023, Validation Loss: 1.2832, Learning Rate: 0.060100\n",
      "Epoch  400, Training Loss: 1.3072, Validation Loss: 1.2760, Learning Rate: 0.060000\n",
      "Epoch  401, Training Loss: 1.3051, Validation Loss: 1.2904, Learning Rate: 0.059900\n",
      "Epoch  402, Training Loss: 1.3054, Validation Loss: 1.2749, Learning Rate: 0.059800\n",
      "Epoch  403, Training Loss: 1.3115, Validation Loss: 1.2766, Learning Rate: 0.059700\n",
      "Epoch  404, Training Loss: 1.2977, Validation Loss: 1.2793, Learning Rate: 0.059600\n",
      "Epoch  405, Training Loss: 1.3003, Validation Loss: 1.2749, Learning Rate: 0.059500\n",
      "Epoch  406, Training Loss: 1.2964, Validation Loss: 1.2750, Learning Rate: 0.059400\n",
      "Epoch  407, Training Loss: 1.2951, Validation Loss: 1.2790, Learning Rate: 0.059300\n",
      "Epoch  408, Training Loss: 1.2940, Validation Loss: 1.2780, Learning Rate: 0.059200\n",
      "Epoch  409, Training Loss: 1.2938, Validation Loss: 1.2787, Learning Rate: 0.059100\n",
      "Epoch  410, Training Loss: 1.2968, Validation Loss: 1.2873, Learning Rate: 0.059000\n",
      "Epoch  411, Training Loss: 1.2972, Validation Loss: 1.2782, Learning Rate: 0.058900\n",
      "Epoch  412, Training Loss: 1.2955, Validation Loss: 1.2787, Learning Rate: 0.058800\n",
      "Epoch  413, Training Loss: 1.2913, Validation Loss: 1.2706, Learning Rate: 0.058700\n",
      "Epoch  414, Training Loss: 1.2982, Validation Loss: 1.2688, Learning Rate: 0.058600\n",
      "Epoch  415, Training Loss: 1.2997, Validation Loss: 1.2692, Learning Rate: 0.058500\n",
      "Epoch  416, Training Loss: 1.3021, Validation Loss: 1.2725, Learning Rate: 0.058400\n",
      "Epoch  417, Training Loss: 1.2895, Validation Loss: 1.2677, Learning Rate: 0.058300\n",
      "Epoch  418, Training Loss: 1.2908, Validation Loss: 1.2769, Learning Rate: 0.058200\n",
      "Epoch  419, Training Loss: 1.2905, Validation Loss: 1.2754, Learning Rate: 0.058100\n",
      "Epoch  420, Training Loss: 1.2883, Validation Loss: 1.2725, Learning Rate: 0.058000\n",
      "Epoch  421, Training Loss: 1.2884, Validation Loss: 1.2738, Learning Rate: 0.057900\n",
      "Epoch  422, Training Loss: 1.2865, Validation Loss: 1.2759, Learning Rate: 0.057800\n",
      "Epoch  423, Training Loss: 1.2835, Validation Loss: 1.2728, Learning Rate: 0.057700\n",
      "Epoch  424, Training Loss: 1.3005, Validation Loss: 1.2631, Learning Rate: 0.057600\n",
      "Epoch  425, Training Loss: 1.2823, Validation Loss: 1.2656, Learning Rate: 0.057500\n",
      "Epoch  426, Training Loss: 1.2886, Validation Loss: 1.2750, Learning Rate: 0.057400\n",
      "Epoch  427, Training Loss: 1.2890, Validation Loss: 1.2852, Learning Rate: 0.057300\n",
      "Epoch  428, Training Loss: 1.2794, Validation Loss: 1.2668, Learning Rate: 0.057200\n",
      "Epoch  429, Training Loss: 1.2809, Validation Loss: 1.2685, Learning Rate: 0.057100\n",
      "Epoch  430, Training Loss: 1.2806, Validation Loss: 1.2763, Learning Rate: 0.057000\n",
      "Epoch  431, Training Loss: 1.2785, Validation Loss: 1.2664, Learning Rate: 0.056900\n",
      "Epoch  432, Training Loss: 1.2787, Validation Loss: 1.2635, Learning Rate: 0.056800\n",
      "Epoch  433, Training Loss: 1.2838, Validation Loss: 1.2822, Learning Rate: 0.056700\n",
      "Epoch  434, Training Loss: 1.2754, Validation Loss: 1.2678, Learning Rate: 0.056600\n",
      "Epoch  435, Training Loss: 1.2775, Validation Loss: 1.2679, Learning Rate: 0.056500\n",
      "Epoch  436, Training Loss: 1.2745, Validation Loss: 1.2610, Learning Rate: 0.056400\n",
      "Epoch  437, Training Loss: 1.2769, Validation Loss: 1.2715, Learning Rate: 0.056300\n",
      "Epoch  438, Training Loss: 1.2757, Validation Loss: 1.2714, Learning Rate: 0.056200\n",
      "Epoch  439, Training Loss: 1.2733, Validation Loss: 1.2591, Learning Rate: 0.056100\n",
      "Epoch  440, Training Loss: 1.2747, Validation Loss: 1.2697, Learning Rate: 0.056000\n",
      "Epoch  441, Training Loss: 1.2716, Validation Loss: 1.2590, Learning Rate: 0.055900\n",
      "Epoch  442, Training Loss: 1.2701, Validation Loss: 1.2615, Learning Rate: 0.055800\n",
      "Epoch  443, Training Loss: 1.2724, Validation Loss: 1.2618, Learning Rate: 0.055700\n",
      "Epoch  444, Training Loss: 1.2699, Validation Loss: 1.2614, Learning Rate: 0.055600\n",
      "Epoch  445, Training Loss: 1.2694, Validation Loss: 1.2706, Learning Rate: 0.055500\n",
      "Epoch  446, Training Loss: 1.2814, Validation Loss: 1.2889, Learning Rate: 0.055400\n",
      "Epoch  447, Training Loss: 1.2742, Validation Loss: 1.2600, Learning Rate: 0.055300\n",
      "Epoch  448, Training Loss: 1.2655, Validation Loss: 1.2660, Learning Rate: 0.055200\n",
      "Epoch  449, Training Loss: 1.2668, Validation Loss: 1.2623, Learning Rate: 0.055100\n",
      "Epoch  450, Training Loss: 1.2655, Validation Loss: 1.2653, Learning Rate: 0.055000\n",
      "Epoch  451, Training Loss: 1.2685, Validation Loss: 1.2680, Learning Rate: 0.054900\n",
      "Epoch  452, Training Loss: 1.2744, Validation Loss: 1.2786, Learning Rate: 0.054800\n",
      "Epoch  453, Training Loss: 1.2665, Validation Loss: 1.2641, Learning Rate: 0.054700\n",
      "Epoch  454, Training Loss: 1.2648, Validation Loss: 1.2605, Learning Rate: 0.054600\n",
      "Epoch  455, Training Loss: 1.2657, Validation Loss: 1.2669, Learning Rate: 0.054500\n",
      "Epoch  456, Training Loss: 1.2669, Validation Loss: 1.2534, Learning Rate: 0.054400\n",
      "Epoch  457, Training Loss: 1.2608, Validation Loss: 1.2606, Learning Rate: 0.054300\n",
      "Epoch  458, Training Loss: 1.2626, Validation Loss: 1.2579, Learning Rate: 0.054200\n",
      "Epoch  459, Training Loss: 1.2743, Validation Loss: 1.2551, Learning Rate: 0.054100\n",
      "Epoch  460, Training Loss: 1.2583, Validation Loss: 1.2583, Learning Rate: 0.054000\n",
      "Epoch  461, Training Loss: 1.2587, Validation Loss: 1.2563, Learning Rate: 0.053900\n",
      "Epoch  462, Training Loss: 1.2596, Validation Loss: 1.2675, Learning Rate: 0.053800\n",
      "Epoch  463, Training Loss: 1.2624, Validation Loss: 1.2725, Learning Rate: 0.053700\n",
      "Epoch  464, Training Loss: 1.2684, Validation Loss: 1.2786, Learning Rate: 0.053600\n",
      "Epoch  465, Training Loss: 1.2616, Validation Loss: 1.2677, Learning Rate: 0.053500\n",
      "Epoch  466, Training Loss: 1.2644, Validation Loss: 1.2488, Learning Rate: 0.053400\n",
      "Epoch  467, Training Loss: 1.2564, Validation Loss: 1.2516, Learning Rate: 0.053300\n",
      "Epoch  468, Training Loss: 1.2551, Validation Loss: 1.2520, Learning Rate: 0.053200\n",
      "Epoch  469, Training Loss: 1.2619, Validation Loss: 1.2720, Learning Rate: 0.053100\n",
      "Epoch  470, Training Loss: 1.2521, Validation Loss: 1.2531, Learning Rate: 0.053000\n",
      "Epoch  471, Training Loss: 1.2586, Validation Loss: 1.2497, Learning Rate: 0.052900\n",
      "Epoch  472, Training Loss: 1.2603, Validation Loss: 1.2487, Learning Rate: 0.052800\n",
      "Epoch  473, Training Loss: 1.2587, Validation Loss: 1.2531, Learning Rate: 0.052700\n",
      "Epoch  474, Training Loss: 1.2555, Validation Loss: 1.2514, Learning Rate: 0.052600\n",
      "Epoch  475, Training Loss: 1.2543, Validation Loss: 1.2557, Learning Rate: 0.052500\n",
      "Epoch  476, Training Loss: 1.2514, Validation Loss: 1.2640, Learning Rate: 0.052400\n",
      "Epoch  477, Training Loss: 1.2556, Validation Loss: 1.2707, Learning Rate: 0.052300\n",
      "Epoch  478, Training Loss: 1.2488, Validation Loss: 1.2594, Learning Rate: 0.052200\n",
      "Epoch  479, Training Loss: 1.2472, Validation Loss: 1.2525, Learning Rate: 0.052100\n",
      "Epoch  480, Training Loss: 1.2553, Validation Loss: 1.2729, Learning Rate: 0.052000\n",
      "Epoch  481, Training Loss: 1.2492, Validation Loss: 1.2633, Learning Rate: 0.051900\n",
      "Epoch  482, Training Loss: 1.2466, Validation Loss: 1.2551, Learning Rate: 0.051800\n",
      "Epoch  483, Training Loss: 1.2470, Validation Loss: 1.2513, Learning Rate: 0.051700\n",
      "Epoch  484, Training Loss: 1.2453, Validation Loss: 1.2513, Learning Rate: 0.051600\n",
      "Epoch  485, Training Loss: 1.2516, Validation Loss: 1.2700, Learning Rate: 0.051500\n",
      "Epoch  486, Training Loss: 1.2467, Validation Loss: 1.2622, Learning Rate: 0.051400\n",
      "Epoch  487, Training Loss: 1.2491, Validation Loss: 1.2661, Learning Rate: 0.051300\n",
      "Epoch  488, Training Loss: 1.2498, Validation Loss: 1.2655, Learning Rate: 0.051200\n",
      "Epoch  489, Training Loss: 1.2465, Validation Loss: 1.2565, Learning Rate: 0.051100\n",
      "Epoch  490, Training Loss: 1.2436, Validation Loss: 1.2469, Learning Rate: 0.051000\n",
      "Epoch  491, Training Loss: 1.2411, Validation Loss: 1.2499, Learning Rate: 0.050900\n",
      "Epoch  492, Training Loss: 1.2413, Validation Loss: 1.2467, Learning Rate: 0.050800\n",
      "Epoch  493, Training Loss: 1.2394, Validation Loss: 1.2534, Learning Rate: 0.050700\n",
      "Epoch  494, Training Loss: 1.2403, Validation Loss: 1.2571, Learning Rate: 0.050600\n",
      "Epoch  495, Training Loss: 1.2403, Validation Loss: 1.2497, Learning Rate: 0.050500\n",
      "Epoch  496, Training Loss: 1.2392, Validation Loss: 1.2515, Learning Rate: 0.050400\n",
      "Epoch  497, Training Loss: 1.2381, Validation Loss: 1.2469, Learning Rate: 0.050300\n",
      "Epoch  498, Training Loss: 1.2365, Validation Loss: 1.2520, Learning Rate: 0.050200\n",
      "Epoch  499, Training Loss: 1.2365, Validation Loss: 1.2477, Learning Rate: 0.050100\n",
      "Epoch  500, Training Loss: 1.2365, Validation Loss: 1.2534, Learning Rate: 0.050000\n",
      "Epoch  501, Training Loss: 1.2359, Validation Loss: 1.2474, Learning Rate: 0.049900\n",
      "Epoch  502, Training Loss: 1.2342, Validation Loss: 1.2482, Learning Rate: 0.049800\n",
      "Epoch  503, Training Loss: 1.2357, Validation Loss: 1.2465, Learning Rate: 0.049700\n",
      "Epoch  504, Training Loss: 1.2342, Validation Loss: 1.2481, Learning Rate: 0.049600\n",
      "Epoch  505, Training Loss: 1.2366, Validation Loss: 1.2556, Learning Rate: 0.049500\n",
      "Epoch  506, Training Loss: 1.2328, Validation Loss: 1.2502, Learning Rate: 0.049400\n",
      "Epoch  507, Training Loss: 1.2321, Validation Loss: 1.2481, Learning Rate: 0.049300\n",
      "Epoch  508, Training Loss: 1.2341, Validation Loss: 1.2441, Learning Rate: 0.049200\n",
      "Epoch  509, Training Loss: 1.2303, Validation Loss: 1.2473, Learning Rate: 0.049100\n",
      "Epoch  510, Training Loss: 1.2314, Validation Loss: 1.2433, Learning Rate: 0.049000\n",
      "Epoch  511, Training Loss: 1.2347, Validation Loss: 1.2428, Learning Rate: 0.048900\n",
      "Epoch  512, Training Loss: 1.2293, Validation Loss: 1.2460, Learning Rate: 0.048800\n",
      "Epoch  513, Training Loss: 1.2298, Validation Loss: 1.2432, Learning Rate: 0.048700\n",
      "Epoch  514, Training Loss: 1.2303, Validation Loss: 1.2540, Learning Rate: 0.048600\n",
      "Epoch  515, Training Loss: 1.2300, Validation Loss: 1.2441, Learning Rate: 0.048500\n",
      "Epoch  516, Training Loss: 1.2289, Validation Loss: 1.2539, Learning Rate: 0.048400\n",
      "Epoch  517, Training Loss: 1.2264, Validation Loss: 1.2468, Learning Rate: 0.048300\n",
      "Epoch  518, Training Loss: 1.2263, Validation Loss: 1.2455, Learning Rate: 0.048200\n",
      "Epoch  519, Training Loss: 1.2278, Validation Loss: 1.2413, Learning Rate: 0.048100\n",
      "Epoch  520, Training Loss: 1.2251, Validation Loss: 1.2465, Learning Rate: 0.048000\n",
      "Epoch  521, Training Loss: 1.2286, Validation Loss: 1.2410, Learning Rate: 0.047900\n",
      "Epoch  522, Training Loss: 1.2267, Validation Loss: 1.2432, Learning Rate: 0.047800\n",
      "Epoch  523, Training Loss: 1.2285, Validation Loss: 1.2591, Learning Rate: 0.047700\n",
      "Epoch  524, Training Loss: 1.2352, Validation Loss: 1.2695, Learning Rate: 0.047600\n",
      "Epoch  525, Training Loss: 1.2302, Validation Loss: 1.2387, Learning Rate: 0.047500\n",
      "Epoch  526, Training Loss: 1.2310, Validation Loss: 1.2404, Learning Rate: 0.047400\n",
      "Epoch  527, Training Loss: 1.2258, Validation Loss: 1.2392, Learning Rate: 0.047300\n",
      "Epoch  528, Training Loss: 1.2300, Validation Loss: 1.2367, Learning Rate: 0.047200\n",
      "Epoch  529, Training Loss: 1.2209, Validation Loss: 1.2465, Learning Rate: 0.047100\n",
      "Epoch  530, Training Loss: 1.2296, Validation Loss: 1.2364, Learning Rate: 0.047000\n",
      "Epoch  531, Training Loss: 1.2206, Validation Loss: 1.2498, Learning Rate: 0.046900\n",
      "Epoch  532, Training Loss: 1.2212, Validation Loss: 1.2495, Learning Rate: 0.046800\n",
      "Epoch  533, Training Loss: 1.2201, Validation Loss: 1.2476, Learning Rate: 0.046700\n",
      "Epoch  534, Training Loss: 1.2202, Validation Loss: 1.2415, Learning Rate: 0.046600\n",
      "Epoch  535, Training Loss: 1.2183, Validation Loss: 1.2425, Learning Rate: 0.046500\n",
      "Epoch  536, Training Loss: 1.2184, Validation Loss: 1.2414, Learning Rate: 0.046400\n",
      "Epoch  537, Training Loss: 1.2179, Validation Loss: 1.2463, Learning Rate: 0.046300\n",
      "Epoch  538, Training Loss: 1.2204, Validation Loss: 1.2392, Learning Rate: 0.046200\n",
      "Epoch  539, Training Loss: 1.2164, Validation Loss: 1.2408, Learning Rate: 0.046100\n",
      "Epoch  540, Training Loss: 1.2159, Validation Loss: 1.2448, Learning Rate: 0.046000\n",
      "Epoch  541, Training Loss: 1.2170, Validation Loss: 1.2434, Learning Rate: 0.045900\n",
      "Epoch  542, Training Loss: 1.2181, Validation Loss: 1.2394, Learning Rate: 0.045800\n",
      "Epoch  543, Training Loss: 1.2158, Validation Loss: 1.2392, Learning Rate: 0.045700\n",
      "Epoch  544, Training Loss: 1.2141, Validation Loss: 1.2446, Learning Rate: 0.045600\n",
      "Epoch  545, Training Loss: 1.2142, Validation Loss: 1.2432, Learning Rate: 0.045500\n",
      "Epoch  546, Training Loss: 1.2142, Validation Loss: 1.2410, Learning Rate: 0.045400\n",
      "Epoch  547, Training Loss: 1.2229, Validation Loss: 1.2354, Learning Rate: 0.045300\n",
      "Epoch  548, Training Loss: 1.2129, Validation Loss: 1.2490, Learning Rate: 0.045200\n",
      "Epoch  549, Training Loss: 1.2153, Validation Loss: 1.2374, Learning Rate: 0.045100\n",
      "Epoch  550, Training Loss: 1.2127, Validation Loss: 1.2509, Learning Rate: 0.045000\n",
      "Epoch  551, Training Loss: 1.2125, Validation Loss: 1.2397, Learning Rate: 0.044900\n",
      "Epoch  552, Training Loss: 1.2107, Validation Loss: 1.2433, Learning Rate: 0.044800\n",
      "Epoch  553, Training Loss: 1.2121, Validation Loss: 1.2378, Learning Rate: 0.044700\n",
      "Epoch  554, Training Loss: 1.2119, Validation Loss: 1.2391, Learning Rate: 0.044600\n",
      "Epoch  555, Training Loss: 1.2098, Validation Loss: 1.2417, Learning Rate: 0.044500\n",
      "Epoch  556, Training Loss: 1.2131, Validation Loss: 1.2554, Learning Rate: 0.044400\n",
      "Epoch  557, Training Loss: 1.2093, Validation Loss: 1.2502, Learning Rate: 0.044300\n",
      "Epoch  558, Training Loss: 1.2079, Validation Loss: 1.2398, Learning Rate: 0.044200\n",
      "Epoch  559, Training Loss: 1.2077, Validation Loss: 1.2469, Learning Rate: 0.044100\n",
      "Epoch  560, Training Loss: 1.2094, Validation Loss: 1.2362, Learning Rate: 0.044000\n",
      "Epoch  561, Training Loss: 1.2069, Validation Loss: 1.2414, Learning Rate: 0.043900\n",
      "Epoch  562, Training Loss: 1.2063, Validation Loss: 1.2430, Learning Rate: 0.043800\n",
      "Epoch  563, Training Loss: 1.2059, Validation Loss: 1.2403, Learning Rate: 0.043700\n",
      "Epoch  564, Training Loss: 1.2094, Validation Loss: 1.2519, Learning Rate: 0.043600\n",
      "Epoch  565, Training Loss: 1.2064, Validation Loss: 1.2409, Learning Rate: 0.043500\n",
      "Epoch  566, Training Loss: 1.2045, Validation Loss: 1.2385, Learning Rate: 0.043400\n",
      "Epoch  567, Training Loss: 1.2046, Validation Loss: 1.2380, Learning Rate: 0.043300\n",
      "Epoch  568, Training Loss: 1.2042, Validation Loss: 1.2370, Learning Rate: 0.043200\n",
      "Epoch  569, Training Loss: 1.2089, Validation Loss: 1.2315, Learning Rate: 0.043100\n",
      "Epoch  570, Training Loss: 1.2076, Validation Loss: 1.2304, Learning Rate: 0.043000\n",
      "Epoch  571, Training Loss: 1.2025, Validation Loss: 1.2395, Learning Rate: 0.042900\n",
      "Epoch  572, Training Loss: 1.2032, Validation Loss: 1.2363, Learning Rate: 0.042800\n",
      "Epoch  573, Training Loss: 1.2050, Validation Loss: 1.2484, Learning Rate: 0.042700\n",
      "Epoch  574, Training Loss: 1.2015, Validation Loss: 1.2345, Learning Rate: 0.042600\n",
      "Epoch  575, Training Loss: 1.2026, Validation Loss: 1.2363, Learning Rate: 0.042500\n",
      "Epoch  576, Training Loss: 1.2042, Validation Loss: 1.2324, Learning Rate: 0.042400\n",
      "Epoch  577, Training Loss: 1.2012, Validation Loss: 1.2428, Learning Rate: 0.042300\n",
      "Epoch  578, Training Loss: 1.2040, Validation Loss: 1.2314, Learning Rate: 0.042200\n",
      "Epoch  579, Training Loss: 1.1993, Validation Loss: 1.2350, Learning Rate: 0.042100\n",
      "Epoch  580, Training Loss: 1.2005, Validation Loss: 1.2441, Learning Rate: 0.042000\n",
      "Epoch  581, Training Loss: 1.2024, Validation Loss: 1.2313, Learning Rate: 0.041900\n",
      "Epoch  582, Training Loss: 1.1991, Validation Loss: 1.2331, Learning Rate: 0.041800\n",
      "Epoch  583, Training Loss: 1.1985, Validation Loss: 1.2357, Learning Rate: 0.041700\n",
      "Epoch  584, Training Loss: 1.1999, Validation Loss: 1.2282, Learning Rate: 0.041600\n",
      "Epoch  585, Training Loss: 1.1977, Validation Loss: 1.2317, Learning Rate: 0.041500\n",
      "Epoch  586, Training Loss: 1.1971, Validation Loss: 1.2331, Learning Rate: 0.041400\n",
      "Epoch  587, Training Loss: 1.1993, Validation Loss: 1.2304, Learning Rate: 0.041300\n",
      "Epoch  588, Training Loss: 1.1977, Validation Loss: 1.2290, Learning Rate: 0.041200\n",
      "Epoch  589, Training Loss: 1.2073, Validation Loss: 1.2272, Learning Rate: 0.041100\n",
      "Epoch  590, Training Loss: 1.1972, Validation Loss: 1.2319, Learning Rate: 0.041000\n",
      "Epoch  591, Training Loss: 1.1991, Validation Loss: 1.2279, Learning Rate: 0.040900\n",
      "Epoch  592, Training Loss: 1.1980, Validation Loss: 1.2298, Learning Rate: 0.040800\n",
      "Epoch  593, Training Loss: 1.1948, Validation Loss: 1.2379, Learning Rate: 0.040700\n",
      "Epoch  594, Training Loss: 1.1940, Validation Loss: 1.2371, Learning Rate: 0.040600\n",
      "Epoch  595, Training Loss: 1.1939, Validation Loss: 1.2374, Learning Rate: 0.040500\n",
      "Epoch  596, Training Loss: 1.1945, Validation Loss: 1.2366, Learning Rate: 0.040400\n",
      "Epoch  597, Training Loss: 1.1954, Validation Loss: 1.2408, Learning Rate: 0.040300\n",
      "Epoch  598, Training Loss: 1.1929, Validation Loss: 1.2342, Learning Rate: 0.040200\n",
      "Epoch  599, Training Loss: 1.1917, Validation Loss: 1.2302, Learning Rate: 0.040100\n",
      "Epoch  600, Training Loss: 1.1913, Validation Loss: 1.2324, Learning Rate: 0.040000\n",
      "Epoch  601, Training Loss: 1.1981, Validation Loss: 1.2265, Learning Rate: 0.039900\n",
      "Epoch  602, Training Loss: 1.1928, Validation Loss: 1.2371, Learning Rate: 0.039800\n",
      "Epoch  603, Training Loss: 1.1929, Validation Loss: 1.2389, Learning Rate: 0.039700\n",
      "Epoch  604, Training Loss: 1.1973, Validation Loss: 1.2492, Learning Rate: 0.039600\n",
      "Epoch  605, Training Loss: 1.1973, Validation Loss: 1.2495, Learning Rate: 0.039500\n",
      "Epoch  606, Training Loss: 1.1961, Validation Loss: 1.2494, Learning Rate: 0.039400\n",
      "Epoch  607, Training Loss: 1.1909, Validation Loss: 1.2414, Learning Rate: 0.039300\n",
      "Epoch  608, Training Loss: 1.1888, Validation Loss: 1.2341, Learning Rate: 0.039200\n",
      "Epoch  609, Training Loss: 1.1913, Validation Loss: 1.2399, Learning Rate: 0.039100\n",
      "Epoch  610, Training Loss: 1.1965, Validation Loss: 1.2478, Learning Rate: 0.039000\n",
      "Epoch  611, Training Loss: 1.1910, Validation Loss: 1.2391, Learning Rate: 0.038900\n",
      "Epoch  612, Training Loss: 1.1879, Validation Loss: 1.2284, Learning Rate: 0.038800\n",
      "Epoch  613, Training Loss: 1.1887, Validation Loss: 1.2308, Learning Rate: 0.038700\n",
      "Epoch  614, Training Loss: 1.1885, Validation Loss: 1.2342, Learning Rate: 0.038600\n",
      "Epoch  615, Training Loss: 1.1868, Validation Loss: 1.2309, Learning Rate: 0.038500\n",
      "Epoch  616, Training Loss: 1.1862, Validation Loss: 1.2278, Learning Rate: 0.038400\n",
      "Epoch  617, Training Loss: 1.1854, Validation Loss: 1.2291, Learning Rate: 0.038300\n",
      "Epoch  618, Training Loss: 1.1855, Validation Loss: 1.2332, Learning Rate: 0.038200\n",
      "Epoch  619, Training Loss: 1.1864, Validation Loss: 1.2375, Learning Rate: 0.038100\n",
      "Epoch  620, Training Loss: 1.1854, Validation Loss: 1.2308, Learning Rate: 0.038000\n",
      "Epoch  621, Training Loss: 1.1855, Validation Loss: 1.2340, Learning Rate: 0.037900\n",
      "Epoch  622, Training Loss: 1.1859, Validation Loss: 1.2334, Learning Rate: 0.037800\n",
      "Epoch  623, Training Loss: 1.1859, Validation Loss: 1.2290, Learning Rate: 0.037700\n",
      "Epoch  624, Training Loss: 1.1841, Validation Loss: 1.2334, Learning Rate: 0.037600\n",
      "Epoch  625, Training Loss: 1.1875, Validation Loss: 1.2430, Learning Rate: 0.037500\n",
      "Epoch  626, Training Loss: 1.1858, Validation Loss: 1.2423, Learning Rate: 0.037400\n",
      "Epoch  627, Training Loss: 1.1843, Validation Loss: 1.2364, Learning Rate: 0.037300\n",
      "Epoch  628, Training Loss: 1.1823, Validation Loss: 1.2345, Learning Rate: 0.037200\n",
      "Epoch  629, Training Loss: 1.1823, Validation Loss: 1.2356, Learning Rate: 0.037100\n",
      "Epoch  630, Training Loss: 1.1826, Validation Loss: 1.2280, Learning Rate: 0.037000\n",
      "Epoch  631, Training Loss: 1.1812, Validation Loss: 1.2289, Learning Rate: 0.036900\n",
      "Epoch  632, Training Loss: 1.1809, Validation Loss: 1.2301, Learning Rate: 0.036800\n",
      "Epoch  633, Training Loss: 1.1842, Validation Loss: 1.2395, Learning Rate: 0.036700\n",
      "Epoch  634, Training Loss: 1.1830, Validation Loss: 1.2254, Learning Rate: 0.036600\n",
      "Epoch  635, Training Loss: 1.1803, Validation Loss: 1.2284, Learning Rate: 0.036500\n",
      "Epoch  636, Training Loss: 1.1801, Validation Loss: 1.2301, Learning Rate: 0.036400\n",
      "Epoch  637, Training Loss: 1.1805, Validation Loss: 1.2294, Learning Rate: 0.036300\n",
      "Epoch  638, Training Loss: 1.1809, Validation Loss: 1.2291, Learning Rate: 0.036200\n",
      "Epoch  639, Training Loss: 1.1808, Validation Loss: 1.2246, Learning Rate: 0.036100\n",
      "Epoch  640, Training Loss: 1.1815, Validation Loss: 1.2310, Learning Rate: 0.036000\n",
      "Epoch  641, Training Loss: 1.1835, Validation Loss: 1.2218, Learning Rate: 0.035900\n",
      "Epoch  642, Training Loss: 1.1981, Validation Loss: 1.2225, Learning Rate: 0.035800\n",
      "Epoch  643, Training Loss: 1.1780, Validation Loss: 1.2316, Learning Rate: 0.035700\n",
      "Epoch  644, Training Loss: 1.1784, Validation Loss: 1.2256, Learning Rate: 0.035600\n",
      "Epoch  645, Training Loss: 1.1793, Validation Loss: 1.2365, Learning Rate: 0.035500\n",
      "Epoch  646, Training Loss: 1.1813, Validation Loss: 1.2396, Learning Rate: 0.035400\n",
      "Epoch  647, Training Loss: 1.1824, Validation Loss: 1.2426, Learning Rate: 0.035300\n",
      "Epoch  648, Training Loss: 1.1790, Validation Loss: 1.2358, Learning Rate: 0.035200\n",
      "Epoch  649, Training Loss: 1.1767, Validation Loss: 1.2316, Learning Rate: 0.035100\n",
      "Epoch  650, Training Loss: 1.1780, Validation Loss: 1.2335, Learning Rate: 0.035000\n",
      "Epoch  651, Training Loss: 1.1757, Validation Loss: 1.2291, Learning Rate: 0.034900\n",
      "Epoch  652, Training Loss: 1.1755, Validation Loss: 1.2266, Learning Rate: 0.034800\n",
      "Epoch  653, Training Loss: 1.1749, Validation Loss: 1.2254, Learning Rate: 0.034700\n",
      "Epoch  654, Training Loss: 1.1757, Validation Loss: 1.2337, Learning Rate: 0.034600\n",
      "Epoch  655, Training Loss: 1.1745, Validation Loss: 1.2258, Learning Rate: 0.034500\n",
      "Epoch  656, Training Loss: 1.1766, Validation Loss: 1.2223, Learning Rate: 0.034400\n",
      "Epoch  657, Training Loss: 1.1765, Validation Loss: 1.2281, Learning Rate: 0.034300\n",
      "Epoch  658, Training Loss: 1.1769, Validation Loss: 1.2235, Learning Rate: 0.034200\n",
      "Epoch  659, Training Loss: 1.1740, Validation Loss: 1.2330, Learning Rate: 0.034100\n",
      "Epoch  660, Training Loss: 1.1776, Validation Loss: 1.2377, Learning Rate: 0.034000\n",
      "Epoch  661, Training Loss: 1.1754, Validation Loss: 1.2288, Learning Rate: 0.033900\n",
      "Epoch  662, Training Loss: 1.1746, Validation Loss: 1.2318, Learning Rate: 0.033800\n",
      "Epoch  663, Training Loss: 1.1768, Validation Loss: 1.2183, Learning Rate: 0.033700\n",
      "Epoch  664, Training Loss: 1.1856, Validation Loss: 1.2168, Learning Rate: 0.033600\n",
      "Epoch  665, Training Loss: 1.1718, Validation Loss: 1.2292, Learning Rate: 0.033500\n",
      "Epoch  666, Training Loss: 1.1718, Validation Loss: 1.2283, Learning Rate: 0.033400\n",
      "Epoch  667, Training Loss: 1.1716, Validation Loss: 1.2229, Learning Rate: 0.033300\n",
      "Epoch  668, Training Loss: 1.1729, Validation Loss: 1.2198, Learning Rate: 0.033200\n",
      "Epoch  669, Training Loss: 1.1707, Validation Loss: 1.2237, Learning Rate: 0.033100\n",
      "Epoch  670, Training Loss: 1.1710, Validation Loss: 1.2236, Learning Rate: 0.033000\n",
      "Epoch  671, Training Loss: 1.1708, Validation Loss: 1.2261, Learning Rate: 0.032900\n",
      "Epoch  672, Training Loss: 1.1709, Validation Loss: 1.2198, Learning Rate: 0.032800\n",
      "Epoch  673, Training Loss: 1.1737, Validation Loss: 1.2180, Learning Rate: 0.032700\n",
      "Epoch  674, Training Loss: 1.1747, Validation Loss: 1.2172, Learning Rate: 0.032600\n",
      "Epoch  675, Training Loss: 1.1700, Validation Loss: 1.2210, Learning Rate: 0.032500\n",
      "Epoch  676, Training Loss: 1.1697, Validation Loss: 1.2271, Learning Rate: 0.032400\n",
      "Epoch  677, Training Loss: 1.1695, Validation Loss: 1.2307, Learning Rate: 0.032300\n",
      "Epoch  678, Training Loss: 1.1684, Validation Loss: 1.2275, Learning Rate: 0.032200\n",
      "Epoch  679, Training Loss: 1.1697, Validation Loss: 1.2289, Learning Rate: 0.032100\n",
      "Epoch  680, Training Loss: 1.1722, Validation Loss: 1.2199, Learning Rate: 0.032000\n",
      "Epoch  681, Training Loss: 1.1681, Validation Loss: 1.2260, Learning Rate: 0.031900\n",
      "Epoch  682, Training Loss: 1.1715, Validation Loss: 1.2214, Learning Rate: 0.031800\n",
      "Epoch  683, Training Loss: 1.1723, Validation Loss: 1.2225, Learning Rate: 0.031700\n",
      "Epoch  684, Training Loss: 1.1681, Validation Loss: 1.2298, Learning Rate: 0.031600\n",
      "Epoch  685, Training Loss: 1.1679, Validation Loss: 1.2318, Learning Rate: 0.031500\n",
      "Epoch  686, Training Loss: 1.1684, Validation Loss: 1.2331, Learning Rate: 0.031400\n",
      "Epoch  687, Training Loss: 1.1694, Validation Loss: 1.2376, Learning Rate: 0.031300\n",
      "Epoch  688, Training Loss: 1.1660, Validation Loss: 1.2299, Learning Rate: 0.031200\n",
      "Epoch  689, Training Loss: 1.1658, Validation Loss: 1.2227, Learning Rate: 0.031100\n",
      "Epoch  690, Training Loss: 1.1657, Validation Loss: 1.2218, Learning Rate: 0.031000\n",
      "Epoch  691, Training Loss: 1.1667, Validation Loss: 1.2306, Learning Rate: 0.030900\n",
      "Epoch  692, Training Loss: 1.1658, Validation Loss: 1.2251, Learning Rate: 0.030800\n",
      "Epoch  693, Training Loss: 1.1650, Validation Loss: 1.2266, Learning Rate: 0.030700\n",
      "Epoch  694, Training Loss: 1.1658, Validation Loss: 1.2239, Learning Rate: 0.030600\n",
      "Epoch  695, Training Loss: 1.1640, Validation Loss: 1.2239, Learning Rate: 0.030500\n",
      "Epoch  696, Training Loss: 1.1643, Validation Loss: 1.2211, Learning Rate: 0.030400\n",
      "Epoch  697, Training Loss: 1.1640, Validation Loss: 1.2282, Learning Rate: 0.030300\n",
      "Epoch  698, Training Loss: 1.1642, Validation Loss: 1.2230, Learning Rate: 0.030200\n",
      "Epoch  699, Training Loss: 1.1632, Validation Loss: 1.2249, Learning Rate: 0.030100\n",
      "Epoch  700, Training Loss: 1.1630, Validation Loss: 1.2251, Learning Rate: 0.030000\n",
      "Epoch  701, Training Loss: 1.1628, Validation Loss: 1.2222, Learning Rate: 0.029900\n",
      "Epoch  702, Training Loss: 1.1638, Validation Loss: 1.2217, Learning Rate: 0.029800\n",
      "Epoch  703, Training Loss: 1.1626, Validation Loss: 1.2272, Learning Rate: 0.029700\n",
      "Epoch  704, Training Loss: 1.1622, Validation Loss: 1.2272, Learning Rate: 0.029600\n",
      "Epoch  705, Training Loss: 1.1617, Validation Loss: 1.2246, Learning Rate: 0.029500\n",
      "Epoch  706, Training Loss: 1.1623, Validation Loss: 1.2281, Learning Rate: 0.029400\n",
      "Epoch  707, Training Loss: 1.1620, Validation Loss: 1.2262, Learning Rate: 0.029300\n",
      "Epoch  708, Training Loss: 1.1633, Validation Loss: 1.2200, Learning Rate: 0.029200\n",
      "Epoch  709, Training Loss: 1.1611, Validation Loss: 1.2250, Learning Rate: 0.029100\n",
      "Epoch  710, Training Loss: 1.1625, Validation Loss: 1.2320, Learning Rate: 0.029000\n",
      "Epoch  711, Training Loss: 1.1616, Validation Loss: 1.2310, Learning Rate: 0.028900\n",
      "Epoch  712, Training Loss: 1.1626, Validation Loss: 1.2207, Learning Rate: 0.028800\n",
      "Epoch  713, Training Loss: 1.1610, Validation Loss: 1.2249, Learning Rate: 0.028700\n",
      "Epoch  714, Training Loss: 1.1605, Validation Loss: 1.2210, Learning Rate: 0.028600\n",
      "Epoch  715, Training Loss: 1.1598, Validation Loss: 1.2234, Learning Rate: 0.028500\n",
      "Epoch  716, Training Loss: 1.1612, Validation Loss: 1.2219, Learning Rate: 0.028400\n",
      "Epoch  717, Training Loss: 1.1600, Validation Loss: 1.2242, Learning Rate: 0.028300\n",
      "Epoch  718, Training Loss: 1.1607, Validation Loss: 1.2231, Learning Rate: 0.028200\n",
      "Epoch  719, Training Loss: 1.1598, Validation Loss: 1.2240, Learning Rate: 0.028100\n",
      "Epoch  720, Training Loss: 1.1623, Validation Loss: 1.2212, Learning Rate: 0.028000\n",
      "Epoch  721, Training Loss: 1.1649, Validation Loss: 1.2185, Learning Rate: 0.027900\n",
      "Epoch  722, Training Loss: 1.1600, Validation Loss: 1.2208, Learning Rate: 0.027800\n",
      "Epoch  723, Training Loss: 1.1583, Validation Loss: 1.2254, Learning Rate: 0.027700\n",
      "Epoch  724, Training Loss: 1.1587, Validation Loss: 1.2307, Learning Rate: 0.027600\n",
      "Epoch  725, Training Loss: 1.1577, Validation Loss: 1.2234, Learning Rate: 0.027500\n",
      "Epoch  726, Training Loss: 1.1572, Validation Loss: 1.2230, Learning Rate: 0.027400\n",
      "Epoch  727, Training Loss: 1.1576, Validation Loss: 1.2272, Learning Rate: 0.027300\n",
      "Epoch  728, Training Loss: 1.1581, Validation Loss: 1.2273, Learning Rate: 0.027200\n",
      "Epoch  729, Training Loss: 1.1574, Validation Loss: 1.2229, Learning Rate: 0.027100\n",
      "Epoch  730, Training Loss: 1.1577, Validation Loss: 1.2247, Learning Rate: 0.027000\n",
      "Epoch  731, Training Loss: 1.1567, Validation Loss: 1.2217, Learning Rate: 0.026900\n",
      "Epoch  732, Training Loss: 1.1580, Validation Loss: 1.2168, Learning Rate: 0.026800\n",
      "Epoch  733, Training Loss: 1.1562, Validation Loss: 1.2217, Learning Rate: 0.026700\n",
      "Epoch  734, Training Loss: 1.1599, Validation Loss: 1.2320, Learning Rate: 0.026600\n",
      "Epoch  735, Training Loss: 1.1570, Validation Loss: 1.2274, Learning Rate: 0.026500\n",
      "Epoch  736, Training Loss: 1.1561, Validation Loss: 1.2226, Learning Rate: 0.026400\n",
      "Epoch  737, Training Loss: 1.1559, Validation Loss: 1.2195, Learning Rate: 0.026300\n",
      "Epoch  738, Training Loss: 1.1561, Validation Loss: 1.2208, Learning Rate: 0.026200\n",
      "Epoch  739, Training Loss: 1.1583, Validation Loss: 1.2175, Learning Rate: 0.026100\n",
      "Epoch  740, Training Loss: 1.1553, Validation Loss: 1.2196, Learning Rate: 0.026000\n",
      "Epoch  741, Training Loss: 1.1553, Validation Loss: 1.2269, Learning Rate: 0.025900\n",
      "Epoch  742, Training Loss: 1.1555, Validation Loss: 1.2267, Learning Rate: 0.025800\n",
      "Epoch  743, Training Loss: 1.1557, Validation Loss: 1.2267, Learning Rate: 0.025700\n",
      "Epoch  744, Training Loss: 1.1557, Validation Loss: 1.2294, Learning Rate: 0.025600\n",
      "Epoch  745, Training Loss: 1.1542, Validation Loss: 1.2224, Learning Rate: 0.025500\n",
      "Epoch  746, Training Loss: 1.1566, Validation Loss: 1.2194, Learning Rate: 0.025400\n",
      "Epoch  747, Training Loss: 1.1540, Validation Loss: 1.2188, Learning Rate: 0.025300\n",
      "Epoch  748, Training Loss: 1.1537, Validation Loss: 1.2242, Learning Rate: 0.025200\n",
      "Epoch  749, Training Loss: 1.1540, Validation Loss: 1.2183, Learning Rate: 0.025100\n",
      "Epoch  750, Training Loss: 1.1531, Validation Loss: 1.2213, Learning Rate: 0.025000\n",
      "Epoch  751, Training Loss: 1.1550, Validation Loss: 1.2288, Learning Rate: 0.024900\n",
      "Epoch  752, Training Loss: 1.1538, Validation Loss: 1.2257, Learning Rate: 0.024800\n",
      "Epoch  753, Training Loss: 1.1533, Validation Loss: 1.2220, Learning Rate: 0.024700\n",
      "Epoch  754, Training Loss: 1.1544, Validation Loss: 1.2185, Learning Rate: 0.024600\n",
      "Epoch  755, Training Loss: 1.1522, Validation Loss: 1.2211, Learning Rate: 0.024500\n",
      "Epoch  756, Training Loss: 1.1528, Validation Loss: 1.2185, Learning Rate: 0.024400\n",
      "Epoch  757, Training Loss: 1.1544, Validation Loss: 1.2157, Learning Rate: 0.024300\n",
      "Epoch  758, Training Loss: 1.1530, Validation Loss: 1.2185, Learning Rate: 0.024200\n",
      "Epoch  759, Training Loss: 1.1523, Validation Loss: 1.2193, Learning Rate: 0.024100\n",
      "Epoch  760, Training Loss: 1.1515, Validation Loss: 1.2230, Learning Rate: 0.024000\n",
      "Epoch  761, Training Loss: 1.1524, Validation Loss: 1.2273, Learning Rate: 0.023900\n",
      "Epoch  762, Training Loss: 1.1554, Validation Loss: 1.2348, Learning Rate: 0.023800\n",
      "Epoch  763, Training Loss: 1.1518, Validation Loss: 1.2275, Learning Rate: 0.023700\n",
      "Epoch  764, Training Loss: 1.1508, Validation Loss: 1.2242, Learning Rate: 0.023600\n",
      "Epoch  765, Training Loss: 1.1537, Validation Loss: 1.2315, Learning Rate: 0.023500\n",
      "Epoch  766, Training Loss: 1.1564, Validation Loss: 1.2349, Learning Rate: 0.023400\n",
      "Epoch  767, Training Loss: 1.1573, Validation Loss: 1.2357, Learning Rate: 0.023300\n",
      "Epoch  768, Training Loss: 1.1511, Validation Loss: 1.2256, Learning Rate: 0.023200\n",
      "Epoch  769, Training Loss: 1.1506, Validation Loss: 1.2251, Learning Rate: 0.023100\n",
      "Epoch  770, Training Loss: 1.1506, Validation Loss: 1.2250, Learning Rate: 0.023000\n",
      "Epoch  771, Training Loss: 1.1513, Validation Loss: 1.2280, Learning Rate: 0.022900\n",
      "Epoch  772, Training Loss: 1.1496, Validation Loss: 1.2178, Learning Rate: 0.022800\n",
      "Epoch  773, Training Loss: 1.1494, Validation Loss: 1.2187, Learning Rate: 0.022700\n",
      "Epoch  774, Training Loss: 1.1498, Validation Loss: 1.2235, Learning Rate: 0.022600\n",
      "Epoch  775, Training Loss: 1.1500, Validation Loss: 1.2212, Learning Rate: 0.022500\n",
      "Epoch  776, Training Loss: 1.1493, Validation Loss: 1.2209, Learning Rate: 0.022400\n",
      "Epoch  777, Training Loss: 1.1518, Validation Loss: 1.2290, Learning Rate: 0.022300\n",
      "Epoch  778, Training Loss: 1.1519, Validation Loss: 1.2284, Learning Rate: 0.022200\n",
      "Epoch  779, Training Loss: 1.1491, Validation Loss: 1.2208, Learning Rate: 0.022100\n",
      "Epoch  780, Training Loss: 1.1496, Validation Loss: 1.2179, Learning Rate: 0.022000\n",
      "Epoch  781, Training Loss: 1.1487, Validation Loss: 1.2189, Learning Rate: 0.021900\n",
      "Epoch  782, Training Loss: 1.1479, Validation Loss: 1.2218, Learning Rate: 0.021800\n",
      "Epoch  783, Training Loss: 1.1484, Validation Loss: 1.2242, Learning Rate: 0.021700\n",
      "Epoch  784, Training Loss: 1.1487, Validation Loss: 1.2265, Learning Rate: 0.021600\n",
      "Epoch  785, Training Loss: 1.1495, Validation Loss: 1.2277, Learning Rate: 0.021500\n",
      "Epoch  786, Training Loss: 1.1483, Validation Loss: 1.2256, Learning Rate: 0.021400\n",
      "Epoch  787, Training Loss: 1.1475, Validation Loss: 1.2214, Learning Rate: 0.021300\n",
      "Epoch  788, Training Loss: 1.1474, Validation Loss: 1.2228, Learning Rate: 0.021200\n",
      "Epoch  789, Training Loss: 1.1494, Validation Loss: 1.2281, Learning Rate: 0.021100\n",
      "Epoch  790, Training Loss: 1.1475, Validation Loss: 1.2227, Learning Rate: 0.021000\n",
      "Epoch  791, Training Loss: 1.1470, Validation Loss: 1.2211, Learning Rate: 0.020900\n",
      "Epoch  792, Training Loss: 1.1467, Validation Loss: 1.2216, Learning Rate: 0.020800\n",
      "Epoch  793, Training Loss: 1.1466, Validation Loss: 1.2181, Learning Rate: 0.020700\n",
      "Epoch  794, Training Loss: 1.1466, Validation Loss: 1.2213, Learning Rate: 0.020600\n",
      "Epoch  795, Training Loss: 1.1480, Validation Loss: 1.2248, Learning Rate: 0.020500\n",
      "Epoch  796, Training Loss: 1.1467, Validation Loss: 1.2161, Learning Rate: 0.020400\n",
      "Epoch  797, Training Loss: 1.1463, Validation Loss: 1.2159, Learning Rate: 0.020300\n",
      "Epoch  798, Training Loss: 1.1459, Validation Loss: 1.2167, Learning Rate: 0.020200\n",
      "Epoch  799, Training Loss: 1.1457, Validation Loss: 1.2151, Learning Rate: 0.020100\n",
      "Epoch  800, Training Loss: 1.1486, Validation Loss: 1.2111, Learning Rate: 0.020000\n",
      "Epoch  801, Training Loss: 1.1492, Validation Loss: 1.2096, Learning Rate: 0.019900\n",
      "Epoch  802, Training Loss: 1.1491, Validation Loss: 1.2102, Learning Rate: 0.019800\n",
      "Epoch  803, Training Loss: 1.1456, Validation Loss: 1.2140, Learning Rate: 0.019700\n",
      "Epoch  804, Training Loss: 1.1462, Validation Loss: 1.2136, Learning Rate: 0.019600\n",
      "Epoch  805, Training Loss: 1.1459, Validation Loss: 1.2143, Learning Rate: 0.019500\n",
      "Epoch  806, Training Loss: 1.1447, Validation Loss: 1.2191, Learning Rate: 0.019400\n",
      "Epoch  807, Training Loss: 1.1451, Validation Loss: 1.2181, Learning Rate: 0.019300\n",
      "Epoch  808, Training Loss: 1.1445, Validation Loss: 1.2205, Learning Rate: 0.019200\n",
      "Epoch  809, Training Loss: 1.1460, Validation Loss: 1.2234, Learning Rate: 0.019100\n",
      "Epoch  810, Training Loss: 1.1458, Validation Loss: 1.2221, Learning Rate: 0.019000\n",
      "Epoch  811, Training Loss: 1.1450, Validation Loss: 1.2207, Learning Rate: 0.018900\n",
      "Epoch  812, Training Loss: 1.1438, Validation Loss: 1.2156, Learning Rate: 0.018800\n",
      "Epoch  813, Training Loss: 1.1453, Validation Loss: 1.2126, Learning Rate: 0.018700\n",
      "Epoch  814, Training Loss: 1.1448, Validation Loss: 1.2129, Learning Rate: 0.018600\n",
      "Epoch  815, Training Loss: 1.1448, Validation Loss: 1.2118, Learning Rate: 0.018500\n",
      "Epoch  816, Training Loss: 1.1435, Validation Loss: 1.2185, Learning Rate: 0.018400\n",
      "Epoch  817, Training Loss: 1.1432, Validation Loss: 1.2161, Learning Rate: 0.018300\n",
      "Epoch  818, Training Loss: 1.1431, Validation Loss: 1.2195, Learning Rate: 0.018200\n",
      "Epoch  819, Training Loss: 1.1429, Validation Loss: 1.2189, Learning Rate: 0.018100\n",
      "Epoch  820, Training Loss: 1.1438, Validation Loss: 1.2221, Learning Rate: 0.018000\n",
      "Epoch  821, Training Loss: 1.1429, Validation Loss: 1.2175, Learning Rate: 0.017900\n",
      "Epoch  822, Training Loss: 1.1438, Validation Loss: 1.2143, Learning Rate: 0.017800\n",
      "Epoch  823, Training Loss: 1.1436, Validation Loss: 1.2135, Learning Rate: 0.017700\n",
      "Epoch  824, Training Loss: 1.1483, Validation Loss: 1.2110, Learning Rate: 0.017600\n",
      "Epoch  825, Training Loss: 1.1425, Validation Loss: 1.2144, Learning Rate: 0.017500\n",
      "Epoch  826, Training Loss: 1.1429, Validation Loss: 1.2138, Learning Rate: 0.017400\n",
      "Epoch  827, Training Loss: 1.1421, Validation Loss: 1.2151, Learning Rate: 0.017300\n",
      "Epoch  828, Training Loss: 1.1423, Validation Loss: 1.2166, Learning Rate: 0.017200\n",
      "Epoch  829, Training Loss: 1.1418, Validation Loss: 1.2148, Learning Rate: 0.017100\n",
      "Epoch  830, Training Loss: 1.1418, Validation Loss: 1.2173, Learning Rate: 0.017000\n",
      "Epoch  831, Training Loss: 1.1421, Validation Loss: 1.2181, Learning Rate: 0.016900\n",
      "Epoch  832, Training Loss: 1.1422, Validation Loss: 1.2185, Learning Rate: 0.016800\n",
      "Epoch  833, Training Loss: 1.1415, Validation Loss: 1.2151, Learning Rate: 0.016700\n",
      "Epoch  834, Training Loss: 1.1417, Validation Loss: 1.2135, Learning Rate: 0.016600\n",
      "Epoch  835, Training Loss: 1.1412, Validation Loss: 1.2161, Learning Rate: 0.016500\n",
      "Epoch  836, Training Loss: 1.1410, Validation Loss: 1.2163, Learning Rate: 0.016400\n",
      "Epoch  837, Training Loss: 1.1412, Validation Loss: 1.2157, Learning Rate: 0.016300\n",
      "Epoch  838, Training Loss: 1.1409, Validation Loss: 1.2149, Learning Rate: 0.016200\n",
      "Epoch  839, Training Loss: 1.1407, Validation Loss: 1.2165, Learning Rate: 0.016100\n",
      "Epoch  840, Training Loss: 1.1414, Validation Loss: 1.2204, Learning Rate: 0.016000\n",
      "Epoch  841, Training Loss: 1.1405, Validation Loss: 1.2156, Learning Rate: 0.015900\n",
      "Epoch  842, Training Loss: 1.1404, Validation Loss: 1.2144, Learning Rate: 0.015800\n",
      "Epoch  843, Training Loss: 1.1405, Validation Loss: 1.2137, Learning Rate: 0.015700\n",
      "Epoch  844, Training Loss: 1.1403, Validation Loss: 1.2129, Learning Rate: 0.015600\n",
      "Epoch  845, Training Loss: 1.1414, Validation Loss: 1.2104, Learning Rate: 0.015500\n",
      "Epoch  846, Training Loss: 1.1412, Validation Loss: 1.2107, Learning Rate: 0.015400\n",
      "Epoch  847, Training Loss: 1.1402, Validation Loss: 1.2143, Learning Rate: 0.015300\n",
      "Epoch  848, Training Loss: 1.1411, Validation Loss: 1.2132, Learning Rate: 0.015200\n",
      "Epoch  849, Training Loss: 1.1400, Validation Loss: 1.2152, Learning Rate: 0.015100\n",
      "Epoch  850, Training Loss: 1.1401, Validation Loss: 1.2190, Learning Rate: 0.015000\n",
      "Epoch  851, Training Loss: 1.1398, Validation Loss: 1.2185, Learning Rate: 0.014900\n",
      "Epoch  852, Training Loss: 1.1397, Validation Loss: 1.2182, Learning Rate: 0.014800\n",
      "Epoch  853, Training Loss: 1.1397, Validation Loss: 1.2179, Learning Rate: 0.014700\n",
      "Epoch  854, Training Loss: 1.1391, Validation Loss: 1.2150, Learning Rate: 0.014600\n",
      "Epoch  855, Training Loss: 1.1401, Validation Loss: 1.2124, Learning Rate: 0.014500\n",
      "Epoch  856, Training Loss: 1.1403, Validation Loss: 1.2133, Learning Rate: 0.014400\n",
      "Epoch  857, Training Loss: 1.1392, Validation Loss: 1.2132, Learning Rate: 0.014300\n",
      "Epoch  858, Training Loss: 1.1386, Validation Loss: 1.2154, Learning Rate: 0.014200\n",
      "Epoch  859, Training Loss: 1.1386, Validation Loss: 1.2141, Learning Rate: 0.014100\n",
      "Epoch  860, Training Loss: 1.1385, Validation Loss: 1.2153, Learning Rate: 0.014000\n",
      "Epoch  861, Training Loss: 1.1384, Validation Loss: 1.2145, Learning Rate: 0.013900\n",
      "Epoch  862, Training Loss: 1.1386, Validation Loss: 1.2176, Learning Rate: 0.013800\n",
      "Epoch  863, Training Loss: 1.1383, Validation Loss: 1.2161, Learning Rate: 0.013700\n",
      "Epoch  864, Training Loss: 1.1400, Validation Loss: 1.2106, Learning Rate: 0.013600\n",
      "Epoch  865, Training Loss: 1.1406, Validation Loss: 1.2105, Learning Rate: 0.013500\n",
      "Epoch  866, Training Loss: 1.1384, Validation Loss: 1.2122, Learning Rate: 0.013400\n",
      "Epoch  867, Training Loss: 1.1380, Validation Loss: 1.2138, Learning Rate: 0.013300\n",
      "Epoch  868, Training Loss: 1.1379, Validation Loss: 1.2133, Learning Rate: 0.013200\n",
      "Epoch  869, Training Loss: 1.1377, Validation Loss: 1.2132, Learning Rate: 0.013100\n",
      "Epoch  870, Training Loss: 1.1376, Validation Loss: 1.2135, Learning Rate: 0.013000\n",
      "Epoch  871, Training Loss: 1.1376, Validation Loss: 1.2145, Learning Rate: 0.012900\n",
      "Epoch  872, Training Loss: 1.1375, Validation Loss: 1.2131, Learning Rate: 0.012800\n",
      "Epoch  873, Training Loss: 1.1377, Validation Loss: 1.2126, Learning Rate: 0.012700\n",
      "Epoch  874, Training Loss: 1.1375, Validation Loss: 1.2144, Learning Rate: 0.012600\n",
      "Epoch  875, Training Loss: 1.1373, Validation Loss: 1.2129, Learning Rate: 0.012500\n",
      "Epoch  876, Training Loss: 1.1372, Validation Loss: 1.2137, Learning Rate: 0.012400\n",
      "Epoch  877, Training Loss: 1.1371, Validation Loss: 1.2124, Learning Rate: 0.012300\n",
      "Epoch  878, Training Loss: 1.1372, Validation Loss: 1.2141, Learning Rate: 0.012200\n",
      "Epoch  879, Training Loss: 1.1373, Validation Loss: 1.2110, Learning Rate: 0.012100\n",
      "Epoch  880, Training Loss: 1.1376, Validation Loss: 1.2095, Learning Rate: 0.012000\n",
      "Epoch  881, Training Loss: 1.1373, Validation Loss: 1.2110, Learning Rate: 0.011900\n",
      "Epoch  882, Training Loss: 1.1371, Validation Loss: 1.2109, Learning Rate: 0.011800\n",
      "Epoch  883, Training Loss: 1.1372, Validation Loss: 1.2106, Learning Rate: 0.011700\n",
      "Epoch  884, Training Loss: 1.1377, Validation Loss: 1.2092, Learning Rate: 0.011600\n",
      "Epoch  885, Training Loss: 1.1371, Validation Loss: 1.2099, Learning Rate: 0.011500\n",
      "Epoch  886, Training Loss: 1.1364, Validation Loss: 1.2128, Learning Rate: 0.011400\n",
      "Epoch  887, Training Loss: 1.1364, Validation Loss: 1.2123, Learning Rate: 0.011300\n",
      "Epoch  888, Training Loss: 1.1363, Validation Loss: 1.2122, Learning Rate: 0.011200\n",
      "Epoch  889, Training Loss: 1.1364, Validation Loss: 1.2118, Learning Rate: 0.011100\n",
      "Epoch  890, Training Loss: 1.1362, Validation Loss: 1.2154, Learning Rate: 0.011000\n",
      "Epoch  891, Training Loss: 1.1361, Validation Loss: 1.2133, Learning Rate: 0.010900\n",
      "Epoch  892, Training Loss: 1.1361, Validation Loss: 1.2149, Learning Rate: 0.010800\n",
      "Epoch  893, Training Loss: 1.1363, Validation Loss: 1.2166, Learning Rate: 0.010700\n",
      "Epoch  894, Training Loss: 1.1360, Validation Loss: 1.2151, Learning Rate: 0.010600\n",
      "Epoch  895, Training Loss: 1.1359, Validation Loss: 1.2153, Learning Rate: 0.010500\n",
      "Epoch  896, Training Loss: 1.1361, Validation Loss: 1.2171, Learning Rate: 0.010400\n",
      "Epoch  897, Training Loss: 1.1362, Validation Loss: 1.2177, Learning Rate: 0.010300\n",
      "Epoch  898, Training Loss: 1.1357, Validation Loss: 1.2142, Learning Rate: 0.010200\n",
      "Epoch  899, Training Loss: 1.1358, Validation Loss: 1.2122, Learning Rate: 0.010100\n",
      "Epoch  900, Training Loss: 1.1358, Validation Loss: 1.2115, Learning Rate: 0.010000\n",
      "Epoch  901, Training Loss: 1.1359, Validation Loss: 1.2110, Learning Rate: 0.009900\n",
      "Epoch  902, Training Loss: 1.1355, Validation Loss: 1.2117, Learning Rate: 0.009800\n",
      "Epoch  903, Training Loss: 1.1356, Validation Loss: 1.2112, Learning Rate: 0.009700\n",
      "Epoch  904, Training Loss: 1.1369, Validation Loss: 1.2088, Learning Rate: 0.009600\n",
      "Epoch  905, Training Loss: 1.1360, Validation Loss: 1.2095, Learning Rate: 0.009500\n",
      "Epoch  906, Training Loss: 1.1366, Validation Loss: 1.2087, Learning Rate: 0.009400\n",
      "Epoch  907, Training Loss: 1.1362, Validation Loss: 1.2088, Learning Rate: 0.009300\n",
      "Epoch  908, Training Loss: 1.1356, Validation Loss: 1.2099, Learning Rate: 0.009200\n",
      "Epoch  909, Training Loss: 1.1352, Validation Loss: 1.2103, Learning Rate: 0.009100\n",
      "Epoch  910, Training Loss: 1.1353, Validation Loss: 1.2100, Learning Rate: 0.009000\n",
      "Epoch  911, Training Loss: 1.1356, Validation Loss: 1.2087, Learning Rate: 0.008900\n",
      "Epoch  912, Training Loss: 1.1349, Validation Loss: 1.2110, Learning Rate: 0.008800\n",
      "Epoch  913, Training Loss: 1.1352, Validation Loss: 1.2096, Learning Rate: 0.008700\n",
      "Epoch  914, Training Loss: 1.1348, Validation Loss: 1.2116, Learning Rate: 0.008600\n",
      "Epoch  915, Training Loss: 1.1347, Validation Loss: 1.2121, Learning Rate: 0.008500\n",
      "Epoch  916, Training Loss: 1.1349, Validation Loss: 1.2102, Learning Rate: 0.008400\n",
      "Epoch  917, Training Loss: 1.1353, Validation Loss: 1.2091, Learning Rate: 0.008300\n",
      "Epoch  918, Training Loss: 1.1346, Validation Loss: 1.2110, Learning Rate: 0.008200\n",
      "Epoch  919, Training Loss: 1.1346, Validation Loss: 1.2122, Learning Rate: 0.008100\n",
      "Epoch  920, Training Loss: 1.1345, Validation Loss: 1.2112, Learning Rate: 0.008000\n",
      "Epoch  921, Training Loss: 1.1345, Validation Loss: 1.2114, Learning Rate: 0.007900\n",
      "Epoch  922, Training Loss: 1.1347, Validation Loss: 1.2101, Learning Rate: 0.007800\n",
      "Epoch  923, Training Loss: 1.1345, Validation Loss: 1.2119, Learning Rate: 0.007700\n",
      "Epoch  924, Training Loss: 1.1345, Validation Loss: 1.2138, Learning Rate: 0.007600\n",
      "Epoch  925, Training Loss: 1.1347, Validation Loss: 1.2150, Learning Rate: 0.007500\n",
      "Epoch  926, Training Loss: 1.1347, Validation Loss: 1.2151, Learning Rate: 0.007400\n",
      "Epoch  927, Training Loss: 1.1346, Validation Loss: 1.2152, Learning Rate: 0.007300\n",
      "Epoch  928, Training Loss: 1.1347, Validation Loss: 1.2155, Learning Rate: 0.007200\n",
      "Epoch  929, Training Loss: 1.1349, Validation Loss: 1.2167, Learning Rate: 0.007100\n",
      "Epoch  930, Training Loss: 1.1345, Validation Loss: 1.2157, Learning Rate: 0.007000\n",
      "Epoch  931, Training Loss: 1.1341, Validation Loss: 1.2148, Learning Rate: 0.006900\n",
      "Epoch  932, Training Loss: 1.1341, Validation Loss: 1.2144, Learning Rate: 0.006800\n",
      "Epoch  933, Training Loss: 1.1341, Validation Loss: 1.2151, Learning Rate: 0.006700\n",
      "Epoch  934, Training Loss: 1.1340, Validation Loss: 1.2148, Learning Rate: 0.006600\n",
      "Epoch  935, Training Loss: 1.1339, Validation Loss: 1.2144, Learning Rate: 0.006500\n",
      "Epoch  936, Training Loss: 1.1338, Validation Loss: 1.2125, Learning Rate: 0.006400\n",
      "Epoch  937, Training Loss: 1.1339, Validation Loss: 1.2127, Learning Rate: 0.006300\n",
      "Epoch  938, Training Loss: 1.1338, Validation Loss: 1.2119, Learning Rate: 0.006200\n",
      "Epoch  939, Training Loss: 1.1340, Validation Loss: 1.2135, Learning Rate: 0.006100\n",
      "Epoch  940, Training Loss: 1.1337, Validation Loss: 1.2129, Learning Rate: 0.006000\n",
      "Epoch  941, Training Loss: 1.1337, Validation Loss: 1.2132, Learning Rate: 0.005900\n",
      "Epoch  942, Training Loss: 1.1338, Validation Loss: 1.2124, Learning Rate: 0.005800\n",
      "Epoch  943, Training Loss: 1.1337, Validation Loss: 1.2117, Learning Rate: 0.005700\n",
      "Epoch  944, Training Loss: 1.1337, Validation Loss: 1.2128, Learning Rate: 0.005600\n",
      "Epoch  945, Training Loss: 1.1336, Validation Loss: 1.2124, Learning Rate: 0.005500\n",
      "Epoch  946, Training Loss: 1.1335, Validation Loss: 1.2124, Learning Rate: 0.005400\n",
      "Epoch  947, Training Loss: 1.1334, Validation Loss: 1.2110, Learning Rate: 0.005300\n",
      "Epoch  948, Training Loss: 1.1333, Validation Loss: 1.2109, Learning Rate: 0.005200\n",
      "Epoch  949, Training Loss: 1.1332, Validation Loss: 1.2121, Learning Rate: 0.005100\n",
      "Epoch  950, Training Loss: 1.1332, Validation Loss: 1.2116, Learning Rate: 0.005000\n",
      "Epoch  951, Training Loss: 1.1333, Validation Loss: 1.2106, Learning Rate: 0.004900\n",
      "Epoch  952, Training Loss: 1.1333, Validation Loss: 1.2104, Learning Rate: 0.004800\n",
      "Epoch  953, Training Loss: 1.1333, Validation Loss: 1.2103, Learning Rate: 0.004700\n",
      "Epoch  954, Training Loss: 1.1332, Validation Loss: 1.2107, Learning Rate: 0.004600\n",
      "Epoch  955, Training Loss: 1.1333, Validation Loss: 1.2103, Learning Rate: 0.004500\n",
      "Epoch  956, Training Loss: 1.1331, Validation Loss: 1.2107, Learning Rate: 0.004400\n",
      "Epoch  957, Training Loss: 1.1332, Validation Loss: 1.2104, Learning Rate: 0.004300\n",
      "Epoch  958, Training Loss: 1.1331, Validation Loss: 1.2111, Learning Rate: 0.004200\n",
      "Epoch  959, Training Loss: 1.1329, Validation Loss: 1.2120, Learning Rate: 0.004100\n",
      "Epoch  960, Training Loss: 1.1329, Validation Loss: 1.2125, Learning Rate: 0.004000\n",
      "Epoch  961, Training Loss: 1.1329, Validation Loss: 1.2134, Learning Rate: 0.003900\n",
      "Epoch  962, Training Loss: 1.1329, Validation Loss: 1.2135, Learning Rate: 0.003800\n",
      "Epoch  963, Training Loss: 1.1329, Validation Loss: 1.2138, Learning Rate: 0.003700\n",
      "Epoch  964, Training Loss: 1.1329, Validation Loss: 1.2137, Learning Rate: 0.003600\n",
      "Epoch  965, Training Loss: 1.1328, Validation Loss: 1.2131, Learning Rate: 0.003500\n",
      "Epoch  966, Training Loss: 1.1328, Validation Loss: 1.2139, Learning Rate: 0.003400\n",
      "Epoch  967, Training Loss: 1.1328, Validation Loss: 1.2143, Learning Rate: 0.003300\n",
      "Epoch  968, Training Loss: 1.1328, Validation Loss: 1.2142, Learning Rate: 0.003200\n",
      "Epoch  969, Training Loss: 1.1328, Validation Loss: 1.2146, Learning Rate: 0.003100\n",
      "Epoch  970, Training Loss: 1.1327, Validation Loss: 1.2142, Learning Rate: 0.003000\n",
      "Epoch  971, Training Loss: 1.1327, Validation Loss: 1.2139, Learning Rate: 0.002900\n",
      "Epoch  972, Training Loss: 1.1326, Validation Loss: 1.2137, Learning Rate: 0.002800\n",
      "Epoch  973, Training Loss: 1.1326, Validation Loss: 1.2139, Learning Rate: 0.002700\n",
      "Epoch  974, Training Loss: 1.1327, Validation Loss: 1.2143, Learning Rate: 0.002600\n",
      "Epoch  975, Training Loss: 1.1327, Validation Loss: 1.2148, Learning Rate: 0.002500\n",
      "Epoch  976, Training Loss: 1.1327, Validation Loss: 1.2145, Learning Rate: 0.002400\n",
      "Epoch  977, Training Loss: 1.1326, Validation Loss: 1.2142, Learning Rate: 0.002300\n",
      "Epoch  978, Training Loss: 1.1327, Validation Loss: 1.2147, Learning Rate: 0.002200\n",
      "Epoch  979, Training Loss: 1.1326, Validation Loss: 1.2142, Learning Rate: 0.002100\n",
      "Epoch  980, Training Loss: 1.1326, Validation Loss: 1.2143, Learning Rate: 0.002000\n",
      "Epoch  981, Training Loss: 1.1326, Validation Loss: 1.2142, Learning Rate: 0.001900\n",
      "Epoch  982, Training Loss: 1.1326, Validation Loss: 1.2143, Learning Rate: 0.001800\n",
      "Epoch  983, Training Loss: 1.1326, Validation Loss: 1.2144, Learning Rate: 0.001700\n",
      "Epoch  984, Training Loss: 1.1325, Validation Loss: 1.2144, Learning Rate: 0.001600\n",
      "Epoch  985, Training Loss: 1.1326, Validation Loss: 1.2148, Learning Rate: 0.001500\n",
      "Epoch  986, Training Loss: 1.1325, Validation Loss: 1.2147, Learning Rate: 0.001400\n",
      "Epoch  987, Training Loss: 1.1325, Validation Loss: 1.2147, Learning Rate: 0.001300\n",
      "Epoch  988, Training Loss: 1.1325, Validation Loss: 1.2146, Learning Rate: 0.001200\n",
      "Epoch  989, Training Loss: 1.1325, Validation Loss: 1.2145, Learning Rate: 0.001100\n",
      "Epoch  990, Training Loss: 1.1325, Validation Loss: 1.2145, Learning Rate: 0.001000\n",
      "Epoch  991, Training Loss: 1.1325, Validation Loss: 1.2146, Learning Rate: 0.000900\n",
      "Epoch  992, Training Loss: 1.1325, Validation Loss: 1.2145, Learning Rate: 0.000800\n",
      "Epoch  993, Training Loss: 1.1325, Validation Loss: 1.2143, Learning Rate: 0.000700\n",
      "Epoch  994, Training Loss: 1.1324, Validation Loss: 1.2143, Learning Rate: 0.000600\n",
      "Epoch  995, Training Loss: 1.1324, Validation Loss: 1.2143, Learning Rate: 0.000500\n",
      "Epoch  996, Training Loss: 1.1324, Validation Loss: 1.2143, Learning Rate: 0.000400\n",
      "Epoch  997, Training Loss: 1.1324, Validation Loss: 1.2143, Learning Rate: 0.000300\n",
      "Epoch  998, Training Loss: 1.1324, Validation Loss: 1.2143, Learning Rate: 0.000200\n",
      "Epoch  999, Training Loss: 1.1324, Validation Loss: 1.2143, Learning Rate: 0.000100\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0, Training Loss: 11.6558, Learning Rate: 0.100000\n",
      "Epoch    1, Training Loss: 11.0106, Learning Rate: 0.099900\n",
      "Epoch    2, Training Loss: 10.4985, Learning Rate: 0.099800\n",
      "Epoch    3, Training Loss: 9.7307, Learning Rate: 0.099700\n",
      "Epoch    4, Training Loss: 8.6622, Learning Rate: 0.099600\n",
      "Epoch    5, Training Loss: 7.5103, Learning Rate: 0.099500\n",
      "Epoch    6, Training Loss: 5.4409, Learning Rate: 0.099400\n",
      "Epoch    7, Training Loss: 4.0351, Learning Rate: 0.099300\n",
      "Epoch    8, Training Loss: 3.6487, Learning Rate: 0.099200\n",
      "Epoch    9, Training Loss: 3.8901, Learning Rate: 0.099100\n",
      "Epoch   10, Training Loss: 3.5024, Learning Rate: 0.099000\n",
      "Epoch   11, Training Loss: 3.2847, Learning Rate: 0.098900\n",
      "Epoch   12, Training Loss: 3.3327, Learning Rate: 0.098800\n",
      "Epoch   13, Training Loss: 3.1830, Learning Rate: 0.098700\n",
      "Epoch   14, Training Loss: 3.1476, Learning Rate: 0.098600\n",
      "Epoch   15, Training Loss: 3.1169, Learning Rate: 0.098500\n",
      "Epoch   16, Training Loss: 3.3616, Learning Rate: 0.098400\n",
      "Epoch   17, Training Loss: 3.1563, Learning Rate: 0.098300\n",
      "Epoch   18, Training Loss: 4.4292, Learning Rate: 0.098200\n",
      "Epoch   19, Training Loss: 3.0748, Learning Rate: 0.098100\n",
      "Epoch   20, Training Loss: 2.9925, Learning Rate: 0.098000\n",
      "Epoch   21, Training Loss: 3.1652, Learning Rate: 0.097900\n",
      "Epoch   22, Training Loss: 2.8978, Learning Rate: 0.097800\n",
      "Epoch   23, Training Loss: 2.7853, Learning Rate: 0.097700\n",
      "Epoch   24, Training Loss: 3.3565, Learning Rate: 0.097600\n",
      "Epoch   25, Training Loss: 3.0019, Learning Rate: 0.097500\n",
      "Epoch   26, Training Loss: 2.7579, Learning Rate: 0.097400\n",
      "Epoch   27, Training Loss: 3.1690, Learning Rate: 0.097300\n",
      "Epoch   28, Training Loss: 2.7246, Learning Rate: 0.097200\n",
      "Epoch   29, Training Loss: 3.2605, Learning Rate: 0.097100\n",
      "Epoch   30, Training Loss: 2.7530, Learning Rate: 0.097000\n",
      "Epoch   31, Training Loss: 2.5427, Learning Rate: 0.096900\n",
      "Epoch   32, Training Loss: 2.6741, Learning Rate: 0.096800\n",
      "Epoch   33, Training Loss: 2.4190, Learning Rate: 0.096700\n",
      "Epoch   34, Training Loss: 2.4083, Learning Rate: 0.096600\n",
      "Epoch   35, Training Loss: 2.7335, Learning Rate: 0.096500\n",
      "Epoch   36, Training Loss: 2.6823, Learning Rate: 0.096400\n",
      "Epoch   37, Training Loss: 2.3646, Learning Rate: 0.096300\n",
      "Epoch   38, Training Loss: 2.4909, Learning Rate: 0.096200\n",
      "Epoch   39, Training Loss: 2.4099, Learning Rate: 0.096100\n",
      "Epoch   40, Training Loss: 2.1718, Learning Rate: 0.096000\n",
      "Epoch   41, Training Loss: 3.4867, Learning Rate: 0.095900\n",
      "Epoch   42, Training Loss: 2.4927, Learning Rate: 0.095800\n",
      "Epoch   43, Training Loss: 2.1034, Learning Rate: 0.095700\n",
      "Epoch   44, Training Loss: 2.5076, Learning Rate: 0.095600\n",
      "Epoch   45, Training Loss: 2.2154, Learning Rate: 0.095500\n",
      "Epoch   46, Training Loss: 2.3009, Learning Rate: 0.095400\n",
      "Epoch   47, Training Loss: 1.9929, Learning Rate: 0.095300\n",
      "Epoch   48, Training Loss: 2.4067, Learning Rate: 0.095200\n",
      "Epoch   49, Training Loss: 2.1827, Learning Rate: 0.095100\n",
      "Epoch   50, Training Loss: 2.6647, Learning Rate: 0.095000\n",
      "Epoch   51, Training Loss: 2.3828, Learning Rate: 0.094900\n",
      "Epoch   52, Training Loss: 2.1169, Learning Rate: 0.094800\n",
      "Epoch   53, Training Loss: 1.7952, Learning Rate: 0.094700\n",
      "Epoch   54, Training Loss: 1.7526, Learning Rate: 0.094600\n",
      "Epoch   55, Training Loss: 2.0839, Learning Rate: 0.094500\n",
      "Epoch   56, Training Loss: 1.9304, Learning Rate: 0.094400\n",
      "Epoch   57, Training Loss: 1.9275, Learning Rate: 0.094300\n",
      "Epoch   58, Training Loss: 1.9447, Learning Rate: 0.094200\n",
      "Epoch   59, Training Loss: 2.1361, Learning Rate: 0.094100\n",
      "Epoch   60, Training Loss: 1.7659, Learning Rate: 0.094000\n",
      "Epoch   61, Training Loss: 3.1647, Learning Rate: 0.093900\n",
      "Epoch   62, Training Loss: 1.8736, Learning Rate: 0.093800\n",
      "Epoch   63, Training Loss: 2.1101, Learning Rate: 0.093700\n",
      "Epoch   64, Training Loss: 1.3772, Learning Rate: 0.093600\n",
      "Epoch   65, Training Loss: 1.9487, Learning Rate: 0.093500\n",
      "Epoch   66, Training Loss: 2.1945, Learning Rate: 0.093400\n",
      "Epoch   67, Training Loss: 1.7994, Learning Rate: 0.093300\n",
      "Epoch   68, Training Loss: 2.4813, Learning Rate: 0.093200\n",
      "Epoch   69, Training Loss: 1.5426, Learning Rate: 0.093100\n",
      "Epoch   70, Training Loss: 2.2850, Learning Rate: 0.093000\n",
      "Epoch   71, Training Loss: 2.6370, Learning Rate: 0.092900\n",
      "Epoch   72, Training Loss: 1.5842, Learning Rate: 0.092800\n",
      "Epoch   73, Training Loss: 2.3778, Learning Rate: 0.092700\n",
      "Epoch   74, Training Loss: 2.2441, Learning Rate: 0.092600\n",
      "Epoch   75, Training Loss: 1.6862, Learning Rate: 0.092500\n",
      "Epoch   76, Training Loss: 1.9338, Learning Rate: 0.092400\n",
      "Epoch   77, Training Loss: 1.4492, Learning Rate: 0.092300\n",
      "Epoch   78, Training Loss: 1.5003, Learning Rate: 0.092200\n",
      "Epoch   79, Training Loss: 1.6701, Learning Rate: 0.092100\n",
      "Epoch   80, Training Loss: 2.1702, Learning Rate: 0.092000\n",
      "Epoch   81, Training Loss: 1.4590, Learning Rate: 0.091900\n",
      "Epoch   82, Training Loss: 1.8498, Learning Rate: 0.091800\n",
      "Epoch   83, Training Loss: 1.8950, Learning Rate: 0.091700\n",
      "Epoch   84, Training Loss: 1.4155, Learning Rate: 0.091600\n",
      "Epoch   85, Training Loss: 1.3371, Learning Rate: 0.091500\n",
      "Epoch   86, Training Loss: 1.9605, Learning Rate: 0.091400\n",
      "Epoch   87, Training Loss: 2.3874, Learning Rate: 0.091300\n",
      "Epoch   88, Training Loss: 1.3563, Learning Rate: 0.091200\n",
      "Epoch   89, Training Loss: 1.7757, Learning Rate: 0.091100\n",
      "Epoch   90, Training Loss: 1.2799, Learning Rate: 0.091000\n",
      "Epoch   91, Training Loss: 1.4850, Learning Rate: 0.090900\n",
      "Epoch   92, Training Loss: 1.4530, Learning Rate: 0.090800\n",
      "Epoch   93, Training Loss: 1.2833, Learning Rate: 0.090700\n",
      "Epoch   94, Training Loss: 1.5795, Learning Rate: 0.090600\n",
      "Epoch   95, Training Loss: 1.8099, Learning Rate: 0.090500\n",
      "Epoch   96, Training Loss: 2.0566, Learning Rate: 0.090400\n",
      "Epoch   97, Training Loss: 2.0501, Learning Rate: 0.090300\n",
      "Epoch   98, Training Loss: 1.4002, Learning Rate: 0.090200\n",
      "Epoch   99, Training Loss: 3.2040, Learning Rate: 0.090100\n",
      "Epoch  100, Training Loss: 2.0309, Learning Rate: 0.090000\n",
      "Epoch  101, Training Loss: 1.9519, Learning Rate: 0.089900\n",
      "Epoch  102, Training Loss: 1.2314, Learning Rate: 0.089800\n",
      "Epoch  103, Training Loss: 1.7366, Learning Rate: 0.089700\n",
      "Epoch  104, Training Loss: 2.0396, Learning Rate: 0.089600\n",
      "Epoch  105, Training Loss: 2.1022, Learning Rate: 0.089500\n",
      "Epoch  106, Training Loss: 1.3146, Learning Rate: 0.089400\n",
      "Epoch  107, Training Loss: 1.3571, Learning Rate: 0.089300\n",
      "Epoch  108, Training Loss: 1.9036, Learning Rate: 0.089200\n",
      "Epoch  109, Training Loss: 1.6863, Learning Rate: 0.089100\n",
      "Epoch  110, Training Loss: 1.2714, Learning Rate: 0.089000\n",
      "Epoch  111, Training Loss: 1.5018, Learning Rate: 0.088900\n",
      "Epoch  112, Training Loss: 1.9213, Learning Rate: 0.088800\n",
      "Epoch  113, Training Loss: 2.8591, Learning Rate: 0.088700\n",
      "Epoch  114, Training Loss: 1.7230, Learning Rate: 0.088600\n",
      "Epoch  115, Training Loss: 1.6526, Learning Rate: 0.088500\n",
      "Epoch  116, Training Loss: 1.5434, Learning Rate: 0.088400\n",
      "Epoch  117, Training Loss: 1.2244, Learning Rate: 0.088300\n",
      "Epoch  118, Training Loss: 1.7323, Learning Rate: 0.088200\n",
      "Epoch  119, Training Loss: 1.6099, Learning Rate: 0.088100\n",
      "Epoch  120, Training Loss: 1.9911, Learning Rate: 0.088000\n",
      "Epoch  121, Training Loss: 1.7225, Learning Rate: 0.087900\n",
      "Epoch  122, Training Loss: 1.8134, Learning Rate: 0.087800\n",
      "Epoch  123, Training Loss: 1.3134, Learning Rate: 0.087700\n",
      "Epoch  124, Training Loss: 2.9783, Learning Rate: 0.087600\n",
      "Epoch  125, Training Loss: 1.3454, Learning Rate: 0.087500\n",
      "Epoch  126, Training Loss: 1.1611, Learning Rate: 0.087400\n",
      "Epoch  127, Training Loss: 1.7203, Learning Rate: 0.087300\n",
      "Epoch  128, Training Loss: 1.4790, Learning Rate: 0.087200\n",
      "Epoch  129, Training Loss: 1.5676, Learning Rate: 0.087100\n",
      "Epoch  130, Training Loss: 1.8090, Learning Rate: 0.087000\n",
      "Epoch  131, Training Loss: 2.4999, Learning Rate: 0.086900\n",
      "Epoch  132, Training Loss: 2.1637, Learning Rate: 0.086800\n",
      "Epoch  133, Training Loss: 2.1746, Learning Rate: 0.086700\n",
      "Epoch  134, Training Loss: 1.9853, Learning Rate: 0.086600\n",
      "Epoch  135, Training Loss: 1.8025, Learning Rate: 0.086500\n",
      "Epoch  136, Training Loss: 2.0014, Learning Rate: 0.086400\n",
      "Epoch  137, Training Loss: 1.6349, Learning Rate: 0.086300\n",
      "Epoch  138, Training Loss: 1.4322, Learning Rate: 0.086200\n",
      "Epoch  139, Training Loss: 1.7142, Learning Rate: 0.086100\n",
      "Epoch  140, Training Loss: 2.1164, Learning Rate: 0.086000\n",
      "Epoch  141, Training Loss: 1.4001, Learning Rate: 0.085900\n",
      "Epoch  142, Training Loss: 1.7865, Learning Rate: 0.085800\n",
      "Epoch  143, Training Loss: 1.7054, Learning Rate: 0.085700\n",
      "Epoch  144, Training Loss: 1.7805, Learning Rate: 0.085600\n",
      "Epoch  145, Training Loss: 2.5775, Learning Rate: 0.085500\n",
      "Epoch  146, Training Loss: 1.6849, Learning Rate: 0.085400\n",
      "Epoch  147, Training Loss: 1.3091, Learning Rate: 0.085300\n",
      "Epoch  148, Training Loss: 3.9323, Learning Rate: 0.085200\n",
      "Epoch  149, Training Loss: 2.1517, Learning Rate: 0.085100\n",
      "Epoch  150, Training Loss: 1.8986, Learning Rate: 0.085000\n",
      "Epoch  151, Training Loss: 1.6099, Learning Rate: 0.084900\n",
      "Epoch  152, Training Loss: 1.6650, Learning Rate: 0.084800\n",
      "Epoch  153, Training Loss: 1.5290, Learning Rate: 0.084700\n",
      "Epoch  154, Training Loss: 1.5851, Learning Rate: 0.084600\n",
      "Epoch  155, Training Loss: 1.6024, Learning Rate: 0.084500\n",
      "Epoch  156, Training Loss: 1.2968, Learning Rate: 0.084400\n",
      "Epoch  157, Training Loss: 1.6567, Learning Rate: 0.084300\n",
      "Epoch  158, Training Loss: 1.2706, Learning Rate: 0.084200\n",
      "Epoch  159, Training Loss: 1.7913, Learning Rate: 0.084100\n",
      "Epoch  160, Training Loss: 2.3291, Learning Rate: 0.084000\n",
      "Epoch  161, Training Loss: 2.5204, Learning Rate: 0.083900\n",
      "Epoch  162, Training Loss: 1.2037, Learning Rate: 0.083800\n",
      "Epoch  163, Training Loss: 1.4266, Learning Rate: 0.083700\n",
      "Epoch  164, Training Loss: 1.3703, Learning Rate: 0.083600\n",
      "Epoch  165, Training Loss: 1.7387, Learning Rate: 0.083500\n",
      "Epoch  166, Training Loss: 1.4941, Learning Rate: 0.083400\n",
      "Epoch  167, Training Loss: 1.8187, Learning Rate: 0.083300\n",
      "Epoch  168, Training Loss: 2.4245, Learning Rate: 0.083200\n",
      "Epoch  169, Training Loss: 2.1147, Learning Rate: 0.083100\n",
      "Epoch  170, Training Loss: 2.5093, Learning Rate: 0.083000\n",
      "Epoch  171, Training Loss: 1.2655, Learning Rate: 0.082900\n",
      "Epoch  172, Training Loss: 1.2928, Learning Rate: 0.082800\n",
      "Epoch  173, Training Loss: 1.3756, Learning Rate: 0.082700\n",
      "Epoch  174, Training Loss: 1.6796, Learning Rate: 0.082600\n",
      "Epoch  175, Training Loss: 1.7665, Learning Rate: 0.082500\n",
      "Epoch  176, Training Loss: 1.5940, Learning Rate: 0.082400\n",
      "Epoch  177, Training Loss: 1.3487, Learning Rate: 0.082300\n",
      "Epoch  178, Training Loss: 1.8820, Learning Rate: 0.082200\n",
      "Epoch  179, Training Loss: 1.1667, Learning Rate: 0.082100\n",
      "Epoch  180, Training Loss: 1.5691, Learning Rate: 0.082000\n",
      "Epoch  181, Training Loss: 2.1038, Learning Rate: 0.081900\n",
      "Epoch  182, Training Loss: 1.5848, Learning Rate: 0.081800\n",
      "Epoch  183, Training Loss: 3.1931, Learning Rate: 0.081700\n",
      "Epoch  184, Training Loss: 2.2938, Learning Rate: 0.081600\n",
      "Epoch  185, Training Loss: 1.4753, Learning Rate: 0.081500\n",
      "Epoch  186, Training Loss: 1.3005, Learning Rate: 0.081400\n",
      "Epoch  187, Training Loss: 2.4939, Learning Rate: 0.081300\n",
      "Epoch  188, Training Loss: 1.4133, Learning Rate: 0.081200\n",
      "Epoch  189, Training Loss: 1.5621, Learning Rate: 0.081100\n",
      "Epoch  190, Training Loss: 1.9609, Learning Rate: 0.081000\n",
      "Epoch  191, Training Loss: 2.1048, Learning Rate: 0.080900\n",
      "Epoch  192, Training Loss: 1.5613, Learning Rate: 0.080800\n",
      "Epoch  193, Training Loss: 2.8938, Learning Rate: 0.080700\n",
      "Epoch  194, Training Loss: 1.7133, Learning Rate: 0.080600\n",
      "Epoch  195, Training Loss: 1.8162, Learning Rate: 0.080500\n",
      "Epoch  196, Training Loss: 2.8372, Learning Rate: 0.080400\n",
      "Epoch  197, Training Loss: 1.2998, Learning Rate: 0.080300\n",
      "Epoch  198, Training Loss: 1.8206, Learning Rate: 0.080200\n",
      "Epoch  199, Training Loss: 2.6503, Learning Rate: 0.080100\n",
      "Epoch  200, Training Loss: 1.3123, Learning Rate: 0.080000\n",
      "Epoch  201, Training Loss: 1.0577, Learning Rate: 0.079900\n",
      "Epoch  202, Training Loss: 1.1717, Learning Rate: 0.079800\n",
      "Epoch  203, Training Loss: 2.4953, Learning Rate: 0.079700\n",
      "Epoch  204, Training Loss: 1.1559, Learning Rate: 0.079600\n",
      "Epoch  205, Training Loss: 1.0940, Learning Rate: 0.079500\n",
      "Epoch  206, Training Loss: 2.4741, Learning Rate: 0.079400\n",
      "Epoch  207, Training Loss: 1.2243, Learning Rate: 0.079300\n",
      "Epoch  208, Training Loss: 1.5289, Learning Rate: 0.079200\n",
      "Epoch  209, Training Loss: 1.2447, Learning Rate: 0.079100\n",
      "Epoch  210, Training Loss: 1.3594, Learning Rate: 0.079000\n",
      "Epoch  211, Training Loss: 1.1224, Learning Rate: 0.078900\n",
      "Epoch  212, Training Loss: 1.7423, Learning Rate: 0.078800\n",
      "Epoch  213, Training Loss: 2.9229, Learning Rate: 0.078700\n",
      "Epoch  214, Training Loss: 1.2392, Learning Rate: 0.078600\n",
      "Epoch  215, Training Loss: 2.3680, Learning Rate: 0.078500\n",
      "Epoch  216, Training Loss: 1.4194, Learning Rate: 0.078400\n",
      "Epoch  217, Training Loss: 1.6271, Learning Rate: 0.078300\n",
      "Epoch  218, Training Loss: 1.7466, Learning Rate: 0.078200\n",
      "Epoch  219, Training Loss: 1.1395, Learning Rate: 0.078100\n",
      "Epoch  220, Training Loss: 1.3674, Learning Rate: 0.078000\n",
      "Epoch  221, Training Loss: 1.4947, Learning Rate: 0.077900\n",
      "Epoch  222, Training Loss: 1.5327, Learning Rate: 0.077800\n",
      "Epoch  223, Training Loss: 1.0857, Learning Rate: 0.077700\n",
      "Epoch  224, Training Loss: 2.8127, Learning Rate: 0.077600\n",
      "Epoch  225, Training Loss: 1.2321, Learning Rate: 0.077500\n",
      "Epoch  226, Training Loss: 1.3416, Learning Rate: 0.077400\n",
      "Epoch  227, Training Loss: 1.6445, Learning Rate: 0.077300\n",
      "Epoch  228, Training Loss: 1.0796, Learning Rate: 0.077200\n",
      "Epoch  229, Training Loss: 1.4501, Learning Rate: 0.077100\n",
      "Epoch  230, Training Loss: 1.1936, Learning Rate: 0.077000\n",
      "Epoch  231, Training Loss: 1.7710, Learning Rate: 0.076900\n",
      "Epoch  232, Training Loss: 1.5133, Learning Rate: 0.076800\n",
      "Epoch  233, Training Loss: 1.6970, Learning Rate: 0.076700\n",
      "Epoch  234, Training Loss: 1.3675, Learning Rate: 0.076600\n",
      "Epoch  235, Training Loss: 2.1738, Learning Rate: 0.076500\n",
      "Epoch  236, Training Loss: 2.4122, Learning Rate: 0.076400\n",
      "Epoch  237, Training Loss: 1.0726, Learning Rate: 0.076300\n",
      "Epoch  238, Training Loss: 2.2036, Learning Rate: 0.076200\n",
      "Epoch  239, Training Loss: 1.3715, Learning Rate: 0.076100\n",
      "Epoch  240, Training Loss: 1.3001, Learning Rate: 0.076000\n",
      "Epoch  241, Training Loss: 1.3636, Learning Rate: 0.075900\n",
      "Epoch  242, Training Loss: 1.3297, Learning Rate: 0.075800\n",
      "Epoch  243, Training Loss: 1.0990, Learning Rate: 0.075700\n",
      "Epoch  244, Training Loss: 1.3558, Learning Rate: 0.075600\n",
      "Epoch  245, Training Loss: 2.0669, Learning Rate: 0.075500\n",
      "Epoch  246, Training Loss: 1.4231, Learning Rate: 0.075400\n",
      "Epoch  247, Training Loss: 2.2213, Learning Rate: 0.075300\n",
      "Epoch  248, Training Loss: 1.2321, Learning Rate: 0.075200\n",
      "Epoch  249, Training Loss: 1.5653, Learning Rate: 0.075100\n",
      "Epoch  250, Training Loss: 1.3439, Learning Rate: 0.075000\n",
      "Epoch  251, Training Loss: 1.2671, Learning Rate: 0.074900\n",
      "Epoch  252, Training Loss: 1.5903, Learning Rate: 0.074800\n",
      "Epoch  253, Training Loss: 1.2180, Learning Rate: 0.074700\n",
      "Epoch  254, Training Loss: 1.2395, Learning Rate: 0.074600\n",
      "Epoch  255, Training Loss: 1.8814, Learning Rate: 0.074500\n",
      "Epoch  256, Training Loss: 1.1431, Learning Rate: 0.074400\n",
      "Epoch  257, Training Loss: 1.4576, Learning Rate: 0.074300\n",
      "Epoch  258, Training Loss: 1.2661, Learning Rate: 0.074200\n",
      "Epoch  259, Training Loss: 1.7044, Learning Rate: 0.074100\n",
      "Epoch  260, Training Loss: 2.8028, Learning Rate: 0.074000\n",
      "Epoch  261, Training Loss: 1.2488, Learning Rate: 0.073900\n",
      "Epoch  262, Training Loss: 1.8169, Learning Rate: 0.073800\n",
      "Epoch  263, Training Loss: 1.5833, Learning Rate: 0.073700\n",
      "Epoch  264, Training Loss: 2.9210, Learning Rate: 0.073600\n",
      "Epoch  265, Training Loss: 1.4333, Learning Rate: 0.073500\n",
      "Epoch  266, Training Loss: 1.3013, Learning Rate: 0.073400\n",
      "Epoch  267, Training Loss: 1.6587, Learning Rate: 0.073300\n",
      "Epoch  268, Training Loss: 1.7887, Learning Rate: 0.073200\n",
      "Epoch  269, Training Loss: 1.3128, Learning Rate: 0.073100\n",
      "Epoch  270, Training Loss: 1.1183, Learning Rate: 0.073000\n",
      "Epoch  271, Training Loss: 1.6870, Learning Rate: 0.072900\n",
      "Epoch  272, Training Loss: 1.6743, Learning Rate: 0.072800\n",
      "Epoch  273, Training Loss: 2.1555, Learning Rate: 0.072700\n",
      "Epoch  274, Training Loss: 2.1807, Learning Rate: 0.072600\n",
      "Epoch  275, Training Loss: 2.9096, Learning Rate: 0.072500\n",
      "Epoch  276, Training Loss: 1.8564, Learning Rate: 0.072400\n",
      "Epoch  277, Training Loss: 1.3907, Learning Rate: 0.072300\n",
      "Epoch  278, Training Loss: 1.4793, Learning Rate: 0.072200\n",
      "Epoch  279, Training Loss: 1.2864, Learning Rate: 0.072100\n",
      "Epoch  280, Training Loss: 1.4180, Learning Rate: 0.072000\n",
      "Epoch  281, Training Loss: 1.6041, Learning Rate: 0.071900\n",
      "Epoch  282, Training Loss: 1.1755, Learning Rate: 0.071800\n",
      "Epoch  283, Training Loss: 1.5853, Learning Rate: 0.071700\n",
      "Epoch  284, Training Loss: 2.3039, Learning Rate: 0.071600\n",
      "Epoch  285, Training Loss: 1.9506, Learning Rate: 0.071500\n",
      "Epoch  286, Training Loss: 1.0674, Learning Rate: 0.071400\n",
      "Epoch  287, Training Loss: 2.1791, Learning Rate: 0.071300\n",
      "Epoch  288, Training Loss: 1.3285, Learning Rate: 0.071200\n",
      "Epoch  289, Training Loss: 1.1355, Learning Rate: 0.071100\n",
      "Epoch  290, Training Loss: 1.3963, Learning Rate: 0.071000\n",
      "Epoch  291, Training Loss: 1.0718, Learning Rate: 0.070900\n",
      "Epoch  292, Training Loss: 1.4027, Learning Rate: 0.070800\n",
      "Epoch  293, Training Loss: 1.4833, Learning Rate: 0.070700\n",
      "Epoch  294, Training Loss: 1.3743, Learning Rate: 0.070600\n",
      "Epoch  295, Training Loss: 1.0899, Learning Rate: 0.070500\n",
      "Epoch  296, Training Loss: 2.3473, Learning Rate: 0.070400\n",
      "Epoch  297, Training Loss: 1.1407, Learning Rate: 0.070300\n",
      "Epoch  298, Training Loss: 1.8226, Learning Rate: 0.070200\n",
      "Epoch  299, Training Loss: 1.1659, Learning Rate: 0.070100\n",
      "Epoch  300, Training Loss: 1.1360, Learning Rate: 0.070000\n",
      "Epoch  301, Training Loss: 1.7408, Learning Rate: 0.069900\n",
      "Epoch  302, Training Loss: 1.1662, Learning Rate: 0.069800\n",
      "Epoch  303, Training Loss: 1.0513, Learning Rate: 0.069700\n",
      "Epoch  304, Training Loss: 1.4570, Learning Rate: 0.069600\n",
      "Epoch  305, Training Loss: 1.0879, Learning Rate: 0.069500\n",
      "Epoch  306, Training Loss: 1.2921, Learning Rate: 0.069400\n",
      "Epoch  307, Training Loss: 2.0192, Learning Rate: 0.069300\n",
      "Epoch  308, Training Loss: 1.5976, Learning Rate: 0.069200\n",
      "Epoch  309, Training Loss: 1.2928, Learning Rate: 0.069100\n",
      "Epoch  310, Training Loss: 2.3804, Learning Rate: 0.069000\n",
      "Epoch  311, Training Loss: 1.7852, Learning Rate: 0.068900\n",
      "Epoch  312, Training Loss: 1.4467, Learning Rate: 0.068800\n",
      "Epoch  313, Training Loss: 2.7151, Learning Rate: 0.068700\n",
      "Epoch  314, Training Loss: 1.5076, Learning Rate: 0.068600\n",
      "Epoch  315, Training Loss: 1.0528, Learning Rate: 0.068500\n",
      "Epoch  316, Training Loss: 2.3421, Learning Rate: 0.068400\n",
      "Epoch  317, Training Loss: 1.2246, Learning Rate: 0.068300\n",
      "Epoch  318, Training Loss: 2.4504, Learning Rate: 0.068200\n",
      "Epoch  319, Training Loss: 2.0720, Learning Rate: 0.068100\n",
      "Epoch  320, Training Loss: 2.0300, Learning Rate: 0.068000\n",
      "Epoch  321, Training Loss: 1.2821, Learning Rate: 0.067900\n",
      "Epoch  322, Training Loss: 1.6598, Learning Rate: 0.067800\n",
      "Epoch  323, Training Loss: 1.6566, Learning Rate: 0.067700\n",
      "Epoch  324, Training Loss: 1.9279, Learning Rate: 0.067600\n",
      "Epoch  325, Training Loss: 1.1532, Learning Rate: 0.067500\n",
      "Epoch  326, Training Loss: 1.5968, Learning Rate: 0.067400\n",
      "Epoch  327, Training Loss: 1.3738, Learning Rate: 0.067300\n",
      "Epoch  328, Training Loss: 2.4197, Learning Rate: 0.067200\n",
      "Epoch  329, Training Loss: 1.6264, Learning Rate: 0.067100\n",
      "Epoch  330, Training Loss: 1.2007, Learning Rate: 0.067000\n",
      "Epoch  331, Training Loss: 1.1699, Learning Rate: 0.066900\n",
      "Epoch  332, Training Loss: 1.6275, Learning Rate: 0.066800\n",
      "Epoch  333, Training Loss: 2.3185, Learning Rate: 0.066700\n",
      "Epoch  334, Training Loss: 1.3768, Learning Rate: 0.066600\n",
      "Epoch  335, Training Loss: 1.6077, Learning Rate: 0.066500\n",
      "Epoch  336, Training Loss: 1.4965, Learning Rate: 0.066400\n",
      "Epoch  337, Training Loss: 2.1337, Learning Rate: 0.066300\n",
      "Epoch  338, Training Loss: 1.3082, Learning Rate: 0.066200\n",
      "Epoch  339, Training Loss: 2.0121, Learning Rate: 0.066100\n",
      "Epoch  340, Training Loss: 1.7078, Learning Rate: 0.066000\n",
      "Epoch  341, Training Loss: 1.2610, Learning Rate: 0.065900\n",
      "Epoch  342, Training Loss: 2.2441, Learning Rate: 0.065800\n",
      "Epoch  343, Training Loss: 1.2875, Learning Rate: 0.065700\n",
      "Epoch  344, Training Loss: 1.3512, Learning Rate: 0.065600\n",
      "Epoch  345, Training Loss: 1.5306, Learning Rate: 0.065500\n",
      "Epoch  346, Training Loss: 1.7753, Learning Rate: 0.065400\n",
      "Epoch  347, Training Loss: 2.5761, Learning Rate: 0.065300\n",
      "Epoch  348, Training Loss: 2.4483, Learning Rate: 0.065200\n",
      "Epoch  349, Training Loss: 1.3382, Learning Rate: 0.065100\n",
      "Epoch  350, Training Loss: 2.5121, Learning Rate: 0.065000\n",
      "Epoch  351, Training Loss: 1.2339, Learning Rate: 0.064900\n",
      "Epoch  352, Training Loss: 1.0489, Learning Rate: 0.064800\n",
      "Epoch  353, Training Loss: 1.1206, Learning Rate: 0.064700\n",
      "Epoch  354, Training Loss: 1.1171, Learning Rate: 0.064600\n",
      "Epoch  355, Training Loss: 1.0239, Learning Rate: 0.064500\n",
      "Epoch  356, Training Loss: 1.7168, Learning Rate: 0.064400\n",
      "Epoch  357, Training Loss: 1.1790, Learning Rate: 0.064300\n",
      "Epoch  358, Training Loss: 1.2641, Learning Rate: 0.064200\n",
      "Epoch  359, Training Loss: 2.0959, Learning Rate: 0.064100\n",
      "Epoch  360, Training Loss: 1.3453, Learning Rate: 0.064000\n",
      "Epoch  361, Training Loss: 2.5832, Learning Rate: 0.063900\n",
      "Epoch  362, Training Loss: 1.1330, Learning Rate: 0.063800\n",
      "Epoch  363, Training Loss: 1.1499, Learning Rate: 0.063700\n",
      "Epoch  364, Training Loss: 1.3091, Learning Rate: 0.063600\n",
      "Epoch  365, Training Loss: 1.0237, Learning Rate: 0.063500\n",
      "Epoch  366, Training Loss: 1.5277, Learning Rate: 0.063400\n",
      "Epoch  367, Training Loss: 1.1435, Learning Rate: 0.063300\n",
      "Epoch  368, Training Loss: 2.4627, Learning Rate: 0.063200\n",
      "Epoch  369, Training Loss: 1.2357, Learning Rate: 0.063100\n",
      "Epoch  370, Training Loss: 1.4447, Learning Rate: 0.063000\n",
      "Epoch  371, Training Loss: 1.4992, Learning Rate: 0.062900\n",
      "Epoch  372, Training Loss: 1.5786, Learning Rate: 0.062800\n",
      "Epoch  373, Training Loss: 1.0096, Learning Rate: 0.062700\n",
      "Epoch  374, Training Loss: 1.5291, Learning Rate: 0.062600\n",
      "Epoch  375, Training Loss: 1.5211, Learning Rate: 0.062500\n",
      "Epoch  376, Training Loss: 1.4568, Learning Rate: 0.062400\n",
      "Epoch  377, Training Loss: 1.7378, Learning Rate: 0.062300\n",
      "Epoch  378, Training Loss: 1.7647, Learning Rate: 0.062200\n",
      "Epoch  379, Training Loss: 2.0482, Learning Rate: 0.062100\n",
      "Epoch  380, Training Loss: 1.2162, Learning Rate: 0.062000\n",
      "Epoch  381, Training Loss: 1.1266, Learning Rate: 0.061900\n",
      "Epoch  382, Training Loss: 1.2189, Learning Rate: 0.061800\n",
      "Epoch  383, Training Loss: 1.0462, Learning Rate: 0.061700\n",
      "Epoch  384, Training Loss: 1.6562, Learning Rate: 0.061600\n",
      "Epoch  385, Training Loss: 1.2822, Learning Rate: 0.061500\n",
      "Epoch  386, Training Loss: 1.7115, Learning Rate: 0.061400\n",
      "Epoch  387, Training Loss: 1.5598, Learning Rate: 0.061300\n",
      "Epoch  388, Training Loss: 2.3784, Learning Rate: 0.061200\n",
      "Epoch  389, Training Loss: 1.1846, Learning Rate: 0.061100\n",
      "Epoch  390, Training Loss: 1.5235, Learning Rate: 0.061000\n",
      "Epoch  391, Training Loss: 1.5899, Learning Rate: 0.060900\n",
      "Epoch  392, Training Loss: 1.5430, Learning Rate: 0.060800\n",
      "Epoch  393, Training Loss: 1.3705, Learning Rate: 0.060700\n",
      "Epoch  394, Training Loss: 1.6177, Learning Rate: 0.060600\n",
      "Epoch  395, Training Loss: 1.1140, Learning Rate: 0.060500\n",
      "Epoch  396, Training Loss: 2.2441, Learning Rate: 0.060400\n",
      "Epoch  397, Training Loss: 1.0219, Learning Rate: 0.060300\n",
      "Epoch  398, Training Loss: 1.9586, Learning Rate: 0.060200\n",
      "Epoch  399, Training Loss: 2.6182, Learning Rate: 0.060100\n",
      "Epoch  400, Training Loss: 1.9204, Learning Rate: 0.060000\n",
      "Epoch  401, Training Loss: 1.4858, Learning Rate: 0.059900\n",
      "Epoch  402, Training Loss: 1.3715, Learning Rate: 0.059800\n",
      "Epoch  403, Training Loss: 1.3220, Learning Rate: 0.059700\n",
      "Epoch  404, Training Loss: 1.2151, Learning Rate: 0.059600\n",
      "Epoch  405, Training Loss: 1.3364, Learning Rate: 0.059500\n",
      "Epoch  406, Training Loss: 1.0468, Learning Rate: 0.059400\n",
      "Epoch  407, Training Loss: 1.1587, Learning Rate: 0.059300\n",
      "Epoch  408, Training Loss: 1.1032, Learning Rate: 0.059200\n",
      "Epoch  409, Training Loss: 1.4038, Learning Rate: 0.059100\n",
      "Epoch  410, Training Loss: 1.6329, Learning Rate: 0.059000\n",
      "Epoch  411, Training Loss: 1.2849, Learning Rate: 0.058900\n",
      "Epoch  412, Training Loss: 1.0649, Learning Rate: 0.058800\n",
      "Epoch  413, Training Loss: 1.6545, Learning Rate: 0.058700\n",
      "Epoch  414, Training Loss: 1.0805, Learning Rate: 0.058600\n",
      "Epoch  415, Training Loss: 1.8721, Learning Rate: 0.058500\n",
      "Epoch  416, Training Loss: 1.3310, Learning Rate: 0.058400\n",
      "Epoch  417, Training Loss: 1.3368, Learning Rate: 0.058300\n",
      "Epoch  418, Training Loss: 1.0628, Learning Rate: 0.058200\n",
      "Epoch  419, Training Loss: 1.2818, Learning Rate: 0.058100\n",
      "Epoch  420, Training Loss: 1.3532, Learning Rate: 0.058000\n",
      "Epoch  421, Training Loss: 1.2511, Learning Rate: 0.057900\n",
      "Epoch  422, Training Loss: 2.1625, Learning Rate: 0.057800\n",
      "Epoch  423, Training Loss: 1.3253, Learning Rate: 0.057700\n",
      "Epoch  424, Training Loss: 1.2823, Learning Rate: 0.057600\n",
      "Epoch  425, Training Loss: 1.2145, Learning Rate: 0.057500\n",
      "Epoch  426, Training Loss: 1.8009, Learning Rate: 0.057400\n",
      "Epoch  427, Training Loss: 1.0410, Learning Rate: 0.057300\n",
      "Epoch  428, Training Loss: 1.2179, Learning Rate: 0.057200\n",
      "Epoch  429, Training Loss: 2.5563, Learning Rate: 0.057100\n",
      "Epoch  430, Training Loss: 1.1917, Learning Rate: 0.057000\n",
      "Epoch  431, Training Loss: 1.8418, Learning Rate: 0.056900\n",
      "Epoch  432, Training Loss: 1.3616, Learning Rate: 0.056800\n",
      "Epoch  433, Training Loss: 1.2305, Learning Rate: 0.056700\n",
      "Epoch  434, Training Loss: 1.1349, Learning Rate: 0.056600\n",
      "Epoch  435, Training Loss: 1.7105, Learning Rate: 0.056500\n",
      "Epoch  436, Training Loss: 1.7398, Learning Rate: 0.056400\n",
      "Epoch  437, Training Loss: 1.4891, Learning Rate: 0.056300\n",
      "Epoch  438, Training Loss: 1.1647, Learning Rate: 0.056200\n",
      "Epoch  439, Training Loss: 1.1302, Learning Rate: 0.056100\n",
      "Epoch  440, Training Loss: 1.6270, Learning Rate: 0.056000\n",
      "Epoch  441, Training Loss: 1.1682, Learning Rate: 0.055900\n",
      "Epoch  442, Training Loss: 1.1244, Learning Rate: 0.055800\n",
      "Epoch  443, Training Loss: 1.1708, Learning Rate: 0.055700\n",
      "Epoch  444, Training Loss: 1.4702, Learning Rate: 0.055600\n",
      "Epoch  445, Training Loss: 1.0193, Learning Rate: 0.055500\n",
      "Epoch  446, Training Loss: 1.0110, Learning Rate: 0.055400\n",
      "Epoch  447, Training Loss: 1.3409, Learning Rate: 0.055300\n",
      "Epoch  448, Training Loss: 1.6365, Learning Rate: 0.055200\n",
      "Epoch  449, Training Loss: 1.0320, Learning Rate: 0.055100\n",
      "Epoch  450, Training Loss: 1.2032, Learning Rate: 0.055000\n",
      "Epoch  451, Training Loss: 1.0905, Learning Rate: 0.054900\n",
      "Epoch  452, Training Loss: 1.2576, Learning Rate: 0.054800\n",
      "Epoch  453, Training Loss: 1.0775, Learning Rate: 0.054700\n",
      "Epoch  454, Training Loss: 1.1802, Learning Rate: 0.054600\n",
      "Epoch  455, Training Loss: 1.4438, Learning Rate: 0.054500\n",
      "Epoch  456, Training Loss: 1.7744, Learning Rate: 0.054400\n",
      "Epoch  457, Training Loss: 1.6296, Learning Rate: 0.054300\n",
      "Epoch  458, Training Loss: 1.2147, Learning Rate: 0.054200\n",
      "Epoch  459, Training Loss: 1.1569, Learning Rate: 0.054100\n",
      "Epoch  460, Training Loss: 1.2167, Learning Rate: 0.054000\n",
      "Epoch  461, Training Loss: 1.5940, Learning Rate: 0.053900\n",
      "Epoch  462, Training Loss: 1.0847, Learning Rate: 0.053800\n",
      "Epoch  463, Training Loss: 1.7400, Learning Rate: 0.053700\n",
      "Epoch  464, Training Loss: 1.3315, Learning Rate: 0.053600\n",
      "Epoch  465, Training Loss: 1.4130, Learning Rate: 0.053500\n",
      "Epoch  466, Training Loss: 1.3933, Learning Rate: 0.053400\n",
      "Epoch  467, Training Loss: 2.0760, Learning Rate: 0.053300\n",
      "Epoch  468, Training Loss: 1.0296, Learning Rate: 0.053200\n",
      "Epoch  469, Training Loss: 1.2740, Learning Rate: 0.053100\n",
      "Epoch  470, Training Loss: 1.3751, Learning Rate: 0.053000\n",
      "Epoch  471, Training Loss: 1.3438, Learning Rate: 0.052900\n",
      "Epoch  472, Training Loss: 1.6372, Learning Rate: 0.052800\n",
      "Epoch  473, Training Loss: 1.0099, Learning Rate: 0.052700\n",
      "Epoch  474, Training Loss: 1.3004, Learning Rate: 0.052600\n",
      "Epoch  475, Training Loss: 1.0182, Learning Rate: 0.052500\n",
      "Epoch  476, Training Loss: 1.0156, Learning Rate: 0.052400\n",
      "Epoch  477, Training Loss: 1.0481, Learning Rate: 0.052300\n",
      "Epoch  478, Training Loss: 1.2734, Learning Rate: 0.052200\n",
      "Epoch  479, Training Loss: 2.2153, Learning Rate: 0.052100\n",
      "Epoch  480, Training Loss: 1.6692, Learning Rate: 0.052000\n",
      "Epoch  481, Training Loss: 1.0028, Learning Rate: 0.051900\n",
      "Epoch  482, Training Loss: 1.0239, Learning Rate: 0.051800\n",
      "Epoch  483, Training Loss: 1.0554, Learning Rate: 0.051700\n",
      "Epoch  484, Training Loss: 1.6380, Learning Rate: 0.051600\n",
      "Epoch  485, Training Loss: 1.5586, Learning Rate: 0.051500\n",
      "Epoch  486, Training Loss: 1.0080, Learning Rate: 0.051400\n",
      "Epoch  487, Training Loss: 1.3222, Learning Rate: 0.051300\n",
      "Epoch  488, Training Loss: 1.0612, Learning Rate: 0.051200\n",
      "Epoch  489, Training Loss: 1.8333, Learning Rate: 0.051100\n",
      "Epoch  490, Training Loss: 1.9777, Learning Rate: 0.051000\n",
      "Epoch  491, Training Loss: 1.2224, Learning Rate: 0.050900\n",
      "Epoch  492, Training Loss: 1.4167, Learning Rate: 0.050800\n",
      "Epoch  493, Training Loss: 1.1148, Learning Rate: 0.050700\n",
      "Epoch  494, Training Loss: 0.9828, Learning Rate: 0.050600\n",
      "Epoch  495, Training Loss: 1.1855, Learning Rate: 0.050500\n",
      "Epoch  496, Training Loss: 1.7331, Learning Rate: 0.050400\n",
      "Epoch  497, Training Loss: 1.1693, Learning Rate: 0.050300\n",
      "Epoch  498, Training Loss: 1.2336, Learning Rate: 0.050200\n",
      "Epoch  499, Training Loss: 1.2673, Learning Rate: 0.050100\n",
      "Epoch  500, Training Loss: 1.0565, Learning Rate: 0.050000\n",
      "Epoch  501, Training Loss: 1.4718, Learning Rate: 0.049900\n",
      "Epoch  502, Training Loss: 1.1803, Learning Rate: 0.049800\n",
      "Epoch  503, Training Loss: 1.0776, Learning Rate: 0.049700\n",
      "Epoch  504, Training Loss: 1.1180, Learning Rate: 0.049600\n",
      "Epoch  505, Training Loss: 1.0390, Learning Rate: 0.049500\n",
      "Epoch  506, Training Loss: 1.2407, Learning Rate: 0.049400\n",
      "Epoch  507, Training Loss: 1.3839, Learning Rate: 0.049300\n",
      "Epoch  508, Training Loss: 0.9807, Learning Rate: 0.049200\n",
      "Epoch  509, Training Loss: 1.2735, Learning Rate: 0.049100\n",
      "Epoch  510, Training Loss: 1.2792, Learning Rate: 0.049000\n",
      "Epoch  511, Training Loss: 1.0155, Learning Rate: 0.048900\n",
      "Epoch  512, Training Loss: 1.9322, Learning Rate: 0.048800\n",
      "Epoch  513, Training Loss: 1.3486, Learning Rate: 0.048700\n",
      "Epoch  514, Training Loss: 1.1393, Learning Rate: 0.048600\n",
      "Epoch  515, Training Loss: 0.9872, Learning Rate: 0.048500\n",
      "Epoch  516, Training Loss: 1.9180, Learning Rate: 0.048400\n",
      "Epoch  517, Training Loss: 0.9824, Learning Rate: 0.048300\n",
      "Epoch  518, Training Loss: 1.4623, Learning Rate: 0.048200\n",
      "Epoch  519, Training Loss: 1.0551, Learning Rate: 0.048100\n",
      "Epoch  520, Training Loss: 1.1286, Learning Rate: 0.048000\n",
      "Epoch  521, Training Loss: 1.0511, Learning Rate: 0.047900\n",
      "Epoch  522, Training Loss: 0.9798, Learning Rate: 0.047800\n",
      "Epoch  523, Training Loss: 1.0499, Learning Rate: 0.047700\n",
      "Epoch  524, Training Loss: 0.9804, Learning Rate: 0.047600\n",
      "Epoch  525, Training Loss: 1.2220, Learning Rate: 0.047500\n",
      "Epoch  526, Training Loss: 1.2910, Learning Rate: 0.047400\n",
      "Epoch  527, Training Loss: 1.0927, Learning Rate: 0.047300\n",
      "Epoch  528, Training Loss: 1.3044, Learning Rate: 0.047200\n",
      "Epoch  529, Training Loss: 1.6555, Learning Rate: 0.047100\n",
      "Epoch  530, Training Loss: 1.2157, Learning Rate: 0.047000\n",
      "Epoch  531, Training Loss: 1.3418, Learning Rate: 0.046900\n",
      "Epoch  532, Training Loss: 1.4866, Learning Rate: 0.046800\n",
      "Epoch  533, Training Loss: 2.0472, Learning Rate: 0.046700\n",
      "Epoch  534, Training Loss: 1.1646, Learning Rate: 0.046600\n",
      "Epoch  535, Training Loss: 1.4343, Learning Rate: 0.046500\n",
      "Epoch  536, Training Loss: 1.0458, Learning Rate: 0.046400\n",
      "Epoch  537, Training Loss: 1.6010, Learning Rate: 0.046300\n",
      "Epoch  538, Training Loss: 1.1558, Learning Rate: 0.046200\n",
      "Epoch  539, Training Loss: 1.0436, Learning Rate: 0.046100\n",
      "Epoch  540, Training Loss: 1.3284, Learning Rate: 0.046000\n",
      "Epoch  541, Training Loss: 1.1363, Learning Rate: 0.045900\n",
      "Epoch  542, Training Loss: 1.1093, Learning Rate: 0.045800\n",
      "Epoch  543, Training Loss: 1.7255, Learning Rate: 0.045700\n",
      "Epoch  544, Training Loss: 1.4381, Learning Rate: 0.045600\n",
      "Epoch  545, Training Loss: 1.6858, Learning Rate: 0.045500\n",
      "Epoch  546, Training Loss: 1.0711, Learning Rate: 0.045400\n",
      "Epoch  547, Training Loss: 1.1128, Learning Rate: 0.045300\n",
      "Epoch  548, Training Loss: 1.0801, Learning Rate: 0.045200\n",
      "Epoch  549, Training Loss: 1.4045, Learning Rate: 0.045100\n",
      "Epoch  550, Training Loss: 1.0000, Learning Rate: 0.045000\n",
      "Epoch  551, Training Loss: 1.1874, Learning Rate: 0.044900\n",
      "Epoch  552, Training Loss: 1.0105, Learning Rate: 0.044800\n",
      "Epoch  553, Training Loss: 1.5559, Learning Rate: 0.044700\n",
      "Epoch  554, Training Loss: 1.0219, Learning Rate: 0.044600\n",
      "Epoch  555, Training Loss: 1.2289, Learning Rate: 0.044500\n",
      "Epoch  556, Training Loss: 1.0225, Learning Rate: 0.044400\n",
      "Epoch  557, Training Loss: 1.1113, Learning Rate: 0.044300\n",
      "Epoch  558, Training Loss: 1.2701, Learning Rate: 0.044200\n",
      "Epoch  559, Training Loss: 1.1246, Learning Rate: 0.044100\n",
      "Epoch  560, Training Loss: 1.1372, Learning Rate: 0.044000\n",
      "Epoch  561, Training Loss: 0.9804, Learning Rate: 0.043900\n",
      "Epoch  562, Training Loss: 0.9838, Learning Rate: 0.043800\n",
      "Epoch  563, Training Loss: 1.2685, Learning Rate: 0.043700\n",
      "Epoch  564, Training Loss: 1.5869, Learning Rate: 0.043600\n",
      "Epoch  565, Training Loss: 1.0596, Learning Rate: 0.043500\n",
      "Epoch  566, Training Loss: 1.0699, Learning Rate: 0.043400\n",
      "Epoch  567, Training Loss: 1.2281, Learning Rate: 0.043300\n",
      "Epoch  568, Training Loss: 1.5090, Learning Rate: 0.043200\n",
      "Epoch  569, Training Loss: 0.9861, Learning Rate: 0.043100\n",
      "Epoch  570, Training Loss: 1.0667, Learning Rate: 0.043000\n",
      "Epoch  571, Training Loss: 1.1532, Learning Rate: 0.042900\n",
      "Epoch  572, Training Loss: 1.2186, Learning Rate: 0.042800\n",
      "Epoch  573, Training Loss: 1.1007, Learning Rate: 0.042700\n",
      "Epoch  574, Training Loss: 0.9965, Learning Rate: 0.042600\n",
      "Epoch  575, Training Loss: 1.0336, Learning Rate: 0.042500\n",
      "Epoch  576, Training Loss: 1.0575, Learning Rate: 0.042400\n",
      "Epoch  577, Training Loss: 1.4772, Learning Rate: 0.042300\n",
      "Epoch  578, Training Loss: 1.3358, Learning Rate: 0.042200\n",
      "Epoch  579, Training Loss: 1.0492, Learning Rate: 0.042100\n",
      "Epoch  580, Training Loss: 1.0578, Learning Rate: 0.042000\n",
      "Epoch  581, Training Loss: 1.1322, Learning Rate: 0.041900\n",
      "Epoch  582, Training Loss: 1.4099, Learning Rate: 0.041800\n",
      "Epoch  583, Training Loss: 1.0035, Learning Rate: 0.041700\n",
      "Epoch  584, Training Loss: 1.3591, Learning Rate: 0.041600\n",
      "Epoch  585, Training Loss: 1.1321, Learning Rate: 0.041500\n",
      "Epoch  586, Training Loss: 1.3633, Learning Rate: 0.041400\n",
      "Epoch  587, Training Loss: 1.6570, Learning Rate: 0.041300\n",
      "Epoch  588, Training Loss: 1.0056, Learning Rate: 0.041200\n",
      "Epoch  589, Training Loss: 1.1431, Learning Rate: 0.041100\n",
      "Epoch  590, Training Loss: 1.2398, Learning Rate: 0.041000\n",
      "Epoch  591, Training Loss: 1.2301, Learning Rate: 0.040900\n",
      "Epoch  592, Training Loss: 1.0893, Learning Rate: 0.040800\n",
      "Epoch  593, Training Loss: 1.2952, Learning Rate: 0.040700\n",
      "Epoch  594, Training Loss: 1.0358, Learning Rate: 0.040600\n",
      "Epoch  595, Training Loss: 1.1249, Learning Rate: 0.040500\n",
      "Epoch  596, Training Loss: 1.0642, Learning Rate: 0.040400\n",
      "Epoch  597, Training Loss: 1.0241, Learning Rate: 0.040300\n",
      "Epoch  598, Training Loss: 0.9573, Learning Rate: 0.040200\n",
      "Epoch  599, Training Loss: 1.1427, Learning Rate: 0.040100\n",
      "Epoch  600, Training Loss: 1.0393, Learning Rate: 0.040000\n",
      "Epoch  601, Training Loss: 1.1046, Learning Rate: 0.039900\n",
      "Epoch  602, Training Loss: 1.0063, Learning Rate: 0.039800\n",
      "Epoch  603, Training Loss: 1.0378, Learning Rate: 0.039700\n",
      "Epoch  604, Training Loss: 1.0278, Learning Rate: 0.039600\n",
      "Epoch  605, Training Loss: 1.1352, Learning Rate: 0.039500\n",
      "Epoch  606, Training Loss: 1.1195, Learning Rate: 0.039400\n",
      "Epoch  607, Training Loss: 1.3016, Learning Rate: 0.039300\n",
      "Epoch  608, Training Loss: 1.0099, Learning Rate: 0.039200\n",
      "Epoch  609, Training Loss: 1.2371, Learning Rate: 0.039100\n",
      "Epoch  610, Training Loss: 1.0085, Learning Rate: 0.039000\n",
      "Epoch  611, Training Loss: 1.0279, Learning Rate: 0.038900\n",
      "Epoch  612, Training Loss: 1.1260, Learning Rate: 0.038800\n",
      "Epoch  613, Training Loss: 1.1742, Learning Rate: 0.038700\n",
      "Epoch  614, Training Loss: 0.9909, Learning Rate: 0.038600\n",
      "Epoch  615, Training Loss: 1.3992, Learning Rate: 0.038500\n",
      "Epoch  616, Training Loss: 1.3843, Learning Rate: 0.038400\n",
      "Epoch  617, Training Loss: 1.2277, Learning Rate: 0.038300\n",
      "Epoch  618, Training Loss: 1.5078, Learning Rate: 0.038200\n",
      "Epoch  619, Training Loss: 1.0055, Learning Rate: 0.038100\n",
      "Epoch  620, Training Loss: 1.3783, Learning Rate: 0.038000\n",
      "Epoch  621, Training Loss: 1.6773, Learning Rate: 0.037900\n",
      "Epoch  622, Training Loss: 1.2427, Learning Rate: 0.037800\n",
      "Epoch  623, Training Loss: 1.0385, Learning Rate: 0.037700\n",
      "Epoch  624, Training Loss: 1.0026, Learning Rate: 0.037600\n",
      "Epoch  625, Training Loss: 1.0487, Learning Rate: 0.037500\n",
      "Epoch  626, Training Loss: 0.9801, Learning Rate: 0.037400\n",
      "Epoch  627, Training Loss: 1.0178, Learning Rate: 0.037300\n",
      "Epoch  628, Training Loss: 1.1464, Learning Rate: 0.037200\n",
      "Epoch  629, Training Loss: 1.0447, Learning Rate: 0.037100\n",
      "Epoch  630, Training Loss: 1.4683, Learning Rate: 0.037000\n",
      "Epoch  631, Training Loss: 1.4116, Learning Rate: 0.036900\n",
      "Epoch  632, Training Loss: 0.9646, Learning Rate: 0.036800\n",
      "Epoch  633, Training Loss: 1.0671, Learning Rate: 0.036700\n",
      "Epoch  634, Training Loss: 1.0842, Learning Rate: 0.036600\n",
      "Epoch  635, Training Loss: 1.1164, Learning Rate: 0.036500\n",
      "Epoch  636, Training Loss: 1.0234, Learning Rate: 0.036400\n",
      "Epoch  637, Training Loss: 1.3183, Learning Rate: 0.036300\n",
      "Epoch  638, Training Loss: 1.0127, Learning Rate: 0.036200\n",
      "Epoch  639, Training Loss: 1.2410, Learning Rate: 0.036100\n",
      "Epoch  640, Training Loss: 1.1758, Learning Rate: 0.036000\n",
      "Epoch  641, Training Loss: 1.2180, Learning Rate: 0.035900\n",
      "Epoch  642, Training Loss: 0.9847, Learning Rate: 0.035800\n",
      "Epoch  643, Training Loss: 1.4713, Learning Rate: 0.035700\n",
      "Epoch  644, Training Loss: 1.0040, Learning Rate: 0.035600\n",
      "Epoch  645, Training Loss: 1.0163, Learning Rate: 0.035500\n",
      "Epoch  646, Training Loss: 0.9874, Learning Rate: 0.035400\n",
      "Epoch  647, Training Loss: 1.1939, Learning Rate: 0.035300\n",
      "Epoch  648, Training Loss: 1.0932, Learning Rate: 0.035200\n",
      "Epoch  649, Training Loss: 1.0113, Learning Rate: 0.035100\n",
      "Epoch  650, Training Loss: 1.3604, Learning Rate: 0.035000\n",
      "Epoch  651, Training Loss: 1.2386, Learning Rate: 0.034900\n",
      "Epoch  652, Training Loss: 1.3040, Learning Rate: 0.034800\n",
      "Epoch  653, Training Loss: 1.2172, Learning Rate: 0.034700\n",
      "Epoch  654, Training Loss: 1.2936, Learning Rate: 0.034600\n",
      "Epoch  655, Training Loss: 1.3411, Learning Rate: 0.034500\n",
      "Epoch  656, Training Loss: 1.1234, Learning Rate: 0.034400\n",
      "Epoch  657, Training Loss: 1.4533, Learning Rate: 0.034300\n",
      "Epoch  658, Training Loss: 1.0891, Learning Rate: 0.034200\n",
      "Epoch  659, Training Loss: 1.0817, Learning Rate: 0.034100\n",
      "Epoch  660, Training Loss: 1.0284, Learning Rate: 0.034000\n",
      "Epoch  661, Training Loss: 1.6803, Learning Rate: 0.033900\n",
      "Epoch  662, Training Loss: 0.9755, Learning Rate: 0.033800\n",
      "Epoch  663, Training Loss: 1.1562, Learning Rate: 0.033700\n",
      "Epoch  664, Training Loss: 1.1819, Learning Rate: 0.033600\n",
      "Epoch  665, Training Loss: 1.5220, Learning Rate: 0.033500\n",
      "Epoch  666, Training Loss: 1.0157, Learning Rate: 0.033400\n",
      "Epoch  667, Training Loss: 1.1272, Learning Rate: 0.033300\n",
      "Epoch  668, Training Loss: 1.1791, Learning Rate: 0.033200\n",
      "Epoch  669, Training Loss: 1.1139, Learning Rate: 0.033100\n",
      "Epoch  670, Training Loss: 1.0626, Learning Rate: 0.033000\n",
      "Epoch  671, Training Loss: 0.9997, Learning Rate: 0.032900\n",
      "Epoch  672, Training Loss: 1.3237, Learning Rate: 0.032800\n",
      "Epoch  673, Training Loss: 0.9575, Learning Rate: 0.032700\n",
      "Epoch  674, Training Loss: 1.0357, Learning Rate: 0.032600\n",
      "Epoch  675, Training Loss: 1.0781, Learning Rate: 0.032500\n",
      "Epoch  676, Training Loss: 1.1058, Learning Rate: 0.032400\n",
      "Epoch  677, Training Loss: 1.0441, Learning Rate: 0.032300\n",
      "Epoch  678, Training Loss: 1.0346, Learning Rate: 0.032200\n",
      "Epoch  679, Training Loss: 1.0060, Learning Rate: 0.032100\n",
      "Epoch  680, Training Loss: 1.4029, Learning Rate: 0.032000\n",
      "Epoch  681, Training Loss: 1.2329, Learning Rate: 0.031900\n",
      "Epoch  682, Training Loss: 1.3962, Learning Rate: 0.031800\n",
      "Epoch  683, Training Loss: 1.0650, Learning Rate: 0.031700\n",
      "Epoch  684, Training Loss: 1.1509, Learning Rate: 0.031600\n",
      "Epoch  685, Training Loss: 1.0940, Learning Rate: 0.031500\n",
      "Epoch  686, Training Loss: 0.9615, Learning Rate: 0.031400\n",
      "Epoch  687, Training Loss: 1.0184, Learning Rate: 0.031300\n",
      "Epoch  688, Training Loss: 0.9627, Learning Rate: 0.031200\n",
      "Epoch  689, Training Loss: 1.0852, Learning Rate: 0.031100\n",
      "Epoch  690, Training Loss: 1.2928, Learning Rate: 0.031000\n",
      "Epoch  691, Training Loss: 1.0159, Learning Rate: 0.030900\n",
      "Epoch  692, Training Loss: 1.0699, Learning Rate: 0.030800\n",
      "Epoch  693, Training Loss: 1.1106, Learning Rate: 0.030700\n",
      "Epoch  694, Training Loss: 1.1699, Learning Rate: 0.030600\n",
      "Epoch  695, Training Loss: 1.0584, Learning Rate: 0.030500\n",
      "Epoch  696, Training Loss: 1.0877, Learning Rate: 0.030400\n",
      "Epoch  697, Training Loss: 1.0813, Learning Rate: 0.030300\n",
      "Epoch  698, Training Loss: 1.0644, Learning Rate: 0.030200\n",
      "Epoch  699, Training Loss: 1.1530, Learning Rate: 0.030100\n",
      "Epoch  700, Training Loss: 1.0504, Learning Rate: 0.030000\n",
      "Epoch  701, Training Loss: 0.9610, Learning Rate: 0.029900\n",
      "Epoch  702, Training Loss: 1.0176, Learning Rate: 0.029800\n",
      "Epoch  703, Training Loss: 1.1634, Learning Rate: 0.029700\n",
      "Epoch  704, Training Loss: 0.9971, Learning Rate: 0.029600\n",
      "Epoch  705, Training Loss: 1.2779, Learning Rate: 0.029500\n",
      "Epoch  706, Training Loss: 1.2193, Learning Rate: 0.029400\n",
      "Epoch  707, Training Loss: 1.1026, Learning Rate: 0.029300\n",
      "Epoch  708, Training Loss: 0.9967, Learning Rate: 0.029200\n",
      "Epoch  709, Training Loss: 0.9954, Learning Rate: 0.029100\n",
      "Epoch  710, Training Loss: 1.0918, Learning Rate: 0.029000\n",
      "Epoch  711, Training Loss: 0.9706, Learning Rate: 0.028900\n",
      "Epoch  712, Training Loss: 1.2899, Learning Rate: 0.028800\n",
      "Epoch  713, Training Loss: 1.3481, Learning Rate: 0.028700\n",
      "Epoch  714, Training Loss: 1.1157, Learning Rate: 0.028600\n",
      "Epoch  715, Training Loss: 1.0188, Learning Rate: 0.028500\n",
      "Epoch  716, Training Loss: 1.0988, Learning Rate: 0.028400\n",
      "Epoch  717, Training Loss: 1.0387, Learning Rate: 0.028300\n",
      "Epoch  718, Training Loss: 1.3292, Learning Rate: 0.028200\n",
      "Epoch  719, Training Loss: 1.1963, Learning Rate: 0.028100\n",
      "Epoch  720, Training Loss: 1.0236, Learning Rate: 0.028000\n",
      "Epoch  721, Training Loss: 1.1794, Learning Rate: 0.027900\n",
      "Epoch  722, Training Loss: 0.9902, Learning Rate: 0.027800\n",
      "Epoch  723, Training Loss: 0.9956, Learning Rate: 0.027700\n",
      "Epoch  724, Training Loss: 1.6032, Learning Rate: 0.027600\n",
      "Epoch  725, Training Loss: 1.0059, Learning Rate: 0.027500\n",
      "Epoch  726, Training Loss: 0.9759, Learning Rate: 0.027400\n",
      "Epoch  727, Training Loss: 0.9819, Learning Rate: 0.027300\n",
      "Epoch  728, Training Loss: 1.1697, Learning Rate: 0.027200\n",
      "Epoch  729, Training Loss: 1.0190, Learning Rate: 0.027100\n",
      "Epoch  730, Training Loss: 0.9538, Learning Rate: 0.027000\n",
      "Epoch  731, Training Loss: 1.0159, Learning Rate: 0.026900\n",
      "Epoch  732, Training Loss: 0.9882, Learning Rate: 0.026800\n",
      "Epoch  733, Training Loss: 1.0049, Learning Rate: 0.026700\n",
      "Epoch  734, Training Loss: 1.0619, Learning Rate: 0.026600\n",
      "Epoch  735, Training Loss: 1.1508, Learning Rate: 0.026500\n",
      "Epoch  736, Training Loss: 0.9760, Learning Rate: 0.026400\n",
      "Epoch  737, Training Loss: 1.1973, Learning Rate: 0.026300\n",
      "Epoch  738, Training Loss: 1.0025, Learning Rate: 0.026200\n",
      "Epoch  739, Training Loss: 0.9672, Learning Rate: 0.026100\n",
      "Epoch  740, Training Loss: 1.3009, Learning Rate: 0.026000\n",
      "Epoch  741, Training Loss: 1.0844, Learning Rate: 0.025900\n",
      "Epoch  742, Training Loss: 1.0542, Learning Rate: 0.025800\n",
      "Epoch  743, Training Loss: 0.9720, Learning Rate: 0.025700\n",
      "Epoch  744, Training Loss: 0.9533, Learning Rate: 0.025600\n",
      "Epoch  745, Training Loss: 1.0888, Learning Rate: 0.025500\n",
      "Epoch  746, Training Loss: 0.9863, Learning Rate: 0.025400\n",
      "Epoch  747, Training Loss: 1.2409, Learning Rate: 0.025300\n",
      "Epoch  748, Training Loss: 1.0046, Learning Rate: 0.025200\n",
      "Epoch  749, Training Loss: 0.9935, Learning Rate: 0.025100\n",
      "Epoch  750, Training Loss: 1.1567, Learning Rate: 0.025000\n",
      "Epoch  751, Training Loss: 0.9819, Learning Rate: 0.024900\n",
      "Epoch  752, Training Loss: 1.1412, Learning Rate: 0.024800\n",
      "Epoch  753, Training Loss: 0.9948, Learning Rate: 0.024700\n",
      "Epoch  754, Training Loss: 0.9475, Learning Rate: 0.024600\n",
      "Epoch  755, Training Loss: 1.0324, Learning Rate: 0.024500\n",
      "Epoch  756, Training Loss: 0.9511, Learning Rate: 0.024400\n",
      "Epoch  757, Training Loss: 1.0547, Learning Rate: 0.024300\n",
      "Epoch  758, Training Loss: 1.0196, Learning Rate: 0.024200\n",
      "Epoch  759, Training Loss: 0.9932, Learning Rate: 0.024100\n",
      "Epoch  760, Training Loss: 1.1090, Learning Rate: 0.024000\n",
      "Epoch  761, Training Loss: 1.0720, Learning Rate: 0.023900\n",
      "Epoch  762, Training Loss: 1.0433, Learning Rate: 0.023800\n",
      "Epoch  763, Training Loss: 1.1237, Learning Rate: 0.023700\n",
      "Epoch  764, Training Loss: 1.1431, Learning Rate: 0.023600\n",
      "Epoch  765, Training Loss: 1.0044, Learning Rate: 0.023500\n",
      "Epoch  766, Training Loss: 0.9929, Learning Rate: 0.023400\n",
      "Epoch  767, Training Loss: 0.9937, Learning Rate: 0.023300\n",
      "Epoch  768, Training Loss: 0.9955, Learning Rate: 0.023200\n",
      "Epoch  769, Training Loss: 0.9512, Learning Rate: 0.023100\n",
      "Epoch  770, Training Loss: 1.1914, Learning Rate: 0.023000\n",
      "Epoch  771, Training Loss: 0.9752, Learning Rate: 0.022900\n",
      "Epoch  772, Training Loss: 0.9709, Learning Rate: 0.022800\n",
      "Epoch  773, Training Loss: 1.0726, Learning Rate: 0.022700\n",
      "Epoch  774, Training Loss: 1.0637, Learning Rate: 0.022600\n",
      "Epoch  775, Training Loss: 1.1345, Learning Rate: 0.022500\n",
      "Epoch  776, Training Loss: 0.9421, Learning Rate: 0.022400\n",
      "Epoch  777, Training Loss: 1.0612, Learning Rate: 0.022300\n",
      "Epoch  778, Training Loss: 0.9709, Learning Rate: 0.022200\n",
      "Epoch  779, Training Loss: 1.0891, Learning Rate: 0.022100\n",
      "Epoch  780, Training Loss: 0.9491, Learning Rate: 0.022000\n",
      "Epoch  781, Training Loss: 0.9445, Learning Rate: 0.021900\n",
      "Epoch  782, Training Loss: 0.9811, Learning Rate: 0.021800\n",
      "Epoch  783, Training Loss: 1.0081, Learning Rate: 0.021700\n",
      "Epoch  784, Training Loss: 1.1101, Learning Rate: 0.021600\n",
      "Epoch  785, Training Loss: 1.1188, Learning Rate: 0.021500\n",
      "Epoch  786, Training Loss: 0.9512, Learning Rate: 0.021400\n",
      "Epoch  787, Training Loss: 0.9786, Learning Rate: 0.021300\n",
      "Epoch  788, Training Loss: 0.9554, Learning Rate: 0.021200\n",
      "Epoch  789, Training Loss: 0.9773, Learning Rate: 0.021100\n",
      "Epoch  790, Training Loss: 0.9762, Learning Rate: 0.021000\n",
      "Epoch  791, Training Loss: 1.0119, Learning Rate: 0.020900\n",
      "Epoch  792, Training Loss: 0.9519, Learning Rate: 0.020800\n",
      "Epoch  793, Training Loss: 1.0056, Learning Rate: 0.020700\n",
      "Epoch  794, Training Loss: 1.0339, Learning Rate: 0.020600\n",
      "Epoch  795, Training Loss: 0.9912, Learning Rate: 0.020500\n",
      "Epoch  796, Training Loss: 0.9682, Learning Rate: 0.020400\n",
      "Epoch  797, Training Loss: 0.9466, Learning Rate: 0.020300\n",
      "Epoch  798, Training Loss: 0.9466, Learning Rate: 0.020200\n",
      "Epoch  799, Training Loss: 0.9537, Learning Rate: 0.020100\n",
      "Epoch  800, Training Loss: 0.9569, Learning Rate: 0.020000\n",
      "Epoch  801, Training Loss: 0.9660, Learning Rate: 0.019900\n",
      "Epoch  802, Training Loss: 1.0230, Learning Rate: 0.019800\n",
      "Epoch  803, Training Loss: 1.1283, Learning Rate: 0.019700\n",
      "Epoch  804, Training Loss: 0.9943, Learning Rate: 0.019600\n",
      "Epoch  805, Training Loss: 0.9765, Learning Rate: 0.019500\n",
      "Epoch  806, Training Loss: 1.0388, Learning Rate: 0.019400\n",
      "Epoch  807, Training Loss: 0.9872, Learning Rate: 0.019300\n",
      "Epoch  808, Training Loss: 0.9817, Learning Rate: 0.019200\n",
      "Epoch  809, Training Loss: 0.9476, Learning Rate: 0.019100\n",
      "Epoch  810, Training Loss: 0.9807, Learning Rate: 0.019000\n",
      "Epoch  811, Training Loss: 1.0487, Learning Rate: 0.018900\n",
      "Epoch  812, Training Loss: 0.9570, Learning Rate: 0.018800\n",
      "Epoch  813, Training Loss: 1.0744, Learning Rate: 0.018700\n",
      "Epoch  814, Training Loss: 0.9605, Learning Rate: 0.018600\n",
      "Epoch  815, Training Loss: 0.9619, Learning Rate: 0.018500\n",
      "Epoch  816, Training Loss: 0.9618, Learning Rate: 0.018400\n",
      "Epoch  817, Training Loss: 0.9496, Learning Rate: 0.018300\n",
      "Epoch  818, Training Loss: 1.1220, Learning Rate: 0.018200\n",
      "Epoch  819, Training Loss: 0.9943, Learning Rate: 0.018100\n",
      "Epoch  820, Training Loss: 1.0488, Learning Rate: 0.018000\n",
      "Epoch  821, Training Loss: 0.9595, Learning Rate: 0.017900\n",
      "Epoch  822, Training Loss: 0.9446, Learning Rate: 0.017800\n",
      "Epoch  823, Training Loss: 1.0336, Learning Rate: 0.017700\n",
      "Epoch  824, Training Loss: 1.0171, Learning Rate: 0.017600\n",
      "Epoch  825, Training Loss: 1.0095, Learning Rate: 0.017500\n",
      "Epoch  826, Training Loss: 1.0457, Learning Rate: 0.017400\n",
      "Epoch  827, Training Loss: 0.9895, Learning Rate: 0.017300\n",
      "Epoch  828, Training Loss: 0.9780, Learning Rate: 0.017200\n",
      "Epoch  829, Training Loss: 0.9594, Learning Rate: 0.017100\n",
      "Epoch  830, Training Loss: 0.9576, Learning Rate: 0.017000\n",
      "Epoch  831, Training Loss: 1.0678, Learning Rate: 0.016900\n",
      "Epoch  832, Training Loss: 0.9474, Learning Rate: 0.016800\n",
      "Epoch  833, Training Loss: 0.9769, Learning Rate: 0.016700\n",
      "Epoch  834, Training Loss: 1.0481, Learning Rate: 0.016600\n",
      "Epoch  835, Training Loss: 0.9653, Learning Rate: 0.016500\n",
      "Epoch  836, Training Loss: 0.9686, Learning Rate: 0.016400\n",
      "Epoch  837, Training Loss: 0.9727, Learning Rate: 0.016300\n",
      "Epoch  838, Training Loss: 0.9619, Learning Rate: 0.016200\n",
      "Epoch  839, Training Loss: 0.9537, Learning Rate: 0.016100\n",
      "Epoch  840, Training Loss: 0.9824, Learning Rate: 0.016000\n",
      "Epoch  841, Training Loss: 0.9799, Learning Rate: 0.015900\n",
      "Epoch  842, Training Loss: 1.0293, Learning Rate: 0.015800\n",
      "Epoch  843, Training Loss: 0.9610, Learning Rate: 0.015700\n",
      "Epoch  844, Training Loss: 1.0450, Learning Rate: 0.015600\n",
      "Epoch  845, Training Loss: 0.9609, Learning Rate: 0.015500\n",
      "Epoch  846, Training Loss: 0.9515, Learning Rate: 0.015400\n",
      "Epoch  847, Training Loss: 1.0657, Learning Rate: 0.015300\n",
      "Epoch  848, Training Loss: 0.9466, Learning Rate: 0.015200\n",
      "Epoch  849, Training Loss: 1.0248, Learning Rate: 0.015100\n",
      "Epoch  850, Training Loss: 0.9445, Learning Rate: 0.015000\n",
      "Epoch  851, Training Loss: 0.9826, Learning Rate: 0.014900\n",
      "Epoch  852, Training Loss: 0.9499, Learning Rate: 0.014800\n",
      "Epoch  853, Training Loss: 0.9615, Learning Rate: 0.014700\n",
      "Epoch  854, Training Loss: 0.9746, Learning Rate: 0.014600\n",
      "Epoch  855, Training Loss: 0.9954, Learning Rate: 0.014500\n",
      "Epoch  856, Training Loss: 0.9577, Learning Rate: 0.014400\n",
      "Epoch  857, Training Loss: 0.9468, Learning Rate: 0.014300\n",
      "Epoch  858, Training Loss: 0.9976, Learning Rate: 0.014200\n",
      "Epoch  859, Training Loss: 0.9634, Learning Rate: 0.014100\n",
      "Epoch  860, Training Loss: 0.9482, Learning Rate: 0.014000\n",
      "Epoch  861, Training Loss: 0.9456, Learning Rate: 0.013900\n",
      "Epoch  862, Training Loss: 0.9918, Learning Rate: 0.013800\n",
      "Epoch  863, Training Loss: 0.9502, Learning Rate: 0.013700\n",
      "Epoch  864, Training Loss: 1.0546, Learning Rate: 0.013600\n",
      "Epoch  865, Training Loss: 0.9577, Learning Rate: 0.013500\n",
      "Epoch  866, Training Loss: 0.9463, Learning Rate: 0.013400\n",
      "Epoch  867, Training Loss: 0.9377, Learning Rate: 0.013300\n",
      "Epoch  868, Training Loss: 0.9531, Learning Rate: 0.013200\n",
      "Epoch  869, Training Loss: 0.9529, Learning Rate: 0.013100\n",
      "Epoch  870, Training Loss: 0.9593, Learning Rate: 0.013000\n",
      "Epoch  871, Training Loss: 0.9439, Learning Rate: 0.012900\n",
      "Epoch  872, Training Loss: 0.9954, Learning Rate: 0.012800\n",
      "Epoch  873, Training Loss: 0.9399, Learning Rate: 0.012700\n",
      "Epoch  874, Training Loss: 0.9865, Learning Rate: 0.012600\n",
      "Epoch  875, Training Loss: 0.9524, Learning Rate: 0.012500\n",
      "Epoch  876, Training Loss: 1.0175, Learning Rate: 0.012400\n",
      "Epoch  877, Training Loss: 0.9771, Learning Rate: 0.012300\n",
      "Epoch  878, Training Loss: 0.9547, Learning Rate: 0.012200\n",
      "Epoch  879, Training Loss: 0.9450, Learning Rate: 0.012100\n",
      "Epoch  880, Training Loss: 0.9407, Learning Rate: 0.012000\n",
      "Epoch  881, Training Loss: 0.9574, Learning Rate: 0.011900\n",
      "Epoch  882, Training Loss: 0.9627, Learning Rate: 0.011800\n",
      "Epoch  883, Training Loss: 0.9798, Learning Rate: 0.011700\n",
      "Epoch  884, Training Loss: 0.9481, Learning Rate: 0.011600\n",
      "Epoch  885, Training Loss: 1.0088, Learning Rate: 0.011500\n",
      "Epoch  886, Training Loss: 0.9504, Learning Rate: 0.011400\n",
      "Epoch  887, Training Loss: 0.9536, Learning Rate: 0.011300\n",
      "Epoch  888, Training Loss: 1.0536, Learning Rate: 0.011200\n",
      "Epoch  889, Training Loss: 0.9625, Learning Rate: 0.011100\n",
      "Epoch  890, Training Loss: 0.9961, Learning Rate: 0.011000\n",
      "Epoch  891, Training Loss: 0.9543, Learning Rate: 0.010900\n",
      "Epoch  892, Training Loss: 0.9656, Learning Rate: 0.010800\n",
      "Epoch  893, Training Loss: 0.9473, Learning Rate: 0.010700\n",
      "Epoch  894, Training Loss: 0.9904, Learning Rate: 0.010600\n",
      "Epoch  895, Training Loss: 0.9630, Learning Rate: 0.010500\n",
      "Epoch  896, Training Loss: 0.9386, Learning Rate: 0.010400\n",
      "Epoch  897, Training Loss: 0.9386, Learning Rate: 0.010300\n",
      "Epoch  898, Training Loss: 0.9740, Learning Rate: 0.010200\n",
      "Epoch  899, Training Loss: 0.9434, Learning Rate: 0.010100\n",
      "Epoch  900, Training Loss: 0.9421, Learning Rate: 0.010000\n",
      "Epoch  901, Training Loss: 0.9661, Learning Rate: 0.009900\n",
      "Epoch  902, Training Loss: 0.9411, Learning Rate: 0.009800\n",
      "Epoch  903, Training Loss: 0.9378, Learning Rate: 0.009700\n",
      "Epoch  904, Training Loss: 0.9724, Learning Rate: 0.009600\n",
      "Epoch  905, Training Loss: 1.0053, Learning Rate: 0.009500\n",
      "Epoch  906, Training Loss: 0.9474, Learning Rate: 0.009400\n",
      "Epoch  907, Training Loss: 0.9484, Learning Rate: 0.009300\n",
      "Epoch  908, Training Loss: 0.9610, Learning Rate: 0.009200\n",
      "Epoch  909, Training Loss: 0.9514, Learning Rate: 0.009100\n",
      "Epoch  910, Training Loss: 0.9375, Learning Rate: 0.009000\n",
      "Epoch  911, Training Loss: 0.9450, Learning Rate: 0.008900\n",
      "Epoch  912, Training Loss: 0.9477, Learning Rate: 0.008800\n",
      "Epoch  913, Training Loss: 0.9551, Learning Rate: 0.008700\n",
      "Epoch  914, Training Loss: 0.9438, Learning Rate: 0.008600\n",
      "Epoch  915, Training Loss: 0.9471, Learning Rate: 0.008500\n",
      "Epoch  916, Training Loss: 0.9553, Learning Rate: 0.008400\n",
      "Epoch  917, Training Loss: 0.9484, Learning Rate: 0.008300\n",
      "Epoch  918, Training Loss: 0.9557, Learning Rate: 0.008200\n",
      "Epoch  919, Training Loss: 0.9373, Learning Rate: 0.008100\n",
      "Epoch  920, Training Loss: 0.9631, Learning Rate: 0.008000\n",
      "Epoch  921, Training Loss: 0.9441, Learning Rate: 0.007900\n",
      "Epoch  922, Training Loss: 0.9478, Learning Rate: 0.007800\n",
      "Epoch  923, Training Loss: 0.9413, Learning Rate: 0.007700\n",
      "Epoch  924, Training Loss: 0.9467, Learning Rate: 0.007600\n",
      "Epoch  925, Training Loss: 0.9429, Learning Rate: 0.007500\n",
      "Epoch  926, Training Loss: 0.9646, Learning Rate: 0.007400\n",
      "Epoch  927, Training Loss: 0.9515, Learning Rate: 0.007300\n",
      "Epoch  928, Training Loss: 0.9533, Learning Rate: 0.007200\n",
      "Epoch  929, Training Loss: 0.9579, Learning Rate: 0.007100\n",
      "Epoch  930, Training Loss: 0.9429, Learning Rate: 0.007000\n",
      "Epoch  931, Training Loss: 0.9360, Learning Rate: 0.006900\n",
      "Epoch  932, Training Loss: 0.9365, Learning Rate: 0.006800\n",
      "Epoch  933, Training Loss: 0.9380, Learning Rate: 0.006700\n",
      "Epoch  934, Training Loss: 0.9471, Learning Rate: 0.006600\n",
      "Epoch  935, Training Loss: 0.9401, Learning Rate: 0.006500\n",
      "Epoch  936, Training Loss: 0.9373, Learning Rate: 0.006400\n",
      "Epoch  937, Training Loss: 0.9372, Learning Rate: 0.006300\n",
      "Epoch  938, Training Loss: 0.9400, Learning Rate: 0.006200\n",
      "Epoch  939, Training Loss: 0.9351, Learning Rate: 0.006100\n",
      "Epoch  940, Training Loss: 0.9409, Learning Rate: 0.006000\n",
      "Epoch  941, Training Loss: 0.9367, Learning Rate: 0.005900\n",
      "Epoch  942, Training Loss: 0.9501, Learning Rate: 0.005800\n",
      "Epoch  943, Training Loss: 0.9372, Learning Rate: 0.005700\n",
      "Epoch  944, Training Loss: 0.9378, Learning Rate: 0.005600\n",
      "Epoch  945, Training Loss: 0.9473, Learning Rate: 0.005500\n",
      "Epoch  946, Training Loss: 0.9389, Learning Rate: 0.005400\n",
      "Epoch  947, Training Loss: 0.9418, Learning Rate: 0.005300\n",
      "Epoch  948, Training Loss: 0.9416, Learning Rate: 0.005200\n",
      "Epoch  949, Training Loss: 0.9357, Learning Rate: 0.005100\n",
      "Epoch  950, Training Loss: 0.9467, Learning Rate: 0.005000\n",
      "Epoch  951, Training Loss: 0.9423, Learning Rate: 0.004900\n",
      "Epoch  952, Training Loss: 0.9446, Learning Rate: 0.004800\n",
      "Epoch  953, Training Loss: 0.9426, Learning Rate: 0.004700\n",
      "Epoch  954, Training Loss: 0.9483, Learning Rate: 0.004600\n",
      "Epoch  955, Training Loss: 0.9371, Learning Rate: 0.004500\n",
      "Epoch  956, Training Loss: 0.9370, Learning Rate: 0.004400\n",
      "Epoch  957, Training Loss: 0.9355, Learning Rate: 0.004300\n",
      "Epoch  958, Training Loss: 0.9389, Learning Rate: 0.004200\n",
      "Epoch  959, Training Loss: 0.9379, Learning Rate: 0.004100\n",
      "Epoch  960, Training Loss: 0.9382, Learning Rate: 0.004000\n",
      "Epoch  961, Training Loss: 0.9389, Learning Rate: 0.003900\n",
      "Epoch  962, Training Loss: 0.9367, Learning Rate: 0.003800\n",
      "Epoch  963, Training Loss: 0.9389, Learning Rate: 0.003700\n",
      "Epoch  964, Training Loss: 0.9429, Learning Rate: 0.003600\n",
      "Epoch  965, Training Loss: 0.9401, Learning Rate: 0.003500\n",
      "Epoch  966, Training Loss: 0.9387, Learning Rate: 0.003400\n",
      "Epoch  967, Training Loss: 0.9369, Learning Rate: 0.003300\n",
      "Epoch  968, Training Loss: 0.9356, Learning Rate: 0.003200\n",
      "Epoch  969, Training Loss: 0.9363, Learning Rate: 0.003100\n",
      "Epoch  970, Training Loss: 0.9350, Learning Rate: 0.003000\n",
      "Epoch  971, Training Loss: 0.9354, Learning Rate: 0.002900\n",
      "Epoch  972, Training Loss: 0.9368, Learning Rate: 0.002800\n",
      "Epoch  973, Training Loss: 0.9358, Learning Rate: 0.002700\n",
      "Epoch  974, Training Loss: 0.9354, Learning Rate: 0.002600\n",
      "Epoch  975, Training Loss: 0.9367, Learning Rate: 0.002500\n",
      "Epoch  976, Training Loss: 0.9364, Learning Rate: 0.002400\n",
      "Epoch  977, Training Loss: 0.9370, Learning Rate: 0.002300\n",
      "Epoch  978, Training Loss: 0.9373, Learning Rate: 0.002200\n",
      "Epoch  979, Training Loss: 0.9363, Learning Rate: 0.002100\n",
      "Epoch  980, Training Loss: 0.9361, Learning Rate: 0.002000\n",
      "Epoch  981, Training Loss: 0.9358, Learning Rate: 0.001900\n",
      "Epoch  982, Training Loss: 0.9378, Learning Rate: 0.001800\n",
      "Epoch  983, Training Loss: 0.9369, Learning Rate: 0.001700\n",
      "Epoch  984, Training Loss: 0.9358, Learning Rate: 0.001600\n",
      "Epoch  985, Training Loss: 0.9378, Learning Rate: 0.001500\n",
      "Epoch  986, Training Loss: 0.9389, Learning Rate: 0.001400\n",
      "Epoch  987, Training Loss: 0.9398, Learning Rate: 0.001300\n",
      "Epoch  988, Training Loss: 0.9372, Learning Rate: 0.001200\n",
      "Epoch  989, Training Loss: 0.9364, Learning Rate: 0.001100\n",
      "Epoch  990, Training Loss: 0.9359, Learning Rate: 0.001000\n",
      "Epoch  991, Training Loss: 0.9358, Learning Rate: 0.000900\n",
      "Epoch  992, Training Loss: 0.9357, Learning Rate: 0.000800\n",
      "Epoch  993, Training Loss: 0.9356, Learning Rate: 0.000700\n",
      "Epoch  994, Training Loss: 0.9357, Learning Rate: 0.000600\n",
      "Epoch  995, Training Loss: 0.9365, Learning Rate: 0.000500\n",
      "Epoch  996, Training Loss: 0.9360, Learning Rate: 0.000400\n",
      "Epoch  997, Training Loss: 0.9361, Learning Rate: 0.000300\n",
      "Epoch  998, Training Loss: 0.9361, Learning Rate: 0.000200\n",
      "Epoch  999, Training Loss: 0.9359, Learning Rate: 0.000100\n",
      "\n",
      "Best Model Test MEE: 0.7658\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lib.data_loader import get_cup_dataset\n",
    "from lib.neural_network import NeuralNetwork\n",
    "from lib.grid_search import grid_search\n",
    "\n",
    "X_dev, y_dev, X_test, y_test = get_cup_dataset()\n",
    "input_size = X_dev.shape[1]\n",
    "hidden_layers = [20, 20]\n",
    "output_size = 3\n",
    "layers = [input_size] + hidden_layers + [output_size]\n",
    "activation_funcs = [\"relu\", \"relu\", \"linear\"]  # <---- l'ultimo layer  linear perch stiamo facendo una regressione\n",
    "\n",
    "### fin qui dovrebbe essere tutto chiaro...\n",
    "\n",
    "def build_nn_model_with_params(learning_rate=0.2, lambda_reg=0.001, reg_type=\"l2\",\n",
    "                                   lr_decay_type=\"linear\", decay_rate=0.001, weight_init=\"base\"):\n",
    "    \n",
    "    ### mi serve una funzione helper da passare alla grid search, coscche possa costruire un modello per ogni combinazione di parametri\n",
    "        \n",
    "    return NeuralNetwork(\n",
    "        layers=layers,\n",
    "        learning_rate=learning_rate,\n",
    "        lambda_reg=lambda_reg,\n",
    "        reg_type=reg_type,\n",
    "        loss_function_name=\"mee\", # <------- micheli richiede mee\n",
    "        activation_function_names=activation_funcs,\n",
    "        task=\"regression\", # <--------- regression\n",
    "        lr_decay_type=lr_decay_type,\n",
    "        decay_rate=decay_rate,\n",
    "        weight_init=weight_init\n",
    "    )\n",
    "\n",
    "param_grid = { # la griglia dei parametri da testare\n",
    "        \"learning_rate\": [0.1, 0.2],\n",
    "        \"lambda_reg\": [0.001, 0.01],\n",
    "        \"lr_decay_type\": [\"linear\", \"none\"],\n",
    "        \"decay_rate\": [0.001, 0.0],\n",
    "        \"weight_init\": [\"base\", \"glorot\"],\n",
    "}\n",
    "\n",
    "best_params, all_results = grid_search(\n",
    "        # la grid search far la seguente cosa. \n",
    "        # per ogni combinazione di parametri:\n",
    "            # 1. crea un modello con quei parametri\n",
    "            # 2. fa il k-fold cross validation, quindi divide il dataset in k parti, e per ogni parte:\n",
    "            #   1. addestra il modello sulle altre k-1 parti\n",
    "            #   2. valuta il modello sulla parte rimanente\n",
    "            #   3. calcola le performance ottenute\n",
    "            # 3. calcola la media delle performance ottenute su tutte le k parti\n",
    "            # 4. restituisce la media delle performance ottenute su tutte le k parti\n",
    "        # dopo di che, restituisce i parametri che hanno ottenuto la migliore media delle performance\n",
    "    \n",
    "        model_builder=build_nn_model_with_params,\n",
    "        param_grid=param_grid,\n",
    "        X=X_dev,\n",
    "        y=y_dev,\n",
    "        k=5,\n",
    "        epochs=500,       \n",
    "        batch_size=32,\n",
    "        early_stopping=True, # nella grid search, l'early stopping pu essere indicato True anche senza un validation set, perch viene usato il k-fold cross validation. Guarda sotto, penultimo train()\n",
    "        patience=10,\n",
    "        min_delta=1e-4,\n",
    "        n_jobs=-1, # usa tutti i core disponibili\n",
    "        maximize=True, # la media va massimizzata, dato il fatto che viene usata l'accuracy [:questo  specificato nel metodo evaluate() della rete!]\n",
    "        verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Grid Search Best Result:\")\n",
    "print(best_params)\n",
    "best_hyperparams = best_params[\"params\"] \n",
    "\n",
    "# mi rebuildo la rete con i migliori params\n",
    "nn_best = build_nn_model_with_params(\n",
    "    learning_rate=best_hyperparams['learning_rate'],\n",
    "    lr_decay_type=best_hyperparams['lr_decay_type'],\n",
    "    lambda_reg=best_hyperparams['lambda_reg'],\n",
    "    weight_init=best_hyperparams['weight_init']\n",
    ")\n",
    "\n",
    "# splitto il monk su dev set e test set, cos da poter poi splittare il dev set in train e validation, e fare i plot sia sul train che sul validation\n",
    "X_dev, y_dev, X_test, y_test = get_cup_dataset()\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_dev, y_dev, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "nn_best.train(\n",
    "    X_train_split, y_train_split,\n",
    "    epochs=1000,\n",
    "    batch_size=64,\n",
    "    verbose=True,\n",
    "    validation_data=(X_val_split, y_val_split) # passo il validation set, cos da poter fare i plot. \n",
    "                                               # Nota anche il fatto che il validation set  NECESSARIO se si intende fare early stopping (non questo caso)\n",
    ")\n",
    "    \n",
    "nn_best.plot_loss_history() # plotto la loss sia sul train che sul validation\n",
    "\n",
    "# OK, quindi abbiamo i nostri plot su training e validation, ora retraino sull'intero dev set, per testare sul test set\n",
    "\n",
    "nn_best = build_nn_model_with_params(\n",
    "    learning_rate=best_hyperparams['learning_rate'],\n",
    "    lr_decay_type=best_hyperparams['lr_decay_type'],\n",
    "    lambda_reg=best_hyperparams['lambda_reg'],\n",
    "    weight_init=best_hyperparams['weight_init']\n",
    ")\n",
    "\n",
    "nn_best.train(\n",
    "    X_dev, y_dev,\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "test_mee = nn_best.evaluate(X_test, y_test)\n",
    "print(f\"\\nBest Model Test MEE: {test_mee:.4f}\")\n",
    "nn_best.plot_loss_history() # questi plot non conterranno il validation set, ma solo i dati relativi al training set (che  il dev set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
