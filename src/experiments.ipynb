{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-1-train.csv\n",
      "Using cached ../datasets/monks/monk-1-test.csv\n",
      "One-hot encoding MONK-1 dataset...\n",
      "Epoch 0, Loss: 0.7363\n",
      "Epoch 10, Loss: 0.6567\n",
      "Epoch 20, Loss: 0.6324\n",
      "Epoch 30, Loss: 0.6056\n",
      "Epoch 40, Loss: 0.5778\n",
      "Epoch 50, Loss: 0.5511\n",
      "Epoch 60, Loss: 0.5274\n",
      "Epoch 70, Loss: 0.5076\n",
      "Epoch 80, Loss: 0.4916\n",
      "Epoch 90, Loss: 0.4788\n",
      "Epoch 100, Loss: 0.4687\n",
      "Epoch 110, Loss: 0.4605\n",
      "Epoch 120, Loss: 0.4536\n",
      "Epoch 130, Loss: 0.4478\n",
      "Epoch 140, Loss: 0.4427\n",
      "Epoch 150, Loss: 0.4380\n",
      "Epoch 160, Loss: 0.4335\n",
      "Epoch 170, Loss: 0.4292\n",
      "Epoch 180, Loss: 0.4249\n",
      "Epoch 190, Loss: 0.4206\n",
      "Epoch 200, Loss: 0.4161\n",
      "Epoch 210, Loss: 0.4113\n",
      "Epoch 220, Loss: 0.4064\n",
      "Epoch 230, Loss: 0.4010\n",
      "Epoch 240, Loss: 0.3953\n",
      "Epoch 250, Loss: 0.3892\n",
      "Epoch 260, Loss: 0.3824\n",
      "Epoch 270, Loss: 0.3751\n",
      "Epoch 280, Loss: 0.3670\n",
      "Epoch 290, Loss: 0.3581\n",
      "Epoch 300, Loss: 0.3485\n",
      "Epoch 310, Loss: 0.3383\n",
      "Epoch 320, Loss: 0.3277\n",
      "Epoch 330, Loss: 0.3170\n",
      "Epoch 340, Loss: 0.3064\n",
      "Epoch 350, Loss: 0.2959\n",
      "Epoch 360, Loss: 0.2857\n",
      "Epoch 370, Loss: 0.2757\n",
      "Epoch 380, Loss: 0.2660\n",
      "Epoch 390, Loss: 0.2566\n",
      "Epoch 400, Loss: 0.2474\n",
      "Epoch 410, Loss: 0.2385\n",
      "Epoch 420, Loss: 0.2297\n",
      "Epoch 430, Loss: 0.2211\n",
      "Epoch 440, Loss: 0.2127\n",
      "Epoch 450, Loss: 0.2046\n",
      "Epoch 460, Loss: 0.1966\n",
      "Epoch 470, Loss: 0.1889\n",
      "Epoch 480, Loss: 0.1813\n",
      "Epoch 490, Loss: 0.1740\n",
      "Epoch 500, Loss: 0.1669\n",
      "Epoch 510, Loss: 0.1601\n",
      "Epoch 520, Loss: 0.1535\n",
      "Epoch 530, Loss: 0.1472\n",
      "Epoch 540, Loss: 0.1411\n",
      "Epoch 550, Loss: 0.1353\n",
      "Epoch 560, Loss: 0.1297\n",
      "Epoch 570, Loss: 0.1244\n",
      "Epoch 580, Loss: 0.1193\n",
      "Epoch 590, Loss: 0.1145\n",
      "Epoch 600, Loss: 0.1099\n",
      "Epoch 610, Loss: 0.1056\n",
      "Epoch 620, Loss: 0.1014\n",
      "Epoch 630, Loss: 0.0975\n",
      "Epoch 640, Loss: 0.0938\n",
      "Epoch 650, Loss: 0.0903\n",
      "Epoch 660, Loss: 0.0870\n",
      "Epoch 670, Loss: 0.0838\n",
      "Epoch 680, Loss: 0.0808\n",
      "Epoch 690, Loss: 0.0780\n",
      "Epoch 700, Loss: 0.0753\n",
      "Epoch 710, Loss: 0.0728\n",
      "Epoch 720, Loss: 0.0704\n",
      "Epoch 730, Loss: 0.0681\n",
      "Epoch 740, Loss: 0.0659\n",
      "Epoch 750, Loss: 0.0639\n",
      "Epoch 760, Loss: 0.0619\n",
      "Epoch 770, Loss: 0.0601\n",
      "Epoch 780, Loss: 0.0583\n",
      "Epoch 790, Loss: 0.0566\n",
      "Epoch 800, Loss: 0.0550\n",
      "Epoch 810, Loss: 0.0535\n",
      "Epoch 820, Loss: 0.0520\n",
      "Epoch 830, Loss: 0.0506\n",
      "Epoch 840, Loss: 0.0493\n",
      "Epoch 850, Loss: 0.0480\n",
      "Epoch 860, Loss: 0.0468\n",
      "Epoch 870, Loss: 0.0456\n",
      "Epoch 880, Loss: 0.0445\n",
      "Epoch 890, Loss: 0.0434\n",
      "Epoch 900, Loss: 0.0424\n",
      "Epoch 910, Loss: 0.0414\n",
      "Epoch 920, Loss: 0.0405\n",
      "Epoch 930, Loss: 0.0396\n",
      "Epoch 940, Loss: 0.0387\n",
      "Epoch 950, Loss: 0.0378\n",
      "Epoch 960, Loss: 0.0370\n",
      "Epoch 970, Loss: 0.0362\n",
      "Epoch 980, Loss: 0.0355\n",
      "Epoch 990, Loss: 0.0348\n",
      "Test Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(1, one_hot_encode=True)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 10  # Number of neurons in hidden layer\n",
    "output_size = y_train.shape[1]\n",
    "learning_rate = 0.6\n",
    "epochs = 1000\n",
    "\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2 / input_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2 / hidden_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def forward_propagation(X):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def backward_propagation(X, y, Z1, A1, Z2, A2):\n",
    "    global W1, b1, W2, b2\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    dZ2 = A2 - y\n",
    "    dW2 = (1/m) * np.dot(A1.T, dZ2)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "    \n",
    "    dZ1 = np.dot(dZ2, W2.T) * sigmoid_derivative(A1)\n",
    "    dW1 = (1/m) * np.dot(X.T, dZ1)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "    \n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    epsilon = 1e-8  # small value to avoid division by zero\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # clip predictions to avoid log(0)\n",
    "    loss = -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / m\n",
    "    loss_history.append(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    Z1, A1, Z2, A2 = forward_propagation(X_train)\n",
    "    backward_propagation(X_train, y_train, Z1, A1, Z2, A2)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        loss = binary_crossentropy_loss(y_train, A2)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "_, _, _, A2_test = forward_propagation(X_test)\n",
    "predictions = (A2_test > 0.5).astype(int)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "pd.Series(loss_history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-1-train.csv\n",
      "Using cached ../datasets/monks/monk-1-test.csv\n",
      "One-hot encoding MONK-1 dataset...\n",
      "Epoch 0, Loss: 0.2698\n",
      "Epoch 10, Loss: 0.2445\n",
      "Epoch 20, Loss: 0.2394\n",
      "Epoch 30, Loss: 0.2370\n",
      "Epoch 40, Loss: 0.2350\n",
      "Epoch 50, Loss: 0.2329\n",
      "Epoch 60, Loss: 0.2308\n",
      "Epoch 70, Loss: 0.2286\n",
      "Epoch 80, Loss: 0.2265\n",
      "Epoch 90, Loss: 0.2243\n",
      "Epoch 100, Loss: 0.2220\n",
      "Epoch 110, Loss: 0.2197\n",
      "Epoch 120, Loss: 0.2174\n",
      "Epoch 130, Loss: 0.2151\n",
      "Epoch 140, Loss: 0.2128\n",
      "Epoch 150, Loss: 0.2104\n",
      "Epoch 160, Loss: 0.2081\n",
      "Epoch 170, Loss: 0.2057\n",
      "Epoch 180, Loss: 0.2034\n",
      "Epoch 190, Loss: 0.2010\n",
      "Epoch 200, Loss: 0.1988\n",
      "Epoch 210, Loss: 0.1965\n",
      "Epoch 220, Loss: 0.1943\n",
      "Epoch 230, Loss: 0.1921\n",
      "Epoch 240, Loss: 0.1900\n",
      "Epoch 250, Loss: 0.1880\n",
      "Epoch 260, Loss: 0.1860\n",
      "Epoch 270, Loss: 0.1841\n",
      "Epoch 280, Loss: 0.1822\n",
      "Epoch 290, Loss: 0.1804\n",
      "Epoch 300, Loss: 0.1787\n",
      "Epoch 310, Loss: 0.1770\n",
      "Epoch 320, Loss: 0.1754\n",
      "Epoch 330, Loss: 0.1739\n",
      "Epoch 340, Loss: 0.1725\n",
      "Epoch 350, Loss: 0.1711\n",
      "Epoch 360, Loss: 0.1697\n",
      "Epoch 370, Loss: 0.1685\n",
      "Epoch 380, Loss: 0.1672\n",
      "Epoch 390, Loss: 0.1661\n",
      "Epoch 400, Loss: 0.1650\n",
      "Epoch 410, Loss: 0.1639\n",
      "Epoch 420, Loss: 0.1629\n",
      "Epoch 430, Loss: 0.1619\n",
      "Epoch 440, Loss: 0.1610\n",
      "Epoch 450, Loss: 0.1601\n",
      "Epoch 460, Loss: 0.1592\n",
      "Epoch 470, Loss: 0.1584\n",
      "Epoch 480, Loss: 0.1576\n",
      "Epoch 490, Loss: 0.1569\n",
      "Epoch 500, Loss: 0.1561\n",
      "Epoch 510, Loss: 0.1554\n",
      "Epoch 520, Loss: 0.1548\n",
      "Epoch 530, Loss: 0.1541\n",
      "Epoch 540, Loss: 0.1535\n",
      "Epoch 550, Loss: 0.1529\n",
      "Epoch 560, Loss: 0.1523\n",
      "Epoch 570, Loss: 0.1517\n",
      "Epoch 580, Loss: 0.1512\n",
      "Epoch 590, Loss: 0.1506\n",
      "Epoch 600, Loss: 0.1501\n",
      "Epoch 610, Loss: 0.1496\n",
      "Epoch 620, Loss: 0.1491\n",
      "Epoch 630, Loss: 0.1486\n",
      "Epoch 640, Loss: 0.1482\n",
      "Epoch 650, Loss: 0.1477\n",
      "Epoch 660, Loss: 0.1473\n",
      "Epoch 670, Loss: 0.1468\n",
      "Epoch 680, Loss: 0.1464\n",
      "Epoch 690, Loss: 0.1460\n",
      "Epoch 700, Loss: 0.1456\n",
      "Epoch 710, Loss: 0.1452\n",
      "Epoch 720, Loss: 0.1448\n",
      "Epoch 730, Loss: 0.1444\n",
      "Epoch 740, Loss: 0.1440\n",
      "Epoch 750, Loss: 0.1436\n",
      "Epoch 760, Loss: 0.1433\n",
      "Epoch 770, Loss: 0.1429\n",
      "Epoch 780, Loss: 0.1426\n",
      "Epoch 790, Loss: 0.1422\n",
      "Epoch 800, Loss: 0.1419\n",
      "Epoch 810, Loss: 0.1415\n",
      "Epoch 820, Loss: 0.1412\n",
      "Epoch 830, Loss: 0.1409\n",
      "Epoch 840, Loss: 0.1405\n",
      "Epoch 850, Loss: 0.1402\n",
      "Epoch 860, Loss: 0.1399\n",
      "Epoch 870, Loss: 0.1396\n",
      "Epoch 880, Loss: 0.1392\n",
      "Epoch 890, Loss: 0.1389\n",
      "Epoch 900, Loss: 0.1386\n",
      "Epoch 910, Loss: 0.1383\n",
      "Epoch 920, Loss: 0.1380\n",
      "Epoch 930, Loss: 0.1377\n",
      "Epoch 940, Loss: 0.1374\n",
      "Epoch 950, Loss: 0.1371\n",
      "Epoch 960, Loss: 0.1368\n",
      "Epoch 970, Loss: 0.1365\n",
      "Epoch 980, Loss: 0.1362\n",
      "Epoch 990, Loss: 0.1359\n",
      "Epoch 1000, Loss: 0.1356\n",
      "Epoch 1010, Loss: 0.1353\n",
      "Epoch 1020, Loss: 0.1351\n",
      "Epoch 1030, Loss: 0.1348\n",
      "Epoch 1040, Loss: 0.1345\n",
      "Epoch 1050, Loss: 0.1342\n",
      "Epoch 1060, Loss: 0.1339\n",
      "Epoch 1070, Loss: 0.1336\n",
      "Epoch 1080, Loss: 0.1334\n",
      "Epoch 1090, Loss: 0.1331\n",
      "Epoch 1100, Loss: 0.1328\n",
      "Epoch 1110, Loss: 0.1325\n",
      "Epoch 1120, Loss: 0.1323\n",
      "Epoch 1130, Loss: 0.1320\n",
      "Epoch 1140, Loss: 0.1317\n",
      "Epoch 1150, Loss: 0.1314\n",
      "Epoch 1160, Loss: 0.1312\n",
      "Epoch 1170, Loss: 0.1309\n",
      "Epoch 1180, Loss: 0.1306\n",
      "Epoch 1190, Loss: 0.1304\n",
      "Epoch 1200, Loss: 0.1301\n",
      "Epoch 1210, Loss: 0.1298\n",
      "Epoch 1220, Loss: 0.1295\n",
      "Epoch 1230, Loss: 0.1293\n",
      "Epoch 1240, Loss: 0.1290\n",
      "Epoch 1250, Loss: 0.1287\n",
      "Epoch 1260, Loss: 0.1285\n",
      "Epoch 1270, Loss: 0.1282\n",
      "Epoch 1280, Loss: 0.1279\n",
      "Epoch 1290, Loss: 0.1277\n",
      "Epoch 1300, Loss: 0.1274\n",
      "Epoch 1310, Loss: 0.1271\n",
      "Epoch 1320, Loss: 0.1269\n",
      "Epoch 1330, Loss: 0.1266\n",
      "Epoch 1340, Loss: 0.1263\n",
      "Epoch 1350, Loss: 0.1261\n",
      "Epoch 1360, Loss: 0.1258\n",
      "Epoch 1370, Loss: 0.1256\n",
      "Epoch 1380, Loss: 0.1253\n",
      "Epoch 1390, Loss: 0.1250\n",
      "Epoch 1400, Loss: 0.1248\n",
      "Epoch 1410, Loss: 0.1245\n",
      "Epoch 1420, Loss: 0.1242\n",
      "Epoch 1430, Loss: 0.1240\n",
      "Epoch 1440, Loss: 0.1237\n",
      "Epoch 1450, Loss: 0.1234\n",
      "Epoch 1460, Loss: 0.1232\n",
      "Epoch 1470, Loss: 0.1229\n",
      "Epoch 1480, Loss: 0.1227\n",
      "Epoch 1490, Loss: 0.1224\n",
      "Epoch 1500, Loss: 0.1221\n",
      "Epoch 1510, Loss: 0.1219\n",
      "Epoch 1520, Loss: 0.1216\n",
      "Epoch 1530, Loss: 0.1213\n",
      "Epoch 1540, Loss: 0.1211\n",
      "Epoch 1550, Loss: 0.1208\n",
      "Epoch 1560, Loss: 0.1206\n",
      "Epoch 1570, Loss: 0.1203\n",
      "Epoch 1580, Loss: 0.1200\n",
      "Epoch 1590, Loss: 0.1198\n",
      "Epoch 1600, Loss: 0.1195\n",
      "Epoch 1610, Loss: 0.1193\n",
      "Epoch 1620, Loss: 0.1190\n",
      "Epoch 1630, Loss: 0.1187\n",
      "Epoch 1640, Loss: 0.1185\n",
      "Epoch 1650, Loss: 0.1182\n",
      "Epoch 1660, Loss: 0.1180\n",
      "Epoch 1670, Loss: 0.1177\n",
      "Epoch 1680, Loss: 0.1175\n",
      "Epoch 1690, Loss: 0.1172\n",
      "Epoch 1700, Loss: 0.1169\n",
      "Epoch 1710, Loss: 0.1167\n",
      "Epoch 1720, Loss: 0.1164\n",
      "Epoch 1730, Loss: 0.1162\n",
      "Epoch 1740, Loss: 0.1159\n",
      "Epoch 1750, Loss: 0.1157\n",
      "Epoch 1760, Loss: 0.1154\n",
      "Epoch 1770, Loss: 0.1152\n",
      "Epoch 1780, Loss: 0.1149\n",
      "Epoch 1790, Loss: 0.1146\n",
      "Epoch 1800, Loss: 0.1144\n",
      "Epoch 1810, Loss: 0.1141\n",
      "Epoch 1820, Loss: 0.1139\n",
      "Epoch 1830, Loss: 0.1136\n",
      "Epoch 1840, Loss: 0.1134\n",
      "Epoch 1850, Loss: 0.1131\n",
      "Epoch 1860, Loss: 0.1129\n",
      "Epoch 1870, Loss: 0.1126\n",
      "Epoch 1880, Loss: 0.1124\n",
      "Epoch 1890, Loss: 0.1122\n",
      "Epoch 1900, Loss: 0.1119\n",
      "Epoch 1910, Loss: 0.1117\n",
      "Epoch 1920, Loss: 0.1114\n",
      "Epoch 1930, Loss: 0.1112\n",
      "Epoch 1940, Loss: 0.1109\n",
      "Epoch 1950, Loss: 0.1107\n",
      "Epoch 1960, Loss: 0.1104\n",
      "Epoch 1970, Loss: 0.1102\n",
      "Epoch 1980, Loss: 0.1100\n",
      "Epoch 1990, Loss: 0.1097\n",
      "Epoch 2000, Loss: 0.1095\n",
      "Epoch 2010, Loss: 0.1092\n",
      "Epoch 2020, Loss: 0.1090\n",
      "Epoch 2030, Loss: 0.1088\n",
      "Epoch 2040, Loss: 0.1085\n",
      "Epoch 2050, Loss: 0.1083\n",
      "Epoch 2060, Loss: 0.1080\n",
      "Epoch 2070, Loss: 0.1078\n",
      "Epoch 2080, Loss: 0.1076\n",
      "Epoch 2090, Loss: 0.1073\n",
      "Epoch 2100, Loss: 0.1071\n",
      "Epoch 2110, Loss: 0.1069\n",
      "Epoch 2120, Loss: 0.1066\n",
      "Epoch 2130, Loss: 0.1064\n",
      "Epoch 2140, Loss: 0.1062\n",
      "Epoch 2150, Loss: 0.1059\n",
      "Epoch 2160, Loss: 0.1057\n",
      "Epoch 2170, Loss: 0.1055\n",
      "Epoch 2180, Loss: 0.1053\n",
      "Epoch 2190, Loss: 0.1050\n",
      "Epoch 2200, Loss: 0.1048\n",
      "Epoch 2210, Loss: 0.1046\n",
      "Epoch 2220, Loss: 0.1043\n",
      "Epoch 2230, Loss: 0.1041\n",
      "Epoch 2240, Loss: 0.1039\n",
      "Epoch 2250, Loss: 0.1037\n",
      "Epoch 2260, Loss: 0.1035\n",
      "Epoch 2270, Loss: 0.1032\n",
      "Epoch 2280, Loss: 0.1030\n",
      "Epoch 2290, Loss: 0.1028\n",
      "Epoch 2300, Loss: 0.1026\n",
      "Epoch 2310, Loss: 0.1023\n",
      "Epoch 2320, Loss: 0.1021\n",
      "Epoch 2330, Loss: 0.1019\n",
      "Epoch 2340, Loss: 0.1017\n",
      "Epoch 2350, Loss: 0.1015\n",
      "Epoch 2360, Loss: 0.1013\n",
      "Epoch 2370, Loss: 0.1010\n",
      "Epoch 2380, Loss: 0.1008\n",
      "Epoch 2390, Loss: 0.1006\n",
      "Epoch 2400, Loss: 0.1004\n",
      "Epoch 2410, Loss: 0.1002\n",
      "Epoch 2420, Loss: 0.1000\n",
      "Epoch 2430, Loss: 0.0998\n",
      "Epoch 2440, Loss: 0.0996\n",
      "Epoch 2450, Loss: 0.0994\n",
      "Epoch 2460, Loss: 0.0991\n",
      "Epoch 2470, Loss: 0.0989\n",
      "Epoch 2480, Loss: 0.0987\n",
      "Epoch 2490, Loss: 0.0985\n",
      "Epoch 2500, Loss: 0.0983\n",
      "Epoch 2510, Loss: 0.0981\n",
      "Epoch 2520, Loss: 0.0979\n",
      "Epoch 2530, Loss: 0.0977\n",
      "Epoch 2540, Loss: 0.0975\n",
      "Epoch 2550, Loss: 0.0973\n",
      "Epoch 2560, Loss: 0.0971\n",
      "Epoch 2570, Loss: 0.0969\n",
      "Epoch 2580, Loss: 0.0967\n",
      "Epoch 2590, Loss: 0.0965\n",
      "Epoch 2600, Loss: 0.0963\n",
      "Epoch 2610, Loss: 0.0961\n",
      "Epoch 2620, Loss: 0.0959\n",
      "Epoch 2630, Loss: 0.0957\n",
      "Epoch 2640, Loss: 0.0955\n",
      "Epoch 2650, Loss: 0.0953\n",
      "Epoch 2660, Loss: 0.0951\n",
      "Epoch 2670, Loss: 0.0950\n",
      "Epoch 2680, Loss: 0.0948\n",
      "Epoch 2690, Loss: 0.0946\n",
      "Epoch 2700, Loss: 0.0944\n",
      "Epoch 2710, Loss: 0.0942\n",
      "Epoch 2720, Loss: 0.0940\n",
      "Epoch 2730, Loss: 0.0938\n",
      "Epoch 2740, Loss: 0.0936\n",
      "Epoch 2750, Loss: 0.0935\n",
      "Epoch 2760, Loss: 0.0933\n",
      "Epoch 2770, Loss: 0.0931\n",
      "Epoch 2780, Loss: 0.0929\n",
      "Epoch 2790, Loss: 0.0927\n",
      "Epoch 2800, Loss: 0.0925\n",
      "Epoch 2810, Loss: 0.0924\n",
      "Epoch 2820, Loss: 0.0922\n",
      "Epoch 2830, Loss: 0.0920\n",
      "Epoch 2840, Loss: 0.0918\n",
      "Epoch 2850, Loss: 0.0917\n",
      "Epoch 2860, Loss: 0.0915\n",
      "Epoch 2870, Loss: 0.0913\n",
      "Epoch 2880, Loss: 0.0911\n",
      "Epoch 2890, Loss: 0.0910\n",
      "Epoch 2900, Loss: 0.0908\n",
      "Epoch 2910, Loss: 0.0906\n",
      "Epoch 2920, Loss: 0.0904\n",
      "Epoch 2930, Loss: 0.0903\n",
      "Epoch 2940, Loss: 0.0901\n",
      "Epoch 2950, Loss: 0.0899\n",
      "Epoch 2960, Loss: 0.0898\n",
      "Epoch 2970, Loss: 0.0896\n",
      "Epoch 2980, Loss: 0.0894\n",
      "Epoch 2990, Loss: 0.0893\n",
      "Epoch 3000, Loss: 0.0891\n",
      "Epoch 3010, Loss: 0.0890\n",
      "Epoch 3020, Loss: 0.0888\n",
      "Epoch 3030, Loss: 0.0886\n",
      "Epoch 3040, Loss: 0.0885\n",
      "Epoch 3050, Loss: 0.0883\n",
      "Epoch 3060, Loss: 0.0882\n",
      "Epoch 3070, Loss: 0.0880\n",
      "Epoch 3080, Loss: 0.0878\n",
      "Epoch 3090, Loss: 0.0877\n",
      "Epoch 3100, Loss: 0.0875\n",
      "Epoch 3110, Loss: 0.0874\n",
      "Epoch 3120, Loss: 0.0872\n",
      "Epoch 3130, Loss: 0.0871\n",
      "Epoch 3140, Loss: 0.0869\n",
      "Epoch 3150, Loss: 0.0868\n",
      "Epoch 3160, Loss: 0.0866\n",
      "Epoch 3170, Loss: 0.0865\n",
      "Epoch 3180, Loss: 0.0863\n",
      "Epoch 3190, Loss: 0.0862\n",
      "Epoch 3200, Loss: 0.0861\n",
      "Epoch 3210, Loss: 0.0859\n",
      "Epoch 3220, Loss: 0.0858\n",
      "Epoch 3230, Loss: 0.0856\n",
      "Epoch 3240, Loss: 0.0855\n",
      "Epoch 3250, Loss: 0.0853\n",
      "Epoch 3260, Loss: 0.0852\n",
      "Epoch 3270, Loss: 0.0851\n",
      "Epoch 3280, Loss: 0.0849\n",
      "Epoch 3290, Loss: 0.0848\n",
      "Epoch 3300, Loss: 0.0847\n",
      "Epoch 3310, Loss: 0.0845\n",
      "Epoch 3320, Loss: 0.0844\n",
      "Epoch 3330, Loss: 0.0843\n",
      "Epoch 3340, Loss: 0.0841\n",
      "Epoch 3350, Loss: 0.0840\n",
      "Epoch 3360, Loss: 0.0839\n",
      "Epoch 3370, Loss: 0.0837\n",
      "Epoch 3380, Loss: 0.0836\n",
      "Epoch 3390, Loss: 0.0835\n",
      "Epoch 3400, Loss: 0.0833\n",
      "Epoch 3410, Loss: 0.0832\n",
      "Epoch 3420, Loss: 0.0831\n",
      "Epoch 3430, Loss: 0.0830\n",
      "Epoch 3440, Loss: 0.0829\n",
      "Epoch 3450, Loss: 0.0827\n",
      "Epoch 3460, Loss: 0.0826\n",
      "Epoch 3470, Loss: 0.0825\n",
      "Epoch 3480, Loss: 0.0824\n",
      "Epoch 3490, Loss: 0.0823\n",
      "Epoch 3500, Loss: 0.0821\n",
      "Epoch 3510, Loss: 0.0820\n",
      "Epoch 3520, Loss: 0.0819\n",
      "Epoch 3530, Loss: 0.0818\n",
      "Epoch 3540, Loss: 0.0817\n",
      "Epoch 3550, Loss: 0.0816\n",
      "Epoch 3560, Loss: 0.0814\n",
      "Epoch 3570, Loss: 0.0813\n",
      "Epoch 3580, Loss: 0.0812\n",
      "Epoch 3590, Loss: 0.0811\n",
      "Epoch 3600, Loss: 0.0810\n",
      "Epoch 3610, Loss: 0.0809\n",
      "Epoch 3620, Loss: 0.0808\n",
      "Epoch 3630, Loss: 0.0807\n",
      "Epoch 3640, Loss: 0.0806\n",
      "Epoch 3650, Loss: 0.0805\n",
      "Epoch 3660, Loss: 0.0804\n",
      "Epoch 3670, Loss: 0.0803\n",
      "Epoch 3680, Loss: 0.0802\n",
      "Epoch 3690, Loss: 0.0800\n",
      "Epoch 3700, Loss: 0.0799\n",
      "Epoch 3710, Loss: 0.0798\n",
      "Epoch 3720, Loss: 0.0797\n",
      "Epoch 3730, Loss: 0.0796\n",
      "Epoch 3740, Loss: 0.0795\n",
      "Epoch 3750, Loss: 0.0795\n",
      "Epoch 3760, Loss: 0.0794\n",
      "Epoch 3770, Loss: 0.0793\n",
      "Epoch 3780, Loss: 0.0792\n",
      "Epoch 3790, Loss: 0.0791\n",
      "Epoch 3800, Loss: 0.0790\n",
      "Epoch 3810, Loss: 0.0789\n",
      "Epoch 3820, Loss: 0.0788\n",
      "Epoch 3830, Loss: 0.0787\n",
      "Epoch 3840, Loss: 0.0786\n",
      "Epoch 3850, Loss: 0.0785\n",
      "Epoch 3860, Loss: 0.0784\n",
      "Epoch 3870, Loss: 0.0783\n",
      "Epoch 3880, Loss: 0.0782\n",
      "Epoch 3890, Loss: 0.0782\n",
      "Epoch 3900, Loss: 0.0781\n",
      "Epoch 3910, Loss: 0.0780\n",
      "Epoch 3920, Loss: 0.0779\n",
      "Epoch 3930, Loss: 0.0778\n",
      "Epoch 3940, Loss: 0.0777\n",
      "Epoch 3950, Loss: 0.0776\n",
      "Epoch 3960, Loss: 0.0776\n",
      "Epoch 3970, Loss: 0.0775\n",
      "Epoch 3980, Loss: 0.0774\n",
      "Epoch 3990, Loss: 0.0773\n",
      "Epoch 4000, Loss: 0.0772\n",
      "Epoch 4010, Loss: 0.0772\n",
      "Epoch 4020, Loss: 0.0771\n",
      "Epoch 4030, Loss: 0.0770\n",
      "Epoch 4040, Loss: 0.0769\n",
      "Epoch 4050, Loss: 0.0768\n",
      "Epoch 4060, Loss: 0.0768\n",
      "Epoch 4070, Loss: 0.0767\n",
      "Epoch 4080, Loss: 0.0766\n",
      "Epoch 4090, Loss: 0.0765\n",
      "Epoch 4100, Loss: 0.0765\n",
      "Epoch 4110, Loss: 0.0764\n",
      "Epoch 4120, Loss: 0.0763\n",
      "Epoch 4130, Loss: 0.0762\n",
      "Epoch 4140, Loss: 0.0762\n",
      "Epoch 4150, Loss: 0.0761\n",
      "Epoch 4160, Loss: 0.0760\n",
      "Epoch 4170, Loss: 0.0760\n",
      "Epoch 4180, Loss: 0.0759\n",
      "Epoch 4190, Loss: 0.0758\n",
      "Epoch 4200, Loss: 0.0758\n",
      "Epoch 4210, Loss: 0.0757\n",
      "Epoch 4220, Loss: 0.0756\n",
      "Epoch 4230, Loss: 0.0755\n",
      "Epoch 4240, Loss: 0.0755\n",
      "Epoch 4250, Loss: 0.0754\n",
      "Epoch 4260, Loss: 0.0753\n",
      "Epoch 4270, Loss: 0.0753\n",
      "Epoch 4280, Loss: 0.0752\n",
      "Epoch 4290, Loss: 0.0752\n",
      "Epoch 4300, Loss: 0.0751\n",
      "Epoch 4310, Loss: 0.0750\n",
      "Epoch 4320, Loss: 0.0750\n",
      "Epoch 4330, Loss: 0.0749\n",
      "Epoch 4340, Loss: 0.0748\n",
      "Epoch 4350, Loss: 0.0748\n",
      "Epoch 4360, Loss: 0.0747\n",
      "Epoch 4370, Loss: 0.0747\n",
      "Epoch 4380, Loss: 0.0746\n",
      "Epoch 4390, Loss: 0.0745\n",
      "Epoch 4400, Loss: 0.0745\n",
      "Epoch 4410, Loss: 0.0744\n",
      "Epoch 4420, Loss: 0.0744\n",
      "Epoch 4430, Loss: 0.0743\n",
      "Epoch 4440, Loss: 0.0742\n",
      "Epoch 4450, Loss: 0.0742\n",
      "Epoch 4460, Loss: 0.0741\n",
      "Epoch 4470, Loss: 0.0741\n",
      "Epoch 4480, Loss: 0.0740\n",
      "Epoch 4490, Loss: 0.0740\n",
      "Epoch 4500, Loss: 0.0739\n",
      "Epoch 4510, Loss: 0.0739\n",
      "Epoch 4520, Loss: 0.0738\n",
      "Epoch 4530, Loss: 0.0738\n",
      "Epoch 4540, Loss: 0.0737\n",
      "Epoch 4550, Loss: 0.0736\n",
      "Epoch 4560, Loss: 0.0736\n",
      "Epoch 4570, Loss: 0.0735\n",
      "Epoch 4580, Loss: 0.0735\n",
      "Epoch 4590, Loss: 0.0734\n",
      "Epoch 4600, Loss: 0.0734\n",
      "Epoch 4610, Loss: 0.0733\n",
      "Epoch 4620, Loss: 0.0733\n",
      "Epoch 4630, Loss: 0.0732\n",
      "Epoch 4640, Loss: 0.0732\n",
      "Epoch 4650, Loss: 0.0731\n",
      "Epoch 4660, Loss: 0.0731\n",
      "Epoch 4670, Loss: 0.0730\n",
      "Epoch 4680, Loss: 0.0730\n",
      "Epoch 4690, Loss: 0.0729\n",
      "Epoch 4700, Loss: 0.0729\n",
      "Epoch 4710, Loss: 0.0729\n",
      "Epoch 4720, Loss: 0.0728\n",
      "Epoch 4730, Loss: 0.0728\n",
      "Epoch 4740, Loss: 0.0727\n",
      "Epoch 4750, Loss: 0.0727\n",
      "Epoch 4760, Loss: 0.0726\n",
      "Epoch 4770, Loss: 0.0726\n",
      "Epoch 4780, Loss: 0.0725\n",
      "Epoch 4790, Loss: 0.0725\n",
      "Epoch 4800, Loss: 0.0724\n",
      "Epoch 4810, Loss: 0.0724\n",
      "Epoch 4820, Loss: 0.0724\n",
      "Epoch 4830, Loss: 0.0723\n",
      "Epoch 4840, Loss: 0.0723\n",
      "Epoch 4850, Loss: 0.0722\n",
      "Epoch 4860, Loss: 0.0722\n",
      "Epoch 4870, Loss: 0.0721\n",
      "Epoch 4880, Loss: 0.0721\n",
      "Epoch 4890, Loss: 0.0721\n",
      "Epoch 4900, Loss: 0.0720\n",
      "Epoch 4910, Loss: 0.0720\n",
      "Epoch 4920, Loss: 0.0719\n",
      "Epoch 4930, Loss: 0.0719\n",
      "Epoch 4940, Loss: 0.0719\n",
      "Epoch 4950, Loss: 0.0718\n",
      "Epoch 4960, Loss: 0.0718\n",
      "Epoch 4970, Loss: 0.0717\n",
      "Epoch 4980, Loss: 0.0717\n",
      "Epoch 4990, Loss: 0.0717\n",
      "Epoch 5000, Loss: 0.0716\n",
      "Epoch 5010, Loss: 0.0716\n",
      "Epoch 5020, Loss: 0.0716\n",
      "Epoch 5030, Loss: 0.0715\n",
      "Epoch 5040, Loss: 0.0715\n",
      "Epoch 5050, Loss: 0.0714\n",
      "Epoch 5060, Loss: 0.0714\n",
      "Epoch 5070, Loss: 0.0714\n",
      "Epoch 5080, Loss: 0.0713\n",
      "Epoch 5090, Loss: 0.0713\n",
      "Epoch 5100, Loss: 0.0713\n",
      "Epoch 5110, Loss: 0.0712\n",
      "Epoch 5120, Loss: 0.0712\n",
      "Epoch 5130, Loss: 0.0712\n",
      "Epoch 5140, Loss: 0.0711\n",
      "Epoch 5150, Loss: 0.0711\n",
      "Epoch 5160, Loss: 0.0711\n",
      "Epoch 5170, Loss: 0.0710\n",
      "Epoch 5180, Loss: 0.0710\n",
      "Epoch 5190, Loss: 0.0710\n",
      "Epoch 5200, Loss: 0.0709\n",
      "Epoch 5210, Loss: 0.0709\n",
      "Epoch 5220, Loss: 0.0709\n",
      "Epoch 5230, Loss: 0.0708\n",
      "Epoch 5240, Loss: 0.0708\n",
      "Epoch 5250, Loss: 0.0708\n",
      "Epoch 5260, Loss: 0.0707\n",
      "Epoch 5270, Loss: 0.0707\n",
      "Epoch 5280, Loss: 0.0707\n",
      "Epoch 5290, Loss: 0.0706\n",
      "Epoch 5300, Loss: 0.0706\n",
      "Epoch 5310, Loss: 0.0706\n",
      "Epoch 5320, Loss: 0.0705\n",
      "Epoch 5330, Loss: 0.0705\n",
      "Epoch 5340, Loss: 0.0705\n",
      "Epoch 5350, Loss: 0.0704\n",
      "Epoch 5360, Loss: 0.0704\n",
      "Epoch 5370, Loss: 0.0704\n",
      "Epoch 5380, Loss: 0.0704\n",
      "Epoch 5390, Loss: 0.0703\n",
      "Epoch 5400, Loss: 0.0703\n",
      "Epoch 5410, Loss: 0.0703\n",
      "Epoch 5420, Loss: 0.0702\n",
      "Epoch 5430, Loss: 0.0702\n",
      "Epoch 5440, Loss: 0.0702\n",
      "Epoch 5450, Loss: 0.0701\n",
      "Epoch 5460, Loss: 0.0701\n",
      "Epoch 5470, Loss: 0.0701\n",
      "Epoch 5480, Loss: 0.0701\n",
      "Epoch 5490, Loss: 0.0700\n",
      "Epoch 5500, Loss: 0.0700\n",
      "Epoch 5510, Loss: 0.0700\n",
      "Epoch 5520, Loss: 0.0699\n",
      "Epoch 5530, Loss: 0.0699\n",
      "Epoch 5540, Loss: 0.0699\n",
      "Epoch 5550, Loss: 0.0699\n",
      "Epoch 5560, Loss: 0.0698\n",
      "Epoch 5570, Loss: 0.0698\n",
      "Epoch 5580, Loss: 0.0698\n",
      "Epoch 5590, Loss: 0.0698\n",
      "Epoch 5600, Loss: 0.0697\n",
      "Epoch 5610, Loss: 0.0697\n",
      "Epoch 5620, Loss: 0.0697\n",
      "Epoch 5630, Loss: 0.0697\n",
      "Epoch 5640, Loss: 0.0696\n",
      "Epoch 5650, Loss: 0.0696\n",
      "Epoch 5660, Loss: 0.0696\n",
      "Epoch 5670, Loss: 0.0695\n",
      "Epoch 5680, Loss: 0.0695\n",
      "Epoch 5690, Loss: 0.0695\n",
      "Epoch 5700, Loss: 0.0695\n",
      "Epoch 5710, Loss: 0.0694\n",
      "Epoch 5720, Loss: 0.0694\n",
      "Epoch 5730, Loss: 0.0694\n",
      "Epoch 5740, Loss: 0.0694\n",
      "Epoch 5750, Loss: 0.0693\n",
      "Epoch 5760, Loss: 0.0693\n",
      "Epoch 5770, Loss: 0.0693\n",
      "Epoch 5780, Loss: 0.0693\n",
      "Epoch 5790, Loss: 0.0693\n",
      "Epoch 5800, Loss: 0.0692\n",
      "Epoch 5810, Loss: 0.0692\n",
      "Epoch 5820, Loss: 0.0692\n",
      "Epoch 5830, Loss: 0.0692\n",
      "Epoch 5840, Loss: 0.0691\n",
      "Epoch 5850, Loss: 0.0691\n",
      "Epoch 5860, Loss: 0.0691\n",
      "Epoch 5870, Loss: 0.0691\n",
      "Epoch 5880, Loss: 0.0690\n",
      "Epoch 5890, Loss: 0.0690\n",
      "Epoch 5900, Loss: 0.0690\n",
      "Epoch 5910, Loss: 0.0690\n",
      "Epoch 5920, Loss: 0.0689\n",
      "Epoch 5930, Loss: 0.0689\n",
      "Epoch 5940, Loss: 0.0689\n",
      "Epoch 5950, Loss: 0.0689\n",
      "Epoch 5960, Loss: 0.0689\n",
      "Epoch 5970, Loss: 0.0688\n",
      "Epoch 5980, Loss: 0.0688\n",
      "Epoch 5990, Loss: 0.0688\n",
      "Epoch 6000, Loss: 0.0688\n",
      "Epoch 6010, Loss: 0.0687\n",
      "Epoch 6020, Loss: 0.0687\n",
      "Epoch 6030, Loss: 0.0687\n",
      "Epoch 6040, Loss: 0.0687\n",
      "Epoch 6050, Loss: 0.0687\n",
      "Epoch 6060, Loss: 0.0686\n",
      "Epoch 6070, Loss: 0.0686\n",
      "Epoch 6080, Loss: 0.0686\n",
      "Epoch 6090, Loss: 0.0686\n",
      "Epoch 6100, Loss: 0.0685\n",
      "Epoch 6110, Loss: 0.0685\n",
      "Epoch 6120, Loss: 0.0685\n",
      "Epoch 6130, Loss: 0.0685\n",
      "Epoch 6140, Loss: 0.0685\n",
      "Epoch 6150, Loss: 0.0684\n",
      "Epoch 6160, Loss: 0.0684\n",
      "Epoch 6170, Loss: 0.0684\n",
      "Epoch 6180, Loss: 0.0684\n",
      "Epoch 6190, Loss: 0.0683\n",
      "Epoch 6200, Loss: 0.0683\n",
      "Epoch 6210, Loss: 0.0683\n",
      "Epoch 6220, Loss: 0.0683\n",
      "Epoch 6230, Loss: 0.0683\n",
      "Epoch 6240, Loss: 0.0682\n",
      "Epoch 6250, Loss: 0.0682\n",
      "Epoch 6260, Loss: 0.0682\n",
      "Epoch 6270, Loss: 0.0682\n",
      "Epoch 6280, Loss: 0.0682\n",
      "Epoch 6290, Loss: 0.0681\n",
      "Epoch 6300, Loss: 0.0681\n",
      "Epoch 6310, Loss: 0.0681\n",
      "Epoch 6320, Loss: 0.0681\n",
      "Epoch 6330, Loss: 0.0681\n",
      "Epoch 6340, Loss: 0.0680\n",
      "Epoch 6350, Loss: 0.0680\n",
      "Epoch 6360, Loss: 0.0680\n",
      "Epoch 6370, Loss: 0.0680\n",
      "Epoch 6380, Loss: 0.0679\n",
      "Epoch 6390, Loss: 0.0679\n",
      "Epoch 6400, Loss: 0.0679\n",
      "Epoch 6410, Loss: 0.0679\n",
      "Epoch 6420, Loss: 0.0679\n",
      "Epoch 6430, Loss: 0.0678\n",
      "Epoch 6440, Loss: 0.0678\n",
      "Epoch 6450, Loss: 0.0678\n",
      "Epoch 6460, Loss: 0.0678\n",
      "Epoch 6470, Loss: 0.0678\n",
      "Epoch 6480, Loss: 0.0677\n",
      "Epoch 6490, Loss: 0.0677\n",
      "Epoch 6500, Loss: 0.0677\n",
      "Epoch 6510, Loss: 0.0677\n",
      "Epoch 6520, Loss: 0.0676\n",
      "Epoch 6530, Loss: 0.0676\n",
      "Epoch 6540, Loss: 0.0676\n",
      "Epoch 6550, Loss: 0.0676\n",
      "Epoch 6560, Loss: 0.0676\n",
      "Epoch 6570, Loss: 0.0675\n",
      "Epoch 6580, Loss: 0.0675\n",
      "Epoch 6590, Loss: 0.0675\n",
      "Epoch 6600, Loss: 0.0675\n",
      "Epoch 6610, Loss: 0.0675\n",
      "Epoch 6620, Loss: 0.0674\n",
      "Epoch 6630, Loss: 0.0674\n",
      "Epoch 6640, Loss: 0.0674\n",
      "Epoch 6650, Loss: 0.0674\n",
      "Epoch 6660, Loss: 0.0673\n",
      "Epoch 6670, Loss: 0.0673\n",
      "Epoch 6680, Loss: 0.0673\n",
      "Epoch 6690, Loss: 0.0673\n",
      "Epoch 6700, Loss: 0.0673\n",
      "Epoch 6710, Loss: 0.0672\n",
      "Epoch 6720, Loss: 0.0672\n",
      "Epoch 6730, Loss: 0.0672\n",
      "Epoch 6740, Loss: 0.0672\n",
      "Epoch 6750, Loss: 0.0671\n",
      "Epoch 6760, Loss: 0.0671\n",
      "Epoch 6770, Loss: 0.0671\n",
      "Epoch 6780, Loss: 0.0671\n",
      "Epoch 6790, Loss: 0.0670\n",
      "Epoch 6800, Loss: 0.0670\n",
      "Epoch 6810, Loss: 0.0670\n",
      "Epoch 6820, Loss: 0.0670\n",
      "Epoch 6830, Loss: 0.0669\n",
      "Epoch 6840, Loss: 0.0669\n",
      "Epoch 6850, Loss: 0.0669\n",
      "Epoch 6860, Loss: 0.0669\n",
      "Epoch 6870, Loss: 0.0668\n",
      "Epoch 6880, Loss: 0.0668\n",
      "Epoch 6890, Loss: 0.0668\n",
      "Epoch 6900, Loss: 0.0667\n",
      "Epoch 6910, Loss: 0.0667\n",
      "Epoch 6920, Loss: 0.0667\n",
      "Epoch 6930, Loss: 0.0667\n",
      "Epoch 6940, Loss: 0.0666\n",
      "Epoch 6950, Loss: 0.0666\n",
      "Epoch 6960, Loss: 0.0666\n",
      "Epoch 6970, Loss: 0.0665\n",
      "Epoch 6980, Loss: 0.0665\n",
      "Epoch 6990, Loss: 0.0665\n",
      "Epoch 7000, Loss: 0.0665\n",
      "Epoch 7010, Loss: 0.0664\n",
      "Epoch 7020, Loss: 0.0664\n",
      "Epoch 7030, Loss: 0.0664\n",
      "Epoch 7040, Loss: 0.0663\n",
      "Epoch 7050, Loss: 0.0663\n",
      "Epoch 7060, Loss: 0.0663\n",
      "Epoch 7070, Loss: 0.0662\n",
      "Epoch 7080, Loss: 0.0662\n",
      "Epoch 7090, Loss: 0.0661\n",
      "Epoch 7100, Loss: 0.0661\n",
      "Epoch 7110, Loss: 0.0661\n",
      "Epoch 7120, Loss: 0.0660\n",
      "Epoch 7130, Loss: 0.0660\n",
      "Epoch 7140, Loss: 0.0659\n",
      "Epoch 7150, Loss: 0.0659\n",
      "Epoch 7160, Loss: 0.0659\n",
      "Epoch 7170, Loss: 0.0658\n",
      "Epoch 7180, Loss: 0.0658\n",
      "Epoch 7190, Loss: 0.0657\n",
      "Epoch 7200, Loss: 0.0657\n",
      "Epoch 7210, Loss: 0.0656\n",
      "Epoch 7220, Loss: 0.0656\n",
      "Epoch 7230, Loss: 0.0655\n",
      "Epoch 7240, Loss: 0.0655\n",
      "Epoch 7250, Loss: 0.0654\n",
      "Epoch 7260, Loss: 0.0654\n",
      "Epoch 7270, Loss: 0.0653\n",
      "Epoch 7280, Loss: 0.0652\n",
      "Epoch 7290, Loss: 0.0652\n",
      "Epoch 7300, Loss: 0.0651\n",
      "Epoch 7310, Loss: 0.0651\n",
      "Epoch 7320, Loss: 0.0650\n",
      "Epoch 7330, Loss: 0.0649\n",
      "Epoch 7340, Loss: 0.0648\n",
      "Epoch 7350, Loss: 0.0648\n",
      "Epoch 7360, Loss: 0.0647\n",
      "Epoch 7370, Loss: 0.0646\n",
      "Epoch 7380, Loss: 0.0645\n",
      "Epoch 7390, Loss: 0.0644\n",
      "Epoch 7400, Loss: 0.0643\n",
      "Epoch 7410, Loss: 0.0642\n",
      "Epoch 7420, Loss: 0.0642\n",
      "Epoch 7430, Loss: 0.0640\n",
      "Epoch 7440, Loss: 0.0639\n",
      "Epoch 7450, Loss: 0.0638\n",
      "Epoch 7460, Loss: 0.0637\n",
      "Epoch 7470, Loss: 0.0636\n",
      "Epoch 7480, Loss: 0.0635\n",
      "Epoch 7490, Loss: 0.0633\n",
      "Epoch 7500, Loss: 0.0632\n",
      "Epoch 7510, Loss: 0.0631\n",
      "Epoch 7520, Loss: 0.0629\n",
      "Epoch 7530, Loss: 0.0628\n",
      "Epoch 7540, Loss: 0.0626\n",
      "Epoch 7550, Loss: 0.0624\n",
      "Epoch 7560, Loss: 0.0622\n",
      "Epoch 7570, Loss: 0.0621\n",
      "Epoch 7580, Loss: 0.0619\n",
      "Epoch 7590, Loss: 0.0617\n",
      "Epoch 7600, Loss: 0.0615\n",
      "Epoch 7610, Loss: 0.0612\n",
      "Epoch 7620, Loss: 0.0610\n",
      "Epoch 7630, Loss: 0.0608\n",
      "Epoch 7640, Loss: 0.0605\n",
      "Epoch 7650, Loss: 0.0603\n",
      "Epoch 7660, Loss: 0.0600\n",
      "Epoch 7670, Loss: 0.0597\n",
      "Epoch 7680, Loss: 0.0594\n",
      "Epoch 7690, Loss: 0.0591\n",
      "Epoch 7700, Loss: 0.0588\n",
      "Epoch 7710, Loss: 0.0585\n",
      "Epoch 7720, Loss: 0.0582\n",
      "Epoch 7730, Loss: 0.0579\n",
      "Epoch 7740, Loss: 0.0575\n",
      "Epoch 7750, Loss: 0.0572\n",
      "Epoch 7760, Loss: 0.0569\n",
      "Epoch 7770, Loss: 0.0565\n",
      "Epoch 7780, Loss: 0.0561\n",
      "Epoch 7790, Loss: 0.0558\n",
      "Epoch 7800, Loss: 0.0554\n",
      "Epoch 7810, Loss: 0.0551\n",
      "Epoch 7820, Loss: 0.0547\n",
      "Epoch 7830, Loss: 0.0543\n",
      "Epoch 7840, Loss: 0.0539\n",
      "Epoch 7850, Loss: 0.0536\n",
      "Epoch 7860, Loss: 0.0532\n",
      "Epoch 7870, Loss: 0.0528\n",
      "Epoch 7880, Loss: 0.0525\n",
      "Epoch 7890, Loss: 0.0521\n",
      "Epoch 7900, Loss: 0.0517\n",
      "Epoch 7910, Loss: 0.0514\n",
      "Epoch 7920, Loss: 0.0510\n",
      "Epoch 7930, Loss: 0.0507\n",
      "Epoch 7940, Loss: 0.0503\n",
      "Epoch 7950, Loss: 0.0499\n",
      "Epoch 7960, Loss: 0.0496\n",
      "Epoch 7970, Loss: 0.0492\n",
      "Epoch 7980, Loss: 0.0489\n",
      "Epoch 7990, Loss: 0.0485\n",
      "Epoch 8000, Loss: 0.0482\n",
      "Epoch 8010, Loss: 0.0478\n",
      "Epoch 8020, Loss: 0.0475\n",
      "Epoch 8030, Loss: 0.0471\n",
      "Epoch 8040, Loss: 0.0468\n",
      "Epoch 8050, Loss: 0.0465\n",
      "Epoch 8060, Loss: 0.0461\n",
      "Epoch 8070, Loss: 0.0458\n",
      "Epoch 8080, Loss: 0.0454\n",
      "Epoch 8090, Loss: 0.0451\n",
      "Epoch 8100, Loss: 0.0448\n",
      "Epoch 8110, Loss: 0.0444\n",
      "Epoch 8120, Loss: 0.0441\n",
      "Epoch 8130, Loss: 0.0437\n",
      "Epoch 8140, Loss: 0.0434\n",
      "Epoch 8150, Loss: 0.0431\n",
      "Epoch 8160, Loss: 0.0427\n",
      "Epoch 8170, Loss: 0.0424\n",
      "Epoch 8180, Loss: 0.0420\n",
      "Epoch 8190, Loss: 0.0417\n",
      "Epoch 8200, Loss: 0.0414\n",
      "Epoch 8210, Loss: 0.0410\n",
      "Epoch 8220, Loss: 0.0407\n",
      "Epoch 8230, Loss: 0.0404\n",
      "Epoch 8240, Loss: 0.0400\n",
      "Epoch 8250, Loss: 0.0397\n",
      "Epoch 8260, Loss: 0.0393\n",
      "Epoch 8270, Loss: 0.0390\n",
      "Epoch 8280, Loss: 0.0387\n",
      "Epoch 8290, Loss: 0.0383\n",
      "Epoch 8300, Loss: 0.0380\n",
      "Epoch 8310, Loss: 0.0377\n",
      "Epoch 8320, Loss: 0.0373\n",
      "Epoch 8330, Loss: 0.0370\n",
      "Epoch 8340, Loss: 0.0367\n",
      "Epoch 8350, Loss: 0.0363\n",
      "Epoch 8360, Loss: 0.0360\n",
      "Epoch 8370, Loss: 0.0357\n",
      "Epoch 8380, Loss: 0.0354\n",
      "Epoch 8390, Loss: 0.0350\n",
      "Epoch 8400, Loss: 0.0347\n",
      "Epoch 8410, Loss: 0.0344\n",
      "Epoch 8420, Loss: 0.0341\n",
      "Epoch 8430, Loss: 0.0337\n",
      "Epoch 8440, Loss: 0.0334\n",
      "Epoch 8450, Loss: 0.0331\n",
      "Epoch 8460, Loss: 0.0328\n",
      "Epoch 8470, Loss: 0.0325\n",
      "Epoch 8480, Loss: 0.0322\n",
      "Epoch 8490, Loss: 0.0318\n",
      "Epoch 8500, Loss: 0.0315\n",
      "Epoch 8510, Loss: 0.0312\n",
      "Epoch 8520, Loss: 0.0309\n",
      "Epoch 8530, Loss: 0.0306\n",
      "Epoch 8540, Loss: 0.0303\n",
      "Epoch 8550, Loss: 0.0300\n",
      "Epoch 8560, Loss: 0.0297\n",
      "Epoch 8570, Loss: 0.0294\n",
      "Epoch 8580, Loss: 0.0291\n",
      "Epoch 8590, Loss: 0.0288\n",
      "Epoch 8600, Loss: 0.0286\n",
      "Epoch 8610, Loss: 0.0283\n",
      "Epoch 8620, Loss: 0.0280\n",
      "Epoch 8630, Loss: 0.0277\n",
      "Epoch 8640, Loss: 0.0274\n",
      "Epoch 8650, Loss: 0.0272\n",
      "Epoch 8660, Loss: 0.0269\n",
      "Epoch 8670, Loss: 0.0266\n",
      "Epoch 8680, Loss: 0.0264\n",
      "Epoch 8690, Loss: 0.0261\n",
      "Epoch 8700, Loss: 0.0258\n",
      "Epoch 8710, Loss: 0.0256\n",
      "Epoch 8720, Loss: 0.0253\n",
      "Epoch 8730, Loss: 0.0251\n",
      "Epoch 8740, Loss: 0.0248\n",
      "Epoch 8750, Loss: 0.0246\n",
      "Epoch 8760, Loss: 0.0243\n",
      "Epoch 8770, Loss: 0.0241\n",
      "Epoch 8780, Loss: 0.0238\n",
      "Epoch 8790, Loss: 0.0236\n",
      "Epoch 8800, Loss: 0.0234\n",
      "Epoch 8810, Loss: 0.0231\n",
      "Epoch 8820, Loss: 0.0229\n",
      "Epoch 8830, Loss: 0.0227\n",
      "Epoch 8840, Loss: 0.0225\n",
      "Epoch 8850, Loss: 0.0222\n",
      "Epoch 8860, Loss: 0.0220\n",
      "Epoch 8870, Loss: 0.0218\n",
      "Epoch 8880, Loss: 0.0216\n",
      "Epoch 8890, Loss: 0.0214\n",
      "Epoch 8900, Loss: 0.0212\n",
      "Epoch 8910, Loss: 0.0210\n",
      "Epoch 8920, Loss: 0.0208\n",
      "Epoch 8930, Loss: 0.0206\n",
      "Epoch 8940, Loss: 0.0204\n",
      "Epoch 8950, Loss: 0.0202\n",
      "Epoch 8960, Loss: 0.0200\n",
      "Epoch 8970, Loss: 0.0198\n",
      "Epoch 8980, Loss: 0.0196\n",
      "Epoch 8990, Loss: 0.0194\n",
      "Epoch 9000, Loss: 0.0192\n",
      "Epoch 9010, Loss: 0.0190\n",
      "Epoch 9020, Loss: 0.0189\n",
      "Epoch 9030, Loss: 0.0187\n",
      "Epoch 9040, Loss: 0.0185\n",
      "Epoch 9050, Loss: 0.0184\n",
      "Epoch 9060, Loss: 0.0182\n",
      "Epoch 9070, Loss: 0.0180\n",
      "Epoch 9080, Loss: 0.0179\n",
      "Epoch 9090, Loss: 0.0177\n",
      "Epoch 9100, Loss: 0.0175\n",
      "Epoch 9110, Loss: 0.0174\n",
      "Epoch 9120, Loss: 0.0172\n",
      "Epoch 9130, Loss: 0.0171\n",
      "Epoch 9140, Loss: 0.0169\n",
      "Epoch 9150, Loss: 0.0168\n",
      "Epoch 9160, Loss: 0.0166\n",
      "Epoch 9170, Loss: 0.0165\n",
      "Epoch 9180, Loss: 0.0163\n",
      "Epoch 9190, Loss: 0.0162\n",
      "Epoch 9200, Loss: 0.0160\n",
      "Epoch 9210, Loss: 0.0159\n",
      "Epoch 9220, Loss: 0.0158\n",
      "Epoch 9230, Loss: 0.0156\n",
      "Epoch 9240, Loss: 0.0155\n",
      "Epoch 9250, Loss: 0.0154\n",
      "Epoch 9260, Loss: 0.0152\n",
      "Epoch 9270, Loss: 0.0151\n",
      "Epoch 9280, Loss: 0.0150\n",
      "Epoch 9290, Loss: 0.0149\n",
      "Epoch 9300, Loss: 0.0147\n",
      "Epoch 9310, Loss: 0.0146\n",
      "Epoch 9320, Loss: 0.0145\n",
      "Epoch 9330, Loss: 0.0144\n",
      "Epoch 9340, Loss: 0.0143\n",
      "Epoch 9350, Loss: 0.0141\n",
      "Epoch 9360, Loss: 0.0140\n",
      "Epoch 9370, Loss: 0.0139\n",
      "Epoch 9380, Loss: 0.0138\n",
      "Epoch 9390, Loss: 0.0137\n",
      "Epoch 9400, Loss: 0.0136\n",
      "Epoch 9410, Loss: 0.0135\n",
      "Epoch 9420, Loss: 0.0134\n",
      "Epoch 9430, Loss: 0.0133\n",
      "Epoch 9440, Loss: 0.0132\n",
      "Epoch 9450, Loss: 0.0131\n",
      "Epoch 9460, Loss: 0.0130\n",
      "Epoch 9470, Loss: 0.0129\n",
      "Epoch 9480, Loss: 0.0128\n",
      "Epoch 9490, Loss: 0.0127\n",
      "Epoch 9500, Loss: 0.0126\n",
      "Epoch 9510, Loss: 0.0125\n",
      "Epoch 9520, Loss: 0.0124\n",
      "Epoch 9530, Loss: 0.0123\n",
      "Epoch 9540, Loss: 0.0122\n",
      "Epoch 9550, Loss: 0.0121\n",
      "Epoch 9560, Loss: 0.0120\n",
      "Epoch 9570, Loss: 0.0120\n",
      "Epoch 9580, Loss: 0.0119\n",
      "Epoch 9590, Loss: 0.0118\n",
      "Epoch 9600, Loss: 0.0117\n",
      "Epoch 9610, Loss: 0.0116\n",
      "Epoch 9620, Loss: 0.0115\n",
      "Epoch 9630, Loss: 0.0115\n",
      "Epoch 9640, Loss: 0.0114\n",
      "Epoch 9650, Loss: 0.0113\n",
      "Epoch 9660, Loss: 0.0112\n",
      "Epoch 9670, Loss: 0.0111\n",
      "Epoch 9680, Loss: 0.0111\n",
      "Epoch 9690, Loss: 0.0110\n",
      "Epoch 9700, Loss: 0.0109\n",
      "Epoch 9710, Loss: 0.0108\n",
      "Epoch 9720, Loss: 0.0108\n",
      "Epoch 9730, Loss: 0.0107\n",
      "Epoch 9740, Loss: 0.0106\n",
      "Epoch 9750, Loss: 0.0106\n",
      "Epoch 9760, Loss: 0.0105\n",
      "Epoch 9770, Loss: 0.0104\n",
      "Epoch 9780, Loss: 0.0103\n",
      "Epoch 9790, Loss: 0.0103\n",
      "Epoch 9800, Loss: 0.0102\n",
      "Epoch 9810, Loss: 0.0102\n",
      "Epoch 9820, Loss: 0.0101\n",
      "Epoch 9830, Loss: 0.0100\n",
      "Epoch 9840, Loss: 0.0100\n",
      "Epoch 9850, Loss: 0.0099\n",
      "Epoch 9860, Loss: 0.0098\n",
      "Epoch 9870, Loss: 0.0098\n",
      "Epoch 9880, Loss: 0.0097\n",
      "Epoch 9890, Loss: 0.0097\n",
      "Epoch 9900, Loss: 0.0096\n",
      "Epoch 9910, Loss: 0.0095\n",
      "Epoch 9920, Loss: 0.0095\n",
      "Epoch 9930, Loss: 0.0094\n",
      "Epoch 9940, Loss: 0.0094\n",
      "Epoch 9950, Loss: 0.0093\n",
      "Epoch 9960, Loss: 0.0092\n",
      "Epoch 9970, Loss: 0.0092\n",
      "Epoch 9980, Loss: 0.0091\n",
      "Epoch 9990, Loss: 0.0091\n",
      "Test Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "# Caricamento del dataset Monks con one-hot encoding per le etichette\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(1, one_hot_encode=True)\n",
    "\n",
    "# Lista per memorizzare la storia della perdita durante l'allenamento\n",
    "loss_history = []\n",
    "\n",
    "# Definizione dell'architettura della rete neurale\n",
    "input_size = X_train.shape[1]   # Numero di input features\n",
    "hidden_size = 10                # Numero di neuroni nello strato nascosto\n",
    "output_size = y_train.shape[1]  # Numero di classi nell'output\n",
    "learning_rate = 0.45             # Tasso di apprendimento\n",
    "epochs = 10000                   # Numero di epoche di allenamento\n",
    "loss_function = \"mse\"  # Funzione di perdita selezionata\n",
    "\n",
    "# Inizializzazione dei pesi e dei bias\n",
    "np.random.seed(42)  # Per la riproducibilità dei risultati\n",
    "W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2 / input_size)  # Pesi strato nascosto\n",
    "b1 = np.zeros((1, hidden_size))  # Bias strato nascosto\n",
    "W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2 / hidden_size)  # Pesi strato output\n",
    "b2 = np.zeros((1, output_size))  # Bias strato output\n",
    "\n",
    "# Funzione di attivazione sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivata della funzione sigmoide\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Propagazione in avanti (Forward Propagation)\n",
    "def forward_propagation(X):\n",
    "    Z1 = np.dot(X, W1) + b1  # Calcolo pre-attivazione strato nascosto\n",
    "    A1 = sigmoid(Z1)         # Applicazione funzione di attivazione\n",
    "    Z2 = np.dot(A1, W2) + b2  # Calcolo pre-attivazione strato output\n",
    "    A2 = sigmoid(Z2)         # Applicazione funzione di attivazione\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "# Funzione di perdita Binary Crossentropy\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]  # Numero di esempi nel batch\n",
    "    epsilon = 1e-8  # Piccolo valore per evitare divisioni per zero\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Evita log(0)\n",
    "    return -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / m\n",
    "\n",
    "# Funzione di perdita Mean Squared Error (MSE)\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Selezione della funzione di perdita\n",
    "def compute_loss(y_true, y_pred):\n",
    "    loss = binary_crossentropy_loss(y_true, y_pred) if loss_function == \"binary_crossentropy\" else mse_loss(y_true, y_pred)\n",
    "    loss_history.append(loss)  # Salvataggio della perdita per analisi successive\n",
    "    return loss\n",
    "\n",
    "# Propagazione all'indietro (Backpropagation)\n",
    "def backward_propagation(X, y, Z1, A1, Z2, A2):\n",
    "    global W1, b1, W2, b2  # Dichiarazione delle variabili globali\n",
    "    \n",
    "    m = X.shape[0]  # Numero di campioni nel batch\n",
    "    \n",
    "    # Calcolo dell'errore dell'output in base alla funzione di perdita scelta\n",
    "    if loss_function == \"binary_crossentropy\":\n",
    "        dZ2 = A2 - y  # Derivata della loss rispetto all'output (cross-entropy)\n",
    "    else:  # Mean Squared Error\n",
    "        dZ2 = (A2 - y) * sigmoid_derivative(A2)  # Derivata della MSE rispetto all'output\n",
    "    \n",
    "    dW2 = (1/m) * np.dot(A1.T, dZ2)  # Gradiente dei pesi dello strato output\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)  # Gradiente del bias output\n",
    "    \n",
    "    # Backpropagation attraverso lo strato nascosto\n",
    "    dZ1 = np.dot(dZ2, W2.T) * sigmoid_derivative(A1)  # Derivata rispetto all'input dello strato nascosto\n",
    "    dW1 = (1/m) * np.dot(X.T, dZ1)  # Gradiente dei pesi dello strato nascosto\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)  # Gradiente del bias strato nascosto\n",
    "    \n",
    "    # Aggiornamento dei pesi e dei bias\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "# Allenamento della rete neurale\n",
    "for epoch in range(epochs):\n",
    "    Z1, A1, Z2, A2 = forward_propagation(X_train)  # Forward propagation\n",
    "    backward_propagation(X_train, y_train, Z1, A1, Z2, A2)  # Backpropagation e aggiornamento pesi\n",
    "    \n",
    "    if epoch % 10 == 0:  # Ogni 10 epoche calcola la perdita\n",
    "        loss = compute_loss(y_train, A2)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Valutazione sul set di test\n",
    "_, _, _, A2_test = forward_propagation(X_test)\n",
    "predictions = (A2_test > 0.5).astype(int)  # Conversione delle probabilità in classi binarie\n",
    "accuracy = np.mean(predictions == y_test)  # Calcolo dell'accuratezza\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Visualizzazione della curva di perdita\n",
    "pd.Series(loss_history).plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-1-train.csv\n",
      "Using cached ../datasets/monks/monk-1-test.csv\n",
      "One-hot encoding MONK-1 dataset...\n",
      "Epoch 0, Loss: 0.7041\n",
      "Epoch 10, Loss: 0.6021\n",
      "Epoch 20, Loss: 0.4808\n",
      "Epoch 30, Loss: 0.4053\n",
      "Epoch 40, Loss: 0.3565\n",
      "Epoch 50, Loss: 0.3177\n",
      "Epoch 60, Loss: 0.2856\n",
      "Epoch 70, Loss: 0.2581\n",
      "Epoch 80, Loss: 0.2335\n",
      "Epoch 90, Loss: 0.2122\n",
      "Epoch 100, Loss: 0.1936\n",
      "Epoch 110, Loss: 0.1768\n",
      "Epoch 120, Loss: 0.1614\n",
      "Epoch 130, Loss: 0.1485\n",
      "Epoch 140, Loss: 0.1350\n",
      "Epoch 150, Loss: 0.1230\n",
      "Epoch 160, Loss: 0.1113\n",
      "Epoch 170, Loss: 0.1014\n",
      "Epoch 180, Loss: 0.0928\n",
      "Epoch 190, Loss: 0.0846\n",
      "Epoch 200, Loss: 0.0774\n",
      "Epoch 210, Loss: 0.0708\n",
      "Epoch 220, Loss: 0.0657\n",
      "Epoch 230, Loss: 0.0599\n",
      "Epoch 240, Loss: 0.0555\n",
      "Epoch 250, Loss: 0.0519\n",
      "Epoch 260, Loss: 0.0473\n",
      "Epoch 270, Loss: 0.0441\n",
      "Epoch 280, Loss: 0.0410\n",
      "Epoch 290, Loss: 0.0384\n",
      "Epoch 300, Loss: 0.0358\n",
      "Epoch 310, Loss: 0.0336\n",
      "Epoch 320, Loss: 0.0315\n",
      "Epoch 330, Loss: 0.0296\n",
      "Epoch 340, Loss: 0.0279\n",
      "Epoch 350, Loss: 0.0264\n",
      "Epoch 360, Loss: 0.0249\n",
      "Epoch 370, Loss: 0.0236\n",
      "Epoch 380, Loss: 0.0224\n",
      "Epoch 390, Loss: 0.0213\n",
      "Epoch 400, Loss: 0.0202\n",
      "Epoch 410, Loss: 0.0193\n",
      "Epoch 420, Loss: 0.0184\n",
      "Epoch 430, Loss: 0.0176\n",
      "Epoch 440, Loss: 0.0168\n",
      "Epoch 450, Loss: 0.0161\n",
      "Epoch 460, Loss: 0.0154\n",
      "Epoch 470, Loss: 0.0148\n",
      "Epoch 480, Loss: 0.0143\n",
      "Epoch 490, Loss: 0.0137\n",
      "Epoch 500, Loss: 0.0132\n",
      "Epoch 510, Loss: 0.0127\n",
      "Epoch 520, Loss: 0.0123\n",
      "Epoch 530, Loss: 0.0118\n",
      "Epoch 540, Loss: 0.0115\n",
      "Epoch 550, Loss: 0.0111\n",
      "Epoch 560, Loss: 0.0107\n",
      "Epoch 570, Loss: 0.0104\n",
      "Epoch 580, Loss: 0.0100\n",
      "Epoch 590, Loss: 0.0097\n",
      "Epoch 600, Loss: 0.0094\n",
      "Epoch 610, Loss: 0.0092\n",
      "Epoch 620, Loss: 0.0089\n",
      "Epoch 630, Loss: 0.0087\n",
      "Epoch 640, Loss: 0.0084\n",
      "Epoch 650, Loss: 0.0082\n",
      "Epoch 660, Loss: 0.0080\n",
      "Epoch 670, Loss: 0.0078\n",
      "Epoch 680, Loss: 0.0076\n",
      "Epoch 690, Loss: 0.0074\n",
      "Epoch 700, Loss: 0.0072\n",
      "Epoch 710, Loss: 0.0070\n",
      "Epoch 720, Loss: 0.0068\n",
      "Epoch 730, Loss: 0.0067\n",
      "Epoch 740, Loss: 0.0065\n",
      "Epoch 750, Loss: 0.0064\n",
      "Epoch 760, Loss: 0.0062\n",
      "Epoch 770, Loss: 0.0061\n",
      "Epoch 780, Loss: 0.0060\n",
      "Epoch 790, Loss: 0.0058\n",
      "Epoch 800, Loss: 0.0057\n",
      "Epoch 810, Loss: 0.0056\n",
      "Epoch 820, Loss: 0.0055\n",
      "Epoch 830, Loss: 0.0054\n",
      "Epoch 840, Loss: 0.0053\n",
      "Epoch 850, Loss: 0.0052\n",
      "Epoch 860, Loss: 0.0051\n",
      "Epoch 870, Loss: 0.0050\n",
      "Epoch 880, Loss: 0.0049\n",
      "Epoch 890, Loss: 0.0048\n",
      "Epoch 900, Loss: 0.0047\n",
      "Epoch 910, Loss: 0.0046\n",
      "Epoch 920, Loss: 0.0045\n",
      "Epoch 930, Loss: 0.0044\n",
      "Epoch 940, Loss: 0.0044\n",
      "Epoch 950, Loss: 0.0043\n",
      "Epoch 960, Loss: 0.0042\n",
      "Epoch 970, Loss: 0.0041\n",
      "Epoch 980, Loss: 0.0041\n",
      "Epoch 990, Loss: 0.0040\n",
      "Test Accuracy: 0.9954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "# caricamento del dataset monks con one-hot encoding per le etichette\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(1, one_hot_encode=True)\n",
    "\n",
    "# lista per memorizzare la storia della perdita durante l'allenamento\n",
    "loss_history = []\n",
    "\n",
    "# definizione dell'architettura della rete neurale\n",
    "input_size = X_train.shape[1]   # numero di input features\n",
    "hidden_size = 10                # numero di neuroni nello strato nascosto\n",
    "output_size = y_train.shape[1]  # numero di classi nell'output\n",
    "learning_rate = 0.1             # tasso di apprendimento\n",
    "epochs = 1000                   # numero di epoche di allenamento\n",
    "batch_size = 32                 # dimensione del batch\n",
    "loss_function = \"binary_crossentropy\"  # funzione di perdita selezionata\n",
    "activation_function = \"relu\"  # funzione di attivazione selezionata\n",
    "\n",
    "# inizializzazione dei pesi e dei bias\n",
    "np.random.seed(42)  # per la riproducibilità dei risultati\n",
    "W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2 / input_size)  # pesi strato nascosto\n",
    "b1 = np.zeros((1, hidden_size))  # bias strato nascosto\n",
    "W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2 / hidden_size)  # pesi strato output\n",
    "b2 = np.zeros((1, output_size))  # bias strato output\n",
    "\n",
    "# definizione delle funzioni di attivazione e delle loro derivate\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def activation(x):\n",
    "    return sigmoid(x) if activation_function == \"sigmoid\" else relu(x)\n",
    "\n",
    "def activation_derivative(x):\n",
    "    return sigmoid_derivative(x) if activation_function == \"sigmoid\" else relu_derivative(x)\n",
    "\n",
    "# propagazione in avanti (forward propagation)\n",
    "def forward_propagation(X):\n",
    "    Z1 = np.dot(X, W1) + b1  # calcolo pre-attivazione strato nascosto\n",
    "    A1 = activation(Z1)      # applicazione funzione di attivazione\n",
    "    Z2 = np.dot(A1, W2) + b2  # calcolo pre-attivazione strato output\n",
    "    A2 = sigmoid(Z2)         # funzione di attivazione finale (sigmoide per classificazione binaria)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "# funzione di perdita binary crossentropy\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]  # numero di esempi nel batch\n",
    "    epsilon = 1e-8  # piccolo valore per evitare divisioni per zero\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # evita log(0)\n",
    "    return -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / m\n",
    "\n",
    "# funzione di perdita mean squared error (mse)\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# selezione della funzione di perdita\n",
    "def compute_loss(y_true, y_pred):\n",
    "    loss = binary_crossentropy_loss(y_true, y_pred) if loss_function == \"binary_crossentropy\" else mse_loss(y_pred, y_true)\n",
    "    loss_history.append(loss)  # salvataggio della perdita per analisi successive\n",
    "    return loss\n",
    "\n",
    "# propagazione all'indietro (backpropagation)\n",
    "def backward_propagation(X, y, Z1, A1, Z2, A2):\n",
    "    global W1, b1, W2, b2  # dichiarazione delle variabili globali\n",
    "    \n",
    "    m = X.shape[0]  # numero di campioni nel batch\n",
    "    \n",
    "    # calcolo dell'errore dell'output in base alla funzione di perdita scelta\n",
    "    if loss_function == \"binary_crossentropy\":\n",
    "        dZ2 = A2 - y  # derivata della loss rispetto all'output (cross-entropy)\n",
    "    else:  # mean squared error\n",
    "        dZ2 = (A2 - y) * sigmoid_derivative(A2)  # derivata della mse rispetto all'output\n",
    "    \n",
    "    dW2 = (1/m) * np.dot(A1.T, dZ2)  # gradiente dei pesi dello strato output\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)  # gradiente del bias output\n",
    "    \n",
    "    # backpropagation attraverso lo strato nascosto\n",
    "    dZ1 = np.dot(dZ2, W2.T) * activation_derivative(A1)  # derivata rispetto all'input dello strato nascosto\n",
    "    dW1 = (1/m) * np.dot(X.T, dZ1)  # gradiente dei pesi dello strato nascosto\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)  # gradiente del bias strato nascosto\n",
    "    \n",
    "    # aggiornamento dei pesi e dei bias\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "# allenamento della rete neurale con mini-batch gradient descent\n",
    "for epoch in range(epochs):\n",
    "    # shuffle dei dati per evitare overfitting\n",
    "    permutation = np.random.permutation(X_train.shape[0])\n",
    "    X_train_shuffled = X_train[permutation]\n",
    "    y_train_shuffled = y_train[permutation]\n",
    "    \n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        X_batch = X_train_shuffled[i:i+batch_size]\n",
    "        y_batch = y_train_shuffled[i:i+batch_size]\n",
    "        \n",
    "        Z1, A1, Z2, A2 = forward_propagation(X_batch)  # forward propagation\n",
    "        backward_propagation(X_batch, y_batch, Z1, A1, Z2, A2)  # backpropagation e aggiornamento pesi\n",
    "    \n",
    "    if epoch % 10 == 0:  # ogni 10 epoche calcola la perdita\n",
    "        loss = compute_loss(y_train, forward_propagation(X_train)[-1])\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# valutazione sul set di test\n",
    "_, _, _, A2_test = forward_propagation(X_test)\n",
    "predictions = (A2_test > 0.5).astype(int)  # conversione delle probabilità in classi binarie\n",
    "accuracy = np.mean(predictions == y_test)  # calcolo dell'accuratezza\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "pd.Series(loss_history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-1-train.csv\n",
      "Using cached ../datasets/monks/monk-1-test.csv\n",
      "One-hot encoding MONK-1 dataset...\n",
      "Epoch 0, Loss: 0.2787\n",
      "Epoch 10, Loss: 0.2360\n",
      "Epoch 20, Loss: 0.2200\n",
      "Epoch 30, Loss: 0.2000\n",
      "Epoch 40, Loss: 0.1757\n",
      "Epoch 50, Loss: 0.1434\n",
      "Epoch 60, Loss: 0.1071\n",
      "Epoch 70, Loss: 0.0780\n",
      "Epoch 80, Loss: 0.0587\n",
      "Epoch 90, Loss: 0.0443\n",
      "Epoch 100, Loss: 0.0335\n",
      "Epoch 110, Loss: 0.0243\n",
      "Epoch 120, Loss: 0.0176\n",
      "Epoch 130, Loss: 0.0127\n",
      "Epoch 140, Loss: 0.0093\n",
      "Epoch 150, Loss: 0.0069\n",
      "Epoch 160, Loss: 0.0051\n",
      "Epoch 170, Loss: 0.0039\n",
      "Epoch 180, Loss: 0.0029\n",
      "Epoch 190, Loss: 0.0022\n",
      "Epoch 200, Loss: 0.0017\n",
      "Epoch 210, Loss: 0.0013\n",
      "Epoch 220, Loss: 0.0011\n",
      "Epoch 230, Loss: 0.0009\n",
      "Epoch 240, Loss: 0.0007\n",
      "Epoch 250, Loss: 0.0006\n",
      "Epoch 260, Loss: 0.0005\n",
      "Epoch 270, Loss: 0.0004\n",
      "Epoch 280, Loss: 0.0003\n",
      "Epoch 290, Loss: 0.0003\n",
      "Epoch 300, Loss: 0.0003\n",
      "Epoch 310, Loss: 0.0002\n",
      "Epoch 320, Loss: 0.0002\n",
      "Epoch 330, Loss: 0.0002\n",
      "Epoch 340, Loss: 0.0001\n",
      "Epoch 350, Loss: 0.0001\n",
      "Epoch 360, Loss: 0.0001\n",
      "Epoch 370, Loss: 0.0001\n",
      "Epoch 380, Loss: 0.0001\n",
      "Epoch 390, Loss: 0.0001\n",
      "Epoch 400, Loss: 0.0001\n",
      "Epoch 410, Loss: 0.0001\n",
      "Epoch 420, Loss: 0.0001\n",
      "Epoch 430, Loss: 0.0001\n",
      "Epoch 440, Loss: 0.0001\n",
      "Epoch 450, Loss: 0.0000\n",
      "Epoch 460, Loss: 0.0000\n",
      "Epoch 470, Loss: 0.0000\n",
      "Epoch 480, Loss: 0.0000\n",
      "Epoch 490, Loss: 0.0000\n",
      "Epoch 500, Loss: 0.0000\n",
      "Epoch 510, Loss: 0.0000\n",
      "Epoch 520, Loss: 0.0000\n",
      "Epoch 530, Loss: 0.0000\n",
      "Epoch 540, Loss: 0.0000\n",
      "Epoch 550, Loss: 0.0000\n",
      "Epoch 560, Loss: 0.0000\n",
      "Epoch 570, Loss: 0.0000\n",
      "Epoch 580, Loss: 0.0000\n",
      "Epoch 590, Loss: 0.0000\n",
      "Epoch 600, Loss: 0.0000\n",
      "Epoch 610, Loss: 0.0000\n",
      "Epoch 620, Loss: 0.0000\n",
      "Epoch 630, Loss: 0.0000\n",
      "Epoch 640, Loss: 0.0000\n",
      "Epoch 650, Loss: 0.0000\n",
      "Epoch 660, Loss: 0.0000\n",
      "Epoch 670, Loss: 0.0000\n",
      "Epoch 680, Loss: 0.0000\n",
      "Epoch 690, Loss: 0.0000\n",
      "Epoch 700, Loss: 0.0000\n",
      "Epoch 710, Loss: 0.0000\n",
      "Epoch 720, Loss: 0.0000\n",
      "Epoch 730, Loss: 0.0000\n",
      "Epoch 740, Loss: 0.0000\n",
      "Epoch 750, Loss: 0.0000\n",
      "Epoch 760, Loss: 0.0000\n",
      "Epoch 770, Loss: 0.0000\n",
      "Epoch 780, Loss: 0.0000\n",
      "Epoch 790, Loss: 0.0000\n",
      "Epoch 800, Loss: 0.0000\n",
      "Epoch 810, Loss: 0.0000\n",
      "Epoch 820, Loss: 0.0000\n",
      "Epoch 830, Loss: 0.0000\n",
      "Epoch 840, Loss: 0.0000\n",
      "Epoch 850, Loss: 0.0000\n",
      "Epoch 860, Loss: 0.0000\n",
      "Epoch 870, Loss: 0.0000\n",
      "Epoch 880, Loss: 0.0000\n",
      "Epoch 890, Loss: 0.0000\n",
      "Epoch 900, Loss: 0.0000\n",
      "Epoch 910, Loss: 0.0000\n",
      "Epoch 920, Loss: 0.0000\n",
      "Epoch 930, Loss: 0.0000\n",
      "Epoch 940, Loss: 0.0000\n",
      "Epoch 950, Loss: 0.0000\n",
      "Epoch 960, Loss: 0.0000\n",
      "Epoch 970, Loss: 0.0000\n",
      "Epoch 980, Loss: 0.0000\n",
      "Epoch 990, Loss: 0.0000\n",
      "Test Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "# Parametri\n",
    "hidden_layers = [10, 8]  # Lista che specifica il numero di neuroni per ogni hidden layer\n",
    "learning_rate = 0.05\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "loss_function = \"mse\"\n",
    "activation_function = \"relu\"\n",
    "\n",
    "# Caricamento dataset\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(1, one_hot_encode=True)\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "# Inizializzazione pesi e bias\n",
    "np.random.seed(42)\n",
    "layers = [input_size] + hidden_layers + [output_size]\n",
    "W = [np.random.randn(layers[i], layers[i+1]) * np.sqrt(2 / layers[i]) for i in range(len(layers) - 1)]\n",
    "b = [np.zeros((1, layers[i+1])) for i in range(len(layers) - 1)]\n",
    "\n",
    "# Funzioni di attivazione\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def activation(x):\n",
    "    return sigmoid(x) if activation_function == \"sigmoid\" else relu(x)\n",
    "\n",
    "def activation_derivative(x):\n",
    "    return sigmoid_derivative(x) if activation_function == \"sigmoid\" else relu_derivative(x)\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X):\n",
    "    A = [X]\n",
    "    Z = []\n",
    "    for i in range(len(W) - 1):\n",
    "        Z.append(np.dot(A[-1], W[i]) + b[i])\n",
    "        A.append(activation(Z[-1]))\n",
    "    \n",
    "    Z.append(np.dot(A[-1], W[-1]) + b[-1])\n",
    "    A.append(sigmoid(Z[-1]))  # Output layer usa sigmoide\n",
    "    return Z, A\n",
    "\n",
    "# Funzione di perdita\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def compute_loss(y_true, y_pred):\n",
    "    return binary_crossentropy_loss(y_true, y_pred) if loss_function == \"binary_crossentropy\" else mse_loss(y_true, y_pred)\n",
    "\n",
    "# Backward propagation\n",
    "def backward_propagation(X, y, Z, A):\n",
    "    global W, b\n",
    "    m = X.shape[0]\n",
    "    dZ = A[-1] - y  # Output layer\n",
    "    \n",
    "    dW = [np.dot(A[-2].T, dZ) / m]\n",
    "    db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "    \n",
    "    for i in range(len(W) - 2, -1, -1):\n",
    "        dZ = np.dot(dZ, W[i+1].T) * activation_derivative(A[i+1])\n",
    "        dW.insert(0, np.dot(A[i].T, dZ) / m)\n",
    "        db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "    \n",
    "    for i in range(len(W)):\n",
    "        W[i] -= learning_rate * dW[i]\n",
    "        b[i] -= learning_rate * db[i]\n",
    "\n",
    "# Training\n",
    "loss_history = []\n",
    "for epoch in range(epochs):\n",
    "    permutation = np.random.permutation(X_train.shape[0])\n",
    "    X_train_shuffled = X_train[permutation]\n",
    "    y_train_shuffled = y_train[permutation]\n",
    "    \n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        X_batch = X_train_shuffled[i:i+batch_size]\n",
    "        y_batch = y_train_shuffled[i:i+batch_size]\n",
    "        \n",
    "        Z, A = forward_propagation(X_batch)\n",
    "        backward_propagation(X_batch, y_batch, Z, A)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        loss = compute_loss(y_train, forward_propagation(X_train)[1][-1])\n",
    "        loss_history.append(loss)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Test\n",
    "_, A_test = forward_propagation(X_test)\n",
    "predictions = (A_test[-1] > 0.5).astype(int)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "pd.Series(loss_history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-3-train.csv\n",
      "Using cached ../datasets/monks/monk-3-test.csv\n",
      "One-hot encoding MONK-3 dataset...\n",
      "Epoch 0, Loss: 0.2639\n",
      "Epoch 10, Loss: 0.2558\n",
      "Epoch 20, Loss: 0.2508\n",
      "Epoch 30, Loss: 0.2464\n",
      "Epoch 40, Loss: 0.2422\n",
      "Epoch 50, Loss: 0.2377\n",
      "Epoch 60, Loss: 0.2324\n",
      "Epoch 70, Loss: 0.2258\n",
      "Epoch 80, Loss: 0.2173\n",
      "Epoch 90, Loss: 0.2077\n",
      "Epoch 100, Loss: 0.1958\n",
      "Epoch 110, Loss: 0.1826\n",
      "Epoch 120, Loss: 0.1693\n",
      "Epoch 130, Loss: 0.1560\n",
      "Epoch 140, Loss: 0.1421\n",
      "Epoch 150, Loss: 0.1282\n",
      "Epoch 160, Loss: 0.1134\n",
      "Epoch 170, Loss: 0.1000\n",
      "Epoch 180, Loss: 0.0889\n",
      "Epoch 190, Loss: 0.0798\n",
      "Epoch 200, Loss: 0.0732\n",
      "Epoch 210, Loss: 0.0672\n",
      "Epoch 220, Loss: 0.0628\n",
      "Epoch 230, Loss: 0.0600\n",
      "Epoch 240, Loss: 0.0578\n",
      "Epoch 250, Loss: 0.0560\n",
      "Epoch 260, Loss: 0.0547\n",
      "Epoch 270, Loss: 0.0534\n",
      "Epoch 280, Loss: 0.0524\n",
      "Epoch 290, Loss: 0.0515\n",
      "Epoch 300, Loss: 0.0507\n",
      "Epoch 310, Loss: 0.0502\n",
      "Epoch 320, Loss: 0.0495\n",
      "Epoch 330, Loss: 0.0487\n",
      "Epoch 340, Loss: 0.0481\n",
      "Epoch 350, Loss: 0.0478\n",
      "Epoch 360, Loss: 0.0473\n",
      "Epoch 370, Loss: 0.0467\n",
      "Epoch 380, Loss: 0.0463\n",
      "Epoch 390, Loss: 0.0460\n",
      "Epoch 400, Loss: 0.0457\n",
      "Epoch 410, Loss: 0.0453\n",
      "Epoch 420, Loss: 0.0451\n",
      "Epoch 430, Loss: 0.0447\n",
      "Epoch 440, Loss: 0.0443\n",
      "Epoch 450, Loss: 0.0440\n",
      "Epoch 460, Loss: 0.0439\n",
      "Epoch 470, Loss: 0.0435\n",
      "Epoch 480, Loss: 0.0434\n",
      "Epoch 490, Loss: 0.0430\n",
      "Epoch 500, Loss: 0.0427\n",
      "Epoch 510, Loss: 0.0426\n",
      "Epoch 520, Loss: 0.0422\n",
      "Epoch 530, Loss: 0.0419\n",
      "Epoch 540, Loss: 0.0417\n",
      "Epoch 550, Loss: 0.0414\n",
      "Epoch 560, Loss: 0.0411\n",
      "Epoch 570, Loss: 0.0408\n",
      "Epoch 580, Loss: 0.0407\n",
      "Epoch 590, Loss: 0.0404\n",
      "Epoch 600, Loss: 0.0400\n",
      "Epoch 610, Loss: 0.0400\n",
      "Epoch 620, Loss: 0.0397\n",
      "Epoch 630, Loss: 0.0396\n",
      "Epoch 640, Loss: 0.0392\n",
      "Epoch 650, Loss: 0.0390\n",
      "Epoch 660, Loss: 0.0388\n",
      "Epoch 670, Loss: 0.0389\n",
      "Epoch 680, Loss: 0.0386\n",
      "Epoch 690, Loss: 0.0384\n",
      "Epoch 700, Loss: 0.0382\n",
      "Epoch 710, Loss: 0.0380\n",
      "Epoch 720, Loss: 0.0377\n",
      "Epoch 730, Loss: 0.0378\n",
      "Epoch 740, Loss: 0.0375\n",
      "Epoch 750, Loss: 0.0374\n",
      "Epoch 760, Loss: 0.0371\n",
      "Epoch 770, Loss: 0.0368\n",
      "Epoch 780, Loss: 0.0367\n",
      "Epoch 790, Loss: 0.0367\n",
      "Epoch 800, Loss: 0.0364\n",
      "Epoch 810, Loss: 0.0363\n",
      "Epoch 820, Loss: 0.0362\n",
      "Epoch 830, Loss: 0.0360\n",
      "Epoch 840, Loss: 0.0358\n",
      "Epoch 850, Loss: 0.0357\n",
      "Epoch 860, Loss: 0.0355\n",
      "Epoch 870, Loss: 0.0354\n",
      "Epoch 880, Loss: 0.0353\n",
      "Epoch 890, Loss: 0.0351\n",
      "Epoch 900, Loss: 0.0349\n",
      "Epoch 910, Loss: 0.0348\n",
      "Epoch 920, Loss: 0.0347\n",
      "Epoch 930, Loss: 0.0346\n",
      "Epoch 940, Loss: 0.0344\n",
      "Epoch 950, Loss: 0.0343\n",
      "Epoch 960, Loss: 0.0342\n",
      "Epoch 970, Loss: 0.0340\n",
      "Epoch 980, Loss: 0.0339\n",
      "Epoch 990, Loss: 0.0338\n",
      "Test Accuracy: 0.9306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "# Parametri per la rete neurale\n",
    "hidden_layers = [4, 4]\n",
    "learning_rate = 0.02\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "loss_function = \"mse\"  # \"mse\" oppure \"binary_crossentropy\"\n",
    "activation_function = \"relu\"\n",
    "lambda_reg = 0.0005  # Coefficiente di regolarizzazione L2 (Tikhonov)\n",
    "\n",
    "# Caricamento dataset\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(3, one_hot_encode=True)\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "# Inizializzazione pesi e bias\n",
    "np.random.seed(42)\n",
    "layers = [input_size] + hidden_layers + [output_size]\n",
    "W = [np.random.randn(layers[i], layers[i+1]) * np.sqrt(2 / layers[i]) for i in range(len(layers) - 1)]\n",
    "b = [np.zeros((1, layers[i+1])) for i in range(len(layers) - 1)]\n",
    "\n",
    "# Funzioni di attivazione\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def activation(x):\n",
    "    return sigmoid(x) if activation_function == \"sigmoid\" else relu(x)\n",
    "\n",
    "def activation_derivative(x):\n",
    "    return sigmoid_derivative(x) if activation_function == \"sigmoid\" else relu_derivative(x)\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X):\n",
    "    A = [X]\n",
    "    Z = []\n",
    "    for i in range(len(W) - 1):\n",
    "        Z.append(np.dot(A[-1], W[i]) + b[i])\n",
    "        A.append(activation(Z[-1]))\n",
    "    Z.append(np.dot(A[-1], W[-1]) + b[-1])\n",
    "    A.append(sigmoid(Z[-1]))  # Output layer usa sigmoide\n",
    "    return Z, A\n",
    "\n",
    "# Funzioni di perdita\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def compute_loss(y_true, y_pred):\n",
    "    loss = binary_crossentropy_loss(y_true, y_pred) if loss_function == \"binary_crossentropy\" else mse_loss(y_true, y_pred)\n",
    "    reg_term = (lambda_reg / 2) * sum(np.sum(W_i ** 2) for W_i in W)  # L2 regularization\n",
    "    return loss + reg_term\n",
    "\n",
    "# Derivata della loss\n",
    "def loss_derivative(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    if loss_function == \"mse\":\n",
    "        return y_pred - y_true\n",
    "    elif loss_function == \"binary_crossentropy\":\n",
    "        return (y_pred - y_true) / (y_pred * (1 - y_pred))\n",
    "\n",
    "# Backward propagation con regolarizzazione L2\n",
    "def backward_propagation(X, y, Z, A):\n",
    "    global W, b\n",
    "    m = X.shape[0]\n",
    "    dZ = loss_derivative(y, A[-1])\n",
    "    \n",
    "    dW = [np.dot(A[-2].T, dZ) / m + lambda_reg * W[-1] / m]  # Aggiunta della regolarizzazione\n",
    "    db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "    \n",
    "    for i in range(len(W) - 2, -1, -1):\n",
    "        dZ = np.dot(dZ, W[i+1].T) * activation_derivative(A[i+1])\n",
    "        dW.insert(0, np.dot(A[i].T, dZ) / m + lambda_reg * W[i] / m)\n",
    "        db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "    \n",
    "    for i in range(len(W)):\n",
    "        W[i] -= learning_rate * dW[i]\n",
    "        b[i] -= learning_rate * db[i]\n",
    "\n",
    "# Training\n",
    "loss_history = []\n",
    "for epoch in range(epochs):\n",
    "    permutation = np.random.permutation(X_train.shape[0])\n",
    "    X_train_shuffled = X_train[permutation]\n",
    "    y_train_shuffled = y_train[permutation]\n",
    "    \n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        X_batch = X_train_shuffled[i:i+batch_size]\n",
    "        y_batch = y_train_shuffled[i:i+batch_size]\n",
    "        Z, A = forward_propagation(X_batch)\n",
    "        backward_propagation(X_batch, y_batch, Z, A)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        loss = compute_loss(y_train, forward_propagation(X_train)[1][-1])\n",
    "        loss_history.append(loss)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Test\n",
    "_, A_test = forward_propagation(X_test)\n",
    "predictions = (A_test[-1] > 0.5).astype(int)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot della loss nel tempo\n",
    "pd.Series(loss_history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-3-train.csv\n",
      "Using cached ../datasets/monks/monk-3-test.csv\n",
      "One-hot encoding MONK-3 dataset...\n",
      "Epoch 0, Loss: 1.5741\n",
      "Epoch 10, Loss: 1.5251\n",
      "Epoch 20, Loss: 1.4939\n",
      "Epoch 30, Loss: 1.4515\n",
      "Epoch 40, Loss: 1.5236\n",
      "Epoch 50, Loss: 1.9696\n",
      "Epoch 60, Loss: 21.0582\n",
      "Epoch 70, Loss: 21.0413\n",
      "Epoch 80, Loss: 147.5815\n",
      "Epoch 90, Loss: 145.9716\n",
      "Epoch 100, Loss: 146.4080\n",
      "Epoch 110, Loss: 145.5904\n",
      "Epoch 120, Loss: 144.8174\n",
      "Epoch 130, Loss: 144.0553\n",
      "Epoch 140, Loss: 143.2988\n",
      "Epoch 150, Loss: 142.5465\n",
      "Epoch 160, Loss: 141.7983\n",
      "Epoch 170, Loss: 141.0540\n",
      "Epoch 180, Loss: 140.3137\n",
      "Epoch 190, Loss: 139.5772\n",
      "Epoch 200, Loss: 138.8446\n",
      "Epoch 210, Loss: 138.1159\n",
      "Epoch 220, Loss: 137.3911\n",
      "Epoch 230, Loss: 136.6700\n",
      "Epoch 240, Loss: 135.9528\n",
      "Epoch 250, Loss: 135.2393\n",
      "Epoch 260, Loss: 134.5296\n",
      "Epoch 270, Loss: 133.8237\n",
      "Epoch 280, Loss: 133.1215\n",
      "Epoch 290, Loss: 132.4230\n",
      "Epoch 300, Loss: 131.7281\n",
      "Epoch 310, Loss: 131.0370\n",
      "Epoch 320, Loss: 130.3494\n",
      "Epoch 330, Loss: 129.6655\n",
      "Epoch 340, Loss: 128.9853\n",
      "Epoch 350, Loss: 128.3086\n",
      "Epoch 360, Loss: 127.6354\n",
      "Epoch 370, Loss: 126.9658\n",
      "Epoch 380, Loss: 126.2998\n",
      "Epoch 390, Loss: 125.6373\n",
      "Epoch 400, Loss: 124.9782\n",
      "Epoch 410, Loss: 124.3227\n",
      "Epoch 420, Loss: 123.6705\n",
      "Epoch 430, Loss: 123.0219\n",
      "Epoch 440, Loss: 122.3766\n",
      "Epoch 450, Loss: 121.7348\n",
      "Epoch 460, Loss: 121.0964\n",
      "Epoch 470, Loss: 120.4612\n",
      "Epoch 480, Loss: 119.8295\n",
      "Epoch 490, Loss: 119.2011\n",
      "Epoch 500, Loss: 118.5760\n",
      "Epoch 510, Loss: 117.9542\n",
      "Epoch 520, Loss: 117.3357\n",
      "Epoch 530, Loss: 116.7204\n",
      "Epoch 540, Loss: 116.1084\n",
      "Epoch 550, Loss: 115.4997\n",
      "Epoch 560, Loss: 114.8941\n",
      "Epoch 570, Loss: 114.2917\n",
      "Epoch 580, Loss: 113.6925\n",
      "Epoch 590, Loss: 113.0965\n",
      "Epoch 600, Loss: 112.5036\n",
      "Epoch 610, Loss: 111.9138\n",
      "Epoch 620, Loss: 111.3272\n",
      "Epoch 630, Loss: 110.7436\n",
      "Epoch 640, Loss: 110.1631\n",
      "Epoch 650, Loss: 109.5857\n",
      "Epoch 660, Loss: 109.0113\n",
      "Epoch 670, Loss: 108.4400\n",
      "Epoch 680, Loss: 107.8717\n",
      "Epoch 690, Loss: 107.3063\n",
      "Epoch 700, Loss: 106.7440\n",
      "Epoch 710, Loss: 106.1846\n",
      "Epoch 720, Loss: 105.6282\n",
      "Epoch 730, Loss: 105.0747\n",
      "Epoch 740, Loss: 104.5241\n",
      "Epoch 750, Loss: 103.9764\n",
      "Epoch 760, Loss: 103.4316\n",
      "Epoch 770, Loss: 102.8897\n",
      "Epoch 780, Loss: 102.3507\n",
      "Epoch 790, Loss: 101.8144\n",
      "Epoch 800, Loss: 101.2811\n",
      "Epoch 810, Loss: 100.7505\n",
      "Epoch 820, Loss: 100.2227\n",
      "Epoch 830, Loss: 99.6977\n",
      "Epoch 840, Loss: 99.1755\n",
      "Epoch 850, Loss: 98.6561\n",
      "Epoch 860, Loss: 98.1393\n",
      "Epoch 870, Loss: 97.6253\n",
      "Epoch 880, Loss: 97.1140\n",
      "Epoch 890, Loss: 96.6055\n",
      "Epoch 900, Loss: 96.0995\n",
      "Epoch 910, Loss: 95.5963\n",
      "Epoch 920, Loss: 95.0957\n",
      "Epoch 930, Loss: 94.5978\n",
      "Epoch 940, Loss: 94.1025\n",
      "Epoch 950, Loss: 93.6098\n",
      "Epoch 960, Loss: 93.1197\n",
      "Epoch 970, Loss: 92.6321\n",
      "Epoch 980, Loss: 92.1472\n",
      "Epoch 990, Loss: 91.6648\n",
      "Test Accuracy: 0.4722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "# Parametri per la rete neurale\n",
    "hidden_layers = [4, 4]\n",
    "learning_rate = 0.02\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "loss_function = \"binary_crossentropy\"  # \"mse\" oppure \"binary_crossentropy\"\n",
    "activation_function = \"relu\"\n",
    "lambda_reg = 0.1  # Coefficiente di regolarizzazione L2 (Tikhonov)\n",
    "\n",
    "# Caricamento dataset\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(3, one_hot_encode=True)\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "# Inizializzazione pesi e bias\n",
    "np.random.seed(42)\n",
    "layers = [input_size] + hidden_layers + [output_size]\n",
    "W = [np.random.randn(layers[i], layers[i+1]) * np.sqrt(2 / layers[i]) for i in range(len(layers) - 1)]\n",
    "b = [np.zeros((1, layers[i+1])) for i in range(len(layers) - 1)]\n",
    "\n",
    "# Funzioni di attivazione\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def activation(x):\n",
    "    return sigmoid(x) if activation_function == \"sigmoid\" else relu(x)\n",
    "\n",
    "def activation_derivative(x):\n",
    "    return sigmoid_derivative(x) if activation_function == \"sigmoid\" else relu_derivative(x)\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X):\n",
    "    A = [X]\n",
    "    Z = []\n",
    "    for i in range(len(W) - 1):\n",
    "        Z.append(np.dot(A[-1], W[i]) + b[i])\n",
    "        A.append(activation(Z[-1]))\n",
    "    Z.append(np.dot(A[-1], W[-1]) + b[-1])\n",
    "    A.append(sigmoid(Z[-1]))  # Output layer usa sigmoide\n",
    "    return Z, A\n",
    "\n",
    "# Funzioni di perdita\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def compute_loss(y_true, y_pred):\n",
    "    loss = binary_crossentropy_loss(y_true, y_pred) if loss_function == \"binary_crossentropy\" else mse_loss(y_true, y_pred)\n",
    "    reg_term = (lambda_reg / 2) * sum(np.sum(W_i ** 2) for W_i in W)  # L2 regularization\n",
    "    return loss + reg_term\n",
    "\n",
    "# Derivata della loss\n",
    "def loss_derivative(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    if loss_function == \"mse\":\n",
    "        return y_pred - y_true\n",
    "    elif loss_function == \"binary_crossentropy\":\n",
    "        return (y_pred - y_true) / (y_pred * (1 - y_pred))\n",
    "\n",
    "# Backward propagation con regolarizzazione L2\n",
    "def backward_propagation(X, y, Z, A):\n",
    "    global W, b\n",
    "    m = X.shape[0]\n",
    "    dZ = loss_derivative(y, A[-1])\n",
    "    \n",
    "    dW = [np.dot(A[-2].T, dZ) / m + lambda_reg * W[-1] / m]  # Aggiunta della regolarizzazione\n",
    "    db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "    \n",
    "    for i in range(len(W) - 2, -1, -1):\n",
    "        dZ = np.dot(dZ, W[i+1].T) * activation_derivative(A[i+1])\n",
    "        dW.insert(0, np.dot(A[i].T, dZ) / m + lambda_reg * W[i] / m)\n",
    "        db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "    \n",
    "    for i in range(len(W)):\n",
    "        W[i] -= learning_rate * dW[i]\n",
    "        b[i] -= learning_rate * db[i]\n",
    "\n",
    "# Training\n",
    "loss_history = []\n",
    "for epoch in range(epochs):\n",
    "    permutation = np.random.permutation(X_train.shape[0])\n",
    "    X_train_shuffled = X_train[permutation]\n",
    "    y_train_shuffled = y_train[permutation]\n",
    "    \n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        X_batch = X_train_shuffled[i:i+batch_size]\n",
    "        y_batch = y_train_shuffled[i:i+batch_size]\n",
    "        Z, A = forward_propagation(X_batch)\n",
    "        backward_propagation(X_batch, y_batch, Z, A)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        loss = compute_loss(y_train, forward_propagation(X_train)[1][-1])\n",
    "        loss_history.append(loss)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Test\n",
    "_, A_test = forward_propagation(X_test)\n",
    "predictions = (A_test[-1] > 0.5).astype(int)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot della loss nel tempo\n",
    "pd.Series(loss_history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-3-train.csv\n",
      "Using cached ../datasets/monks/monk-3-test.csv\n",
      "One-hot encoding MONK-3 dataset...\n",
      "Epoch 0, Loss: 0.8021\n",
      "Epoch 10, Loss: 0.7914\n",
      "Epoch 20, Loss: 0.7829\n",
      "Epoch 30, Loss: 0.7759\n",
      "Epoch 40, Loss: 0.7700\n",
      "Epoch 50, Loss: 0.7648\n",
      "Epoch 60, Loss: 0.7603\n",
      "Epoch 70, Loss: 0.7560\n",
      "Epoch 80, Loss: 0.7519\n",
      "Epoch 90, Loss: 0.7482\n",
      "Epoch 100, Loss: 0.7445\n",
      "Epoch 110, Loss: 0.7410\n",
      "Epoch 120, Loss: 0.7377\n",
      "Epoch 130, Loss: 0.7345\n",
      "Epoch 140, Loss: 0.7315\n",
      "Epoch 150, Loss: 0.7285\n",
      "Epoch 160, Loss: 0.7253\n",
      "Epoch 170, Loss: 0.7221\n",
      "Epoch 180, Loss: 0.7187\n",
      "Epoch 190, Loss: 0.7152\n",
      "Epoch 200, Loss: 0.7115\n",
      "Epoch 210, Loss: 0.7076\n",
      "Epoch 220, Loss: 0.7034\n",
      "Epoch 230, Loss: 0.6992\n",
      "Epoch 240, Loss: 0.6948\n",
      "Epoch 250, Loss: 0.6903\n",
      "Epoch 260, Loss: 0.6858\n",
      "Epoch 270, Loss: 0.6808\n",
      "Epoch 280, Loss: 0.6756\n",
      "Epoch 290, Loss: 0.6701\n",
      "Epoch 300, Loss: 0.6645\n",
      "Epoch 310, Loss: 0.6586\n",
      "Epoch 320, Loss: 0.6523\n",
      "Epoch 330, Loss: 0.6461\n",
      "Epoch 340, Loss: 0.6397\n",
      "Epoch 350, Loss: 0.6326\n",
      "Epoch 360, Loss: 0.6249\n",
      "Epoch 370, Loss: 0.6172\n",
      "Epoch 380, Loss: 0.6092\n",
      "Epoch 390, Loss: 0.6005\n",
      "Epoch 400, Loss: 0.5919\n",
      "Epoch 410, Loss: 0.5836\n",
      "Epoch 420, Loss: 0.5747\n",
      "Epoch 430, Loss: 0.5640\n",
      "Epoch 440, Loss: 0.5529\n",
      "Epoch 450, Loss: 0.5410\n",
      "Epoch 460, Loss: 0.5293\n",
      "Epoch 470, Loss: 0.5164\n",
      "Epoch 480, Loss: 0.5036\n",
      "Epoch 490, Loss: 0.4907\n",
      "Epoch 500, Loss: 0.4788\n",
      "Epoch 510, Loss: 0.4674\n",
      "Epoch 520, Loss: 0.4571\n",
      "Epoch 530, Loss: 0.4477\n",
      "Epoch 540, Loss: 0.4390\n",
      "Epoch 550, Loss: 0.4313\n",
      "Epoch 560, Loss: 0.4241\n",
      "Epoch 570, Loss: 0.4174\n",
      "Epoch 580, Loss: 0.4110\n",
      "Epoch 590, Loss: 0.4050\n",
      "Epoch 600, Loss: 0.3995\n",
      "Epoch 610, Loss: 0.3947\n",
      "Epoch 620, Loss: 0.3899\n",
      "Epoch 630, Loss: 0.3857\n",
      "Epoch 640, Loss: 0.3817\n",
      "Epoch 650, Loss: 0.3777\n",
      "Epoch 660, Loss: 0.3744\n",
      "Epoch 670, Loss: 0.3710\n",
      "Epoch 680, Loss: 0.3680\n",
      "Epoch 690, Loss: 0.3651\n",
      "Epoch 700, Loss: 0.3626\n",
      "Epoch 710, Loss: 0.3603\n",
      "Epoch 720, Loss: 0.3578\n",
      "Epoch 730, Loss: 0.3556\n",
      "Epoch 740, Loss: 0.3539\n",
      "Epoch 750, Loss: 0.3523\n",
      "Epoch 760, Loss: 0.3506\n",
      "Epoch 770, Loss: 0.3489\n",
      "Epoch 780, Loss: 0.3475\n",
      "Epoch 790, Loss: 0.3459\n",
      "Epoch 800, Loss: 0.3446\n",
      "Epoch 810, Loss: 0.3436\n",
      "Epoch 820, Loss: 0.3424\n",
      "Epoch 830, Loss: 0.3410\n",
      "Epoch 840, Loss: 0.3400\n",
      "Epoch 850, Loss: 0.3391\n",
      "Epoch 860, Loss: 0.3381\n",
      "Epoch 870, Loss: 0.3374\n",
      "Epoch 880, Loss: 0.3365\n",
      "Epoch 890, Loss: 0.3358\n",
      "Epoch 900, Loss: 0.3350\n",
      "Epoch 910, Loss: 0.3341\n",
      "Epoch 920, Loss: 0.3334\n",
      "Epoch 930, Loss: 0.3327\n",
      "Epoch 940, Loss: 0.3321\n",
      "Epoch 950, Loss: 0.3315\n",
      "Epoch 960, Loss: 0.3309\n",
      "Epoch 970, Loss: 0.3304\n",
      "Epoch 980, Loss: 0.3300\n",
      "Epoch 990, Loss: 0.3295\n",
      "Test Accuracy: 0.9722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "# Parametri per la rete neurale\n",
    "hidden_layers = [4, 4]\n",
    "learning_rate = 0.9\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "loss_function = \"binary_crossentropy\"  # \"mse\" oppure \"binary_crossentropy\"\n",
    "activation_function = \"relu\"  # Attivazione per i layer nascosti\n",
    "output_activation_function = \"sigmoid\"  # Attivazione per l'output layer\n",
    "lambda_reg = 0.01  # Coefficiente di regolarizzazione L2 (Tikhonov)\n",
    "\n",
    "# Caricamento dataset\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(3, one_hot_encode=True)\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "# Inizializzazione pesi e bias\n",
    "np.random.seed(42)\n",
    "layers = [input_size] + hidden_layers + [output_size]\n",
    "W = [np.random.randn(layers[i], layers[i+1]) * np.sqrt(2 / layers[i]) for i in range(len(layers) - 1)]\n",
    "b = [np.zeros((1, layers[i+1])) for i in range(len(layers) - 1)]\n",
    "\n",
    "# Funzioni di attivazione\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    # a = sigmoid(x)\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Funzione per i layer nascosti (parametrica)\n",
    "def activation(x):\n",
    "    return sigmoid(x) if activation_function == \"sigmoid\" else relu(x)\n",
    "\n",
    "def activation_derivative(a):\n",
    "    return sigmoid_derivative(a) if activation_function == \"sigmoid\" else relu_derivative(a)\n",
    "\n",
    "# Funzione per l'output layer (parametrica)\n",
    "def output_activation(x):\n",
    "    return sigmoid(x) if output_activation_function == \"sigmoid\" else relu(x)\n",
    "\n",
    "def output_activation_derivative(a):\n",
    "    return sigmoid_derivative(a) if output_activation_function == \"sigmoid\" else relu_derivative(a)\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X):\n",
    "    A = [X]\n",
    "    Z = []\n",
    "    # Calcolo per i layer nascosti\n",
    "    for i in range(len(W) - 1):\n",
    "        Z_curr = np.dot(A[-1], W[i]) + b[i]\n",
    "        Z.append(Z_curr)\n",
    "        A.append(activation(Z_curr))\n",
    "    # Output layer: usiamo la funzione di attivazione parametrica\n",
    "    Z_output = np.dot(A[-1], W[-1]) + b[-1]\n",
    "    Z.append(Z_output)\n",
    "    A.append(output_activation(Z_output))\n",
    "    return Z, A\n",
    "\n",
    "# Funzioni di perdita\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def compute_loss(y_true, y_pred):\n",
    "    loss = binary_crossentropy_loss(y_true, y_pred) if loss_function == \"binary_crossentropy\" else mse_loss(y_true, y_pred)\n",
    "    reg_term = (lambda_reg / 2) * sum(np.sum(W_i ** 2) for W_i in W)  # L2 regularization\n",
    "    return loss + reg_term\n",
    "\n",
    "# Derivata della loss rispetto all'output A (senza considerare l'attivazione)\n",
    "def loss_derivative(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    if loss_function == \"mse\":\n",
    "        return (y_pred - y_true) / m\n",
    "    elif loss_function == \"binary_crossentropy\":\n",
    "        # Nota: se l'output activation è sigmoid, la derivata combinata (loss + sigmoide) si semplifica\n",
    "        # Tuttavia, per mantenere tutto parametrico, calcoliamo la derivata della loss e poi moltiplichiamo per quella dell'attivazione\n",
    "        return (y_pred - y_true) / m\n",
    "\n",
    "# Backward propagation con regolarizzazione L2\n",
    "def backward_propagation(X, y, Z, A):\n",
    "    global W, b\n",
    "    m = X.shape[0]\n",
    "    # Calcolo del gradiente per l'output layer\n",
    "    dA_output = loss_derivative(y, A[-1])\n",
    "    # Moltiplichiamo per la derivata della funzione di attivazione dell'output\n",
    "    dZ_output = dA_output * output_activation_derivative(A[-1])\n",
    "    dW = [np.dot(A[-2].T, dZ_output) / m + lambda_reg * W[-1] / m]\n",
    "    db = [np.sum(dZ_output, axis=0, keepdims=True) / m]\n",
    "    \n",
    "    # Propagazione all'indietro per i layer nascosti\n",
    "    dZ = dZ_output  # inizialmente dZ per l'output layer\n",
    "    for i in range(len(W) - 2, -1, -1):\n",
    "        # dA del layer corrente (calcolato tramite il layer successivo)\n",
    "        dA = np.dot(dZ, W[i+1].T)\n",
    "        # Moltiplichiamo per la derivata dell'attivazione del layer nascosto\n",
    "        dZ = dA * activation_derivative(A[i+1])\n",
    "        dW.insert(0, np.dot(A[i].T, dZ) / m + lambda_reg * W[i] / m)\n",
    "        db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "    \n",
    "    # Aggiornamento dei pesi e bias\n",
    "    for i in range(len(W)):\n",
    "        W[i] -= learning_rate * dW[i]\n",
    "        b[i] -= learning_rate * db[i]\n",
    "\n",
    "# Training\n",
    "loss_history = []\n",
    "for epoch in range(epochs):\n",
    "    permutation = np.random.permutation(X_train.shape[0])\n",
    "    X_train_shuffled = X_train[permutation]\n",
    "    y_train_shuffled = y_train[permutation]\n",
    "    \n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        X_batch = X_train_shuffled[i:i+batch_size]\n",
    "        y_batch = y_train_shuffled[i:i+batch_size]\n",
    "        Z, A = forward_propagation(X_batch)\n",
    "        backward_propagation(X_batch, y_batch, Z, A)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        loss = compute_loss(y_train, forward_propagation(X_train)[1][-1])\n",
    "        loss_history.append(loss)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Test\n",
    "_, A_test = forward_propagation(X_test)\n",
    "predictions = (A_test[-1] > 0.5).astype(int)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot della loss nel tempo\n",
    "pd.Series(loss_history).plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-3-train.csv\n",
      "Using cached ../datasets/monks/monk-3-test.csv\n",
      "One-hot encoding MONK-3 dataset...\n",
      "Epoch 0, Loss: 0.2171\n",
      "Epoch 10, Loss: 0.2129\n",
      "Epoch 20, Loss: 0.2092\n",
      "Epoch 30, Loss: 0.2059\n",
      "Epoch 40, Loss: 0.2030\n",
      "Epoch 50, Loss: 0.2003\n",
      "Epoch 60, Loss: 0.1978\n",
      "Epoch 70, Loss: 0.1954\n",
      "Epoch 80, Loss: 0.1932\n",
      "Epoch 90, Loss: 0.1910\n",
      "Epoch 100, Loss: 0.1889\n",
      "Epoch 110, Loss: 0.1869\n",
      "Epoch 120, Loss: 0.1850\n",
      "Epoch 130, Loss: 0.1832\n",
      "Epoch 140, Loss: 0.1814\n",
      "Epoch 150, Loss: 0.1797\n",
      "Epoch 160, Loss: 0.1779\n",
      "Epoch 170, Loss: 0.1762\n",
      "Epoch 180, Loss: 0.1745\n",
      "Epoch 190, Loss: 0.1728\n",
      "Epoch 200, Loss: 0.1710\n",
      "Epoch 210, Loss: 0.1693\n",
      "Epoch 220, Loss: 0.1675\n",
      "Epoch 230, Loss: 0.1657\n",
      "Epoch 240, Loss: 0.1640\n",
      "Epoch 250, Loss: 0.1622\n",
      "Epoch 260, Loss: 0.1605\n",
      "Epoch 270, Loss: 0.1587\n",
      "Epoch 280, Loss: 0.1569\n",
      "Epoch 290, Loss: 0.1550\n",
      "Epoch 300, Loss: 0.1531\n",
      "Epoch 310, Loss: 0.1512\n",
      "Epoch 320, Loss: 0.1493\n",
      "Epoch 330, Loss: 0.1474\n",
      "Epoch 340, Loss: 0.1455\n",
      "Epoch 350, Loss: 0.1434\n",
      "Epoch 360, Loss: 0.1413\n",
      "Epoch 370, Loss: 0.1392\n",
      "Epoch 380, Loss: 0.1370\n",
      "Epoch 390, Loss: 0.1347\n",
      "Epoch 400, Loss: 0.1325\n",
      "Epoch 410, Loss: 0.1304\n",
      "Epoch 420, Loss: 0.1281\n",
      "Epoch 430, Loss: 0.1255\n",
      "Epoch 440, Loss: 0.1228\n",
      "Epoch 450, Loss: 0.1199\n",
      "Epoch 460, Loss: 0.1172\n",
      "Epoch 470, Loss: 0.1142\n",
      "Epoch 480, Loss: 0.1114\n",
      "Epoch 490, Loss: 0.1086\n",
      "Epoch 500, Loss: 0.1061\n",
      "Epoch 510, Loss: 0.1037\n",
      "Epoch 520, Loss: 0.1016\n",
      "Epoch 530, Loss: 0.0997\n",
      "Epoch 540, Loss: 0.0980\n",
      "Epoch 550, Loss: 0.0965\n",
      "Epoch 560, Loss: 0.0951\n",
      "Epoch 570, Loss: 0.0938\n",
      "Epoch 580, Loss: 0.0926\n",
      "Epoch 590, Loss: 0.0915\n",
      "Epoch 600, Loss: 0.0905\n",
      "Epoch 610, Loss: 0.0896\n",
      "Epoch 620, Loss: 0.0888\n",
      "Epoch 630, Loss: 0.0880\n",
      "Epoch 640, Loss: 0.0873\n",
      "Epoch 650, Loss: 0.0866\n",
      "Epoch 660, Loss: 0.0860\n",
      "Epoch 670, Loss: 0.0854\n",
      "Epoch 680, Loss: 0.0849\n",
      "Epoch 690, Loss: 0.0844\n",
      "Epoch 700, Loss: 0.0839\n",
      "Epoch 710, Loss: 0.0835\n",
      "Epoch 720, Loss: 0.0830\n",
      "Epoch 730, Loss: 0.0826\n",
      "Epoch 740, Loss: 0.0823\n",
      "Epoch 750, Loss: 0.0819\n",
      "Epoch 760, Loss: 0.0816\n",
      "Epoch 770, Loss: 0.0812\n",
      "Epoch 780, Loss: 0.0809\n",
      "Epoch 790, Loss: 0.0806\n",
      "Epoch 800, Loss: 0.0803\n",
      "Epoch 810, Loss: 0.0801\n",
      "Epoch 820, Loss: 0.0798\n",
      "Epoch 830, Loss: 0.0795\n",
      "Epoch 840, Loss: 0.0793\n",
      "Epoch 850, Loss: 0.0791\n",
      "Epoch 860, Loss: 0.0788\n",
      "Epoch 870, Loss: 0.0786\n",
      "Epoch 880, Loss: 0.0784\n",
      "Epoch 890, Loss: 0.0782\n",
      "Epoch 900, Loss: 0.0780\n",
      "Epoch 910, Loss: 0.0778\n",
      "Epoch 920, Loss: 0.0776\n",
      "Epoch 930, Loss: 0.0775\n",
      "Epoch 940, Loss: 0.0773\n",
      "Epoch 950, Loss: 0.0771\n",
      "Epoch 960, Loss: 0.0770\n",
      "Epoch 970, Loss: 0.0768\n",
      "Epoch 980, Loss: 0.0767\n",
      "Epoch 990, Loss: 0.0765\n",
      "Test Accuracy: 0.9722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "# Parametri\n",
    "hidden_layers = [4, 4]\n",
    "learning_rate = 0.9\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "loss_function = \"mse\"  # oppure \"mse\"\n",
    "activation_function = \"relu\"           # oppure \"sigmoid\"\n",
    "output_activation_function = \"sigmoid\" # oppure \"relu\"\n",
    "lambda_reg = 0.01\n",
    "\n",
    "# Caricamento del dataset\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(3, one_hot_encode=True)\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "# Inizializzazione dei pesi e bias\n",
    "np.random.seed(42)\n",
    "layers = [input_size] + hidden_layers + [output_size]\n",
    "W = [np.random.randn(layers[i], layers[i+1]) * np.sqrt(2 / layers[i])\n",
    "     for i in range(len(layers) - 1)]\n",
    "b = [np.zeros((1, layers[i+1])) for i in range(len(layers) - 1)]\n",
    "\n",
    "# Funzioni di attivazione e loro derivate\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def activation(x):\n",
    "    if activation_function == \"sigmoid\":\n",
    "        return sigmoid(x)\n",
    "    else:\n",
    "        return relu(x)\n",
    "\n",
    "def activation_derivative(z, a):\n",
    "    \"\"\"\n",
    "    Utilizza i valori pre-attivazione (z) per il calcolo della derivata,\n",
    "    utile soprattutto per ReLU.\n",
    "    \"\"\"\n",
    "    if activation_function == \"sigmoid\":\n",
    "        return sigmoid_derivative(a)\n",
    "    else:\n",
    "        return relu_derivative(z)\n",
    "\n",
    "def output_activation(x):\n",
    "    if output_activation_function == \"sigmoid\":\n",
    "        return sigmoid(x)\n",
    "    else:\n",
    "        return relu(x)\n",
    "\n",
    "def output_activation_derivative(z, a):\n",
    "    if output_activation_function == \"sigmoid\":\n",
    "        return sigmoid_derivative(a)\n",
    "    else:\n",
    "        return relu_derivative(z)\n",
    "\n",
    "# Forward propagation: memorizza sia Z (pre-attivazione) che A (attivazione)\n",
    "def forward_propagation(X):\n",
    "    A = [X]\n",
    "    Z = []\n",
    "    # Layer nascosti\n",
    "    for i in range(len(W) - 1):\n",
    "        Z_curr = np.dot(A[-1], W[i]) + b[i]\n",
    "        Z.append(Z_curr)\n",
    "        A.append(activation(Z_curr))\n",
    "    # Layer di output\n",
    "    Z_output = np.dot(A[-1], W[-1]) + b[-1]\n",
    "    Z.append(Z_output)\n",
    "    A.append(output_activation(Z_output))\n",
    "    return Z, A\n",
    "\n",
    "# Funzioni di loss\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    # Usando 0.5 per semplificare la derivata\n",
    "    return 0.5 * np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def compute_loss(y_true, y_pred):\n",
    "    loss = (binary_crossentropy_loss(y_true, y_pred)\n",
    "            if loss_function == \"binary_crossentropy\" else mse_loss(y_true, y_pred))\n",
    "    reg_term = (lambda_reg / 2) * sum(np.sum(W_i ** 2) for W_i in W)\n",
    "    return loss + reg_term\n",
    "\n",
    "def loss_derivative(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    # Per entrambe le loss (con sigmoid in output e one-hot encoding per binary_crossentropy)\n",
    "    # la derivata risulta (y_pred - y_true) se la loss è definita in questo modo.\n",
    "    return (y_pred - y_true) / m\n",
    "\n",
    "# Backward propagation: calcola i gradienti e aggiorna i pesi e i bias\n",
    "def backward_propagation(X, y, Z, A):\n",
    "    global W, b\n",
    "    m = X.shape[0]\n",
    "    # Layer di output\n",
    "    dA = loss_derivative(y, A[-1])\n",
    "    dZ = dA * output_activation_derivative(Z[-1], A[-1])\n",
    "    dW = [np.dot(A[-2].T, dZ) / m + lambda_reg * W[-1] / m]\n",
    "    db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "\n",
    "    # Layer nascosti (iterando all'indietro)\n",
    "    for i in range(len(W) - 2, -1, -1):\n",
    "        dA = np.dot(dZ, W[i+1].T)\n",
    "        dZ = dA * activation_derivative(Z[i], A[i+1])\n",
    "        dW.insert(0, np.dot(A[i].T, dZ) / m + lambda_reg * W[i] / m)\n",
    "        db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "\n",
    "    # Aggiornamento dei pesi e dei bias\n",
    "    for i in range(len(W)):\n",
    "        W[i] -= learning_rate * dW[i]\n",
    "        b[i] -= learning_rate * db[i]\n",
    "\n",
    "# Ciclo di training\n",
    "loss_history = []\n",
    "for epoch in range(epochs):\n",
    "    permutation = np.random.permutation(X_train.shape[0])\n",
    "    X_train_shuffled = X_train[permutation]\n",
    "    y_train_shuffled = y_train[permutation]\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        X_batch = X_train_shuffled[i:i + batch_size]\n",
    "        y_batch = y_train_shuffled[i:i + batch_size]\n",
    "        Z, A = forward_propagation(X_batch)\n",
    "        backward_propagation(X_batch, y_batch, Z, A)\n",
    "    if epoch % 10 == 0:\n",
    "        Z_train, A_train = forward_propagation(X_train)\n",
    "        loss = compute_loss(y_train, A_train[-1])\n",
    "        loss_history.append(loss)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Valutazione sul test set\n",
    "Z_test, A_test = forward_propagation(X_test)\n",
    "if output_size == 1:\n",
    "    predictions = (A_test[-1] > 0.5).astype(int)\n",
    "    accuracy = np.mean(predictions == y_test)\n",
    "else:\n",
    "    predictions = np.argmax(A_test[-1], axis=1)\n",
    "    labels = np.argmax(y_test, axis=1)\n",
    "    accuracy = np.mean(predictions == labels)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "pd.Series(loss_history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-3-train.csv\n",
      "Using cached ../datasets/monks/monk-3-test.csv\n",
      "One-hot encoding MONK-3 dataset...\n",
      "Epoch 0, Loss: 0.2595\n",
      "Epoch 15, Loss: 0.2474\n",
      "Epoch 30, Loss: 0.2393\n",
      "Epoch 45, Loss: 0.2309\n",
      "Epoch 60, Loss: 0.2188\n",
      "Epoch 75, Loss: 0.2028\n",
      "Epoch 90, Loss: 0.1809\n",
      "Epoch 105, Loss: 0.1584\n",
      "Epoch 120, Loss: 0.1360\n",
      "Epoch 135, Loss: 0.1122\n",
      "Epoch 150, Loss: 0.0920\n",
      "Epoch 165, Loss: 0.0778\n",
      "Epoch 180, Loss: 0.0684\n",
      "Epoch 195, Loss: 0.0621\n",
      "Epoch 210, Loss: 0.0578\n",
      "Epoch 225, Loss: 0.0548\n",
      "Epoch 240, Loss: 0.0525\n",
      "Epoch 255, Loss: 0.0506\n",
      "Epoch 270, Loss: 0.0491\n",
      "Epoch 285, Loss: 0.0478\n",
      "Test Accuracy: 0.9606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPcpJREFUeJzt3Qd4VfX9x/FP9oAkjEBYYS9ZAVmCIlYRRBQElFErrqrl7yyioC1gqxUVpValqLQoalVEi3VQHAioEETZsveGQCCb7Pt/fr+QNEiAJCScO96v5znmnHvPvXyPJ5f74fzG8XO5XC4BAAC4MX+nCwAAADgXAgsAAHB7BBYAAOD2CCwAAMDtEVgAAIDbI7AAAAC3R2ABAABuj8ACAADcXqC8QH5+vg4cOKCIiAj5+fk5XQ4AACgFM3dtamqq6tWrJ39/f+8PLCasxMbGOl0GAAAoh71796pBgwbeH1jMlZXCA46MjHS6HAAAUAopKSn2gkPh97jXB5bCZiATVggsAAB4ltJ056DTLQAAcHsEFgAA4PYILAAAwO0RWAAAgNsjsAAAALdHYAEAAG6PwAIAANwegQUAALg9AgsAAHB7BBYAAOD2CCwAAMDtEVgAAIDbI7Ccw3PzN2nawm1yuVxOlwIAgM/yirs1V5bVe5P090Xb7fqGgymacmMHhQfzvwwAgAuNKyxn0TG2mp66oZ0C/f30+dqDGjo9XnuPZThdFgAAPofAcg6/uaSR3r3rEtWsEqyNB1M0aNoSxW9PdLosAAB8CoGlFLo1qaFP7r9M7epH6lh6tn7zzx80a+ku+rUAAHCBEFhKqX61MM25p6cGdaynvHyXJn2yXuM/Wqes3DynSwMAwOsRWMogLDhALw7vqMevbS1/P2n2T3s18vVlSkjJdLo0AAC8GoGljPz8/HT35c0087auiggN1Mo9Sbr+le/tiCIAAFA5CCzldEWr2vrkvsvUvHZVHU7J0rDX4vXRin1OlwUAgFcisJyHJtFVNPf/eqrPRbWVnZuvh+es0Z8/3aDcvHynSwMAwKsQWM5TRGiQXr+lix64srndnrlkp259Y7mOp2c7XRoAAF6DwFIB/P39NKZvK02/+WKFBwdoybZEO1/L5kOpTpcGAIDvBpZp06apcePGCg0NVffu3bV8+fIz7jtjxgz16tVL1atXt0ufPn1O2/+2226znVmLL9dcc408Tf/2dfXR6J5qUD1Me45laPDfl2j+zwedLgsAAN8LLLNnz9aYMWM0adIkrVy5UnFxcerXr58SEhJK3H/RokUaOXKkFi5cqPj4eMXGxqpv377av3//KfuZgHLw4MGi5b333pMnuqhupO2M27NZTWVk5+l376zU1K+2KD+fSeYAACgvP1cZp2s1V1S6du2qV155xW7n5+fbEHL//fdr/Pjx53x9Xl6evdJiXj9q1KiiKyxJSUn6+OOPy3UQKSkpioqKUnJysiIjI+UOTMfbv8zbqDeW7LLbV7eJ0V+Hd1TVEG6eCABAWb+/y3SFJTs7WytWrLDNOkVv4O9vt83Vk9LIyMhQTk6OatSocdqVmNq1a6tVq1YaPXq0EhM9+349gQH+mnR9W3uH5+AAf3214bAGT1uiXUfTnS4NAACPU6bAcvToUXuFJCYm5pTHzfahQ4dK9R7jxo1TvXr1Tgk9pjnorbfe0oIFC/Tss89q8eLF6t+/v/2zSpKVlWVTWfHFXd3UJVaz77lEtSNCtDUhTQNf+V7fbjnidFkAAHiUCzpK6JlnntH777+vuXPn2g67hUaMGKGBAweqffv2uuGGG/TZZ5/pxx9/tFddSjJ58mR7CalwMU1S7qxTw+r69P7L1KlhNaVk5uq2N5Zrxrc7uHkiAACVEViio6MVEBCgw4cPn/K42a5Tp85ZX/v888/bwPLll1+qQ4cOZ923adOm9s/atm1bic8/9thjtr2rcNm7d6/cXUxkqN6/+xLd1LmBTP9b079lzAdrlJnDzRMBAKjQwBIcHKzOnTvbpptCptOt2e7Ro8cZX/fcc8/pySef1Pz589WlS5dz/jn79u2zfVjq1q1b4vMhISG2c07xxROEBAbouRs76Inr2yjA309zV+3XTa/Ga3/SCadLAwDAu5qEzJBmM7fKrFmztHHjRttBNj09Xbfffrt93oz8MVdACpk+KRMmTNDMmTPt3C2mr4tZ0tLS7PPm5yOPPKJly5Zp165dNvwMGjRIzZs3t8OlvY2ZY+a2S5vo7Tu6qXp4kNbtT9ZVLyzSlC82KSUzx+nyAADwjsAyfPhw27wzceJEdezYUatXr7ZXTgo74u7Zs8fOo1Jo+vTpdnTRjTfeaK+YFC7mPQzTxLR27Vrbh6Vly5a688477VWc7777zl5J8VY9m0fb+Vq6NKquzJx8TVu4XVdMWaQ3luy09yUCAADnMQ+LO3LHeVhKy/zv/3LDYT07f5N2HCkY8tywRrge6ddKA9rXtdP+AwDgjcry/U1gcRNmornZP+3Vi19v1ZHULPtYhwZRGt+/tXo2i3a6PAAAKhyBxYOlZ+Xqn9/v1GuLtys9u2AE0a9a1dK4/q3Vuo5nHxsAAMURWLzA0bQsvbRgq979YY9y813y85OGXtxAY65uqXrVwpwuDwCA80Zg8SI7j6bbEUTz1hXMJBwS6K/bL22i0Vc0U1RYkNPlAQBQbgQWL7Rqz3FN/u8mLd95zG5XCw/Sfb9qrlt6NLLzuwAA4GkILF7KnKoFGxPsiCJzXyKjQfUwje3bSgPj6jGiCADgUQgsXs6MKPpo5T5N/WqLDqcUjChqWy9Sj/W/SJe1YEQRAMAzEFh8xInsPM1cslOvLtqu1Kxc+9jlLWtp/DWt1aae7/x/AAB4JgKLjzmWnq2Xv9mqd5btVk5ewYiiwR3ra0zflmpQPdzp8gAAKBGBxUftSczQlC8369M1B+x2cKC/buvZWPde0VxR4YwoAgC4FwKLj1u7L0mT521S/I7EohFFD17VQr+5pJGCAsp8+ygAACoFgQV2RNGiLUc0ed5GbTlcMKKoSXQVPda/ta5uE2PvGg0AgJMILDhlRNEHP5kRRZt1NC3bPta9SQ1NuK6N2tWPcro8AIAPSyGw4JfSsnI1fdE2/eO7ncrKzbcdc4d0amDvCl0nKtTp8gAAPiiFwIIz2Z90QlPmb9LHqws65oYG+evuXk11T+9mqhIS6HR5AAAfkkJgwbms2Zukpz7foB93HbfbtSJCNLZvS93YOVYBzJgLALgACCwoFXPqv1h/yN6jaHdihn2sdZ0I/XFAG2bMBQBUOgILyiQ7N19vxe/SSwu2KiWzYMbcK1vX1uPXtlbz2hFOlwcA8FIEFpTL8fRsvfTNVr0dv1u5+S7bNPTrbg31UJ8Wqlk1xOnyAABehsCC87LjSJqe+e8mfbnhsN2OCAnUvVc2t7PmhgYFOF0eAMBLEFhQIZbtSLQdc3/en2K361cL0/j+rXVdh7pMPAcAOG8EFlSY/HyXPl69X8/N36xDKZn2sU4Nq9mOuZ0bVXe6PACAByOwoMKdyM7TjO926NXF25WRnWcfG9ChrsZf01qxNbgjNACgcr+/uRMeSiUsOEAPXNVCi8ZeoeFdYu1MuZ+vPag+UxfroxX7nC4PAODlCCwok9qRoXr2xg6a90Av9Wha007z//CcNfrzpxvsfYsAAKgMBBaUy0V1I/Wv33a3V12MmUt2atTM5TqWXnCDRQAAKhKBBeXm7++nMVe31Ku/6awqwQFauj1RA1/5XhsOFIwqAgCgohBYcN6uaVdHc++9VI1rhmvf8RMaMn2JPl1TcHNFAAAqAoEFFaJlTIT+c+9lurxlLWXm5Ov+91bZyefy8j1+EBoAwA0QWFBhosKD9MZtXfW73s3sthkCfcebPyo5I8fp0gAAHo7Aggpl7j9kZsN9aWQnhQb5a/GWIxo07XttOZzqdGkAAA9GYEGlGBhXTx+N7mmn89+VmKHB05boi/WHnC4LAOChCCyoNG3rRenT+y+z87WkZ+fpnrdX6K9fbbHT/QMAUBYEFlSqGlWC9dad3XT7pY3t9t8WbNXdb69Qaib9WgAApUdgQaULCvDXpOvbasqNHRQc6K+vNx7W4L8v1Y4jaU6XBgDwEAQWXDA3dYnVB/f0UJ3IUG1LSNOgaUu0cFOC02UBADwAgQUXVMfYavrk/kvVuVF1pWbm6o5ZP+rvi7bJC24aDgCoRAQWXHC1I0L13l2X6NfdG8rklOfmb9Z9765SRnau06UBANwUgQWOMH1Znh7cXn8Z3E5BAX76fN1BDfn7Uu09luF0aQAAN0RggaNu7t5I7951iaKrhmjToVRd/8r3WrLtqNNlAQDcDIEFjuvauIY+vf9SxTWIUlJGjm755w/6x3c76NcCAChCYIFbqBsVptn39NDQixvIzCv31Ocb9fAHa5SZk+d0aQAAN0BggdsIDQrQ8zd10MTr2th7Ev171X6NeH0Zk8wBAAgscC9+fn6647ImevvObqoWHqTVe5P021k/caUFAHwcgQVuqWezaL19R3dVDQnUDzuP6d5/rVROXr7TZQEAHEJggdtq3yBK/7y1i0IC/bVgU4LGzlnDjRMBwEcRWODWujetqem/uViB/n76z+oDmvjJz4weAgAfRGCB27uydYymDu8oPz/pnWV79PyXm50uCQBwgRFY4BEGxtXTUze0s+vTFm7Xa4u3O10SAOACIrDAo2bFHXdNa7s++b+b9N7yPU6XBAC4QAgs8Cijr2im3/VuZtcfn7tOn6454HRJAIALgMACjzPumlZFd3r+/ezVWrg5wemSAACVjMACj5xc7slB7XR9XD3l5rs0+p0VWr7zmNNlAQAqEYEFHslM3T91WJx+1aqWMnPydeebP+rn/clOlwUAqCQEFnisoAB//f3mzurWpIZSs3J168zl2n4kzemyAACVgMACjxYWHGBnw21XP1KJ6dm65R8/aH/SCafLAgBUMAILPF5EaJBm3d5NzWpV0YHkTP3mHz/oSGqW02UBACoQgQVeoWbVEL3z2+6qXy1MO4+ma9TM5Uo+keN0WQCACkJggdeoGxVmQ0t01RBtPJhiO+JmZOc6XRYAoAIQWOBVmkRX0dt3dlNkaKB+2n1cv3tnpbJz850uCwBwnggs8DoX1Y3UG7d3U1hQgL7dcsROLpeXzx2eAcCTEVjglTo3qq7XR3VWUICfPl93UI//e51cZmpcAIBHIrDAa/VqUUsvjegkfz9p9k979fS8jYQWAPBQBBZ4tf7t6+qZoR3s+ozvdmrawm1OlwQAKAcCC7zesC6xmnBdG7v+/JdbNGvpLqdLAgCUEYEFPuHOy5rogata2PVJn6zX3FX7nC4JAFAGBBb4jN/3aaHbeja262PnrNVXGw47XRIAoJQILPAZfn5+mnhdGw25uL4d5nzvuysVvz3R6bIAAKVAYIFP8ff303NDO6hvmxg7odz9763SsfRsp8sCAFRGYJk2bZoaN26s0NBQde/eXcuXLz/jvjNmzFCvXr1UvXp1u/Tp0+e0/c1Q04kTJ6pu3boKCwuz+2zdurU8pQHnFBjgr5dGdlLz2lV1NC1LEz7+meHOAOBtgWX27NkaM2aMJk2apJUrVyouLk79+vVTQkJCifsvWrRII0eO1MKFCxUfH6/Y2Fj17dtX+/fvL9rnueee00svvaRXX31VP/zwg6pUqWLfMzMz8/yODjiD0KAATR0WpwD/gonlPllzwOmSAABn4ecq4z8tzRWVrl276pVXXrHb+fn5NoTcf//9Gj9+/Dlfn5eXZ6+0mNePGjXK/su2Xr16evjhhzV27Fi7T3JysmJiYvTmm29qxIgR53zPlJQURUVF2ddFRkaW5XDg4/761Rb9bcFWRYUF6cvfX66YyFCnSwIAn5FShu/vMl1hyc7O1ooVK2yTTdEb+PvbbXP1pDQyMjKUk5OjGjVq2O2dO3fq0KFDp7ynKd4EozO9Z1ZWlj3I4gtQHvdd2Vzt60cp+USOxn20lqYhAHBTZQosR48etVdIzNWP4sy2CR2lMW7cOHtFpTCgFL6uLO85efJkG2oKF3OFByiPoAB/2zQUHOivRZuP6P0f9zpdEgDA6VFCzzzzjN5//33NnTvXdtgtr8cee8xePipc9u7lSwbl1yImQo/0bWXXn/psg/Yey3C6JADA+QSW6OhoBQQE6PDhUyfcMtt16tQ562uff/55G1i+/PJLdehQcG8Xo/B1ZXnPkJAQ29ZVfAHOxx2XNVG3xjWUnp2nh+esUX4+TUMA4LGBJTg4WJ07d9aCBQuKHjOdbs12jx49zvg6MwroySef1Pz589WlS5dTnmvSpIkNJsXf0/RJMaOFzvaeQEUyo4WevylO4cEBWr7zmGYu2el0SQCA82kSMkOazdwqs2bN0saNGzV69Gilp6fr9ttvt8+bkT+myabQs88+qwkTJmjmzJl27hbTL8UsaWlpRbOPPvTQQ3rqqaf0ySefaN26dfY9TD+XG264oazlAeXWsGa4/jig4CaJz32xWVsPpzpdEgDgpECV0fDhw3XkyBE70ZsJHh07drRXTgo7ze7Zs8eOHCo0ffp0O7roxhtvPOV9zDwuTzzxhF1/9NFHbei5++67lZSUpMsuu8y+5/n0cwHKY2S3WH254ZDtgDvmgzX69//1tB1zAQAeNg+LO2IeFlSkwymZ6vvXb+1Q54f6tNBDfVo6XRIAeKVKm4cF8AVm8rg/D2pr11/5ZpvW7Ut2uiQA8HkEFqAEA+PqaUD7usrNd2nMB6uVmZPndEkA4NMILEAJTGfwJ29op+iqIdqakKapX21xuiQA8GkEFuAMalQJ1rND29v1Gd/tsMOdAQDOILAAZ3HVRTEa1qWBTNf0sXPWKD0r1+mSAMAnEViAc5hwXRvVrxamPccy9Jd5G50uBwB8EoEFOIeI0CBNuangdhLv/rBHizYnOF0SAPgcAgtQCj2bReu2no3t+riP1io5I8fpkgDApxBYgFIad01rNY2uosMpWZr4yc9OlwMAPoXAApRSWHCAXhgWJ38/6T+rD2jeuoNOlwQAPoPAApRBp4bV9X9XNLfrf5i7TgmpmU6XBAA+gcAClNEDV7VQm7qROp6Ro8f/vU5ecDsuAHB7BBagjIID/TV1eJyCA/z19cYEfbhin9MlAYDXI7AA5dC6TqR+f3XBXZz//OkG7U864XRJAODVCCxAOd19eVN1blRdqVm5emTOGuXn0zQEAJWFwAKUU4C/n164KU5hQQFauj1Rb8XvcrokAPBaBBbgPDSOrqLHr21t15+Zv0k7jqQ5XRIAeCUCC3CefnNJI/VqEa3MnHyN+WCNcvPynS4JALwOgQU4T35+fnruxg6KCA3U6r1Jeu3bHU6XBABeh8ACVIC6UWH608C2dv3Fr7dow4EUp0sCAK9CYAEqyOBO9dWvbYxy8lwa88FqZeXmOV0SAHgNAgtQgU1DTw9ur5pVgrXpUKpe/Hqr0yUBgNcgsAAVqGbVED09pL1df23xdq3YfdzpkgDAKxBYgArWr20dDbm4vsw8co98uEaZOTQNAcD5IrAAlWDSdW1VOyJEO46k669fb3G6HADweAQWoBJEhQfZ/izGjG93aNUemoYA4HwQWIBK0qdNjB05ZJqGHv1wLU1DAHAeCCxAJZp0fRtFVw3R1oQ0vbSAUUMAUF4EFqASVQsP1l8Gt7PrZgbctfuSnC4JADwSgQW4AKOGBsbVU16+S4/MWcuEcgBQDgQW4AJ4YmBbRVcN1ubDqXrlm21OlwMAHofAAlwANaoE68lBBU1Df1+0XT/vT3a6JADwKAQW4ALp376uBnSoa5uGxs5Zo+zcfKdLAgCPQWABLqA/D2xrr7aYew1NW0jTEACUFoEFuMD3GvrzoLZ23QSWDQdSnC4JADwCgQW4wAa0r6v+7eoo92TTUE4eTUMAcC4EFuAC8/Pz058HtVP18CBtOJii6Yu2O10SALg9AgvggFoRIXaos/HyN1u16RBNQwBwNgQWwCFmMrm+bWKUk0fTEACcC4EFcLBp6KnB7RQVFqSf96fo9W93OF0SALgtAgvgoNoRoXpiYBu7/revt2rL4VSnSwIAt0RgARx2Q8f6uqp1bWXn5euROWuUS9MQAJyGwAK4QdPQ00PaKzI0UGv2JWvGdzudLgkA3A6BBXADMZGhmnh9waihv369RdsSaBoCgOIILICbGHpxfV3Rqpa9x9DYOWvtPYcAAAUILIAbNQ1NHtJeESGBWr03Sf/8nlFDAFCIwAK4kbpRYZpwXcGooRe+3KLtR9KcLgkA3AKBBXAzN3VpoMtb1lJWbr4e/ZCmIQAwCCyAmzYNVQ0J1Irdx/Xm0l1OlwQAjiOwAG6ofrUwPX7tRXZ9yhebtPNoutMlAYCjCCyAmxrZLVaXNY9WZk6+xn24Vvk0DQHwYQQWwM2bhqoEB2j5rmN6K56mIQC+i8ACuLHYGuEaf7Jp6Nn5m7U7kaYhAL6JwAK4uZu7NVSPpjV1IifPjhqiaQiALyKwAG7O399Pz93YQeHBAfph5zH964fdTpcEABccgQXwkKahcde0tuuT/7tJe49lOF0SAFxQBBbAQ9xySSN1a1JDGdl5GvfRWrlcNA0B8B0EFsCTmoaGdlBokL+Wbk/Uu8v3OF0SAFwwBBbAgzSOrqJH+xU0DT39+UbtO07TEADfQGABPMxtPRura+PqSqdpCIAPIbAAHjlqKM42DS3Zlqh//UDTEADvR2ABPFCTYk1Dk+dtZNQQAK9HYAE8uGmoW+MaRU1DTCgHwJsRWAAPn1CucNTQvxg1BMCLEVgADx81VDShHE1DALwYgQXwcLf2aFw0oRz3GgLgrQgsgBc0DU25sYPCggIUvyNR73CvIQBeiMACeIFGNU3TUCu7PnneJu1JpGkIgHcpV2CZNm2aGjdurNDQUHXv3l3Lly8/477r16/X0KFD7f5+fn568cUXT9vniSeesM8VX1q3LmiXB1A6o3o0VvcmNXQiJ0+PfLiGpiEAvh1YZs+erTFjxmjSpElauXKl4uLi1K9fPyUkJJS4f0ZGhpo2bapnnnlGderUOeP7tm3bVgcPHixavv/++7KWBvi0gqahOIUHB+iHncf09jKahgD4cGCZOnWq7rrrLt1+++1q06aNXn31VYWHh2vmzJkl7t+1a1dNmTJFI0aMUEhIyBnfNzAw0AaawiU6OrqspQE+r2HNcI3vX3B18pn/btLuxHSnSwKACx9YsrOztWLFCvXp0+d/b+Dvb7fj4+PPq5CtW7eqXr169mrMzTffrD17zjynRFZWllJSUk5ZABT4TfdGuqRpYdMQo4YA+GBgOXr0qPLy8hQTE3PK42b70KFD5S7C9IN58803NX/+fE2fPl07d+5Ur169lJqaWuL+kydPVlRUVNESGxtb7j8b8OamoeU7j+mt+F1OlwQA3jFKqH///rrpppvUoUMH2x9m3rx5SkpK0gcffFDi/o899piSk5OLlr17917wmgF3FlsjXI8VNg3N36RdR2kaAuBDgcX0KwkICNDhw4dPedxsn61DbVlVq1ZNLVu21LZt20p83vSFiYyMPGUBcKqbuzdSj6Y1lZmTz4RyAHwrsAQHB6tz585asGBB0WP5+fl2u0ePHhVWVFpamrZv3666detW2HsCvnqvoSqmaWjXMb25lKYhAD7UJGSGNM+YMUOzZs3Sxo0bNXr0aKWnp9tRQ8aoUaNsk03xjrqrV6+2i1nfv3+/XS9+9WTs2LFavHixdu3apaVLl2rw4MH2Ss7IkSMr6jgB320auvYiu/7cF5u0k6YhAB4qsKwvGD58uI4cOaKJEyfajrYdO3a0nWULO+Ka0T1m5FChAwcOqFOnTkXbzz//vF169+6tRYsW2cf27dtnw0liYqJq1aqlyy67TMuWLbPrAM7Pzd0b6r8/H9SSbYl69MM1mn13D3v1BQA8iZ/L5fL4hm0zrNmMFjIdcOnPApxu3/EM9fvrt0rPztOE69rozsuaOF0SAKgs399uMUoIQOVqUD1cjw842TQ0f5N2HElzuiQAKBMCC+Ajft2toS5rHq2s3Hw7oVweo4YAeBACC+AjzE1FnxnaXlVDArVi93G9sWSn0yUBQKkRWAAfaxr6w8mmoSlfbNZ2moYAeAgCC+BjRnSNVa8WJ5uG5qyhaQiARyCwAD7ZNNTBNg2t3JOkmd/TNATA/RFYAB9Uv1qY/niyaej5LzdrWwJNQwDcG4EF8FHDu8bq8pa1To4aomkIgHsjsAC+3DQ0pL0iQgK1ak+S/vHdDqdLAoAzIrAAPqyeaRq6rqBp6IWvtmhbQqrTJQFAiQgsgI8b1iVWvVvWUnZuvsbOYUI5AO6JwAL4uMIJ5SJCA7V6b5Jm0DQEwA0RWACoblSYvSmiMfWrLdp6mKYhAO6FwALAuqlzA/2qVWHT0Brl5uU7XRIAFCGwAChqGpo8pINtGlqzL1mv0zQEwI0QWAAUqRMVqoknm4Ze/HqrdnCvIQBugsAC4BQ3dm5g7zVkmob++PHPcrkYNQTAeQQWAKc1DT11QzuFBPpr6fZE/XvlfqdLAgACC4DTNapZRQ/2aWHXn/p8g46lZztdEgAfR2ABUKK7ejVVq5gIHc/I0dPzNjpdDgAfR2ABUKKgAH89PaS9/PykD1fs09LtR50uCYAPI7AAOKPOjarr5u4N7fof5/6szJw8p0sC4KMILADO6tFrWqtWRIh2HE3X3xdtd7ocAD6KwALgrCJDg/TE9W3t+vRF27ijMwBHEFgAnNO17evoyta1lZPn0uP//ln53NEZwAVGYAFQqrlZ/jSwrcKCArR81zHNWbHX6ZIA+BgCC4BSia0RrjFXt7TrT8/bpKNpWU6XBMCHEFgAlNrtlzZWm7qRSj6Ro6c+2+B0OQB8CIEFQKkFBvhr8pD28veTPl59QN9tPeJ0SQB8BIEFQJnExVbTqB6N7fof5v6sE9nMzQKg8hFYAJTZ2H6tVCcyVHuOZejlb7Y6XQ4AH0BgAVBmVUMC9adBBXOzvP7tDm06lOJ0SQC8HIEFQLn0a1tHfdvEKDffzM2yjrlZAFQqAguAcntiYFtVCQ7Qyj1Jenf5HqfLAeDFCCwAyq1etTDbn8V4dv4mJaRkOl0SAC9FYAFwXsyIoQ4NopSamas/MTcLgEpCYAFwXgL8/fT04Pb25+drD2rhpgSnSwLghQgsAM5bu/pRuuPSgrlZ/vjxz8rIznW6JABehsACoEL8/uqWql8tTPuTTujFr5mbBUDFIrAAqBDhwYF68oaCuVn++f1OrT+Q7HRJALwIgQVAhbmydYwGtK+rvJNzs5ifAFARCCwAKtSk69soIiRQa/Yl6+34XU6XA8BLEFgAVKjakaF6tH9ruz7li806mHzC6ZIAeAECC4AKd3O3hurUsJrSs/P0xCfrnS4HgBcgsACocP7+fpo8pL0C/f30xfrD+nL9IadLAuDhCCwAKkXrOpG66/Kmdn3SJ+uVlsXcLADKj8ACoNI8cGULNawRroPJmXrhy81OlwPAgxFYAFSasOAAPXVDO7s+a+kurd2X5HRJADwUgQVApbq8ZS0N6lhPZkqW8R+tU25evtMlAfBABBYAlW7CdW0UFRakDQdT9OZS5mYBUHYEFgCVLrpqiB47OTfLC19u0b7jGU6XBMDDEFgAXBDDusSqW+MaOpGTp4n/WS+Xi2n7AZQegQXABZub5ekh7RQU4KdvNiVo3jrmZgFQegQWABdM89oRGt27mV1/fO467U9i2n4ApUNgAXBB3XdlC8U1iFLyiRw98N4q5TBqCEApEFgAXFDBgf56eeTF9o7OK3Yf19SvtjhdEgAPQGABcME1rBmuZ4Z2sOvTF23X4i1HnC4JgJsjsABwxIAOdfWbSxra9TGzVyshJdPpkgC4MQILAMf8cUAbta4TocT0bD34/mrlmelwAaAEBBYAjgkNCtC0my9WeHCA4nck6pVvtjldEgA3RWAB4KhmtarqL4MLbpD4twVbFL890emSALghAgsAxw3u1EA3dm5gb5D44PurlJiW5XRJANwMgQWAW/jzoLZqVquKElKzNOaDNcqnPwuAYggsANxCeHCg7c8SEuhvhznP+G6H0yUBcCMEFgBuo3WdSD0xsK1dn/LFZjuxHAAYBBYAbmVE11hdH1dPufkuO3V/ckaO0yUBcAMEFgBuxc/PT08PbqdGNcPtzREf+XCNXC76swC+jsACwO1EhAbplZEXKyjAT19uOKxZS3c5XRIATwws06ZNU+PGjRUaGqru3btr+fLlZ9x3/fr1Gjp0qN3f/MvpxRdfPO/3BOD92jeI0uPXXmTXn563ST/vT3a6JACeFFhmz56tMWPGaNKkSVq5cqXi4uLUr18/JSQklLh/RkaGmjZtqmeeeUZ16tSpkPcE4Btu69lYV7eJUXZevu57d6VSM+nPAvgqP1cZG4fN1Y+uXbvqlVdesdv5+fmKjY3V/fffr/Hjx5/1teYKykMPPWSXinpPIyUlRVFRUUpOTlZkZGRZDgeAm0vKyNaAl763/VlMZ9yXRnS0V2sBeL6yfH+X6QpLdna2VqxYoT59+vzvDfz97XZ8fHy5iq2M9wTgPaqFB+ulkZ0U4O+nT9cc0Owf9zpdEgAHlCmwHD16VHl5eYqJiTnlcbN96NChchVQnvfMysqyqaz4AsB7dW5UXWP7trLrkz5Zr82HUp0uCcAF5pGjhCZPnmwvIRUupvkIgHe75/KmurxlLWXl5uved1cqIzvX6ZIAuGtgiY6OVkBAgA4fPnzK42b7TB1qK+M9H3vsMdveVbjs3cslYsDb+fv7aeqwONWOCNG2hDQ98cl6p0sC4K6BJTg4WJ07d9aCBQuKHjMdZM12jx49ylVAed4zJCTEds4pvgDwftFVQ/S3EZ3k7yd98NM+zV21z+mSALhrk5AZfjxjxgzNmjVLGzdu1OjRo5Wenq7bb7/dPj9q1Ch7BaR4p9rVq1fbxazv37/frm/btq3U7wkAhXo0q6kHrmph1/8w92ftOJLmdEkALoDAsr5g+PDhOnLkiCZOnGg7xXbs2FHz588v6jS7Z88eO8qn0IEDB9SpU6ei7eeff94uvXv31qJFi0r1ngBQ3P1XttCyHYlatuOY7n13leb+X0+FBgU4XRYAd5qHxR0xDwvgew6nZOrav32nxPRs3XJJIz15QzunSwLgLvOwAIC7iIkM1QvD4uz628t267/rDjpdEoBKRGAB4LGuaFVbv+vdzK4/+tFa7T2W4XRJACoJgQWAR3u4b0td3LCaUjNzdd97q5Sdm+90SQAqAYEFgEcLCvC3U/dHhQVpzd4kTflik9MlAagEBBYAHq9B9XA9d2MHuz7ju51asPHUiSgBeD4CCwCv0K9tHd3Ws7Fdf3jOGu1JpD8L4E0ILAC8xmPXtla7+pFKysjRsNfi7RT+ALwDgQWA1wgJDNDMW7uqRe2qOpSSqeGvxWv9gWSnywJQAQgsALxK7chQzb6nh73SYiaVG/n6Mq3YfdzpsgCcJwILAK9To0qw3r3rEnVpVF0pmbm65Z8/aOm2o06XBeA8EFgAeKXI0CC9dWc39WoRrYzsPN325o/6ZhOjhwBPRWAB4LXCgwM1Y1QXXd0mxk4od/dbK/TZ2gNOlwWgHAgsALyauYvz32++WIM61lNuvksPvLdKH/y01+myAJQRgQWAT8yGO3VYR43sFqt8l/Toh2v15pKdTpcFoAwILAB8QoC/n54e3F6/vayJ3X7i0w2atnCb02UBKCUCCwCf4efnpz8MuEgPXtXCbk/5YrOenb9JLpfL6dIAnAOBBYDPhZbfX91Sf7j2Irs9fdF2PfHJeuWbtiIAbovAAsAn3XV5U/1lcDv5+Umz4nfr0Y/WKjcv3+myAJwBgQWAz7q5eyNNHRZn+7d8uGKfHnh/lR3+DMD9EFgA+LTBnRpo2q8vVnCAv+atO6R73v5JmTl5TpcF4BcILAB83jXt6mjGrV0UGuSvhZuP6LY3listK9fpsgAUQ2ABAEm9W9bSW3d0V9WQQC3bcUy/+ccPSs7IcbosACcRWADgpG5Naujdu7qrWniQVu9N0vDX43UkNcvpsgAQWADgVB0aVNPsu3soumqINh1K1fDX4nUg6YTTZQE+j8ACAL/Qqk6E5vyuh+pXC9OOo+m66dV47U5Md7oswKcRWACgBE2iq+iD3/WwP/cnnbChZevhVKfLAnwWgQUAzsBcYZl9zyVqFROhhNQsDXstXj/vT3a6LMAnEVgA4CxqR4Ta0BLXIErHM3I08vVl+mnXMafLAnwOgQUAzqFaeLDe+W13O4ooNStXv57xg/729VZl5TLBHHChEFgAoBQiQoM06/Zu6tsmRtl5+frr11t07d++0/KdXG0BLgQCCwCUUlhwgF67pbNeHtnJDnvefiTd9msZ9+FaJWVkO10e4NUILABQBn5+fro+rp4WjOmtX3dvaB+b/dNeXfXCYn28ar9cLpfTJQJeicACAOUQFR6kpwe314e/66EWtasqMT1bD81erVEzlzNnC1AJCCwAcB66NK6hzx/opbF9Wyo40F/fbT2qvn/9VtMWblN2br7T5QFeg8ACAOfJBJX7rmyhLx+6XJc2r6ms3HxN+WKzrn/5e63YTadcoCIQWACggjSOrqJ37uyuqcPiVKNKsDYfTtXQ6fH6w9x1Sj7BnZ+B80FgAYAK7pQ75OIGtlPuTZ0b2Mf+9cMe9Zm6WJ+tPUCnXKCcCCwAUAmqVwnWlJvi9N5dl6hpdBUdSc3Sfe+u0h1v/qi9xzKcLg/wOAQWAKhEPZrV1H8f6qUHr2qh4AB/Ldx8xHbKff3b7crNo1MuUFoEFgCoZCGBAfr91S0178Fe6t6khk7k5OnpeZt0/StLtHpvktPlAR6BwAIAF0jz2lX1/t2X6LmhHRQVFqSNB1M0+O9L9MQn65WaSadc4GwILABwgTvlDusaqwUP99bgTvVl+uC+uXSXrp76reb/fMjp8gC3RWABAAeYexH9dXhHvX1nNzWqGa5DKZn63TsrdNdbP9EpFyiBn8sLxtilpKQoKipKycnJioyMdLocACiTzJw8vfzNVr22eIdy813y95P6XBSjUT0a24nozFUZwBuV5fubwAIAbmLL4VQ9+dkGO71/oWa1qtjgMuTi+ooIDXK0PqCiEVgAwINtPZyqt5ft1kcr9ik9O88+ViU4wE5IN6pHI7WIiXC6RKBCEFgAwAuYkUP/Xrlfs+J3aceR/90BumezmvaqS5+LaiswgK6I8FwEFgDwIuav6aXbEzVr6S59vfGw8k/+rV0vKlQ3X9JIw7vG2k68gKchsACAl9p3PMPem2j2j3t1LD3bPmZm0L2uQ12N6tlYHWOrOV0iUGoEFgDwgZFFn689qLfid2nNvuSixzs0iLLNRSbAhAYFOFojcC4EFgDwIWZ6fxNcPltzUNkn709UPTxII7o11M3dG6pB9XCnSwRKRGABAB+UmJal93/cq38t260DyZn2MTOny1UXxehW5nSBGyKwAIAPM3eBXrApwV51WbItsejxpmZOl0sa6doOdVU7ItTRGgGDwAIAsLYlpOrt+N36sNicLkb7+lH6VatauqJ1bcU1qKYAcykGuMAILACA0+Z0mbtqvw0ua4t10i3s79K7ZS39qnVtXd6ilqpXCXasTviWFAILAOBMElIz9e2Wo1q4OUHfbjmi1MzcoufMhRYzNPpXrWrbANOmbqT8ufqCSkJgAQCUSk5evlbtSbLhZeGmBG06lHrK87UiQnTFyasvl7WIViT3M0IFIrAAAMrlYPIJLdp8xIaX77cdVUaxfi+B/n7q3Ki6DS/mCkzLmKqMOsJ5IbAAAM5bVm6eftp13IYXcwVme7H7GRXeGsB02r2yVW31bF5T4cGBjtUKz0RgAQBUuD2JGVq0paDpyNzbKCu3YJK6wtsDdG9aw3bejYutprb1IgkwOCcCCwCg0m8NEL8jUYs2JeibzQnae+zEKc+bfrrNa1dV+/rV7O0C2jeIsh14uV0AiiOwAAAuGPM1suNour3ysmzHMf28P1mHUgpm2i3OzPXSonbVkwGmmjrUj1KrOhGEGB+WQmABADgpISVT6/Yn2zlfCn8eTcs6bT/TkdeEFhtiTl6NaRkToeBAf0fqxoVFYAEAuBXzVWOuuqwrFmDMz2Pp2afta/rDtK4bYWfjLQwyLWKqKiiAEONtCCwAALdnvn72J52wIWbt/mTblGSCTPKJnNP2NVdcTB+YNvUi1TS6iprVqmrvjWTuRM1tBTwXgQUA4JHMV5LpwLt2f1JBkNlXEGRSs/43G+8vr8Y0qhluw0tTE2Ki//eTWwy4PwILAMBr5Oe7tPtYhtbuS9KWw6nacSTdLjsT05VdbGj1L5l7JBVeiSkeZhrWCKePjJsgsAAAvF5evksHkk5o+5E07TxaEGJ2HE2zPw8mnz5KqZBpQjKhxQSYJoVXZGyoqaJaVUOYvddNv7+Z1QcA4JFM8IitEW6XK1qd+lx6Vm5BiLFBJu2UMGNuN2CeM8svhQUFqEH1sJNL+C9+hqlGlWACjUPKFVimTZumKVOm6NChQ4qLi9PLL7+sbt26nXH/OXPmaMKECdq1a5datGihZ599Vtdee23R87fddptmzZp1ymv69eun+fPnl6c8AICPqxISqHb1o+xSnGlUOJySZUPM9l+EmX3HT+hETp62JqTZpSQm0NQvCjSnh5qaBBr3CSyzZ8/WmDFj9Oqrr6p79+568cUXbbjYvHmzateufdr+S5cu1ciRIzV58mRdd911evfdd3XDDTdo5cqVateuXdF+11xzjd54442i7ZCQkPM5LgAATmPCRJ2oULv0bB592r2TDiRlat/xDBte9h8/UbRulsOpmTbQbEtIs0tJQoP8bXipX+2XgSbMBp3oKiHyZ1RTuZS5D4sJKV27dtUrr7xit/Pz8xUbG6v7779f48ePP23/4cOHKz09XZ999lnRY5dccok6duxoQ0/hFZakpCR9/PHH5ToI+rAAACqbCTQHbaApHmRODTTn+kY1E+XVjghRTFSoYiJCFRP5v3UToux2ZKiqhgT6xJWalMrqw5Kdna0VK1boscceK3rM399fffr0UXx8fImvMY+bKzLFmSsyvwwnixYtsldoqlevriuvvFJPPfWUatasWeJ7ZmVl2aX4AQMAUJlCAgPUOLqKXUpiRiwdTD5xWpApXDcT5+WajsLJmXY5m/DgANWJDFXtkwGmjl0v+FkYasxzpiZfUabAcvToUeXl5SkmJuaUx832pk2bSnyN6edS0v7m8eLNQUOGDFGTJk20fft2Pf744+rfv78NOwEBp58M07z0pz/9qSylAwBQqcxQ6UY1q9ilJDl5+fb2BIeSM20/msMp5memDTIJJ7fNempmru0YbDsMl9Ax+JdDt014KVhCVLNqiKLtEqyaVUIUHVHw03QW9vQJ9txilNCIESOK1tu3b68OHTqoWbNm9qrLVVddddr+5gpP8as25gqLaZYCAMBdmVsL1I0Ks8vZZGTnnhJoCpask8GmINSYbXNF53hGjl02HUo963ua1qUa4cGqWRRkQmwHYRNsTMAxQcc8V+vkz/Bgt4gHpyhTRdHR0faKx+HDh0953GzXqVOnxNeYx8uyv9G0aVP7Z23btq3EwGI65NIpFwDgjUxYaBJtlpKv1Bim+2lSRo7tN2PDTXKmElIzdTQtW4np2TqamqXE9CwlpmXrWEa27VtjHjeLVHKH4V+Ohiq8OvO/UBOsh/q0dOyeTmUKLMHBwercubMWLFhgR/oUdro12/fdd1+Jr+nRo4d9/qGHHip67KuvvrKPn8m+ffuUmJiounXrlqU8AAB8gp+fn731gFlan/nf/1ZuXsGVGBNgjqaa0JJVEGzSzM+CUHP0ZMgx21m5+XY0lLlFglmKN3mN7fuLCW8uoDJf8zFNMbfeequ6dOli514xw5rNKKDbb7/dPj9q1CjVr1/f9jMxHnzwQfXu3VsvvPCCBgwYoPfff18//fSTXn/9dft8Wlqa7Y8ydOhQe9XF9GF59NFH1bx5c9s5FwAAlF9ggL9qRYTYRecIN+bKjek/Y4LL/0JNwc/svHxHRy6VObCYYcpHjhzRxIkTbcdZMzzZTPBW2LF2z549duRQoZ49e9q5V/74xz/azrRm4jgzQqhwDhbTxLR27Vo7cZwZ2lyvXj317dtXTz75JM0+AABcQH5+fnbSPbOcqfOwU7iXEAAAcPvvb25XCQAA3B6BBQAAuD0CCwAAcHsEFgAA4PYILAAAwO0RWAAAgNsjsAAAALdHYAEAAG6PwAIAANwegQUAALg9AgsAAHB7BBYAAOD2yny3ZndUeP9GcxMlAADgGQq/t0tzH2avCCypqan2Z2xsrNOlAACAcnyPm7s2n42fqzSxxs3l5+frwIEDioiIkJ+fX4WnPxOE9u7de85bX3s6XzpWXztejtV7+dLxcqzex0QQE1bq1asnf39/77/CYg6yQYMGlfpnmF8Yb/6l8dVj9bXj5Vi9ly8dL8fqXc51ZaUQnW4BAIDbI7AAAAC3R2A5h5CQEE2aNMn+9Ha+dKy+drwcq/fypePlWH2bV3S6BQAA3o0rLAAAwO0RWAAAgNsjsAAAALdHYAEAAG6PwCJp2rRpaty4sUJDQ9W9e3ctX778rPvPmTNHrVu3tvu3b99e8+bNk7ubPHmyunbtamcDrl27tm644QZt3rz5rK9588037czBxRdzzJ7giSeeOK12c8687bwa5nf3l8dqlnvvvdcrzuu3336r66+/3s6EaWr9+OOPT3nejBuYOHGi6tatq7CwMPXp00dbt26t8M+908eak5OjcePG2d/NKlWq2H1GjRplZ/mu6M+CO5zX22677bS6r7nmGo88r6U53pI+w2aZMmWKx53byuLzgWX27NkaM2aMHT62cuVKxcXFqV+/fkpISChx/6VLl2rkyJG68847tWrVKvvFb5aff/5Z7mzx4sX2C2zZsmX66quv7F9+ffv2VXp6+llfZ2ZYPHjwYNGye/dueYq2bdueUvv3339/xn099bwaP/744ynHac6vcdNNN3nFeTW/o+Zzab6ISvLcc8/ppZde0quvvqoffvjBfpmbz3BmZmaFfe7d4VgzMjJsrRMmTLA///3vf9t/dAwcOLBCPwvucl4NE1CK1/3ee++d9T3d9byW5niLH6dZZs6caQPI0KFDPe7cVhqXj+vWrZvr3nvvLdrOy8tz1atXzzV58uQS9x82bJhrwIABpzzWvXt31z333OPyJAkJCWY4u2vx4sVn3OeNN95wRUVFuTzRpEmTXHFxcaXe31vOq/Hggw+6mjVr5srPz/e682p+Z+fOnVu0bY6xTp06rilTphQ9lpSU5AoJCXG99957Ffa5d4djLcny5cvtfrt3766wz4K7HOutt97qGjRoUJnexxPOa2nPrTn2K6+88qz7TPKAc1uRfPoKS3Z2tlasWGEvIRe/L5HZjo+PL/E15vHi+xsmwZ9pf3eVnJxsf9aoUeOs+6WlpalRo0b2JlyDBg3S+vXr5SlMs4C5/Nq0aVPdfPPN2rNnzxn39Zbzan6n33nnHd1xxx1nvRGoJ5/X4nbu3KlDhw6dcu7MfUlMU8CZzl15Pvfu/Dk257latWoV9llwJ4sWLbJN2K1atdLo0aOVmJh4xn296bwePnxYn3/+ub3iey5bPfTclodPB5ajR48qLy9PMTExpzxuts1fgiUxj5dlf3e9u/VDDz2kSy+9VO3atTvjfuYvCXNZ8j//+Y/9EjSv69mzp/bt2yd3Z76wTF+N+fPna/r06faLrVevXvauoN56Xg3TLp6UlGTb/73xvP5S4fkpy7krz+feHZkmL9OnxTRlnu3meGX9LLgL0xz01ltvacGCBXr22Wdts3b//v3tufPm82rMmjXL9jccMmTIWffr7qHntry84m7NKBvTl8X0zThXW2ePHj3sUsh8qV100UV67bXX9OSTT8qdmb/YCnXo0MF+sM0VhQ8++KBU/2rxVP/85z/tsZt/cXnjeUUB0wdt2LBhtsOx+aLyxs/CiBEjitZNR2NTe7NmzexVl6uuukrezPyDwlwtOVdn+P4eem7Ly6evsERHRysgIMBefivObNepU6fE15jHy7K/u7nvvvv02WefaeHChWrQoEGZXhsUFKROnTpp27Zt8jTmknnLli3PWLunn1fDdJz9+uuv9dvf/tZnzmvh+SnLuSvP594dw4o536aD9dmurpTns+CuTJOHOXdnqtvTz2uh7777znamLuvn2JPPbWn5dGAJDg5W586d7SXHQubyuNku/i/Q4szjxfc3zF8aZ9rfXZh/iZmwMnfuXH3zzTdq0qRJmd/DXG5dt26dHT7qaUyfje3bt5+xdk89r8W98cYbtr1/wIABPnNeze+x+TIqfu5SUlLsaKEznbvyfO7dLayYfgsmnNasWbPCPwvuyjRZmj4sZ6rbk8/rL6+SmuMwI4p85dyWmsvHvf/++3ZEwZtvvunasGGD6+6773ZVq1bNdejQIfv8Lbfc4ho/fnzR/kuWLHEFBga6nn/+edfGjRttL+2goCDXunXrXO5s9OjRdmTIokWLXAcPHixaMjIyivb55bH+6U9/cn3xxReu7du3u1asWOEaMWKEKzQ01LV+/XqXu3v44Yftse7cudOesz59+riio6Pt6ChvOq/FR0M0bNjQNW7cuNOe8/Tzmpqa6lq1apVdzF9ZU6dOteuFI2OeeeYZ+5n9z3/+41q7dq0dXdGkSRPXiRMnit7DjLZ4+eWXS/25d8djzc7Odg0cONDVoEED1+rVq0/5HGdlZZ3xWM/1WXDHYzXPjR071hUfH2/r/vrrr10XX3yxq0WLFq7MzEyPO6+l+T02kpOTXeHh4a7p06eX+B5Xesi5rSw+H1gM8wtg/rIPDg62w+KWLVtW9Fzv3r3t8LriPvjgA1fLli3t/m3btnV9/vnnLndnPiAlLWaI65mO9aGHHir6/xITE+O69tprXStXrnR5guHDh7vq1q1ra69fv77d3rZtm9ed10ImgJjzuXnz5tOe8/TzunDhwhJ/dwuPyQxtnjBhgj0W82V11VVXnfb/oVGjRjaElvZz747Har6UzvQ5Nq8707Ge67Pgjsdq/iHVt29fV61atew/HMwx3XXXXacFD085r6X5PTZee+01V1hYmB2aX5JGHnJuK4uf+U/pr8cAAABceD7dhwUAAHgGAgsAAHB7BBYAAOD2CCwAAMDtEVgAAIDbI7AAAAC3R2ABAABuj8ACAADcHoEFAAC4PQILAABwewQWAADg9ggsAABA7u7/AchMZqUSzSQ7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "# ============================\n",
    "# Funzioni di attivazione e derivate\n",
    "# ============================\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(z, a):\n",
    "    # z non viene usato, ma lo manteniamo per avere la stessa firma\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(z, a):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "activation_functions = {\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"relu\": relu\n",
    "}\n",
    "\n",
    "activation_derivatives = {\n",
    "    \"sigmoid\": lambda z, a: sigmoid_derivative(z, a),\n",
    "    \"relu\": lambda z, a: relu_derivative(z, a)\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Funzioni di loss e derivate\n",
    "# ============================\n",
    "\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_crossentropy_derivative(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    # Derivata elementwise: dL/dy_pred = - y_true/y_pred + (1-y_true)/(1-y_pred)\n",
    "    return - (y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "loss_functions = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_loss,\n",
    "    \"mse\": mse_loss\n",
    "}\n",
    "\n",
    "loss_derivatives = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_derivative,\n",
    "    \"mse\": mse_derivative\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Classe della Rete Neurale\n",
    "# ============================\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.9, lambda_reg=0.01,\n",
    "                 loss_function_name=\"binary_crossentropy\",\n",
    "                 activation_function_name=\"relu\",\n",
    "                 output_activation_function_name=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        :param layers: lista con la dimensione di ogni layer (input, hidden, output)\n",
    "        :param learning_rate: tasso di apprendimento\n",
    "        :param lambda_reg: coefficiente di regolarizzazione L2\n",
    "        :param loss_function_name: nome della funzione di loss (deve essere presente in loss_functions e loss_derivatives)\n",
    "        :param activation_function_name: nome della funzione di attivazione per i layer nascosti\n",
    "        :param output_activation_function_name: nome della funzione di attivazione per il layer di output\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.loss_function_name = loss_function_name\n",
    "        self.activation_function_name = activation_function_name\n",
    "        self.output_activation_function_name = output_activation_function_name\n",
    "        self._initialize_parameters()\n",
    "        \n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Inizializza i pesi e bias utilizzando un metodo di inizializzazione (He in questo esempio).\"\"\"\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            weight = np.random.randn(self.layers[i], self.layers[i + 1]) * np.sqrt(2 / self.layers[i])\n",
    "            self.W.append(weight)\n",
    "            self.b.append(np.zeros((1, self.layers[i + 1])))\n",
    "    \n",
    "    def _apply_activation(self, x, func_name):\n",
    "        if func_name not in activation_functions:\n",
    "            raise ValueError(f\"Attivazione non supportata: {func_name}\")\n",
    "        return activation_functions[func_name](x)\n",
    "    \n",
    "    def _apply_activation_derivative(self, z, a, func_name):\n",
    "        if func_name not in activation_derivatives:\n",
    "            raise ValueError(f\"Derivata dell'attivazione non supportata: {func_name}\")\n",
    "        return activation_derivatives[func_name](z, a)\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        \"\"\"\n",
    "        Esegue la forward propagation e ritorna le liste:\n",
    "        - Z: valori pre-attivazione per ogni layer\n",
    "        - A: output attivati per ogni layer (incluso l'input come A[0])\n",
    "        \"\"\"\n",
    "        A = [X]\n",
    "        Z = []\n",
    "        # Propagazione attraverso i layer nascosti\n",
    "        for i in range(len(self.W) - 1):\n",
    "            z_curr = np.dot(A[-1], self.W[i]) + self.b[i]\n",
    "            Z.append(z_curr)\n",
    "            a_curr = self._apply_activation(z_curr, self.activation_function_name)\n",
    "            A.append(a_curr)\n",
    "        # Propagazione nel layer di output\n",
    "        z_out = np.dot(A[-1], self.W[-1]) + self.b[-1]\n",
    "        Z.append(z_out)\n",
    "        a_out = self._apply_activation(z_out, self.output_activation_function_name)\n",
    "        A.append(a_out)\n",
    "        return Z, A\n",
    "    \n",
    "    def _backward(self, X, y, Z, A):\n",
    "        \"\"\"\n",
    "        Esegue la backward propagation calcolando i gradienti e aggiornando i parametri.\n",
    "        La catena del gradiente viene calcolata in maniera modulare.\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        if self.loss_function_name not in loss_derivatives:\n",
    "            raise ValueError(f\"Derivata della loss non supportata: {self.loss_function_name}\")\n",
    "        # Calcola dL/dy_pred per l'output\n",
    "        dA = loss_derivatives[self.loss_function_name](y, A[-1])\n",
    "        # Calcola dL/dz nel layer di output\n",
    "        dZ = dA * self._apply_activation_derivative(Z[-1], A[-1], self.output_activation_function_name)\n",
    "        dW = [np.dot(A[-2].T, dZ) / m + self.lambda_reg * self.W[-1] / m]\n",
    "        db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "        \n",
    "        # Backpropagation nei layer nascosti\n",
    "        for i in range(len(self.W) - 2, -1, -1):\n",
    "            dA = np.dot(dZ, self.W[i + 1].T)\n",
    "            dZ = dA * self._apply_activation_derivative(Z[i], A[i + 1], self.activation_function_name)\n",
    "            dW.insert(0, np.dot(A[i].T, dZ) / m + self.lambda_reg * self.W[i] / m)\n",
    "            db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "        \n",
    "        # Aggiorna i parametri\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] -= self.learning_rate * dW[i]\n",
    "            self.b[i] -= self.learning_rate * db[i]\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, batch_size=32, verbose=True):\n",
    "        \"\"\"\n",
    "        Esegue il training della rete mediante mini-batch gradient descent.\n",
    "        Ritorna la lista degli errori per ogni epoca (ogni 10 epoche).\n",
    "        \"\"\"\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            permutation = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                Z, A = self._forward(X_batch)\n",
    "                self._backward(X_batch, y_batch, Z, A)\n",
    "            if epoch % int(epochs/20) == 0:\n",
    "                _, A_full = self._forward(X)\n",
    "                loss = loss_functions[self.loss_function_name](y, A_full[-1])\n",
    "                reg_term = (self.lambda_reg / 2) * sum(np.sum(w ** 2) for w in self.W)\n",
    "                total_loss = loss + reg_term\n",
    "                loss_history.append(total_loss)\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "        return loss_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Restituisce le predizioni:\n",
    "          - Se l'output ha un solo neurone, viene usata una soglia a 0.5.\n",
    "          - Se l'output è one-hot encoded, viene usato argmax.\n",
    "        \"\"\"\n",
    "        _, A = self._forward(X)\n",
    "        output = A[-1]\n",
    "        if output.shape[1] == 1:\n",
    "            return (output > 0.5).astype(int)\n",
    "        else:\n",
    "            return np.argmax(output, axis=1)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Calcola l'accuratezza.\n",
    "        Se y è one-hot encoded, converte le etichette in formato indice.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        if y.ndim > 1 and y.shape[1] > 1:\n",
    "            y_true = np.argmax(y, axis=1)\n",
    "        else:\n",
    "            y_true = y\n",
    "        return np.mean(predictions == y_true)\n",
    "\n",
    "# ============================\n",
    "# Esecuzione: Caricamento dataset, training e valutazione\n",
    "# ============================\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(3, one_hot_encode=True)\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "layers = [input_size, 4, 4, output_size]\n",
    "\n",
    "# Istanziamo la rete con i parametri desiderati (nessuna assunzione implicita)\n",
    "nn = NeuralNetwork(\n",
    "    layers,\n",
    "    learning_rate=0.05,\n",
    "    lambda_reg=0.00001,\n",
    "    loss_function_name=\"mse\",\n",
    "    activation_function_name=\"relu\",\n",
    "    output_activation_function_name=\"sigmoid\"\n",
    ")\n",
    "\n",
    "loss_history = nn.train(X_train, y_train, epochs=300, batch_size=32, verbose=True)\n",
    "accuracy = nn.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "pd.Series(loss_history).plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-3-train.csv\n",
      "Using cached ../datasets/monks/monk-3-test.csv\n",
      "One-hot encoding MONK-3 dataset...\n",
      "Epoch 0, Loss: 0.2596\n",
      "Epoch 15, Loss: 0.2474\n",
      "Epoch 30, Loss: 0.2392\n",
      "Epoch 45, Loss: 0.2308\n",
      "Epoch 60, Loss: 0.2186\n",
      "Epoch 75, Loss: 0.2024\n",
      "Epoch 90, Loss: 0.1803\n",
      "Epoch 105, Loss: 0.1579\n",
      "Epoch 120, Loss: 0.1358\n",
      "Epoch 135, Loss: 0.1123\n",
      "Epoch 150, Loss: 0.0921\n",
      "Epoch 165, Loss: 0.0778\n",
      "Epoch 180, Loss: 0.0684\n",
      "Epoch 195, Loss: 0.0620\n",
      "Epoch 210, Loss: 0.0577\n",
      "Epoch 225, Loss: 0.0547\n",
      "Epoch 240, Loss: 0.0524\n",
      "Epoch 255, Loss: 0.0506\n",
      "Epoch 270, Loss: 0.0490\n",
      "Epoch 285, Loss: 0.0477\n",
      "Test Accuracy: 0.9630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "# ============================\n",
    "# Funzioni di attivazione e derivate\n",
    "# ============================\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(z, a):\n",
    "    # z non viene usato, ma lo manteniamo per avere la stessa firma\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(z, a):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "activation_functions = {\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"relu\": relu\n",
    "}\n",
    "\n",
    "activation_derivatives = {\n",
    "    \"sigmoid\": lambda z, a: sigmoid_derivative(z, a),\n",
    "    \"relu\": lambda z, a: relu_derivative(z, a)\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Funzioni di loss e derivate\n",
    "# ============================\n",
    "\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_crossentropy_derivative(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    # Derivata elementwise: dL/dy_pred = - y_true/y_pred + (1-y_true)/(1-y_pred)\n",
    "    return - (y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "loss_functions = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_loss,\n",
    "    \"mse\": mse_loss\n",
    "}\n",
    "\n",
    "loss_derivatives = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_derivative,\n",
    "    \"mse\": mse_derivative\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Classe della Rete Neurale\n",
    "# ============================\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.9, lambda_reg=0, reg_type=\"l2\",\n",
    "                 loss_function_name=\"binary_crossentropy\",\n",
    "                 activation_function_name=\"relu\",\n",
    "                 output_activation_function_name=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        :param layers: lista con la dimensione di ogni layer (input, hidden, output)\n",
    "        :param learning_rate: tasso di apprendimento\n",
    "        :param lambda_reg: coefficiente di regolarizzazione\n",
    "        :param reg_type: tipo di regolarizzazione (\"l2\" o \"l1\")\n",
    "        :param loss_function_name: nome della funzione di loss (deve essere presente in loss_functions e loss_derivatives)\n",
    "        :param activation_function_name: nome della funzione di attivazione per i layer nascosti\n",
    "        :param output_activation_function_name: nome della funzione di attivazione per il layer di output\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.reg_type = reg_type\n",
    "        self.loss_function_name = loss_function_name\n",
    "        self.activation_function_name = activation_function_name\n",
    "        self.output_activation_function_name = output_activation_function_name\n",
    "        self._initialize_parameters()\n",
    "        \n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Inizializza i pesi e bias utilizzando un metodo di inizializzazione (He in questo esempio).\"\"\"\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            weight = np.random.randn(self.layers[i], self.layers[i + 1]) * np.sqrt(2 / self.layers[i])\n",
    "            self.W.append(weight)\n",
    "            self.b.append(np.zeros((1, self.layers[i + 1])))\n",
    "    \n",
    "    def _apply_activation(self, x, func_name):\n",
    "        if func_name not in activation_functions:\n",
    "            raise ValueError(f\"Attivazione non supportata: {func_name}\")\n",
    "        return activation_functions[func_name](x)\n",
    "    \n",
    "    def _apply_activation_derivative(self, z, a, func_name):\n",
    "        if func_name not in activation_derivatives:\n",
    "            raise ValueError(f\"Derivata dell'attivazione non supportata: {func_name}\")\n",
    "        return activation_derivatives[func_name](z, a)\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        \"\"\"\n",
    "        Esegue la forward propagation e ritorna le liste:\n",
    "        - Z: valori pre-attivazione per ogni layer\n",
    "        - A: output attivati per ogni layer (incluso l'input come A[0])\n",
    "        \"\"\"\n",
    "        A = [X]\n",
    "        Z = []\n",
    "        # Propagazione attraverso i layer nascosti\n",
    "        for i in range(len(self.W) - 1):\n",
    "            z_curr = np.dot(A[-1], self.W[i]) + self.b[i]\n",
    "            Z.append(z_curr)\n",
    "            a_curr = self._apply_activation(z_curr, self.activation_function_name)\n",
    "            A.append(a_curr)\n",
    "        # Propagazione nel layer di output\n",
    "        z_out = np.dot(A[-1], self.W[-1]) + self.b[-1]\n",
    "        Z.append(z_out)\n",
    "        a_out = self._apply_activation(z_out, self.output_activation_function_name)\n",
    "        A.append(a_out)\n",
    "        return Z, A\n",
    "    \n",
    "    def _backward(self, X, y, Z, A):\n",
    "        \"\"\"\n",
    "        Esegue la backward propagation calcolando i gradienti e aggiornando i parametri.\n",
    "        La catena del gradiente viene calcolata in maniera modulare.\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        if self.loss_function_name not in loss_derivatives:\n",
    "            raise ValueError(f\"Derivata della loss non supportata: {self.loss_function_name}\")\n",
    "        # Calcola dL/dy_pred per l'output\n",
    "        dA = loss_derivatives[self.loss_function_name](y, A[-1])\n",
    "        # Calcola dL/dz nel layer di output\n",
    "        dZ = dA * self._apply_activation_derivative(Z[-1], A[-1], self.output_activation_function_name)\n",
    "        # Calcolo del termine di regolarizzazione per il layer di output\n",
    "        if self.reg_type == \"l2\":\n",
    "            reg_term = self.lambda_reg * self.W[-1] / m\n",
    "        elif self.reg_type == \"l1\":\n",
    "            reg_term = self.lambda_reg * np.sign(self.W[-1]) / m\n",
    "        else:\n",
    "            reg_term = 0\n",
    "        dW = [np.dot(A[-2].T, dZ) / m + reg_term]\n",
    "        db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "        \n",
    "        # Backpropagation nei layer nascosti\n",
    "        for i in range(len(self.W) - 2, -1, -1):\n",
    "            dA = np.dot(dZ, self.W[i + 1].T)\n",
    "            dZ = dA * self._apply_activation_derivative(Z[i], A[i + 1], self.activation_function_name)\n",
    "            if self.reg_type == \"l2\":\n",
    "                reg_term = self.lambda_reg * self.W[i] / m\n",
    "            elif self.reg_type == \"l1\":\n",
    "                reg_term = self.lambda_reg * np.sign(self.W[i]) / m\n",
    "            else:\n",
    "                reg_term = 0\n",
    "            dW.insert(0, np.dot(A[i].T, dZ) / m + reg_term)\n",
    "            db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "        \n",
    "        # Aggiorna i parametri\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] -= self.learning_rate * dW[i]\n",
    "            self.b[i] -= self.learning_rate * db[i]\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, batch_size=32, verbose=True):\n",
    "        \"\"\"\n",
    "        Esegue il training della rete mediante mini-batch gradient descent.\n",
    "        Ritorna la lista degli errori per ogni epoca (ogni 10 epoche).\n",
    "        \"\"\"\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            permutation = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                Z, A = self._forward(X_batch)\n",
    "                self._backward(X_batch, y_batch, Z, A)\n",
    "            if epoch % int(epochs/20) == 0:\n",
    "                _, A_full = self._forward(X)\n",
    "                loss = loss_functions[self.loss_function_name](y, A_full[-1])\n",
    "                # Calcolo del termine di regolarizzazione per il monitoraggio della loss\n",
    "                if self.reg_type == \"l2\":\n",
    "                    reg_term = (self.lambda_reg / 2) * sum(np.sum(w ** 2) for w in self.W)\n",
    "                elif self.reg_type == \"l1\":\n",
    "                    reg_term = self.lambda_reg * sum(np.sum(np.abs(w)) for w in self.W)\n",
    "                else:\n",
    "                    reg_term = 0\n",
    "                total_loss = loss + reg_term\n",
    "                loss_history.append(total_loss)\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "        return loss_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Restituisce le predizioni:\n",
    "          - Se l'output ha un solo neurone, viene usata una soglia a 0.5.\n",
    "          - Se l'output è one-hot encoded, viene usato argmax.\n",
    "        \"\"\"\n",
    "        _, A = self._forward(X)\n",
    "        output = A[-1]\n",
    "        if output.shape[1] == 1:\n",
    "            return (output > 0.5).astype(int)\n",
    "        else:\n",
    "            return np.argmax(output, axis=1)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Calcola l'accuratezza.\n",
    "        Se y è one-hot encoded, converte le etichette in formato indice.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        if y.ndim > 1 and y.shape[1] > 1:\n",
    "            y_true = np.argmax(y, axis=1)\n",
    "        else:\n",
    "            y_true = y\n",
    "        return np.mean(predictions == y_true)\n",
    "\n",
    "# ============================\n",
    "# Esecuzione: Caricamento dataset, training e valutazione\n",
    "# ============================\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(3, one_hot_encode=True)\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "layers = [input_size, 4, 4, output_size]\n",
    "\n",
    "# Istanziamo la rete con i parametri desiderati\n",
    "nn = NeuralNetwork(\n",
    "    layers,\n",
    "    learning_rate=0.05,\n",
    "    lambda_reg=0.00001,\n",
    "    reg_type=\"l2\",  # Cambia in \"l2\" per usare la regolarizzazione L2\n",
    "    loss_function_name=\"mse\",\n",
    "    activation_function_name=\"relu\",\n",
    "    output_activation_function_name=\"sigmoid\"\n",
    ")\n",
    "\n",
    "loss_history = nn.train(X_train, y_train, epochs=300, batch_size=32, verbose=True)\n",
    "accuracy = nn.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "pd.Series(loss_history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-3-train.csv\n",
      "Using cached ../datasets/monks/monk-3-test.csv\n",
      "One-hot encoding MONK-3 dataset...\n",
      "Epoch 0, Loss: 3.0975\n",
      "Epoch 15, Loss: 2.9687\n",
      "Epoch 30, Loss: 2.8550\n",
      "Epoch 45, Loss: 2.7612\n",
      "Epoch 60, Loss: 2.6981\n",
      "Epoch 75, Loss: 2.6535\n",
      "Epoch 90, Loss: 2.6255\n",
      "Epoch 105, Loss: 2.5981\n",
      "Epoch 120, Loss: 2.5715\n",
      "Epoch 135, Loss: 2.5580\n",
      "Epoch 150, Loss: 2.5442\n",
      "Epoch 165, Loss: 2.5207\n",
      "Epoch 180, Loss: 2.4915\n",
      "Epoch 195, Loss: 2.4578\n",
      "Epoch 210, Loss: 2.4239\n",
      "Epoch 225, Loss: 2.3912\n",
      "Epoch 240, Loss: 2.3535\n",
      "Epoch 255, Loss: 2.3160\n",
      "Epoch 270, Loss: 2.2779\n",
      "Epoch 285, Loss: 2.2428\n",
      "Test Accuracy: 0.9699\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "# ============================\n",
    "# Funzioni di attivazione e derivate\n",
    "# ============================\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(z, a):\n",
    "    # z non viene usato, ma lo manteniamo per avere la stessa firma\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(z, a):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "activation_functions = {\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"relu\": relu\n",
    "}\n",
    "\n",
    "activation_derivatives = {\n",
    "    \"sigmoid\": lambda z, a: sigmoid_derivative(z, a),\n",
    "    \"relu\": lambda z, a: relu_derivative(z, a)\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Funzioni di loss e derivate\n",
    "# ============================\n",
    "\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_crossentropy_derivative(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    # Derivata elementwise: dL/dy_pred = - y_true/y_pred + (1-y_true)/(1-y_pred)\n",
    "    return - (y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "loss_functions = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_loss,\n",
    "    \"mse\": mse_loss\n",
    "}\n",
    "\n",
    "loss_derivatives = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_derivative,\n",
    "    \"mse\": mse_derivative\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Funzioni per la regolarizzazione (modulari)\n",
    "# ============================\n",
    "\n",
    "def compute_reg_gradient(W, lambda_reg, reg_type, m):\n",
    "    \"\"\"\n",
    "    Calcola il termine di regolarizzazione da aggiungere al gradiente dei pesi.\n",
    "    \n",
    "    :param W: matrice dei pesi\n",
    "    :param lambda_reg: coefficiente di regolarizzazione\n",
    "    :param reg_type: tipo di regolarizzazione (\"l2\" o \"l1\")\n",
    "    :param m: numero di esempi del batch\n",
    "    :return: termine di regolarizzazione per il gradiente\n",
    "    \"\"\"\n",
    "    if reg_type == \"l2\":\n",
    "        return lambda_reg * W / m\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * np.sign(W) / m\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_reg_loss(W_list, lambda_reg, reg_type):\n",
    "    \"\"\"\n",
    "    Calcola il termine di loss della regolarizzazione per un elenco di pesi.\n",
    "    \n",
    "    :param W_list: lista delle matrici dei pesi\n",
    "    :param lambda_reg: coefficiente di regolarizzazione\n",
    "    :param reg_type: tipo di regolarizzazione (\"l2\" o \"l1\")\n",
    "    :return: valore della regolarizzazione da sommare alla loss\n",
    "    \"\"\"\n",
    "    if reg_type == \"l2\":\n",
    "        return (lambda_reg / 2) * sum(np.sum(W ** 2) for W in W_list)\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * sum(np.sum(np.abs(W)) for W in W_list)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# ============================\n",
    "# Classe della Rete Neurale\n",
    "# ============================\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.9, lambda_reg=0.01, reg_type=\"l2\",\n",
    "                 loss_function_name=\"binary_crossentropy\",\n",
    "                 activation_function_name=\"relu\",\n",
    "                 output_activation_function_name=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        :param layers: lista con la dimensione di ogni layer (input, hidden, output)\n",
    "        :param learning_rate: tasso di apprendimento\n",
    "        :param lambda_reg: coefficiente di regolarizzazione\n",
    "        :param reg_type: tipo di regolarizzazione (\"l2\" o \"l1\")\n",
    "        :param loss_function_name: nome della funzione di loss (deve essere presente in loss_functions e loss_derivatives)\n",
    "        :param activation_function_name: nome della funzione di attivazione per i layer nascosti\n",
    "        :param output_activation_function_name: nome della funzione di attivazione per il layer di output\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.reg_type = reg_type\n",
    "        self.loss_function_name = loss_function_name\n",
    "        self.activation_function_name = activation_function_name\n",
    "        self.output_activation_function_name = output_activation_function_name\n",
    "        self._initialize_parameters()\n",
    "        \n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Inizializza i pesi e bias utilizzando un metodo di inizializzazione (He in questo esempio).\"\"\"\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            weight = np.random.randn(self.layers[i], self.layers[i + 1]) * np.sqrt(2 / self.layers[i])\n",
    "            self.W.append(weight)\n",
    "            self.b.append(np.zeros((1, self.layers[i + 1])))\n",
    "    \n",
    "    def _apply_activation(self, x, func_name):\n",
    "        if func_name not in activation_functions:\n",
    "            raise ValueError(f\"Attivazione non supportata: {func_name}\")\n",
    "        return activation_functions[func_name](x)\n",
    "    \n",
    "    def _apply_activation_derivative(self, z, a, func_name):\n",
    "        if func_name not in activation_derivatives:\n",
    "            raise ValueError(f\"Derivata dell'attivazione non supportata: {func_name}\")\n",
    "        return activation_derivatives[func_name](z, a)\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        \"\"\"\n",
    "        Esegue la forward propagation e ritorna le liste:\n",
    "        - Z: valori pre-attivazione per ogni layer\n",
    "        - A: output attivati per ogni layer (incluso l'input come A[0])\n",
    "        \"\"\"\n",
    "        A = [X]\n",
    "        Z = []\n",
    "        # Propagazione attraverso i layer nascosti\n",
    "        for i in range(len(self.W) - 1):\n",
    "            z_curr = np.dot(A[-1], self.W[i]) + self.b[i]\n",
    "            Z.append(z_curr)\n",
    "            a_curr = self._apply_activation(z_curr, self.activation_function_name)\n",
    "            A.append(a_curr)\n",
    "        # Propagazione nel layer di output\n",
    "        z_out = np.dot(A[-1], self.W[-1]) + self.b[-1]\n",
    "        Z.append(z_out)\n",
    "        a_out = self._apply_activation(z_out, self.output_activation_function_name)\n",
    "        A.append(a_out)\n",
    "        return Z, A\n",
    "    \n",
    "    def _backward(self, X, y, Z, A):\n",
    "        \"\"\"\n",
    "        Esegue la backward propagation calcolando i gradienti e aggiornando i parametri.\n",
    "        La catena del gradiente viene calcolata in maniera modulare.\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        if self.loss_function_name not in loss_derivatives:\n",
    "            raise ValueError(f\"Derivata della loss non supportata: {self.loss_function_name}\")\n",
    "        # Calcola dL/dy_pred per l'output\n",
    "        dA = loss_derivatives[self.loss_function_name](y, A[-1])\n",
    "        # Calcola dL/dz nel layer di output\n",
    "        dZ = dA * self._apply_activation_derivative(Z[-1], A[-1], self.output_activation_function_name)\n",
    "        # Calcolo del gradiente con regolarizzazione per l'ultimo layer\n",
    "        reg_term = compute_reg_gradient(self.W[-1], self.lambda_reg, self.reg_type, m)\n",
    "        dW = [np.dot(A[-2].T, dZ) / m + reg_term]\n",
    "        db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "        \n",
    "        # Backpropagation nei layer nascosti\n",
    "        for i in range(len(self.W) - 2, -1, -1):\n",
    "            dA = np.dot(dZ, self.W[i + 1].T)\n",
    "            dZ = dA * self._apply_activation_derivative(Z[i], A[i + 1], self.activation_function_name)\n",
    "            reg_term = compute_reg_gradient(self.W[i], self.lambda_reg, self.reg_type, m)\n",
    "            dW.insert(0, np.dot(A[i].T, dZ) / m + reg_term)\n",
    "            db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "        \n",
    "        # Aggiorna i parametri\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] -= self.learning_rate * dW[i]\n",
    "            self.b[i] -= self.learning_rate * db[i]\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, batch_size=32, verbose=True):\n",
    "        \"\"\"\n",
    "        Esegue il training della rete mediante mini-batch gradient descent.\n",
    "        Ritorna la lista degli errori per ogni epoca (ogni 10 epoche).\n",
    "        \"\"\"\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            permutation = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                Z, A = self._forward(X_batch)\n",
    "                self._backward(X_batch, y_batch, Z, A)\n",
    "            if epoch % int(epochs / 20) == 0:\n",
    "                _, A_full = self._forward(X)\n",
    "                loss = loss_functions[self.loss_function_name](y, A_full[-1])\n",
    "                reg_loss = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "                total_loss = loss + reg_loss\n",
    "                loss_history.append(total_loss)\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "        return loss_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Restituisce le predizioni:\n",
    "          - Se l'output ha un solo neurone, viene usata una soglia a 0.5.\n",
    "          - Se l'output è one-hot encoded, viene usato argmax.\n",
    "        \"\"\"\n",
    "        _, A = self._forward(X)\n",
    "        output = A[-1]\n",
    "        if output.shape[1] == 1:\n",
    "            return (output > 0.5).astype(int)\n",
    "        else:\n",
    "            return np.argmax(output, axis=1)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Calcola l'accuratezza.\n",
    "        Se y è one-hot encoded, converte le etichette in formato indice.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        if y.ndim > 1 and y.shape[1] > 1:\n",
    "            y_true = np.argmax(y, axis=1)\n",
    "        else:\n",
    "            y_true = y\n",
    "        return np.mean(predictions == y_true)\n",
    "\n",
    "# ============================\n",
    "# Esecuzione: Caricamento dataset, training e valutazione\n",
    "# ============================\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(3, one_hot_encode=True)\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "layers = [input_size, 4, 4, output_size]\n",
    "\n",
    "# Istanziamo la rete con i parametri desiderati\n",
    "nn = NeuralNetwork(\n",
    "    layers,\n",
    "    learning_rate=0.05,\n",
    "    lambda_reg=0.1,\n",
    "    reg_type=\"l1\",  # Cambia in \"l2\" per usare la regolarizzazione L2\n",
    "    loss_function_name=\"mse\",\n",
    "    activation_function_name=\"relu\",\n",
    "    output_activation_function_name=\"sigmoid\"\n",
    ")\n",
    "\n",
    "loss_history = nn.train(X_train, y_train, epochs=300, batch_size=32, verbose=True)\n",
    "accuracy = nn.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "pd.Series(loss_history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training della rete neurale per regressione...\n",
      "Epoch 0, Loss: 2.0243\n",
      "Epoch 15, Loss: 0.8982\n",
      "Epoch 30, Loss: 0.7697\n",
      "Epoch 45, Loss: 0.7187\n",
      "Epoch 60, Loss: 0.6852\n",
      "Epoch 75, Loss: 0.6588\n",
      "Epoch 90, Loss: 0.6437\n",
      "Epoch 105, Loss: 0.6298\n",
      "Epoch 120, Loss: 0.6185\n",
      "Epoch 135, Loss: 0.6087\n",
      "Epoch 150, Loss: 0.5996\n",
      "Epoch 165, Loss: 0.5909\n",
      "Epoch 180, Loss: 0.5824\n",
      "Epoch 195, Loss: 0.5745\n",
      "Epoch 210, Loss: 0.5656\n",
      "Epoch 225, Loss: 0.5596\n",
      "Epoch 240, Loss: 0.5519\n",
      "Epoch 255, Loss: 0.5461\n",
      "Epoch 270, Loss: 0.5395\n",
      "Epoch 285, Loss: 0.5326\n",
      "\n",
      "Neural Network Test MSE (sul target scalato): 0.5626\n",
      "Baseline Linear Regression Test MSE (sul target scalato): 0.4891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import per il dataset e la baseline lineare\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ============================\n",
    "# Funzioni di attivazione e derivate\n",
    "# ============================\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(z, a):\n",
    "    # z non viene usato, ma lo manteniamo per avere la stessa firma\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(z, a):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(z, a):\n",
    "    return np.ones_like(a)\n",
    "\n",
    "activation_functions = {\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"relu\": relu,\n",
    "    \"linear\": linear\n",
    "}\n",
    "\n",
    "activation_derivatives = {\n",
    "    \"sigmoid\": lambda z, a: sigmoid_derivative(z, a),\n",
    "    \"relu\": lambda z, a: relu_derivative(z, a),\n",
    "    \"linear\": lambda z, a: linear_derivative(z, a)\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Funzioni di loss e derivate\n",
    "# ============================\n",
    "\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_crossentropy_derivative(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return - (y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "loss_functions = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_loss,\n",
    "    \"mse\": mse_loss\n",
    "}\n",
    "\n",
    "loss_derivatives = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_derivative,\n",
    "    \"mse\": mse_derivative\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Funzioni per la regolarizzazione (modulari)\n",
    "# ============================\n",
    "\n",
    "def compute_reg_gradient(W, lambda_reg, reg_type, m):\n",
    "    if reg_type == \"l2\":\n",
    "        return lambda_reg * W / m\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * np.sign(W) / m\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_reg_loss(W_list, lambda_reg, reg_type):\n",
    "    if reg_type == \"l2\":\n",
    "        return (lambda_reg / 2) * sum(np.sum(W ** 2) for W in W_list)\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * sum(np.sum(np.abs(W)) for W in W_list)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# ============================\n",
    "# Classe della Rete Neurale\n",
    "# ============================\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.01, lambda_reg=0.001, reg_type=\"l2\",\n",
    "                 loss_function_name=None,\n",
    "                 activation_function_name=\"relu\",\n",
    "                 output_activation_function_name=None,\n",
    "                 task=\"classification\"):\n",
    "        \"\"\"\n",
    "        :param layers: lista con la dimensione di ogni layer (input, hidden, output)\n",
    "        :param learning_rate: tasso di apprendimento\n",
    "        :param lambda_reg: coefficiente di regolarizzazione\n",
    "        :param reg_type: tipo di regolarizzazione (\"l2\" o \"l1\")\n",
    "        :param loss_function_name: nome della funzione di loss (se None, viene settata in base al task)\n",
    "        :param activation_function_name: attivazione per i layer nascosti\n",
    "        :param output_activation_function_name: attivazione per il layer di output (se None, viene settata in base al task)\n",
    "        :param task: \"classification\" o \"regression\"\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.reg_type = reg_type\n",
    "        self.task = task\n",
    "        \n",
    "        # Impostiamo default diversi in base al task\n",
    "        if self.task == \"regression\":\n",
    "            self.loss_function_name = loss_function_name or \"mse\"\n",
    "            self.output_activation_function_name = output_activation_function_name or \"linear\"\n",
    "        else:\n",
    "            self.loss_function_name = loss_function_name or \"binary_crossentropy\"\n",
    "            self.output_activation_function_name = output_activation_function_name or \"sigmoid\"\n",
    "            \n",
    "        self.activation_function_name = activation_function_name\n",
    "        self._initialize_parameters()\n",
    "        \n",
    "    def _initialize_parameters(self):\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            # Utilizziamo l'inizializzazione di He (ottimale per ReLU)\n",
    "            weight = np.random.randn(self.layers[i], self.layers[i + 1]) * np.sqrt(2 / self.layers[i])\n",
    "            self.W.append(weight)\n",
    "            self.b.append(np.zeros((1, self.layers[i + 1])))\n",
    "    \n",
    "    def _apply_activation(self, x, func_name):\n",
    "        if func_name not in activation_functions:\n",
    "            raise ValueError(f\"Attivazione non supportata: {func_name}\")\n",
    "        return activation_functions[func_name](x)\n",
    "    \n",
    "    def _apply_activation_derivative(self, z, a, func_name):\n",
    "        if func_name not in activation_derivatives:\n",
    "            raise ValueError(f\"Derivata dell'attivazione non supportata: {func_name}\")\n",
    "        return activation_derivatives[func_name](z, a)\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        A = [X]\n",
    "        Z = []\n",
    "        # Propagazione attraverso i layer nascosti\n",
    "        for i in range(len(self.W) - 1):\n",
    "            z_curr = np.dot(A[-1], self.W[i]) + self.b[i]\n",
    "            Z.append(z_curr)\n",
    "            a_curr = self._apply_activation(z_curr, self.activation_function_name)\n",
    "            A.append(a_curr)\n",
    "        # Propagazione nel layer di output\n",
    "        z_out = np.dot(A[-1], self.W[-1]) + self.b[-1]\n",
    "        Z.append(z_out)\n",
    "        a_out = self._apply_activation(z_out, self.output_activation_function_name)\n",
    "        A.append(a_out)\n",
    "        return Z, A\n",
    "    \n",
    "    def _backward(self, X, y, Z, A):\n",
    "        m = X.shape[0]\n",
    "        if self.loss_function_name not in loss_derivatives:\n",
    "            raise ValueError(f\"Derivata della loss non supportata: {self.loss_function_name}\")\n",
    "        # Calcola dL/dy_pred per l'output\n",
    "        dA = loss_derivatives[self.loss_function_name](y, A[-1])\n",
    "        # Calcola dL/dz nel layer di output\n",
    "        dZ = dA * self._apply_activation_derivative(Z[-1], A[-1], self.output_activation_function_name)\n",
    "        reg_term = compute_reg_gradient(self.W[-1], self.lambda_reg, self.reg_type, m)\n",
    "        dW = [np.dot(A[-2].T, dZ) / m + reg_term]\n",
    "        db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "        \n",
    "        # Backpropagation nei layer nascosti\n",
    "        for i in range(len(self.W) - 2, -1, -1):\n",
    "            dA = np.dot(dZ, self.W[i + 1].T)\n",
    "            dZ = dA * self._apply_activation_derivative(Z[i], A[i + 1], self.activation_function_name)\n",
    "            reg_term = compute_reg_gradient(self.W[i], self.lambda_reg, self.reg_type, m)\n",
    "            dW.insert(0, np.dot(A[i].T, dZ) / m + reg_term)\n",
    "            db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "        \n",
    "        # Aggiorna i parametri\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] -= self.learning_rate * dW[i]\n",
    "            self.b[i] -= self.learning_rate * db[i]\n",
    "    \n",
    "    def train(self, X, y, epochs=300, batch_size=32, verbose=True):\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            permutation = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                Z, A = self._forward(X_batch)\n",
    "                self._backward(X_batch, y_batch, Z, A)\n",
    "            if epoch % max(1, int(epochs / 20)) == 0:\n",
    "                _, A_full = self._forward(X)\n",
    "                loss = loss_functions[self.loss_function_name](y, A_full[-1])\n",
    "                reg_loss = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "                total_loss = loss + reg_loss\n",
    "                loss_history.append(total_loss)\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "        return loss_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, A = self._forward(X)\n",
    "        output = A[-1]\n",
    "        if self.task == \"classification\":\n",
    "            if output.shape[1] == 1:\n",
    "                return (output > 0.5).astype(int)\n",
    "            else:\n",
    "                return np.argmax(output, axis=1)\n",
    "        else:  # regressione\n",
    "            return output\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        if self.task == \"regression\":\n",
    "            return mse_loss(y, predictions)\n",
    "        else:\n",
    "            if y.ndim > 1 and y.shape[1] > 1:\n",
    "                y_true = np.argmax(y, axis=1)\n",
    "            else:\n",
    "                y_true = y\n",
    "            return np.mean(predictions == y_true)\n",
    "\n",
    "# ============================\n",
    "# Esperimento di regressione con il dataset Diabetes\n",
    "# ============================\n",
    "\n",
    "# Caricamento e scaling del dataset\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target.reshape(-1, 1)  # rendiamo y bidimensionale\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "# Suddividiamo in train e test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_scaled, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Definiamo l'architettura della rete per regressione\n",
    "input_size = X_train.shape[1]    # 10 feature\n",
    "layers = [input_size, 10, 10, 1]   # 2 hidden layer con 10 neuroni ciascuno, 1 output\n",
    "\n",
    "# Istanziamo la rete neurale per regressione\n",
    "nn_reg = NeuralNetwork(\n",
    "    layers=layers,\n",
    "    learning_rate=0.001,\n",
    "    lambda_reg=0.001,\n",
    "    reg_type=\"l2\",\n",
    "    loss_function_name=\"mse\",               # per regressione usiamo MSE\n",
    "    activation_function_name=\"relu\",\n",
    "    output_activation_function_name=\"linear\",  # output lineare per regressione\n",
    "    task=\"regression\"\n",
    ")\n",
    "\n",
    "print(\"Training della rete neurale per regressione...\")\n",
    "loss_history = nn_reg.train(X_train, y_train, epochs=300, batch_size=32, verbose=True)\n",
    "nn_mse = nn_reg.evaluate(X_test, y_test)\n",
    "print(f\"\\nNeural Network Test MSE (sul target scalato): {nn_mse:.4f}\")\n",
    "\n",
    "# Baseline: regressione lineare di sklearn\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "lr_mse = np.mean((y_test - y_pred_lr) ** 2)\n",
    "print(f\"Baseline Linear Regression Test MSE (sul target scalato): {lr_mse:.4f}\")\n",
    "\n",
    "pd.Series(loss_history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training della rete neurale per regressione...\n",
      "Epoch 0, Loss: 2.0243\n",
      "Epoch 15, Loss: 0.8982\n",
      "Epoch 30, Loss: 0.7697\n",
      "Epoch 45, Loss: 0.7187\n",
      "Epoch 60, Loss: 0.6852\n",
      "Epoch 75, Loss: 0.6588\n",
      "Epoch 90, Loss: 0.6437\n",
      "Epoch 105, Loss: 0.6298\n",
      "Epoch 120, Loss: 0.6185\n",
      "Epoch 135, Loss: 0.6087\n",
      "Epoch 150, Loss: 0.5996\n",
      "Epoch 165, Loss: 0.5909\n",
      "Epoch 180, Loss: 0.5824\n",
      "Epoch 195, Loss: 0.5745\n",
      "Epoch 210, Loss: 0.5656\n",
      "Epoch 225, Loss: 0.5596\n",
      "Epoch 240, Loss: 0.5519\n",
      "Epoch 255, Loss: 0.5461\n",
      "Epoch 270, Loss: 0.5395\n",
      "Epoch 285, Loss: 0.5326\n",
      "\n",
      "Neural Network Test MSE (sul target scalato): 0.5626\n",
      "Baseline Linear Regression Test MSE (sul target scalato): 0.4891\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import per il dataset e la baseline lineare\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ============================\n",
    "# Funzioni di attivazione e derivate\n",
    "# ============================\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(z, a):\n",
    "    # z non viene usato, ma lo manteniamo per avere la stessa firma\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(z, a):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(z, a):\n",
    "    return np.ones_like(a)\n",
    "\n",
    "activation_functions = {\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"relu\": relu,\n",
    "    \"linear\": linear\n",
    "}\n",
    "\n",
    "activation_derivatives = {\n",
    "    \"sigmoid\": lambda z, a: sigmoid_derivative(z, a),\n",
    "    \"relu\": lambda z, a: relu_derivative(z, a),\n",
    "    \"linear\": lambda z, a: linear_derivative(z, a)\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Funzioni di loss e derivate\n",
    "# ============================\n",
    "\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_crossentropy_derivative(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return - (y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "loss_functions = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_loss,\n",
    "    \"mse\": mse_loss\n",
    "}\n",
    "\n",
    "loss_derivatives = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_derivative,\n",
    "    \"mse\": mse_derivative\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Funzioni per la regolarizzazione (modulari)\n",
    "# ============================\n",
    "\n",
    "def compute_reg_gradient(W, lambda_reg, reg_type, m):\n",
    "    if reg_type == \"l2\":\n",
    "        return lambda_reg * W / m\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * np.sign(W) / m\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_reg_loss(W_list, lambda_reg, reg_type):\n",
    "    if reg_type == \"l2\":\n",
    "        return (lambda_reg / 2) * sum(np.sum(W ** 2) for W in W_list)\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * sum(np.sum(np.abs(W)) for W in W_list)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# ============================\n",
    "# Classe della Rete Neurale\n",
    "# ============================\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.01, lambda_reg=0.001, reg_type=\"l2\",\n",
    "                 loss_function_name=None,\n",
    "                 activation_function_name=\"relu\",\n",
    "                 output_activation_function_name=None,\n",
    "                 activation_function_names=None,\n",
    "                 task=\"classification\"):\n",
    "        \"\"\"\n",
    "        :param layers: lista con la dimensione di ogni layer (input, hidden, output)\n",
    "        :param learning_rate: tasso di apprendimento\n",
    "        :param lambda_reg: coefficiente di regolarizzazione\n",
    "        :param reg_type: tipo di regolarizzazione (\"l2\" o \"l1\")\n",
    "        :param loss_function_name: nome della funzione di loss (se None, viene settata in base al task)\n",
    "        :param activation_function_name: attivazione da usare per i layer nascosti (se non viene specificato activation_function_names)\n",
    "        :param output_activation_function_name: attivazione per il layer di output (se None, viene settata in base al task)\n",
    "        :param activation_function_names: lista di nomi di funzioni di attivazione per ogni layer (lunghezza = len(layers)-1)\n",
    "        :param task: \"classification\" o \"regression\"\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.reg_type = reg_type\n",
    "        self.task = task\n",
    "        \n",
    "        # Impostiamo i default in base al task\n",
    "        if self.task == \"regression\":\n",
    "            self.loss_function_name = loss_function_name or \"mse\"\n",
    "            output_activation_function_name = output_activation_function_name or \"linear\"\n",
    "        else:\n",
    "            self.loss_function_name = loss_function_name or \"binary_crossentropy\"\n",
    "            output_activation_function_name = output_activation_function_name or \"sigmoid\"\n",
    "        \n",
    "        # Se non è specificata una lista di attivazioni per ogni layer, usiamo la stessa per tutti i layer nascosti\n",
    "        # e quella di output per l'ultimo layer\n",
    "        if activation_function_names is None:\n",
    "            # Creo una lista di lunghezza len(layers)-1\n",
    "            self.activation_function_names = [activation_function_name] * (len(layers) - 1)\n",
    "            self.activation_function_names[-1] = output_activation_function_name\n",
    "        else:\n",
    "            if len(activation_function_names) != len(layers) - 1:\n",
    "                raise ValueError(\"La lista activation_function_names deve avere una lunghezza pari a len(layers)-1.\")\n",
    "            self.activation_function_names = activation_function_names\n",
    "        \n",
    "        self._initialize_parameters()\n",
    "        \n",
    "    def _initialize_parameters(self):\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            # Inizializzazione di He (ottimale per ReLU)\n",
    "            weight = np.random.randn(self.layers[i], self.layers[i + 1]) * np.sqrt(2 / self.layers[i])\n",
    "            self.W.append(weight)\n",
    "            self.b.append(np.zeros((1, self.layers[i + 1])))\n",
    "    \n",
    "    def _apply_activation(self, x, func_name):\n",
    "        if func_name not in activation_functions:\n",
    "            raise ValueError(f\"Attivazione non supportata: {func_name}\")\n",
    "        return activation_functions[func_name](x)\n",
    "    \n",
    "    def _apply_activation_derivative(self, z, a, func_name):\n",
    "        if func_name not in activation_derivatives:\n",
    "            raise ValueError(f\"Derivata dell'attivazione non supportata: {func_name}\")\n",
    "        return activation_derivatives[func_name](z, a)\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        A = [X]\n",
    "        Z = []\n",
    "        # Propagazione attraverso i layer nascosti (tutti tranne l'output)\n",
    "        for i in range(len(self.W) - 1):\n",
    "            z_curr = np.dot(A[-1], self.W[i]) + self.b[i]\n",
    "            Z.append(z_curr)\n",
    "            a_curr = self._apply_activation(z_curr, self.activation_function_names[i])\n",
    "            A.append(a_curr)\n",
    "        # Propagazione nel layer di output\n",
    "        z_out = np.dot(A[-1], self.W[-1]) + self.b[-1]\n",
    "        Z.append(z_out)\n",
    "        a_out = self._apply_activation(z_out, self.activation_function_names[-1])\n",
    "        A.append(a_out)\n",
    "        return Z, A\n",
    "    \n",
    "    def _backward(self, X, y, Z, A):\n",
    "        m = X.shape[0]\n",
    "        if self.loss_function_name not in loss_derivatives:\n",
    "            raise ValueError(f\"Derivata della loss non supportata: {self.loss_function_name}\")\n",
    "        # Calcola dL/dy_pred per l'output\n",
    "        dA = loss_derivatives[self.loss_function_name](y, A[-1])\n",
    "        # Calcola dL/dz per il layer di output\n",
    "        dZ = dA * self._apply_activation_derivative(Z[-1], A[-1], self.activation_function_names[-1])\n",
    "        reg_term = compute_reg_gradient(self.W[-1], self.lambda_reg, self.reg_type, m)\n",
    "        dW = [np.dot(A[-2].T, dZ) / m + reg_term]\n",
    "        db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "        \n",
    "        # Backpropagation nei layer nascosti\n",
    "        for i in range(len(self.W) - 2, -1, -1):\n",
    "            dA = np.dot(dZ, self.W[i + 1].T)\n",
    "            dZ = dA * self._apply_activation_derivative(Z[i], A[i + 1], self.activation_function_names[i])\n",
    "            reg_term = compute_reg_gradient(self.W[i], self.lambda_reg, self.reg_type, m)\n",
    "            dW.insert(0, np.dot(A[i].T, dZ) / m + reg_term)\n",
    "            db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "        \n",
    "        # Aggiorna i parametri\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] -= self.learning_rate * dW[i]\n",
    "            self.b[i] -= self.learning_rate * db[i]\n",
    "    \n",
    "    def train(self, X, y, epochs=300, batch_size=32, verbose=True):\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            permutation = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                Z, A = self._forward(X_batch)\n",
    "                self._backward(X_batch, y_batch, Z, A)\n",
    "            if epoch % max(1, int(epochs / 20)) == 0:\n",
    "                _, A_full = self._forward(X)\n",
    "                loss = loss_functions[self.loss_function_name](y, A_full[-1])\n",
    "                reg_loss = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "                total_loss = loss + reg_loss\n",
    "                loss_history.append(total_loss)\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "        return loss_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, A = self._forward(X)\n",
    "        output = A[-1]\n",
    "        if self.task == \"classification\":\n",
    "            if output.shape[1] == 1:\n",
    "                return (output > 0.5).astype(int)\n",
    "            else:\n",
    "                return np.argmax(output, axis=1)\n",
    "        else:  # regressione\n",
    "            return output\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        if self.task == \"regression\":\n",
    "            return mse_loss(y, predictions)\n",
    "        else:\n",
    "            if y.ndim > 1 and y.shape[1] > 1:\n",
    "                y_true = np.argmax(y, axis=1)\n",
    "            else:\n",
    "                y_true = y\n",
    "            return np.mean(predictions == y_true)\n",
    "\n",
    "# ============================\n",
    "# Esperimento di regressione con il dataset Diabetes\n",
    "# ============================\n",
    "\n",
    "# Caricamento e scaling del dataset\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target.reshape(-1, 1)  # rendiamo y bidimensionale\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "# Suddividiamo in train e test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_scaled, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Definiamo l'architettura della rete per regressione\n",
    "input_size = X_train.shape[1]    # 10 feature\n",
    "layers = [input_size, 10, 10, 1]   # 2 hidden layer con 10 neuroni ciascuno, 1 output\n",
    "\n",
    "# Esempio: specifica di funzioni di attivazione diverse per ogni layer\n",
    "# In questo caso, per i due layer nascosti usiamo \"relu\" e per l'output \"linear\"\n",
    "activation_funcs = [\"relu\", \"relu\", \"linear\"]\n",
    "\n",
    "# Istanziamo la rete neurale per regressione\n",
    "nn_reg = NeuralNetwork(\n",
    "    layers=layers,\n",
    "    learning_rate=0.001,\n",
    "    lambda_reg=0.001,\n",
    "    reg_type=\"l2\",\n",
    "    loss_function_name=\"mse\",  # per regressione usiamo MSE\n",
    "    activation_function_names=activation_funcs,\n",
    "    task=\"regression\"\n",
    ")\n",
    "\n",
    "print(\"Training della rete neurale per regressione...\")\n",
    "loss_history = nn_reg.train(X_train, y_train, epochs=300, batch_size=32, verbose=True)\n",
    "nn_mse = nn_reg.evaluate(X_test, y_test)\n",
    "print(f\"\\nNeural Network Test MSE (sul target scalato): {nn_mse:.4f}\")\n",
    "\n",
    "# Baseline: regressione lineare di sklearn\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "lr_mse = np.mean((y_test - y_pred_lr) ** 2)\n",
    "print(f\"Baseline Linear Regression Test MSE (sul target scalato): {lr_mse:.4f}\")\n",
    "\n",
    "pd.Series(loss_history).plot()\n",
    "plt.xlabel(\"Epoch (intervalli)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss History\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training della rete neurale per classificazione...\n",
      "Epoch 0, Loss: 0.6555\n",
      "Epoch 15, Loss: 0.2408\n",
      "Epoch 30, Loss: 0.1524\n",
      "Epoch 45, Loss: 0.1215\n",
      "Epoch 60, Loss: 0.1066\n",
      "Epoch 75, Loss: 0.0972\n",
      "Epoch 90, Loss: 0.0905\n",
      "Epoch 105, Loss: 0.0856\n",
      "Epoch 120, Loss: 0.0816\n",
      "Epoch 135, Loss: 0.0776\n",
      "Epoch 150, Loss: 0.0743\n",
      "Epoch 165, Loss: 0.0715\n",
      "Epoch 180, Loss: 0.0689\n",
      "Epoch 195, Loss: 0.0665\n",
      "Epoch 210, Loss: 0.0644\n",
      "Epoch 225, Loss: 0.0624\n",
      "Epoch 240, Loss: 0.0604\n",
      "Epoch 255, Loss: 0.0588\n",
      "Epoch 270, Loss: 0.0574\n",
      "Epoch 285, Loss: 0.0561\n",
      "\n",
      "Neural Network Classification Accuracy: 0.9825\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming the NeuralNetwork class and related functions have been defined as in the previous code snippet.\n",
    "\n",
    "# ============================\n",
    "# Example for Binary Classification using the Breast Cancer Dataset\n",
    "# ============================\n",
    "\n",
    "# Caricamento del dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target.reshape(-1, 1)  # rendiamo y bidimensionale\n",
    "\n",
    "# Scaling delle features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Suddivisione in train e test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Definizione dell'architettura della rete:\n",
    "# - Input: numero di features (30)\n",
    "# - Due hidden layer con 16 e 8 neuroni rispettivamente\n",
    "# - Output: 1 neurone per la classificazione binaria\n",
    "layers = [X_train.shape[1], 16, 8, 1]\n",
    "\n",
    "# Specifica delle funzioni di attivazione per ogni layer:\n",
    "# - Hidden layers: \"relu\"\n",
    "# - Output layer: \"sigmoid\" (per ottenere una probabilità)\n",
    "activation_funcs = [\"relu\", \"relu\", \"sigmoid\"]\n",
    "\n",
    "# Istanziamo la rete neurale per classificazione\n",
    "nn_clf = NeuralNetwork(\n",
    "    layers=layers,\n",
    "    learning_rate=0.01,\n",
    "    lambda_reg=0.001,\n",
    "    reg_type=\"l2\",\n",
    "    activation_function_names=activation_funcs,\n",
    "    task=\"classification\"  # Importante: specifica che si tratta di un task di classificazione\n",
    ")\n",
    "\n",
    "print(\"Training della rete neurale per classificazione...\")\n",
    "loss_history = nn_clf.train(X_train, y_train, epochs=300, batch_size=32, verbose=True)\n",
    "\n",
    "# Valutazione: la funzione evaluate restituisce l'accuratezza per il task di classificazione\n",
    "accuracy = nn_clf.evaluate(X_test, y_test)\n",
    "print(f\"\\nNeural Network Classification Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot della storia del loss\n",
    "pd.Series(loss_history).plot()\n",
    "plt.xlabel(\"Epoch (intervalli)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss History - Classification\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-1-train.csv\n",
      "Using cached ../datasets/monks/monk-1-test.csv\n",
      "One-hot encoding MONK-1 dataset...\n",
      "Training della rete neurale per classificazione...\n",
      "Epoch 0, Loss: 0.2689\n",
      "Epoch 50, Loss: 0.1071\n",
      "Epoch 100, Loss: 0.0679\n",
      "Epoch 150, Loss: 0.0309\n",
      "Epoch 200, Loss: 0.0097\n",
      "Epoch 250, Loss: 0.0052\n",
      "Epoch 300, Loss: 0.0034\n",
      "Epoch 350, Loss: 0.0025\n",
      "Epoch 400, Loss: 0.0019\n",
      "Epoch 450, Loss: 0.0016\n",
      "Epoch 500, Loss: 0.0013\n",
      "Epoch 550, Loss: 0.0011\n",
      "Epoch 600, Loss: 0.0010\n",
      "Epoch 650, Loss: 0.0009\n",
      "Epoch 700, Loss: 0.0008\n",
      "Epoch 750, Loss: 0.0007\n",
      "Epoch 800, Loss: 0.0007\n",
      "Epoch 850, Loss: 0.0006\n",
      "Epoch 900, Loss: 0.0006\n",
      "Epoch 950, Loss: 0.0005\n",
      "\n",
      "Neural Network Classification Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(1, one_hot_encode=True)\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "layers = [input_size, 4, output_size]\n",
    "\n",
    "activation_funcs = [\"relu\",  \"sigmoid\"]\n",
    "\n",
    "nn_clf = NeuralNetwork(\n",
    "    layers=layers,\n",
    "    learning_rate=0.5,\n",
    "    lambda_reg=0,\n",
    "    reg_type=\"l2\",\n",
    "    loss_function_name=\"mse\",\n",
    "    activation_function_names=activation_funcs,\n",
    "    task=\"classification\"  # Importante: specifica che si tratta di un task di classificazione\n",
    ")\n",
    "\n",
    "print(\"Training della rete neurale per classificazione...\")\n",
    "loss_history = nn_clf.train(X_train, y_train, epochs=1000, batch_size=32, verbose=True)\n",
    "\n",
    "accuracy = nn_clf.evaluate(X_test, y_test)\n",
    "print(f\"\\nNeural Network Classification Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "pd.Series(loss_history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-1-train.csv\n",
      "Using cached ../datasets/monks/monk-1-test.csv\n",
      "One-hot encoding MONK-1 dataset...\n",
      "Training della rete neurale per classificazione...\n",
      "Epoch 0, Loss: 0.2707\n",
      "Epoch 50, Loss: 0.1064\n",
      "Epoch 100, Loss: 0.0704\n",
      "Epoch 150, Loss: 0.0485\n",
      "Epoch 200, Loss: 0.0123\n",
      "Epoch 250, Loss: 0.0060\n",
      "Epoch 300, Loss: 0.0037\n",
      "Epoch 350, Loss: 0.0027\n",
      "Epoch 400, Loss: 0.0020\n",
      "Epoch 450, Loss: 0.0016\n",
      "Epoch 500, Loss: 0.0014\n",
      "Epoch 550, Loss: 0.0012\n",
      "Epoch 600, Loss: 0.0010\n",
      "Epoch 650, Loss: 0.0009\n",
      "Epoch 700, Loss: 0.0008\n",
      "Epoch 750, Loss: 0.0007\n",
      "Epoch 800, Loss: 0.0007\n",
      "Epoch 850, Loss: 0.0006\n",
      "Epoch 900, Loss: 0.0006\n",
      "Epoch 950, Loss: 0.0005\n",
      "\n",
      "Neural Network Classification Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import per il dataset e la baseline lineare\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ============================\n",
    "# Funzioni di attivazione e derivate\n",
    "# ============================\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(z, a):\n",
    "    # z non viene usato, ma lo manteniamo per avere la stessa firma\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(z, a):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(z, a):\n",
    "    return np.ones_like(a)\n",
    "\n",
    "activation_functions = {\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"relu\": relu,\n",
    "    \"linear\": linear\n",
    "}\n",
    "\n",
    "activation_derivatives = {\n",
    "    \"sigmoid\": lambda z, a: sigmoid_derivative(z, a),\n",
    "    \"relu\": lambda z, a: relu_derivative(z, a),\n",
    "    \"linear\": lambda z, a: linear_derivative(z, a)\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Funzioni di loss e derivate\n",
    "# ============================\n",
    "\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_crossentropy_derivative(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return - (y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "loss_functions = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_loss,\n",
    "    \"mse\": mse_loss\n",
    "}\n",
    "\n",
    "loss_derivatives = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_derivative,\n",
    "    \"mse\": mse_derivative\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Funzioni per la regolarizzazione (modulari)\n",
    "# ============================\n",
    "\n",
    "def compute_reg_gradient(W, lambda_reg, reg_type, m):\n",
    "    if reg_type == \"l2\":\n",
    "        return lambda_reg * W / m\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * np.sign(W) / m\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_reg_loss(W_list, lambda_reg, reg_type):\n",
    "    if reg_type == \"l2\":\n",
    "        return (lambda_reg / 2) * sum(np.sum(W ** 2) for W in W_list)\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * sum(np.sum(np.abs(W)) for W in W_list)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# ============================\n",
    "# Classe della Rete Neurale\n",
    "# ============================\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.01, lambda_reg=0.001, reg_type=\"l2\",\n",
    "                 loss_function_name=None,\n",
    "                 activation_function_name=\"relu\",\n",
    "                 output_activation_function_name=None,\n",
    "                 activation_function_names=None,\n",
    "                 task=\"classification\"):\n",
    "        \"\"\"\n",
    "        :param layers: lista con la dimensione di ogni layer (input, hidden, output)\n",
    "        :param learning_rate: tasso di apprendimento\n",
    "        :param lambda_reg: coefficiente di regolarizzazione\n",
    "        :param reg_type: tipo di regolarizzazione (\"l2\" o \"l1\")\n",
    "        :param loss_function_name: nome della funzione di loss (se None, viene settata in base al task)\n",
    "        :param activation_function_name: attivazione da usare per i layer nascosti (se non viene specificato activation_function_names)\n",
    "        :param output_activation_function_name: attivazione per il layer di output (se None, viene settata in base al task)\n",
    "        :param activation_function_names: lista di nomi di funzioni di attivazione per ogni layer (lunghezza = len(layers)-1)\n",
    "        :param task: \"classification\" o \"regression\"\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.reg_type = reg_type\n",
    "        self.task = task\n",
    "        \n",
    "        # Impostiamo i default in base al task\n",
    "        if self.task == \"regression\":\n",
    "            self.loss_function_name = loss_function_name or \"mse\"\n",
    "            output_activation_function_name = output_activation_function_name or \"linear\"\n",
    "        else:\n",
    "            self.loss_function_name = loss_function_name or \"binary_crossentropy\"\n",
    "            output_activation_function_name = output_activation_function_name or \"sigmoid\"\n",
    "        \n",
    "        # Se non è specificata una lista di attivazioni per ogni layer, usiamo la stessa per tutti i layer nascosti\n",
    "        # e quella di output per l'ultimo layer\n",
    "        if activation_function_names is None:\n",
    "            # Creo una lista di lunghezza len(layers)-1\n",
    "            self.activation_function_names = [activation_function_name] * (len(layers) - 1)\n",
    "            self.activation_function_names[-1] = output_activation_function_name\n",
    "        else:\n",
    "            if len(activation_function_names) != len(layers) - 1:\n",
    "                raise ValueError(\"La lista activation_function_names deve avere una lunghezza pari a len(layers)-1.\")\n",
    "            self.activation_function_names = activation_function_names\n",
    "        \n",
    "        self._initialize_parameters()\n",
    "        \n",
    "    def _initialize_parameters(self):\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            # Inizializzazione di He (ottimale per ReLU)\n",
    "            weight = np.random.randn(self.layers[i], self.layers[i + 1]) * np.sqrt(2 / self.layers[i])\n",
    "            self.W.append(weight)\n",
    "            self.b.append(np.zeros((1, self.layers[i + 1])))\n",
    "    \n",
    "    def _apply_activation(self, x, func_name):\n",
    "        if func_name not in activation_functions:\n",
    "            raise ValueError(f\"Attivazione non supportata: {func_name}\")\n",
    "        return activation_functions[func_name](x)\n",
    "    \n",
    "    def _apply_activation_derivative(self, z, a, func_name):\n",
    "        if func_name not in activation_derivatives:\n",
    "            raise ValueError(f\"Derivata dell'attivazione non supportata: {func_name}\")\n",
    "        return activation_derivatives[func_name](z, a)\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        A = [X]\n",
    "        Z = []\n",
    "        # Propagazione attraverso i layer nascosti (tutti tranne l'output)\n",
    "        for i in range(len(self.W) - 1):\n",
    "            z_curr = np.dot(A[-1], self.W[i]) + self.b[i]\n",
    "            Z.append(z_curr)\n",
    "            a_curr = self._apply_activation(z_curr, self.activation_function_names[i])\n",
    "            A.append(a_curr)\n",
    "        # Propagazione nel layer di output\n",
    "        z_out = np.dot(A[-1], self.W[-1]) + self.b[-1]\n",
    "        Z.append(z_out)\n",
    "        a_out = self._apply_activation(z_out, self.activation_function_names[-1])\n",
    "        A.append(a_out)\n",
    "        return Z, A\n",
    "    \n",
    "    def _backward(self, X, y, Z, A):\n",
    "        m = X.shape[0]\n",
    "        if self.loss_function_name not in loss_derivatives:\n",
    "            raise ValueError(f\"Derivata della loss non supportata: {self.loss_function_name}\")\n",
    "        # Calcola dL/dy_pred per l'output\n",
    "        dA = loss_derivatives[self.loss_function_name](y, A[-1])\n",
    "        # Calcola dL/dz per il layer di output\n",
    "        dZ = dA * self._apply_activation_derivative(Z[-1], A[-1], self.activation_function_names[-1])\n",
    "        reg_term = compute_reg_gradient(self.W[-1], self.lambda_reg, self.reg_type, m)\n",
    "        dW = [np.dot(A[-2].T, dZ) / m + reg_term]\n",
    "        db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "        \n",
    "        # Backpropagation nei layer nascosti\n",
    "        for i in range(len(self.W) - 2, -1, -1):\n",
    "            dA = np.dot(dZ, self.W[i + 1].T)\n",
    "            dZ = dA * self._apply_activation_derivative(Z[i], A[i + 1], self.activation_function_names[i])\n",
    "            reg_term = compute_reg_gradient(self.W[i], self.lambda_reg, self.reg_type, m)\n",
    "            dW.insert(0, np.dot(A[i].T, dZ) / m + reg_term)\n",
    "            db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "        \n",
    "        # Aggiorna i parametri\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] -= self.learning_rate * dW[i]\n",
    "            self.b[i] -= self.learning_rate * db[i]\n",
    "    \n",
    "    def train(self, X, y, epochs=300, batch_size=32, verbose=True):\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            permutation = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                Z, A = self._forward(X_batch)\n",
    "                self._backward(X_batch, y_batch, Z, A)\n",
    "            if epoch % max(1, int(epochs / 20)) == 0:\n",
    "                _, A_full = self._forward(X)\n",
    "                loss = loss_functions[self.loss_function_name](y, A_full[-1])\n",
    "                reg_loss = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "                total_loss = loss + reg_loss\n",
    "                loss_history.append(total_loss)\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "        return loss_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, A = self._forward(X)\n",
    "        output = A[-1]\n",
    "        if self.task == \"classification\":\n",
    "            if output.shape[1] == 1:\n",
    "                return (output > 0.5).astype(int)\n",
    "            else:\n",
    "                return np.argmax(output, axis=1)\n",
    "        else:  # regressione\n",
    "            return output\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        if self.task == \"regression\":\n",
    "            return mse_loss(y, predictions)\n",
    "        else:\n",
    "            if y.ndim > 1 and y.shape[1] > 1:\n",
    "                y_true = np.argmax(y, axis=1)\n",
    "            else:\n",
    "                y_true = y\n",
    "            return np.mean(predictions == y_true)\n",
    "\n",
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(1, one_hot_encode=True)\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "layers = [input_size, 4, output_size]\n",
    "\n",
    "activation_funcs = [\"relu\",  \"sigmoid\"]\n",
    "\n",
    "nn_clf = NeuralNetwork(\n",
    "    layers=layers,\n",
    "    learning_rate=0.5,\n",
    "    lambda_reg=0,\n",
    "    reg_type=\"l2\",\n",
    "    loss_function_name=\"mse\",\n",
    "    activation_function_names=activation_funcs,\n",
    "    task=\"classification\"  # Importante: specifica che si tratta di un task di classificazione\n",
    ")\n",
    "\n",
    "print(\"Training della rete neurale per classificazione...\")\n",
    "loss_history = nn_clf.train(X_train, y_train, epochs=1000, batch_size=32, verbose=True)\n",
    "\n",
    "accuracy = nn_clf.evaluate(X_test, y_test)\n",
    "print(f\"\\nNeural Network Classification Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "pd.Series(loss_history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the neural network on the breast cancer dataset...\n",
      "Epoch    0, Loss: 0.1498, Learning Rate: 0.100000\n",
      "Epoch   50, Loss: 0.0285, Learning Rate: 0.095123\n",
      "Epoch  100, Loss: 0.0258, Learning Rate: 0.090484\n",
      "Epoch  150, Loss: 0.0249, Learning Rate: 0.086071\n",
      "Epoch  200, Loss: 0.0242, Learning Rate: 0.081873\n",
      "Epoch  250, Loss: 0.0237, Learning Rate: 0.077880\n",
      "Epoch  300, Loss: 0.0233, Learning Rate: 0.074082\n",
      "Epoch  350, Loss: 0.0231, Learning Rate: 0.070469\n",
      "Epoch  400, Loss: 0.0230, Learning Rate: 0.067032\n",
      "Epoch  450, Loss: 0.0231, Learning Rate: 0.063763\n",
      "Epoch  500, Loss: 0.0232, Learning Rate: 0.060653\n",
      "Epoch  550, Loss: 0.0233, Learning Rate: 0.057695\n",
      "Epoch  600, Loss: 0.0234, Learning Rate: 0.054881\n",
      "Epoch  650, Loss: 0.0235, Learning Rate: 0.052205\n",
      "Epoch  700, Loss: 0.0236, Learning Rate: 0.049659\n",
      "Epoch  750, Loss: 0.0238, Learning Rate: 0.047237\n",
      "Epoch  800, Loss: 0.0239, Learning Rate: 0.044933\n",
      "Epoch  850, Loss: 0.0240, Learning Rate: 0.042741\n",
      "Epoch  900, Loss: 0.0241, Learning Rate: 0.040657\n",
      "Epoch  950, Loss: 0.0242, Learning Rate: 0.038674\n",
      "\n",
      "Neural Network Classification Accuracy: 0.9649\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import for sklearn dataset and preprocessing\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============================\n",
    "# Activation functions and their derivatives\n",
    "# ============================\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(z, a):\n",
    "    # Note: z is not used here; kept for uniform signature.\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(z, a):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(z, a):\n",
    "    return np.ones_like(a)\n",
    "\n",
    "activation_functions = {\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"relu\": relu,\n",
    "    \"linear\": linear\n",
    "}\n",
    "\n",
    "activation_derivatives = {\n",
    "    \"sigmoid\": lambda z, a: sigmoid_derivative(z, a),\n",
    "    \"relu\": lambda z, a: relu_derivative(z, a),\n",
    "    \"linear\": lambda z, a: linear_derivative(z, a)\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Loss functions and their derivatives\n",
    "# ============================\n",
    "\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_crossentropy_derivative(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return - (y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "loss_functions = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_loss,\n",
    "    \"mse\": mse_loss\n",
    "}\n",
    "\n",
    "loss_derivatives = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_derivative,\n",
    "    \"mse\": mse_derivative\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Regularization functions (modular)\n",
    "# ============================\n",
    "\n",
    "def compute_reg_gradient(W, lambda_reg, reg_type, m):\n",
    "    if reg_type == \"l2\":\n",
    "        return lambda_reg * W / m\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * np.sign(W) / m\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_reg_loss(W_list, lambda_reg, reg_type):\n",
    "    if reg_type == \"l2\":\n",
    "        return (lambda_reg / 2) * sum(np.sum(W ** 2) for W in W_list)\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * sum(np.sum(np.abs(W)) for W in W_list)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# ============================\n",
    "# Neural Network Class with Learning Rate Decay Options\n",
    "# ============================\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.01, lambda_reg=0.001, reg_type=\"l2\",\n",
    "                 loss_function_name=None,\n",
    "                 activation_function_name=\"relu\",\n",
    "                 output_activation_function_name=None,\n",
    "                 activation_function_names=None,\n",
    "                 task=\"classification\",\n",
    "                 lr_decay_type=\"none\",  # Options: \"none\", \"exponential\", \"linear\"\n",
    "                 decay_rate=0.0):\n",
    "        \"\"\"\n",
    "        :param layers: List containing the size of each layer (input, hidden, output)\n",
    "        :param learning_rate: Initial learning rate\n",
    "        :param lambda_reg: Regularization coefficient\n",
    "        :param reg_type: Type of regularization (\"l2\", \"l1\", or other for none)\n",
    "        :param loss_function_name: Name of the loss function (if None, set based on task)\n",
    "        :param activation_function_name: Activation to use for hidden layers (if activation_function_names not provided)\n",
    "        :param output_activation_function_name: Activation for the output layer (if None, set based on task)\n",
    "        :param activation_function_names: List of activation function names for each layer (length = len(layers)-1)\n",
    "        :param task: \"classification\" or \"regression\"\n",
    "        :param lr_decay_type: Learning rate decay strategy (\"none\", \"exponential\", \"linear\")\n",
    "        :param decay_rate: Decay rate used in the learning rate schedule\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.reg_type = reg_type\n",
    "        self.task = task\n",
    "        self.lr_decay_type = lr_decay_type\n",
    "        self.decay_rate = decay_rate\n",
    "        \n",
    "        # Set defaults based on task\n",
    "        if self.task == \"regression\":\n",
    "            self.loss_function_name = loss_function_name or \"mse\"\n",
    "            output_activation_function_name = output_activation_function_name or \"linear\"\n",
    "        else:\n",
    "            self.loss_function_name = loss_function_name or \"binary_crossentropy\"\n",
    "            output_activation_function_name = output_activation_function_name or \"sigmoid\"\n",
    "        \n",
    "        # If no list of activations is provided, use the same activation for all hidden layers and set the output activation\n",
    "        if activation_function_names is None:\n",
    "            self.activation_function_names = [activation_function_name] * (len(layers) - 1)\n",
    "            self.activation_function_names[-1] = output_activation_function_name\n",
    "        else:\n",
    "            if len(activation_function_names) != len(layers) - 1:\n",
    "                raise ValueError(\"activation_function_names must have length equal to len(layers)-1.\")\n",
    "            self.activation_function_names = activation_function_names\n",
    "        \n",
    "        self._initialize_parameters()\n",
    "        \n",
    "    def _initialize_parameters(self):\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            # He initialization (good for ReLU)\n",
    "            weight = np.random.randn(self.layers[i], self.layers[i + 1]) * np.sqrt(2 / self.layers[i])\n",
    "            self.W.append(weight)\n",
    "            self.b.append(np.zeros((1, self.layers[i + 1])))\n",
    "    \n",
    "    def _apply_activation(self, x, func_name):\n",
    "        if func_name not in activation_functions:\n",
    "            raise ValueError(f\"Unsupported activation: {func_name}\")\n",
    "        return activation_functions[func_name](x)\n",
    "    \n",
    "    def _apply_activation_derivative(self, z, a, func_name):\n",
    "        if func_name not in activation_derivatives:\n",
    "            raise ValueError(f\"Unsupported activation derivative: {func_name}\")\n",
    "        return activation_derivatives[func_name](z, a)\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        A = [X]\n",
    "        Z = []\n",
    "        # Forward propagation through hidden layers\n",
    "        for i in range(len(self.W) - 1):\n",
    "            z_curr = np.dot(A[-1], self.W[i]) + self.b[i]\n",
    "            Z.append(z_curr)\n",
    "            a_curr = self._apply_activation(z_curr, self.activation_function_names[i])\n",
    "            A.append(a_curr)\n",
    "        # Forward propagation through output layer\n",
    "        z_out = np.dot(A[-1], self.W[-1]) + self.b[-1]\n",
    "        Z.append(z_out)\n",
    "        a_out = self._apply_activation(z_out, self.activation_function_names[-1])\n",
    "        A.append(a_out)\n",
    "        return Z, A\n",
    "    \n",
    "    def _backward(self, X, y, Z, A):\n",
    "        m = X.shape[0]\n",
    "        if self.loss_function_name not in loss_derivatives:\n",
    "            raise ValueError(f\"Unsupported loss derivative: {self.loss_function_name}\")\n",
    "        # Compute derivative of loss with respect to output activation\n",
    "        dA = loss_derivatives[self.loss_function_name](y, A[-1])\n",
    "        # Compute derivative with respect to z at output layer\n",
    "        dZ = dA * self._apply_activation_derivative(Z[-1], A[-1], self.activation_function_names[-1])\n",
    "        reg_term = compute_reg_gradient(self.W[-1], self.lambda_reg, self.reg_type, m)\n",
    "        dW = [np.dot(A[-2].T, dZ) / m + reg_term]\n",
    "        db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "        \n",
    "        # Backpropagation through hidden layers\n",
    "        for i in range(len(self.W) - 2, -1, -1):\n",
    "            dA = np.dot(dZ, self.W[i + 1].T)\n",
    "            dZ = dA * self._apply_activation_derivative(Z[i], A[i + 1], self.activation_function_names[i])\n",
    "            reg_term = compute_reg_gradient(self.W[i], self.lambda_reg, self.reg_type, m)\n",
    "            dW.insert(0, np.dot(A[i].T, dZ) / m + reg_term)\n",
    "            db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "        \n",
    "        # Update parameters using current learning rate\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] -= self.learning_rate * dW[i]\n",
    "            self.b[i] -= self.learning_rate * db[i]\n",
    "    \n",
    "    def train(self, X, y, epochs=300, batch_size=32, verbose=True):\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            # Update learning rate according to the decay schedule\n",
    "            if self.lr_decay_type == \"exponential\":\n",
    "                self.learning_rate = self.initial_learning_rate * np.exp(-self.decay_rate * epoch)\n",
    "            elif self.lr_decay_type == \"linear\":\n",
    "                # Ensure learning rate does not go negative.\n",
    "                self.learning_rate = self.initial_learning_rate * max(0, 1 - self.decay_rate * epoch)\n",
    "            # Otherwise (\"none\"), keep the initial learning rate.\n",
    "            \n",
    "            permutation = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                Z, A = self._forward(X_batch)\n",
    "                self._backward(X_batch, y_batch, Z, A)\n",
    "            if epoch % max(1, int(epochs / 20)) == 0:\n",
    "                _, A_full = self._forward(X)\n",
    "                loss = loss_functions[self.loss_function_name](y, A_full[-1])\n",
    "                reg_loss = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "                total_loss = loss + reg_loss\n",
    "                loss_history.append(total_loss)\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch:4d}, Loss: {total_loss:.4f}, Learning Rate: {self.learning_rate:.6f}\")\n",
    "        return loss_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, A = self._forward(X)\n",
    "        output = A[-1]\n",
    "        if self.task == \"classification\":\n",
    "            # For binary classification, threshold at 0.5\n",
    "            if output.shape[1] == 1:\n",
    "                return (output > 0.5).astype(int)\n",
    "            else:\n",
    "                return np.argmax(output, axis=1)\n",
    "        else:  # regression\n",
    "            return output\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        if self.task == \"regression\":\n",
    "            return mse_loss(y, predictions)\n",
    "        else:\n",
    "            # If y is one-hot encoded, convert to class labels\n",
    "            if y.ndim > 1 and y.shape[1] > 1:\n",
    "                y_true = np.argmax(y, axis=1)\n",
    "            else:\n",
    "                y_true = y\n",
    "            return np.mean(predictions == y_true)\n",
    "\n",
    "# ============================\n",
    "# Testing on a sklearn Classification Dataset\n",
    "# ============================\n",
    "\n",
    "# Load the breast cancer dataset (binary classification)\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target.reshape(-1, 1)  # reshape y to be a column vector\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the network architecture\n",
    "input_size = X_train.shape[1]\n",
    "hidden_units = 10\n",
    "output_size = 1  # binary classification\n",
    "layers = [input_size, hidden_units, output_size]\n",
    "\n",
    "# Define activation functions for hidden and output layers\n",
    "activation_funcs = [\"relu\", \"sigmoid\"]\n",
    "\n",
    "# Create the NeuralNetwork instance with learning rate decay.\n",
    "# Change lr_decay_type to \"exponential\", \"linear\", or \"none\" as desired.\n",
    "nn_clf = NeuralNetwork(\n",
    "    layers=layers,\n",
    "    learning_rate=0.1,\n",
    "    lambda_reg=0.001,\n",
    "    reg_type=\"l2\",\n",
    "    loss_function_name=\"mse\",\n",
    "    activation_function_names=activation_funcs,\n",
    "    task=\"classification\",\n",
    "    lr_decay_type=\"exponential\",  # Try \"exponential\", \"linear\", or \"none\"\n",
    "    decay_rate=0.001              # Adjust decay rate as needed\n",
    ")\n",
    "\n",
    "print(\"Training the neural network on the breast cancer dataset...\")\n",
    "loss_history = nn_clf.train(X_train, y_train, epochs=1000, batch_size=32, verbose=True)\n",
    "\n",
    "accuracy = nn_clf.evaluate(X_test, y_test)\n",
    "print(f\"\\nNeural Network Classification Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot the training loss history\n",
    "pd.Series(loss_history).plot(title=\"Training Loss History\")\n",
    "plt.xlabel(\"Checkpoint (Epochs)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the neural network on the breast cancer dataset...\n",
      "Epoch    0, Loss: 0.1747, Learning Rate: 0.100000\n",
      "Epoch   50, Loss: 0.0237, Learning Rate: 0.095123\n",
      "Epoch  100, Loss: 0.0211, Learning Rate: 0.090484\n",
      "Epoch  150, Loss: 0.0202, Learning Rate: 0.086071\n",
      "Epoch  200, Loss: 0.0196, Learning Rate: 0.081873\n",
      "Epoch  250, Loss: 0.0192, Learning Rate: 0.077880\n",
      "Epoch  300, Loss: 0.0189, Learning Rate: 0.074082\n",
      "Epoch  350, Loss: 0.0187, Learning Rate: 0.070469\n",
      "Epoch  400, Loss: 0.0187, Learning Rate: 0.067032\n",
      "Epoch  450, Loss: 0.0187, Learning Rate: 0.063763\n",
      "Epoch  500, Loss: 0.0188, Learning Rate: 0.060653\n",
      "Epoch  550, Loss: 0.0190, Learning Rate: 0.057695\n",
      "Epoch  600, Loss: 0.0191, Learning Rate: 0.054881\n",
      "Epoch  650, Loss: 0.0192, Learning Rate: 0.052205\n",
      "Epoch  700, Loss: 0.0193, Learning Rate: 0.049659\n",
      "Epoch  750, Loss: 0.0195, Learning Rate: 0.047237\n",
      "Epoch  800, Loss: 0.0196, Learning Rate: 0.044933\n",
      "Epoch  850, Loss: 0.0197, Learning Rate: 0.042741\n",
      "Epoch  900, Loss: 0.0198, Learning Rate: 0.040657\n",
      "Epoch  950, Loss: 0.0200, Learning Rate: 0.038674\n",
      "\n",
      "Neural Network Classification Accuracy: 0.9649\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAASqJJREFUeJzt3Q2cTXX+wPHvPM8YzEzE5LmWQkR5ilpsWZSKUmHtsmpVFpFqo/VQ2RZ5WBV/0i5qS2R36YGUhK018lyhVCpEDMVMxpgxM/f/+v5m7nXvzJ1HM3POuffzfr1Oc8+555x77r2m853v7/v7/UJcLpdLAAAAgkio1RcAAABQ2QiAAABA0CEAAgAAQYcACAAABB0CIAAAEHQIgAAAQNAhAAIAAEGHAAgAAAQdAiAAABB0CIAAyO9//3tp1KhRmY594oknJCQkpNyvKZht2LDBfKb6E0DFIAACbExvgiVZgvVGqYFb1apVxe66du0qLVq08Pvcd999Z77DGTNmXPDr/PWvf5WVK1de8HmAYBBu9QUAKNw///lPn/WXX35Z1q5dW2B7s2bNLuh1XnzxRcnJySnTsePHj5exY8de0OvDV+fOnSU9PV0iIyNLHQDdeeed0qdPnwq7NiBQEAABNvbb3/7WZ33z5s0mAMq/Pb8zZ85IlSpVSvw6ERERZb7G8PBws6D8hIaGSnR0tNhBWlqaxMbGWn0ZQLmjCQxwOHfzyvbt203mQAOfxx9/3Dz3xhtvSK9evaROnToSFRUlv/jFL2Ty5MmSnZ1dZA2Qd7PMggULzHF6fLt27WTr1q3F1gDp+ogRI0xzjF6bHnvllVfKmjVrCly/Nt+1bdvW3PD1dV544YVyrytavny5tGnTRmJiYqRmzZomgDx8+LDPPkePHpUhQ4ZIvXr1zPVecskl0rt3b/NZuG3btk169OhhzqHnuvTSS+Wee+6RyqgB+uqrr6Rv376SmJhoPiu9zv79+0tKSop5XvfXYOWll17yNI3q9+q2c+dOuemmm6R69eqm2fDGG280AbW3xYsXm+M2btwof/zjH6VWrVrmddavX2+2r1ixosC1LlmyxDyXlJRU7p8DUJH4sw0IAD/++KO5uekNUW/utWvX9tzQ9GY3ZswY8/ODDz6QiRMnSmpqqkyfPr3Y8+rN7eeff5b777/f3OSeeeYZueOOO+Sbb74pNmv00UcfyX/+8x9zI61WrZo899xz5gZ+8OBBqVGjhuem3LNnTxNsPPnkkyYwe+qpp+Tiiy8up08m9zPQwEaDtylTpsixY8fk2Weflf/973/m9ePj481+em179uyRkSNHmmAwOTnZZNv0et3r3bt3N9emTX56nAZH+h5LQt/biRMnCmw/efJkscdmZmaawCsjI8NcnwZBGsC9/fbbcurUKYmLizPNon/4wx+kffv2ct9995njNKBU+r5++ctfmuDnT3/6k/nuNNDU4FmDnQ4dOvi8nn5n+j7134oGVbpf/fr15dVXX5Xbb7/dZ1/dpq/TsWPHEn0OgG24ADjG8OHDXfl/bbt06WK2zZ8/v8D+Z86cKbDt/vvvd1WpUsV19uxZz7bBgwe7GjZs6Fn/9ttvzTlr1Kjh+umnnzzb33jjDbP9rbfe8mybNGlSgWvS9cjISNfXX3/t2fbJJ5+Y7c8//7xn26233mqu5fDhw55tX331lSs8PLzAOf3R646NjS30+czMTFetWrVcLVq0cKWnp3u2v/322+b8EydONOsnT54069OnTy/0XCtWrDD7bN261VVa7u+oqMX7tdevX2+26U+1c+dOs758+fIiX0c/C/1M8uvTp4/5Pvbv3+/ZduTIEVe1atVcnTt39mxbtGiReZ3rr7/elZWV5XOOcePGuaKiolynTp3ybEtOTjbflf4bAJyGJjAgAGiTjWY58tNmGjfN5GgGQjMBWiP0xRdfFHvefv36SUJCgmddj1WaASpOt27dPBkIddVVV5kMhPtYzYi8//77pmBXm+jcGjdubLJZ5UGbrDRzoxkN75oabRZs2rSprFq1yvM5acGxNjkVlpFxZ4o063Lu3LlSX4tmkTSjlH955ZVXij1WMzzq3XffNd9daejn/N5775nP+bLLLvNs16zbb37zG5Op04ygt6FDh0pYWJjPtkGDBpkM1L/+9S/PtmXLlklWVlaxNWmAHREAAQGgbt26fnsMadOHNlnoDVSDD23WcN+s3LUjRWnQoIHPujsYKkmzTf5j3ce7j9XARHs6acCTn79tZXHgwAHz84orrijwnAZA7uc1gJw2bZq88847pvlQa6m0uU/rgty6dOlimsm0qU5rgLQ+aNGiRSYoKAktJNagMP9y3XXXFXus1hppM+bf//5389raHDZ37twSfYfHjx83QZO/z0B7D2rvv0OHDhV4PX+flzYjapOXmz6+9tpry+37AioTARAQALwzPW5aG6I37U8++cTU1bz11lsm46A3elWSbu/5swBuua1cFXesFUaPHi1ffvmlqRPSbNGECRNMgKB1QkproDT7ocW+WuCtNThaAK3F1adPn67w65s5c6Z8+umnpsBdA8cHH3zQFJZ///33lfLvyZ0F0pohfc39+/ebImqyP3AqAiAgQGlzjhZHaxHwqFGj5JZbbjEZB+8mLStpDyMNNL7++usCz/nbVhYNGzY0P/ft21fgOd3mft5Nm+wefvhh02S0e/duU3ysgYc3zXg8/fTTpnlNMyCaZVu6dKlUhpYtW5pxl/773//Khx9+aIKw+fPne57313NOs37aM9DfZ6DNoNrlXgucS0KL7DWwfe2118x712JqbSYFnIgACAhQ7gyMd8ZFb+j/93//J3a5Pg3ItKv8kSNHfIIfbYoqD9q9XgMtDRK8m6r0/J9//rmpBVLaRHT27NkCwZD2XnMfp013+bNXrVu3Nj9L2gxWVlqjo7U2+YMhDV68X1ub2TTzl/9z1t5rOiSCd5d+7Q2nvfyuv/560zxaEtr8pvVZWrekAZD24NNtgBPRDR4IUJ06dTLZnsGDB5vmEs0OaFdpOzVB6Xg/mm3ROphhw4aZgt05c+aYsYN27dpVonNoQfJf/vKXAtsvuugiU/ysTX5aIK7NgQMGDPB0g9ei5Iceesjsq01fOi7O3XffLc2bNzcDO+qYN7qvZj2Ujq+jwaPWVGlwpEXlOoK2Bg8333yzVCQdvkCb3e666y65/PLLTTCk36UGN1qX5KbNcVpYPmvWLFNYrrU82sVdPx9t/tRgRz8TfX/aDV6DJ611Kg1tBtPRppWOKQU4FQEQEKB0rB3tsaRNOtpsosGQ1mvojV6LaO1Ab9iajXnkkUdMzY02xWi9kmZnStJLzZ3V0mPz0yBFb/Y6GKA2AU2dOlUee+wxkyXRIEYDI3fPLn1dDY7WrVtnAgsNELTo9/XXX/cEGBpAbdmyxTR3aWCkheU65o5mQvwVDZenVq1ame9M67i02Uvfj27Tz06b5Nw08NExgPT71johDX41ANJaIW0yGzdunKlx0vov3a6ZnPxjABXn1ltvNf+W9By33XZbBbxboHKEaF/4SnotACgR7bKttTU6+jHsRbNPml3SQOgf//iH1ZcDlBk1QAAspZkKbxr0rF692ow+DPvRmi3tWq9NYYCTkQECYCkdkE+bqXSQPh2XZ968eaY2RbufN2nSxOrLQ56PP/7YdMPXuh8tfN6xY4fVlwRcEGqAAFhKexJpt2oddFAHJNQ5pf76178S/NiMBqZaM6Q933RoBcDpyAABAICgQw0QAAAIOgRAAAAg6FAD5IeOb6Ej0+oosP6GlgcAAPajVT06SKkO1aAjpReFAMgPDX5KOjcOAACwl0OHDkm9evXsHQDNnTtXpk+fbnqA6Mimzz//vBld1R8dGG3ixImyfft20132b3/7m5nB2ZsOpa/D62tvBT2nRoHaxVZHRi1pNkczP+4PsKRz5AAAAGvpvHmawHDfx20bAC1btkzGjBljJirU4dhnz55thnvXWYt1AsP8dMJCHStE58Nxz+GTnw5vr901dd4eHf5dZ2zWeYB02HqdD6kk3IGSBj8EQAAAOEtJEh6WdoPXoKddu3Zm8kN37Y1GbiNHjpSxY8cWeaxOZKjZn/wZoFtuuUVq167tM0S7zuUTExNjskIljSA1YEpJSSEAAgDAIUpz/7asF5hOYKhNWd26dTt/MaGhZj0pKemCZsDWCQ11dmf1ySefyEcffSQ33XRTocfoqLP6oXkvAAAgcFnWBHbixAlTr6PZGm+6XtJZoP3RzJEGMDqTc1hYmHmNp59+WgYOHFjoMTo78pNPPlnm1wQAAM4ScOMAvf766/Lqq6/KkiVLzFw1Wgs0Y8YM87Mw48aNM+ky96LFzwAAIHBZlgHSyfQ0Q3Ps2DGf7bqemJhY5vM++uijJgvUv39/s96yZUvTY0yzPIMHD/Z7jM4/pAsAAAgOlmWAIiMjpU2bNqZex02LoHVdJ0MsK+0pln/wIw209NwAAACWd4PXLvCalWnbtq0Z+0e7waelpZlu62rQoEFSt25dk71xF07v3bvX8/jw4cOya9cuqVq1qjRu3Nhsv/XWW03NT4MGDUw3+J07d8qsWbPknnvusfCdAgAAO7F8NnjtAu8eCLF169by3HPPme7xqmvXrqa7++LFi836d999J5deemmBc3Tp0kU2bNhgHusQ2BMmTJAVK1ZIcnKyGQhxwIABZgBFzTqVBN3gAQBwntLcvy0PgOyIAAgAAOdxxDhAAAAAViEAAgAAQYcACAAABB3LZ4MPJplZOfJjWoZo1VWd+BirLwcAgKBFBqgSrdx5WDpO+UAeX/GZ1ZcCAEBQIwCqRPFVIszPU2fOWX0pAAAENQKgSpQQmzsO0akzmVZfCgAAQY0AqBLFx+RmgE6SAQIAwFIEQJUovkpuBij17DnJzmH8SQAArEIAZEENkPYCS00nCwQAgFUIgCpRRFioVI3KHXngJHVAAABYhgDIoiwQdUAAAFiHAKiSJeTVAaWkkwECAMAqBEBWZYDSyAABAGAVAiCLeoJRAwQAgHUIgCpZQl4GKIVeYAAAWIYAyLLBEMkAAQBgFQIgy5rAyAABAGAVAqBKlhCb1wRGAAQAgGUIgCoZRdAAAFiPAMiiGqBTZIAAALAMAZBFAyGeIgMEAIBlCIAsCoDSMrMlMyvH6ssBACAoEQBVsmrR4RIakvuYLBAAANYgAKpkoaEhEucZC4g6IAAArEAAZAHqgAAAsBYBkAXi3BOikgECAMASBEAWIAMEAIC1CIAsEJ+XATrFhKgAAFiCAMgC8TGMBg0AgJUIgCyQ4M4ApZEBAgDACgRAFoiPzasBSicDBABAUAZAc+fOlUaNGkl0dLR06NBBtmzZUui+e/bskb59+5r9Q0JCZPbs2X73O3z4sPz2t7+VGjVqSExMjLRs2VK2bdsmdpsPjF5gAAAEYQC0bNkyGTNmjEyaNEl27NghrVq1kh49ekhycrLf/c+cOSOXXXaZTJ06VRITE/3uc/LkSbnuuuskIiJC3nnnHdm7d6/MnDlTEhISxC7oBQYAgLXCrXzxWbNmydChQ2XIkCFmff78+bJq1SpZuHChjB07tsD+7dq1M4vy97yaNm2a1K9fXxYtWuTZdumll4ote4GRAQIAILgyQJmZmbJ9+3bp1q3b+YsJDTXrSUlJZT7vm2++KW3btpW77rpLatWqJVdffbW8+OKLYicJ7hqgM+fE5XJZfTkAAAQdywKgEydOSHZ2ttSuXdtnu64fPXq0zOf95ptvZN68edKkSRN59913ZdiwYfLggw/KSy+9VOgxGRkZkpqa6rNURg1QZnaOnMnMrtDXAgAANiyCLm85OTlyzTXXyF//+leT/bnvvvtMM5s2rxVmypQpEhcX51m0Ca0iVYkMk8iw3I+ewRABAAiiAKhmzZoSFhYmx44d89mu64UVOJfEJZdcIs2bN/fZ1qxZMzl48GChx4wbN05SUlI8y6FDh6QiaQ82dx3QyTQKoQEACJoAKDIyUtq0aSPr1q3zyd7oeseOHct8Xu0Btm/fPp9tX375pTRs2LDQY6KioqR69eo+S0WjEBoAgCDtBaZd4AcPHmyKltu3b2/G9UlLS/P0Chs0aJDUrVvXNFG5C6e1W7v7sY73s2vXLqlatao0btzYbH/ooYekU6dOpgns7rvvNuMKLViwwCx2Ep/XFZ7pMAAACLIAqF+/fnL8+HGZOHGiKXxu3bq1rFmzxlMYrc1W2jPM7ciRI6aux23GjBlm6dKli2zYsMFs027yK1asMM1aTz31lOkCr4HVwIEDxZbTYVADBABApQtx0Q+7AO0FpsXQWg9UUc1hj/3rU1m27ZA8/OvLZeSNTSrkNQAACCappbh/B1wvMKeIj2U6DAAArEIAZPV0GEyICgBApSMAsoh7MER6gQEAUPkIgCxCLzAAAKxDAGRxL7AUMkAAAFQ6AiCLkAECAMA6BEBWZ4DSz0lODiMRAABQmQiALM4Aaezz89ksqy8HAICgQgBkkcjwUImNDDOPaQYDAKByEQBZiDogAACsQQBkIc+M8MwHBgBApSIAssNo0GSAAACoVARAForLywCdTCMDBABAZSIAskFXeDJAAABULgIgW0yISgYIAIDKRABkobi8CVFPMh0GAACVigDIQhRBAwBgDQIgCyXEumuAyAABAFCZCIAsFBfDQIgAAFiBAMgWvcDIAAEAUJkIgGxQA3Q6I0vOZedYfTkAAAQNAiALVY+JkJCQ3MdkgQAAqDwEQBYKCw2R6tEMhggAQGUjALJLHRCDIQIAUGkIgCwWn1cHdDKNDBAAAJWFAMhi8fQEAwCg0hEA2WY+MDJAAABUFgIgm2SAmA8MAIDKQwBksfi80aDpBQYAQOUhALLJfGAn08gAAQBQWQiAbNILjBogAAAqDwGQxeJj6AUGAEBQBkBz586VRo0aSXR0tHTo0EG2bNlS6L579uyRvn37mv1DQkJk9uzZRZ576tSpZr/Ro0eLnXuBMSM8AABBFAAtW7ZMxowZI5MmTZIdO3ZIq1atpEePHpKcnOx3/zNnzshll11mApvExMQiz71161Z54YUX5KqrrhK7YhwgAACCMACaNWuWDB06VIYMGSLNmzeX+fPnS5UqVWThwoV+92/Xrp1Mnz5d+vfvL1FRUYWe9/Tp0zJw4EB58cUXJSEhQeweAGVk5Uh6ZrbVlwMAQFCwNADKzMyU7du3S7du3c5fUGioWU9KSrqgcw8fPlx69erlc247qhoVLuGhuVPC0wwGAEDlCBcLnThxQrKzs6V27do+23X9iy++KPN5ly5daprTtAmsJDIyMszilpqaKpVF65O0J9iJ0xmmGaxOfEylvTYAAMHK8iaw8nbo0CEZNWqUvPrqq6aouiSmTJkicXFxnqV+/fpiTR0QGSAAAAI+AKpZs6aEhYXJsWPHfLbrenEFzoXRJjUtoL7mmmskPDzcLBs3bpTnnnvOPNaMU37jxo2TlJQUz6JBVGVKYDoMAACCJwCKjIyUNm3ayLp16zzbcnJyzHrHjh3LdM4bb7xRPvvsM9m1a5dnadu2rSmI1scacOWnxdTVq1f3WSoTgyECABBENUBKu8APHjzYBCnt27c34/qkpaWZXmFq0KBBUrduXdNM5S6c3rt3r+fx4cOHTWBTtWpVady4sVSrVk1atGjh8xqxsbFSo0aNAtvtgsEQAQAIsgCoX79+cvz4cZk4caIcPXpUWrduLWvWrPEURh88eND0DHM7cuSIXH311Z71GTNmmKVLly6yYcMGcaKE2LzBENPIAAEAEBQBkBoxYoRZ/Mkf1OgI0C6Xq1Tnt3tg5CmCTicDBABAZQi4XmBO5J4Og15gAABUDgIgG9UA0QsMAIDKQQBkA+5eYIwEDQBA5SAAsoGE2NwMUAoZIAAAKgUBkA3Ex7jHATpX6gJvAABQegRANuoFlp3jktSzWVZfDgAAAY8AyAaiI8IkJiJ3hGqawQAAqHgEQDbLAlEIDQBAxSMAsgl6ggEAUHkIgGzCPSN8CqNBAwBQ4QiA7NYExnxgAABUOAIg2zWBkQECAKCiEQDZBE1gAABUHgIgmw2GSBE0AAAVjwDIdt3gyQABAFDRCIBsIiGvBiiFDBAAABWOAMhmE6KSAQIAoOIRANlEHDVAAABUGgIgm/UC+/lslmRl51h9OQAABDQCIJuIi8kNgBRd4QEAqFgEQDYRHhYq1aLDzWPqgAAAqFgEQDbsCXaKOiAAACoUAZAN64BOkQECAKBCEQDZSJxnPjAyQAAAVCQCIBshAwQAQOUgALJjDVA6GSAAACoSAZANu8LTCwwAgIpFAGTLJjAyQAAAVCQCIBtJiHV3gycDBABARSIAshGawAAAqBwEQDbCQIgAAFQOAiBbBkBkgAAACPgAaO7cudKoUSOJjo6WDh06yJYtWwrdd8+ePdK3b1+zf0hIiMyePbvAPlOmTJF27dpJtWrVpFatWtKnTx/Zt2+f2F18bG4TWPq5bDl7LtvqywEAIGBZHgAtW7ZMxowZI5MmTZIdO3ZIq1atpEePHpKcnOx3/zNnzshll10mU6dOlcTERL/7bNy4UYYPHy6bN2+WtWvXyrlz56R79+6SlpYmdlYtKlzCQkPMY7JAAABUnBCXy+USC2nGR7M1c+bMMes5OTlSv359GTlypIwdO7bIYzULNHr0aLMU5fjx4yYTpIFR586di72m1NRUiYuLk5SUFKlevbpUpjaT18qPaZnyzqhfSrNLKve1AQBwstLcvy3NAGVmZsr27dulW7du5y8oNNSsJyUlldvr6AehLrroIrG7eKbDAACgwoWLhU6cOCHZ2dlSu3Ztn+26/sUXX5TLa2hGSTNE1113nbRo0cLvPhkZGWbxjiCtEm8KodPoCQYAQCDXAFU0rQXavXu3LF26tNB9tGhaU2buRZvgrB4NmrGAAAAI0ACoZs2aEhYWJseOHfPZruuFFTiXxogRI+Ttt9+W9evXS7169Qrdb9y4caaZzL0cOnRIrM0AMSEqAAABGwBFRkZKmzZtZN26dT5NVrresWPHMp9X67o1+FmxYoV88MEHcumllxa5f1RUlCmW8l6sEp83GjQ1QAAABGgNkNIu8IMHD5a2bdtK+/btzbg+2l19yJAh5vlBgwZJ3bp1TTOVu3B67969nseHDx+WXbt2SdWqVaVx48aeZq8lS5bIG2+8YcYCOnr0qNmuzVsxMTHihPnATqaRAQIAIGADoH79+plu6hMnTjSBSuvWrWXNmjWewuiDBw+anmFuR44ckauvvtqzPmPGDLN06dJFNmzYYLbNmzfP/OzatavPay1atEh+//vfiyN6gaWTAQIAIGADIKXNVbr44w5qvMf+KW7oIouHNrog8THMBwYAQEUL+F5gTkMvMAAAKh4BkM14eoERAAEAUGEIgGw7EnSmo5vyAACwMwIgm0nIywBl5bjkdEaW1ZcDAEBAIgCymZjIMIkKz/1aaAYDAKBiEADZOAtEAAQAQMUgALJxHdBJusIDAFAhCIBsiAAIAICKRQBk4yawFEaDBgCgQhAA2TkDlEYABABARSAAsvFgiDSBAQBQMQiAbDwdBk1gAABUDAIgG0+ISgYIAICKQQBk615gZIAAAKgIBEA2lBCb1wuMDBAAABWCAMiG4mPIAAEAUJEIgGzcCyz17DnJzmFGeAAAyhsBkI1rgFwukVR6ggEAUO4IgGwoIixUqkaFm8f0BAMAoPwRANkUPcEAAKg4BEC2nw+MDBAAAOWNAMimmA8MAICKQwBkU8wHBgBAxSEAsvl8YKeoAQIAoNwRANk8A3SKGiAAAModAZBNMRo0AAAVhwDIphJi3U1gZIAAAChvBEB2bwIjAwQAQLkjALJ5ExgBEAAA5Y8AyOYDIdINHgCA8kcAZPMA6ExmtmRkZVt9OQAABBQCIJuqFh0uoSG5j1NoBgMAwPoA6NChQ/L999971rds2SKjR4+WBQsWlOki5s6dK40aNZLo6Gjp0KGDOV9h9uzZI3379jX7h4SEyOzZsy/4nHYUGhoicXSFBwDAPgHQb37zG1m/fr15fPToUfn1r39tAow///nP8tRTT5XqXMuWLZMxY8bIpEmTZMeOHdKqVSvp0aOHJCcn+93/zJkzctlll8nUqVMlMTGxXM5p92YwusIDAGCDAGj37t3Svn178/j111+XFi1ayKZNm+TVV1+VxYsXl+pcs2bNkqFDh8qQIUOkefPmMn/+fKlSpYosXLjQ7/7t2rWT6dOnS//+/SUqKqpczmlXce4JUckAAQBgfQB07tw5T/Dx/vvvy2233WYeN23aVH744YcSnyczM1O2b98u3bp1O39BoaFmPSkpqSyXViHntAoZIAAAbBQAXXnllSar8uGHH8ratWulZ8+eZvuRI0ekRo0aJT7PiRMnJDs7W2rXru2zXde1aa0synLOjIwMSU1N9VnsIN49IWo6GSAAACwPgKZNmyYvvPCCdO3aVQYMGGBqbNSbb77paRpzkilTpkhcXJxnqV+/vtgBYwEBAFAxwstykAY+mmnRTElCQoJn+3333WdqbUqqZs2aEhYWJseOHfPZruuFFThXxDnHjRtniqbd9H3ZIQjyjAadRgYIAADLM0Dp6emm2cgd/Bw4cMB0R9+3b5/UqlWrxOeJjIyUNm3ayLp16zzbcnJyzHrHjh3LcmllOqfWM1WvXt1nsYP4WDJAAADYJgPUu3dvueOOO+SBBx6QU6dOmXF2IiIiTFZIe2ANGzasxOfSzMvgwYOlbdu2pvlMA6m0tDTTg0sNGjRI6tata5qp3EXOe/fu9Tw+fPiw7Nq1S6pWrSqNGzcu0TmdIoEaIAAA7BMA6dg6f/vb38zjf/3rX6bAeOfOnfLvf/9bJk6cWKoAqF+/fnL8+HFznBYpt27dWtasWeMpYj548KDpxeWmhdZXX321Z33GjBlm6dKli2zYsKFE53SK+Bh6gQEAUBFCXC6Xq7QHaZ3PF198IQ0aNJC7777b9ArTQQd1hOgrrrjCDFboZFoDpMXQKSkpljaH7T6cIrc8/5FcXC1Ktv75fLd+AABwYffvMtUAaVPTypUrTcDz7rvvSvfu3c12HWnZLvUzgSAhrwZI5wIrQ5wKAADKMwDSpqVHHnnEzLWlNTbu4uL33nvPp3kK5dMLLDM7x8wKDwAALKwBuvPOO+X66683oz67xwBSN954o9x+++3ldGmoEhkmkWGhJgDSnmCxUWX6ugAAQD5lvqPqmDq6uGeFr1evniMHQbQzne1eR4NO/jlDTp05J/XOD7kEAAAquwlMx9XRWd+10Khhw4ZmiY+Pl8mTJ5vnUAHTYTAhKgAA1maA/vznP8s//vEPmTp1qlx33XVm20cffSRPPPGEnD17Vp5++unyu8IgF890GAAA2CMAeumll+Tvf/+7ZxZ4ddVVV5kBC//4xz8SAJUjBkMEAMAmTWA//fSTNG3atMB23abPoQIGQ0wjAwQAgKUBkPb8mjNnToHtuk0zQSg/8bG5GaCT1AABAGBtE9gzzzwjvXr1kvfff98zBlBSUpIZGHH16tXld3WQhLwaoFPpZIAAALA0A6Tzbn355ZdmzB+dDFUXnRx1z5498s9//rPcLg5eNUBkgAAAsH4coDp16hQodv7kk09M77AFCxaUx7VBROLyaoDoBQYAgMUZIFQeMkAAAJQ/AiCHTIh6igwQAADlhgDIIROipqSfk5wcZoQHAKDSa4C00LkoWgyNihkJWmOf1LPnPOsAAKCSAiCd+6u45wcNGnQBl4P8IsNDJTYyTNIys00dEAEQAACVHAAtWrSoHF4SpaVBT1pmuukJ1khirb4cAAAcjxogB2BGeAAAyhcBkAMwGjQAAOWLAMgB4vIyQCfTyAABAFAeCIAcNRgiGSAAAMoDAZCjmsDIAAEAUB4IgBwgLm8wxJMUQQMAUC4IgJyUAaIJDACAckEA5AAJsXSDBwCgPBEAOUBcTG4GSAdCBAAAF44AyFG9wMgAAQBQHgiAHFQDdDojSzKzcqy+HAAAHI8AyAGqx0RISEju4xS6wgMAcMEIgBwgLDREqkczGCIAAOWFAMhhdUCMBQQAwIUjAHKIeMYCAgAgsAKguXPnSqNGjSQ6Olo6dOggW7ZsKXL/5cuXS9OmTc3+LVu2lNWrV/s8f/r0aRkxYoTUq1dPYmJipHnz5jJ//nxxsnh6ggEAEDgB0LJly2TMmDEyadIk2bFjh7Rq1Up69OghycnJfvfftGmTDBgwQO69917ZuXOn9OnTxyy7d+/27KPnW7Nmjbzyyivy+eefy+jRo01A9Oabb4rTe4IxFhAAAAEQAM2aNUuGDh0qQ4YM8WRqqlSpIgsXLvS7/7PPPis9e/aURx99VJo1ayaTJ0+Wa665RubMmeMTJA0ePFi6du1qMkv33XefCayKyyw5IgNELzAAAJwdAGVmZsr27dulW7du5y8oNNSsJyUl+T1Gt3vvrzRj5L1/p06dTLbn8OHD4nK5ZP369fLll19K9+7d/Z4zIyNDUlNTfRa7ic8bDZoaIAAAHB4AnThxQrKzs6V27do+23X96NGjfo/R7cXt//zzz5tsktYARUZGmoyR1hl17tzZ7zmnTJkicXFxnqV+/fpi1/nATqaRAQIAwPFNYBVBA6DNmzebLJBmmGbOnCnDhw+X999/3+/+48aNk5SUFM9y6NAhsW0vsHQyQAAAXKhwsVDNmjUlLCxMjh075rNd1xMTE/0eo9uL2j89PV0ef/xxWbFihfTq1ctsu+qqq2TXrl0yY8aMAs1nKioqyix2Fh9DLzAAAAIiA6TNU23atJF169Z5tuXk5Jj1jh07+j1Gt3vvr9auXevZ/9y5c2bRWiJvGmjpuZ2KXmAAAARIBsjdZV17bLVt21bat28vs2fPlrS0NNMrTA0aNEjq1q1r6nTUqFGjpEuXLqZZSzM8S5culW3btsmCBQvM89WrVzfPay8xHQOoYcOGsnHjRnn55ZdNjzOnYhwgAAACKADq16+fHD9+XCZOnGgKmVu3bm3G8HEXOh88eNAnm6M9vJYsWSLjx483TV1NmjSRlStXSosWLTz7aFCkdT0DBw6Un376yQRBTz/9tDzwwAPi9AAoIytH0jOzJSYyzOpLAgDAsUJc2k8cPrQbvPYG04JozSjZgX5NTf78jmTluGTT2BukTnyM1ZcEAIBj798B2QssEIWEhHh6glEHBADAhSEAcuCM8CnUAQEAcEEIgBxYB3SSAAgAgAtCAOQgNIEBAFA+CICc2ATGhKgAAFwQAiAnZoDSyAABAHAhCIAchBogAADKBwGQA6fDSGFCVAAALggBkIO4J0QlAwQAwIUhAHIQeoEBAFA+CIAcJCGWgRABACgPBEAOEh+TmwE6lX7OzA0GAADKhgDIgb3AsnNckno2y+rLAQDAsQiAHCQ6IkxiIsLMY5rBAAAoOwIgx44FRCE0AABlRQDkMPQEAwDgwhEAOXQ+sFM0gQEAUGYEQA4dDfoUGSAAAMqMAMhh4pgPDACAC0YA5NgmMDJAAACUFQGQU5vA0skAAQBQVgRADhPHhKgAAFwwAiCHoQgaAIALRwDk0AlR6QYPAEDZEQA5TFzehKgMhAgAQNkRADm0F9jPZ7MkKzvH6ssBAMCRCIAcWgStUugJBgBAmRAAOUx4WKhUiw43j+kJBgBA2RAAORA9wQAAuDAEQA7EhKgAAFwYAiAHisvLANETDACAsiEAciAyQAAABEAANHfuXGnUqJFER0dLhw4dZMuWLUXuv3z5cmnatKnZv2XLlrJ69eoC+3z++edy2223SVxcnMTGxkq7du3k4MGDEkg1QGSAAABwaAC0bNkyGTNmjEyaNEl27NghrVq1kh49ekhycrLf/Tdt2iQDBgyQe++9V3bu3Cl9+vQxy+7duz377N+/X66//noTJG3YsEE+/fRTmTBhggmYAkG8OwNEN3gAAMokxOVyucRCmvHR7MycOXPMek5OjtSvX19GjhwpY8eOLbB/v379JC0tTd5++23PtmuvvVZat24t8+fPN+v9+/eXiIgI+ec//1mma0pNTTWZo5SUFKlevbrYzeL/fStPvLVXbm6ZKP83sI3VlwMAgC2U5v5taQYoMzNTtm/fLt26dTt/QaGhZj0pKcnvMbrde3+lGSP3/hpArVq1Si6//HKzvVatWibIWrlyZaHXkZGRYT4078XOEmLzmsDSyAABAFAWlgZAJ06ckOzsbKldu7bPdl0/evSo32N0e1H7a9PZ6dOnZerUqdKzZ09577335Pbbb5c77rhDNm7c6PecU6ZMMRGje9EMlJ3Fu8cBogkMAABn1gCVN80Aqd69e8tDDz1kmsa0Ke2WW27xNJHlN27cOJMucy+HDh0SO4vPmw6DgRABACib3DkVLFKzZk0JCwuTY8eO+WzX9cTERL/H6Pai9tdzhoeHS/PmzX32adasmXz00Ud+zxkVFWUWp6AXGAAADs4ARUZGSps2bWTdunU+GRxd79ixo99jdLv3/mrt2rWe/fWcWlS9b98+n32+/PJLadiwoQSC+NjcDNDZczly9ly21ZcDAIDjWJoBUtoFfvDgwdK2bVtp3769zJ492/TyGjJkiHl+0KBBUrduXVOno0aNGiVdunSRmTNnSq9evWTp0qWybds2WbBggeecjz76qOkt1rlzZ/nVr34la9askbfeest0iQ8E1aLCJSw0RLJzXGYwxMS4MKsvCQAAR7E8ANJA5fjx4zJx4kRTyKw1OxqwuAuddfBC7Rnm1qlTJ1myZImMHz9eHn/8cWnSpInp4dWiRQvPPlr0rPU+GjQ9+OCDcsUVV8i///1vMzZQIAgJCTF1QD+mZZpmsMS4wBjfCACAoBkHyI7sPg6QunHmBtl/PE1eG3qtdPxFDasvBwAAyzlmHCCUQ1d4CqEBACg1AiCHT4h6kglRAQAoNQIghzo/GCIZIAAASosAyKHOD4ZIBggAgNIiAHKo8/OBkQECAKC0CIAcKp4aIAAAyowAyKHc02GkUAMEAECpEQA5vAaIDBAAAKVHAORQjAMEAEDZEQA5VELs+V5gDOYNAEDpEAA5VHxMbgYoK8clpzOyrL4cAAAchQDIoWIiwyQqPPfrYywgAABKhwAoAHqCEQABAFA6BEABMRYQhdAAAJQGAZCDEQABAFA2BEABMRgiTWAAAJQGAVAgZIDSCIAAACgNAqAAGAyRJjAAAEqHAMjBEvIyQDSBAQBQOgRAATAYIhkgAABKhwAoIHqBkQECAKA0CIAcLCGWCVEBACgLAqAAqAFiJGgAAEqHAMjB4vJqgFLPnpPsHGaEBwCgpAiAAqAGyOWiJxgAAKVBAORgEWGhUi0q3DymDggAgJIjAHK4OHqCAQBQagRAATIfGBkgAABKjgAoQOqA6AkGAEDJEQA5HPOBAQBQegRADsdYQAAAlB4BUIBkgE6lkwECAMBRAdDcuXOlUaNGEh0dLR06dJAtW7YUuf/y5culadOmZv+WLVvK6tWrC933gQcekJCQEJk9e7YEovgYeoEBAOC4AGjZsmUyZswYmTRpkuzYsUNatWolPXr0kOTkZL/7b9q0SQYMGCD33nuv7Ny5U/r06WOW3bt3F9h3xYoVsnnzZqlTp44EqoRYdxMYGSAAABwTAM2aNUuGDh0qQ4YMkebNm8v8+fOlSpUqsnDhQr/7P/vss9KzZ0959NFHpVmzZjJ58mS55pprZM6cOT77HT58WEaOHCmvvvqqRETkBgkB3QRGBggAAGcEQJmZmbJ9+3bp1q3b+QsKDTXrSUlJfo/R7d77K80Yee+fk5Mjv/vd70yQdOWVVxZ7HRkZGZKamuqzOK0JjAAIAACHBEAnTpyQ7OxsqV27ts92XT969KjfY3R7cftPmzZNwsPD5cEHHyzRdUyZMkXi4uI8S/369cVpAyHSDR4AAAc1gZU3zShpM9nixYtN8XNJjBs3TlJSUjzLoUOHxGkB0JnMbMnIyrb6cgAAcARLA6CaNWtKWFiYHDt2zGe7ricmJvo9RrcXtf+HH35oCqgbNGhgskC6HDhwQB5++GHT08yfqKgoqV69us/iFNWiwyU0L85LoRkMAAD7B0CRkZHSpk0bWbdunU/9jq537NjR7zG63Xt/tXbtWs/+Wvvz6aefyq5duzyL9gLTeqB3331XAk1oaIjE0RUeAIBSCReLaRf4wYMHS9u2baV9+/ZmvJ60tDTTK0wNGjRI6tata+p01KhRo6RLly4yc+ZM6dWrlyxdulS2bdsmCxYsMM/XqFHDLN60F5hmiK644goJRNoMpsEPdUAAADgkAOrXr58cP35cJk6caAqZW7duLWvWrPEUOh88eND0DHPr1KmTLFmyRMaPHy+PP/64NGnSRFauXCktWrSQYMWEqAAAlE6Iy+VylfKYgKfd4LU3mBZEO6Ee6J7FW+WDL5Jl6h0tpX/7BlZfDgAAtr9/B1wvsGDOAFEDBABAyRAABQB3V3gmRAUAoGQIgAKAZzToNDJAAACUBAFQAIiPZTRoAABKgwAoACS4e4GlkwECAKAkCIACQHyMe0Z4MkAAAJQEAVAAoBcYAAClQwAUABLyaoB0LjCGdQIAoHgEQAHUCywzO8fMCg8AAIpGABQAqkSGSWRY7ldJTzAAAIpHABQAQkJCmA8MAIBSIAAKEARAAACUHAFQgIjPmw6DJjAAAIpHABRogyESAAEAUCwCoECbEJUmMAAAikUAFCDiGAwRAIASIwAKuAwQTWAAABSHAChAMCEqAAAlRwAUIOLyJkSlFxgAAMUjAAq4XmBkgAAAKA4BUIBNiEoNEAAAxSMACrAJUVPSz0lODjPCAwBQFAKgABsJWmOf1LM0gwEAUBQCoAARGR4qsZFh5jF1QAAAFI0AKIAwHxgAACVDABRAmBEeAICSIQAKwNGgyQABAFC08GKehwPnA/u/Dftl37GfpV3Di6RNwwRPF3kAAJCLACiAXHtZDVn16Q/ydfJps7wg35jtTWpVlbaNLpL2lyZI24YXSb2EGAkJCbH6cgEAsEyIy+Vi0Jh8UlNTJS4uTlJSUqR69eriJAd/PCNbv/vJs+w/nlZgn8Tq0dK2UYK0a3SR+dk0sbqEhRIQAQCC5/5NABRgAVB+P57OkO0HTsq2AydNQPTZ9ymSlW+gxGpR4XJ1wwRp3yjBZIpa14+X6IjcLvUAADgFAdAFCqQAKL/0zGzZdeiUbNMM0YGTsuPASTmdkeWzT0RYiLSoG5ebIWqYGxRdRB0RAMDmHBcAzZ07V6ZPny5Hjx6VVq1ayfPPPy/t27cvdP/ly5fLhAkT5LvvvpMmTZrItGnT5OabbzbPnTt3TsaPHy+rV6+Wb775xnwQ3bp1k6lTp0qdOnUk2AOg/LJzXPLF0VTZ+m1uQKQ/k3/OKLBf41pVpV2jBBMYXVw1SmpWi5KasVFSo2qkVIkMo6YIAGA5RwVAy5Ytk0GDBsn8+fOlQ4cOMnv2bBPg7Nu3T2rVqlVg/02bNknnzp1lypQpcsstt8iSJUtMALRjxw5p0aKFedN33nmnDB061ARTJ0+elFGjRkl2drZs27atRNcUTAFQfvrP4fuT6Xk1RLnNZlpQXZToiFCpERslNatGSo2qUVIjNtIESOanruv2vOe1R1pEGKMvAACCPADSoKddu3YyZ84cs56TkyP169eXkSNHytixYwvs369fP0lLS5O3337bs+3aa6+V1q1bmyDKn61bt5qM0oEDB6RBgwbFXlMwB0D+/JSWaeqI3MHQj2mZprboxOkMOXsup0wDNprAKF+ApD/1uapR4VItOlxio8JzH0dFSGxUmIQTOAEAyun+bWk3+MzMTNm+fbuMGzfOsy00NNQ0WSUlJfk9RrePGTPGZ1uPHj1k5cqVhb6OfhDaRBMfH+/3+YyMDLN4f4A4T+t/ft28tlnyO5OZJSd+zpQTaRny4+ncwEgDJA2OzHre9hOnM+WntAwzWauOVK3L16W8jpiIMKkarQFRuPkZGxnus67Bkv/nIzzPa3OdFnjT6w3AhdDcgZYQZLt/5rgkJ0c86zne23226aTV/rfrY3NeP9u99/Wcx71dt5nH5/f1fi19zmcfz2PX+X28z5/3PnLyHeN+z7ruPt489jne+7qkkGNy129oWksm3NLcsu/Q0gDoxIkTpmmqdm3fG6uuf/HFF36P0Tohf/vrdn/Onj0rjz32mAwYMKDQaFCb05588skyv49gViUyXBrU0KVKsfvqL8GpM5m+AZInYMrdlpp+zhRlm+VslvyckSWZWblZpvRz2WY57qdGqbQiw0JN011MZJgJrKLzFn3suy3Us837efd2321h5rwR4aHmp1n0cXgoARcqnfum53vz9L1pav7f+6Z3frvvjdrsl/+GWMKbtW8QcP7m6/N83jbtoXr+us+/vv9zej3v95x5n4H3814/9Xi9ft/XLPz185/H+upZ50suh/+XX4iAHghRC6Lvvvtu84983rx5he6nGSjvrJJmgLQZDuVLgwBTI1Q1Si6vXa3Ex2kAlJYXFP18Ni84yjh3/vHZLPP8z3mP3QGU9/PubZ5zZueYJfWsbw+4iqLxjwZCWv8UlRcgeQKlvO0mWPL66f18ZFiI2Uc/w9DQEAnXnyEhZt2z5F/Pt837uHA/2/QaQzw/9arPb9NV3ab7iddj/anPluQYpTcNvanovcPl/VP/UvR+3qzn/fR6fP5YczZzI87dx/cvUc9j900/76bu/Rey+3zuG7z7OHcA4H2sv7/c898o899g3cFGdiF/fXsCEq+bbFZOjglE9KfewLP1Z477p1fGIfv8DVtv4D43aW7QtqC/B+Z3zP07qL9b7t87s03MthDv39H8x+Q7Xn+V8m93/+66f5/Ndv298zx2bz9/bvfrh/rbx/M4b5+8/d3Hu6/X/Xueu//5/dzPea+7r9v9/wz3a2gZRNAGQDVr1pSwsDA5duyYz3ZdT0xM9HuMbi/J/u7gR+t+PvjggyLbAqOioswCe8rNouQWUF8IvelkZOXI2bxMklkysz3rWs9kfmaef97znGdb7vFmu/d+ep6sHBOsmeAqL2vleW2XmPPr8vMFfh5AefG+WXnfTM2N2nNz9roJ5t20cx/7HlvwZl9IAOB+LfP4/GsVFtx7Bwuh+QKJ89vOBwC++0kxr+/9B4Pv51DUe/L+48LnnPmCA9ibpQFQZGSktGnTRtatWyd9+vTxFEHr+ogRI/we07FjR/P86NGjPdvWrl1rtucPfr766itZv3691KhRoxLeDexO/0dlmrciwyShgl/LnVo3AVFWjpzLzjHBl/50B0jubbmPc/c1z2flSEZ2jpzLC6bcP3XJzRzkZg2yfLIDeT/zsg5ZhWzzzi54NznotvNZmLzMincGxivz4p2F8c7i5GZj/D2Xe4wna2QyQ+7skXfW6XzWKH8GyecYn6xT7nP6H/eN2fuvTe+slvuvV++/Tr1v4v73zf/XbP6/ovP9pZz/Buy+wRf5F7xu0+xe3s+87e4snd/F6wZd6H4m0xfqE4jk/0seCGaWN4Fp09PgwYOlbdu2pqeWdoPXXl5Dhgwxz2sX+bp165o6HaVd2rt06SIzZ86UXr16ydKlS0339gULFniCH+0Gr93itaeY1hi564MuuugiE3QBFU1vLhF5zVaxJBcBwHYsD4C0W/vx48dl4sSJJlDR7uxr1qzxFDofPHjQ9Axz69Spkxn7Rwc7fPzxx81AiNoDTMcAUocPH5Y333zTPNZzedNsUNeuXSv1/QEAAPuxfBwgO2IcIAAAAvv+zchyAAAg6BAAAQCAoEMABAAAgg4BEAAACDoEQAAAIOgQAAEAgKBDAAQAAIIOARAAAAg6BEAAACDoEAABAICgQwAEAACCDgEQAAAIOpbPBm9H7vlhdVI1AADgDO77dknmeScA8uPnn382P+vXr2/1pQAAgDLcx3VW+KKEuEoSJgWZnJwcOXLkiFSrVk1CQkLKPTrVwOrQoUNSvXp1CWS818AVTO+X9xq4gun9Bst7dblcJvipU6eOhIYWXeVDBsgP/dDq1atXoa+h/wAD+R+hN95r4Aqm98t7DVzB9H6D4b3GFZP5caMIGgAABB0CIAAAEHQIgCpZVFSUTJo0yfwMdLzXwBVM75f3GriC6f0G03stKYqgAQBA0CEDBAAAgg4BEAAACDoEQAAAIOgQAAEAgKBDAFQB5s6dK40aNZLo6Gjp0KGDbNmypcj9ly9fLk2bNjX7t2zZUlavXi12N2XKFGnXrp0ZLbtWrVrSp08f2bdvX5HHLF682Iys7b3oe7a7J554osB16/cVaN+pm/7bzf9+dRk+fLjjv9f//ve/cuutt5pRYvU6V65c6fO89gmZOHGiXHLJJRITEyPdunWTr776qtx/5+3wfs+dOyePPfaY+fcZGxtr9hk0aJAZBb+8fx/s8N3+/ve/L3DdPXv2dOR3W9x79ff7q8v06dMd971WJAKgcrZs2TIZM2aM6W64Y8cOadWqlfTo0UOSk5P97r9p0yYZMGCA3HvvvbJz504TSOiye/dusbONGzeaG+LmzZtl7dq15n+m3bt3l7S0tCKP0xFIf/jhB89y4MABcYIrr7zS57o/+uijQvd16nfqtnXrVp/3qt+vuuuuuxz/veq/T/2d1JuaP88884w899xzMn/+fPn4449NYKC/v2fPni2333m7vN8zZ86Y650wYYL5+Z///Mf8EXPbbbeV6++DXb5bpQGP93W/9tprRZ7Trt9tce/V+z3qsnDhQhPQ9O3b13Hfa4XSbvAoP+3bt3cNHz7cs56dne2qU6eOa8qUKX73v/vuu129evXy2dahQwfX/fff73KS5ORkHU7BtXHjxkL3WbRokSsuLs7lNJMmTXK1atWqxPsHynfqNmrUKNcvfvELV05OTkB9r/rvdcWKFZ51fX+JiYmu6dOne7adOnXKFRUV5XrttdfK7XfeLu/Xny1btpj9Dhw4UG6/D3Z5r4MHD3b17t27VOdxwndbku9V3/cNN9xQ5D6THPC9ljcyQOUoMzNTtm/fbtLm3vOK6XpSUpLfY3S79/5K/8IobH+7SklJMT8vuuiiIvc7ffq0NGzY0EzK17t3b9mzZ484gTaDaLr5sssuk4EDB8rBgwcL3TdQvlP3v+lXXnlF7rnnniInBnbq9+rt22+/laNHj/p8dzqnkDZ7FPbdleV33u6/x/o9x8fHl9vvg51s2LDBNNlfccUVMmzYMPnxxx8L3TdQvttjx47JqlWrTEa6OF859HstKwKgcnTixAnJzs6W2rVr+2zXdf0fqz+6vTT721FOTo6MHj1arrvuOmnRokWh++n/dDQV+8Ybb5ibqh7XqVMn+f7778XO9AaodS5r1qyRefPmmRvlL3/5SzPjcKB+p25aW3Dq1ClTPxFo32t+7u+nNN9dWX7n7Uqb+bQmSJtvi5oss7S/D3ahzV8vv/yyrFu3TqZNm2aa8W+66Sbz/QXyd/vSSy+ZWs077rijyP06OPR7vRDMBo8LprVAWt9SXHtxx44dzeKmN8lmzZrJCy+8IJMnTxa70v9Jul111VXmfxSa7Xj99ddL9FeVk/3jH/8w71//Kgy07xXnaQ3f3XffbYrA9eYXiL8P/fv39zzWwm+99l/84hcmK3TjjTdKoNI/TjSbU1zHhJsc+r1eCDJA5ahmzZoSFhZmUo7edD0xMdHvMbq9NPvbzYgRI+Ttt9+W9evXS7169Up1bEREhFx99dXy9ddfi5No88Dll19e6HU7/Tt100Lm999/X/7whz8Exffq/n5K892V5XfersGPft9a8F5U9qcsvw92pc08+v0Vdt2B8N1++OGHprC9tL/DTv5eS4MAqBxFRkZKmzZtTIrVTZsDdN37L2Rvut17f6X/Eypsf7vQvxQ1+FmxYoV88MEHcumll5b6HJpe/uyzz0yXYyfRepf9+/cXet1O/U7zW7RokamX6NWrV1B8r/pvWG9s3t9damqq6Q1W2HdXlt95OwY/WvuhwW6NGjXK/ffBrrSJVmuACrtup3+37gyuvgftMRYs32upWF2FHWiWLl1qeo0sXrzYtXfvXtd9993nio+Pdx09etQ8/7vf/c41duxYz/7/+9//XOHh4a4ZM2a4Pv/8c1OJHxER4frss89cdjZs2DDT82fDhg2uH374wbOcOXPGs0/+9/rkk0+63n33Xdf+/ftd27dvd/Xv398VHR3t2rNnj8vOHn74YfM+v/32W/N9devWzVWzZk3T8y2QvlNv2tulQYMGrscee6zAc07+Xn/++WfXzp07zaL/+5s1a5Z57O71NHXqVPP7+sYbb7g+/fRT03vm0ksvdaWnp3vOob1pnn/++RL/ztv1/WZmZrpuu+02V7169Vy7du3y+T3OyMgo9P0W9/tgx/eqzz3yyCOupKQkc93vv/++65prrnE1adLEdfbsWcd9t8X9O1YpKSmuKlWquObNm+f3HDc45HutSARAFUD/UenNIzIy0nSj3Lx5s+e5Ll26mO6Y3l5//XXX5Zdfbva/8sorXatWrXLZnf7S+Vu0S3Rh73X06NGez6V27dqum2++2bVjxw6X3fXr1891ySWXmOuuW7euWf/6668D7jv1pgGNfp/79u0r8JyTv9f169f7/Xfrfj/aFX7ChAnmfeiN78YbbyzwGTRs2NAEtSX9nbfr+9UbXWG/x3pcYe+3uN8HO75X/cOse/furosvvtj8MaLvaejQoQUCGad8t8X9O1YvvPCCKyYmxgzl4E9Dh3yvFSlE/1O6nBEAAICzUQMEAACCDgEQAAAIOgRAAAAg6BAAAQCAoEMABAAAgg4BEAAACDoEQAAAIOgQAAGQkJAQM/N7Rfnuu+/Ma+zatUsqks5a36dPnwqdWqB79+5iN40aNZLZs2eX6djMzExz/LZt28r9ugA7IwACAtzRo0dl5MiRZvLHqKgoqV+/vtx6660F5isLBM8++6wsXry4QoK/s2fPyoQJE2TSpEmebU888YQ5Pv/StGlTcQqd8+qRRx6Rxx57zOpLASpVeOW+HIDKpJmX6667zszsPH36dGnZsqWZAPPdd9+V4cOHyxdffCGBJC4ursLO/a9//cvMlK6fp7crr7zSTCTqLTzcWf9rHThwoDz88MOyZ88e836AYEAGCAhgf/zjH01GYsuWLdK3b1+5/PLLzQ1uzJgxsnnzZp99T5w4IbfffrtUqVJFmjRpIm+++abP87t375abbrpJqlatKrVr15bf/e535hjvmbKfeeYZady4sck0NWjQQJ5++ulCZ4y/5557TKbk4MGDZpte57x588xrxMTEmIyVBh3edJb5G264wTyvM5ffd999ZtbqwprAunbtKg8++KD86U9/kosuusjM9q5ZGzdt+lH6vvX13ev+LF261GTO8tNgR8/rvdSsWdPnNSZPniwDBgyQ2NhYqVu3rsydO9fnHPoZ9O7d23y2GmTpDO3Hjh3z2eett96Sdu3aSXR0tDm/XrO3M2fOmM+0WrVq5rNfsGCBTzPXiBEjzMzeenzDhg1lypQpnucTEhJMYKfvEQgWBEBAgPrpp59kzZo1JtOjN978NCvk7cknnzQ33k8//VRuvvlmkxXQc6hTp06ZwOPqq682tSJ6Xr1B6/5u48aNk6lTp5pmor1798qSJUtMoJRfRkaG3HXXXaYe6MMPPzQ3azc9VgO1Tz75xLx+//795fPPPzfPpaWlSY8ePczNeuvWrbJ8+XKTedEbe1Feeukl8/4//vhjE6A99dRTsnbtWvOcnkctWrRIfvjhB8+6Px999JG0bdtWykKzb61atZKdO3fK2LFjZdSoUZ5r0MBRgx/9rDdu3Gi2f/PNN9KvXz/P8atWrTIBj34veg5tvmzfvr3Pa8ycOdNcnz6vge+wYcNk37595rnnnnvOBLSvv/662fbqq68WCPb0fPp9AEHD6tlYAVSMjz/+2MwQ/Z///KfYfXW/8ePHe9ZPnz5ttr3zzjtmffLkyWY2bW+HDh3yzBifmppqZk9/8cUX/Z7fPfP4hx9+aGZYv/766wvMUq3PP/DAAz7bOnTo4Bo2bJh5vGDBAldCQoK5NrdVq1a5QkNDPbN662zYvXv39pm5Xl/LW7t27VyPPfaYz+uuWLGiyM/n5MmTZr///ve/Ptt1Nm19/djYWJ/l/vvv95l1u2fPnj7H6UzbN910k3n83nvvucLCwlwHDx70PL9nzx7zelu2bDHrHTt2dA0cOLDQ69PX+O1vf+tZ11nta9Wq5Zo3b55ZHzlypOuGG24w2wvz7LPPuho1alTk5wAEEmc1VAMosdx7e8ldddVVnseaMdGmmOTkZLOuGZn169ebJpr89u/fbzJEmtm58cYbi3wNbQaqV6+efPDBB6YZK7+OHTsWWHf3HNNMkGZRvLNZ2myjGRTNavjLNuV/X0qbgdzvq6TS09PNT20+yu+KK64o0Fyon11x78vda0vflxam6+LWvHlzk6HT57TZSz+DoUOHFnmN3u9Tm/O0Kc79PrVp8Ne//rW51p49e8ott9xSoDebfh/ajAYECwIgIEBpHY/eCEta6BwREeGzrsdqcKG0zkbrX6ZNm1bgOA0otMmmJLQJ55VXXpGkpCTTpFYZinpfJaX1RnrcyZMn/fai0rqniuQvWCzN+7zmmmvk22+/lXfeecc0G2rTZbdu3XxqrLQJ7uKLL66AqwfsiRogIEBp0a/WzGjBrdbP5KdZm5LSG6j2ENK6Eb3Zey+akdFgS2/SxXWt17oUrRO67bbbTL1LfvkLs3W9WbNm5rH+1EyU93v53//+J6GhoSazUVYaOGhRdlE0yNGsjNY2lUVx7+vQoUNmcdPX0e9HX9Od3bnQYQs0K6V1RS+++KIsW7ZM/v3vf3tqvNxF7lrjBQQLAiAggGnwozd3LXDVG95XX31lmlW0KDZ/s0xRtJBab5bahKWFwtrspV3phwwZYs6vTUM6joz2tnr55ZfN83qT14ED89Mxif7yl7+YZhgtLPamhc0LFy6UL7/80oy3o73X3EXOWhStrzN48GBzs9YmOT2X9kYrrPmrJDSo0+BCx0vyl+Fx02Ay//WqrKwsc6z3kr8HlwZqWoCt70u/E32fWgitNBOjwxPo+9uxY4d5z4MGDZIuXbp4iq71s3jttdfMT/3+tDecv2xcYWbNmmWO12ygXoO+vjaReRfCawG0HQd5BCoKARAQwLQrud5Uf/WrX5lxXlq0aGFqQfSGr13OS6pOnTrmJq7Bjt4k9YY9evRocwPVDIy7B5e+xsSJE01WQ7MNhdXa6LHa60ybxDZt2uTZrtu0K7ZmPDSQ0pu2Owui3fM16NJATOti7rzzTlNzNGfOnAv6jLT3lPa80hqcojIg9957r6xevVpSUlJ8tmtmTJsBvRftZu5NPxftPafn1+BPAxINqNxNVW+88Ybp3da5c2cTEOn3plka7+78GrRorVHr1q1N86EGSiWlXeM1ANOASj87HR9K34v7u9MmSX1f+pkCwSJEK6GtvggA0EBgxYoVFTqVxYXS7vvaHKhd/kuTYdKATxe70mBVC8wff/xxqy8FqDRkgACgFOP5+OsJ52Q6SKJm9B566CGrLwWoVPQCA4BSZHO07iiQaIH3+PHjrb4MoNLRBAYAAIIOTWAAACDoEAABAICgQwAEAACCDgEQAAAIOgRAAAAg6BAAAQCAoEMABAAAgg4BEAAACDoEQAAAIOj8P++n8E3g4ZyqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import for sklearn dataset and preprocessing\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============================\n",
    "# Activation functions and their derivatives\n",
    "# ============================\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(z, a):\n",
    "    # Note: z is not used here; kept for uniform signature.\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(z, a):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(z, a):\n",
    "    return np.ones_like(a)\n",
    "\n",
    "activation_functions = {\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"relu\": relu,\n",
    "    \"linear\": linear\n",
    "}\n",
    "\n",
    "activation_derivatives = {\n",
    "    \"sigmoid\": lambda z, a: sigmoid_derivative(z, a),\n",
    "    \"relu\": lambda z, a: relu_derivative(z, a),\n",
    "    \"linear\": lambda z, a: linear_derivative(z, a)\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Loss functions and their derivatives\n",
    "# ============================\n",
    "\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_crossentropy_derivative(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return - (y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "loss_functions = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_loss,\n",
    "    \"mse\": mse_loss\n",
    "}\n",
    "\n",
    "loss_derivatives = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_derivative,\n",
    "    \"mse\": mse_derivative\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Regularization functions (modular)\n",
    "# ============================\n",
    "\n",
    "def compute_reg_gradient(W, lambda_reg, reg_type, m):\n",
    "    if reg_type == \"l2\":\n",
    "        return lambda_reg * W / m\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * np.sign(W) / m\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_reg_loss(W_list, lambda_reg, reg_type):\n",
    "    if reg_type == \"l2\":\n",
    "        return (lambda_reg / 2) * sum(np.sum(W ** 2) for W in W_list)\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * sum(np.sum(np.abs(W)) for W in W_list)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# ============================\n",
    "# Neural Network Class with Learning Rate Decay Options and Custom Weight Initialization\n",
    "# ============================\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.01, lambda_reg=0.001, reg_type=\"l2\",\n",
    "                 loss_function_name=None,\n",
    "                 activation_function_name=\"relu\",\n",
    "                 output_activation_function_name=None,\n",
    "                 activation_function_names=None,\n",
    "                 task=\"classification\",\n",
    "                 lr_decay_type=\"none\",  # Options: \"none\", \"exponential\", \"linear\"\n",
    "                 decay_rate=0.0,\n",
    "                 weight_init=\"base\"):  # New parameter for weight initialization strategy\n",
    "        \"\"\"\n",
    "        :param layers: List containing the size of each layer (input, hidden, output)\n",
    "        :param learning_rate: Initial learning rate\n",
    "        :param lambda_reg: Regularization coefficient\n",
    "        :param reg_type: Type of regularization (\"l2\", \"l1\", or other for none)\n",
    "        :param loss_function_name: Name of the loss function (if None, set based on task)\n",
    "        :param activation_function_name: Activation to use for hidden layers (if activation_function_names not provided)\n",
    "        :param output_activation_function_name: Activation for the output layer (if None, set based on task)\n",
    "        :param activation_function_names: List of activation function names for each layer (length = len(layers)-1)\n",
    "        :param task: \"classification\" or \"regression\"\n",
    "        :param lr_decay_type: Learning rate decay strategy (\"none\", \"exponential\", \"linear\")\n",
    "        :param decay_rate: Decay rate used in the learning rate schedule\n",
    "        :param weight_init: Weight initialization strategy (\"base\" uses fan-in scaling or \"glorot\")\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.reg_type = reg_type\n",
    "        self.task = task\n",
    "        self.lr_decay_type = lr_decay_type\n",
    "        self.decay_rate = decay_rate\n",
    "        self.weight_init = weight_init  # Store the weight initialization strategy\n",
    "        \n",
    "        # Set defaults based on task\n",
    "        if self.task == \"regression\":\n",
    "            self.loss_function_name = loss_function_name or \"mse\"\n",
    "            output_activation_function_name = output_activation_function_name or \"linear\"\n",
    "        else:\n",
    "            self.loss_function_name = loss_function_name or \"binary_crossentropy\"\n",
    "            output_activation_function_name = output_activation_function_name or \"sigmoid\"\n",
    "        \n",
    "        # If no list of activations is provided, use the same activation for all hidden layers and set the output activation\n",
    "        if activation_function_names is None:\n",
    "            self.activation_function_names = [activation_function_name] * (len(layers) - 1)\n",
    "            self.activation_function_names[-1] = output_activation_function_name\n",
    "        else:\n",
    "            if len(activation_function_names) != len(layers) - 1:\n",
    "                raise ValueError(\"activation_function_names must have length equal to len(layers)-1.\")\n",
    "            self.activation_function_names = activation_function_names\n",
    "        \n",
    "        self._initialize_parameters()\n",
    "        \n",
    "    def _initialize_parameters(self):\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            fan_in = self.layers[i]\n",
    "            fan_out = self.layers[i + 1]\n",
    "            if self.weight_init == \"base\":\n",
    "                # Standard random initialization scaled by fan-in\n",
    "                std = np.sqrt(1.0 / fan_in)\n",
    "            elif self.weight_init == \"glorot\":\n",
    "                # Glorot (Xavier) initialization\n",
    "                std = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported weight initialization strategy. Use 'base' or 'glorot'.\")\n",
    "            weight = np.random.randn(fan_in, fan_out) * std\n",
    "            self.W.append(weight)\n",
    "            self.b.append(np.zeros((1, fan_out)))\n",
    "    \n",
    "    def _apply_activation(self, x, func_name):\n",
    "        if func_name not in activation_functions:\n",
    "            raise ValueError(f\"Unsupported activation: {func_name}\")\n",
    "        return activation_functions[func_name](x)\n",
    "    \n",
    "    def _apply_activation_derivative(self, z, a, func_name):\n",
    "        if func_name not in activation_derivatives:\n",
    "            raise ValueError(f\"Unsupported activation derivative: {func_name}\")\n",
    "        return activation_derivatives[func_name](z, a)\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        A = [X]\n",
    "        Z = []\n",
    "        # Forward propagation through hidden layers\n",
    "        for i in range(len(self.W) - 1):\n",
    "            z_curr = np.dot(A[-1], self.W[i]) + self.b[i]\n",
    "            Z.append(z_curr)\n",
    "            a_curr = self._apply_activation(z_curr, self.activation_function_names[i])\n",
    "            A.append(a_curr)\n",
    "        # Forward propagation through output layer\n",
    "        z_out = np.dot(A[-1], self.W[-1]) + self.b[-1]\n",
    "        Z.append(z_out)\n",
    "        a_out = self._apply_activation(z_out, self.activation_function_names[-1])\n",
    "        A.append(a_out)\n",
    "        return Z, A\n",
    "    \n",
    "    def _backward(self, X, y, Z, A):\n",
    "        m = X.shape[0]\n",
    "        if self.loss_function_name not in loss_derivatives:\n",
    "            raise ValueError(f\"Unsupported loss derivative: {self.loss_function_name}\")\n",
    "        # Compute derivative of loss with respect to output activation\n",
    "        dA = loss_derivatives[self.loss_function_name](y, A[-1])\n",
    "        # Compute derivative with respect to z at output layer\n",
    "        dZ = dA * self._apply_activation_derivative(Z[-1], A[-1], self.activation_function_names[-1])\n",
    "        reg_term = compute_reg_gradient(self.W[-1], self.lambda_reg, self.reg_type, m)\n",
    "        dW = [np.dot(A[-2].T, dZ) / m + reg_term]\n",
    "        db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "        \n",
    "        # Backpropagation through hidden layers\n",
    "        for i in range(len(self.W) - 2, -1, -1):\n",
    "            dA = np.dot(dZ, self.W[i + 1].T)\n",
    "            dZ = dA * self._apply_activation_derivative(Z[i], A[i + 1], self.activation_function_names[i])\n",
    "            reg_term = compute_reg_gradient(self.W[i], self.lambda_reg, self.reg_type, m)\n",
    "            dW.insert(0, np.dot(A[i].T, dZ) / m + reg_term)\n",
    "            db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "        \n",
    "        # Update parameters using current learning rate\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] -= self.learning_rate * dW[i]\n",
    "            self.b[i] -= self.learning_rate * db[i]\n",
    "    \n",
    "    def train(self, X, y, epochs=300, batch_size=32, verbose=True):\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            # Update learning rate according to the decay schedule\n",
    "            if self.lr_decay_type == \"exponential\":\n",
    "                self.learning_rate = self.initial_learning_rate * np.exp(-self.decay_rate * epoch)\n",
    "            elif self.lr_decay_type == \"linear\":\n",
    "                # Ensure learning rate does not go negative.\n",
    "                self.learning_rate = self.initial_learning_rate * max(0, 1 - self.decay_rate * epoch)\n",
    "            # Otherwise (\"none\"), keep the initial learning rate.\n",
    "            \n",
    "            permutation = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                Z, A = self._forward(X_batch)\n",
    "                self._backward(X_batch, y_batch, Z, A)\n",
    "            if epoch % max(1, int(epochs / 20)) == 0:\n",
    "                _, A_full = self._forward(X)\n",
    "                loss = loss_functions[self.loss_function_name](y, A_full[-1])\n",
    "                reg_loss = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "                total_loss = loss + reg_loss\n",
    "                loss_history.append(total_loss)\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch:4d}, Loss: {total_loss:.4f}, Learning Rate: {self.learning_rate:.6f}\")\n",
    "        return loss_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, A = self._forward(X)\n",
    "        output = A[-1]\n",
    "        if self.task == \"classification\":\n",
    "            # For binary classification, threshold at 0.5\n",
    "            if output.shape[1] == 1:\n",
    "                return (output > 0.5).astype(int)\n",
    "            else:\n",
    "                return np.argmax(output, axis=1)\n",
    "        else:  # regression\n",
    "            return output\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        if self.task == \"regression\":\n",
    "            return mse_loss(y, predictions)\n",
    "        else:\n",
    "            # If y is one-hot encoded, convert to class labels\n",
    "            if y.ndim > 1 and y.shape[1] > 1:\n",
    "                y_true = np.argmax(y, axis=1)\n",
    "            else:\n",
    "                y_true = y\n",
    "            return np.mean(predictions == y_true)\n",
    "\n",
    "# ============================\n",
    "# Testing on a sklearn Classification Dataset\n",
    "# ============================\n",
    "\n",
    "# Load the breast cancer dataset (binary classification)\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target.reshape(-1, 1)  # reshape y to be a column vector\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the network architecture\n",
    "input_size = X_train.shape[1]\n",
    "hidden_units = 10\n",
    "output_size = 1  # binary classification\n",
    "layers = [input_size, hidden_units, output_size]\n",
    "\n",
    "# Define activation functions for hidden and output layers\n",
    "activation_funcs = [\"relu\", \"sigmoid\"]\n",
    "\n",
    "# Create the NeuralNetwork instance with learning rate decay and custom weight initialization.\n",
    "# Set weight_init to \"base\" (fan-in scaling) or \"glorot\" as desired.\n",
    "nn_clf = NeuralNetwork(\n",
    "    layers=layers,\n",
    "    learning_rate=0.1,\n",
    "    lambda_reg=0.001,\n",
    "    reg_type=\"l2\",\n",
    "    loss_function_name=\"mse\",\n",
    "    activation_function_names=activation_funcs,\n",
    "    task=\"classification\",\n",
    "    lr_decay_type=\"exponential\",  # Try \"exponential\", \"linear\", or \"none\"\n",
    "    decay_rate=0.001,             # Adjust decay rate as needed\n",
    "    weight_init=\"base\"          # Choose between \"base\" or \"glorot\"\n",
    ")\n",
    "\n",
    "print(\"Training the neural network on the breast cancer dataset...\")\n",
    "loss_history = nn_clf.train(X_train, y_train, epochs=1000, batch_size=32, verbose=True)\n",
    "\n",
    "accuracy = nn_clf.evaluate(X_test, y_test)\n",
    "print(f\"\\nNeural Network Classification Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot the training loss history\n",
    "pd.Series(loss_history).plot(title=\"Training Loss History\")\n",
    "plt.xlabel(\"Checkpoint (Epochs)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the neural network on the breast cancer dataset...\n",
      "Epoch    0, Loss: 0.1739, Learning Rate: 0.100000\n",
      "Epoch   50, Loss: 0.0237, Learning Rate: 0.095123\n",
      "Epoch  100, Loss: 0.0211, Learning Rate: 0.090484\n",
      "Epoch  150, Loss: 0.0202, Learning Rate: 0.086071\n",
      "Epoch  200, Loss: 0.0196, Learning Rate: 0.081873\n",
      "Epoch  250, Loss: 0.0191, Learning Rate: 0.077880\n",
      "Epoch  300, Loss: 0.0189, Learning Rate: 0.074082\n",
      "Epoch  350, Loss: 0.0187, Learning Rate: 0.070469\n",
      "Epoch  400, Loss: 0.0187, Learning Rate: 0.067032\n",
      "Epoch  450, Loss: 0.0187, Learning Rate: 0.063763\n",
      "Epoch  500, Loss: 0.0188, Learning Rate: 0.060653\n",
      "Epoch  550, Loss: 0.0190, Learning Rate: 0.057695\n",
      "Epoch  600, Loss: 0.0191, Learning Rate: 0.054881\n",
      "Epoch  650, Loss: 0.0192, Learning Rate: 0.052205\n",
      "Epoch  700, Loss: 0.0194, Learning Rate: 0.049659\n",
      "Epoch  750, Loss: 0.0195, Learning Rate: 0.047237\n",
      "Epoch  800, Loss: 0.0196, Learning Rate: 0.044933\n",
      "Epoch  850, Loss: 0.0198, Learning Rate: 0.042741\n",
      "Epoch  900, Loss: 0.0199, Learning Rate: 0.040657\n",
      "Epoch  950, Loss: 0.0200, Learning Rate: 0.038674\n",
      "\n",
      "Neural Network Classification Accuracy: 0.9649\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import for sklearn dataset and preprocessing\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============================\n",
    "# Activation functions and their derivatives\n",
    "# ============================\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(z, a):\n",
    "    # Note: z is not used here; kept for uniform signature.\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(z, a):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(z, a):\n",
    "    return np.ones_like(a)\n",
    "\n",
    "activation_functions = {\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"relu\": relu,\n",
    "    \"linear\": linear\n",
    "}\n",
    "\n",
    "activation_derivatives = {\n",
    "    \"sigmoid\": lambda z, a: sigmoid_derivative(z, a),\n",
    "    \"relu\": lambda z, a: relu_derivative(z, a),\n",
    "    \"linear\": lambda z, a: linear_derivative(z, a)\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Loss functions and their derivatives\n",
    "# ============================\n",
    "\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_crossentropy_derivative(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return - (y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "loss_functions = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_loss,\n",
    "    \"mse\": mse_loss\n",
    "}\n",
    "\n",
    "loss_derivatives = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_derivative,\n",
    "    \"mse\": mse_derivative\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Regularization functions (modular)\n",
    "# ============================\n",
    "\n",
    "def compute_reg_gradient(W, lambda_reg, reg_type, m):\n",
    "    if reg_type == \"l2\":\n",
    "        return lambda_reg * W / m\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * np.sign(W) / m\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_reg_loss(W_list, lambda_reg, reg_type):\n",
    "    if reg_type == \"l2\":\n",
    "        return (lambda_reg / 2) * sum(np.sum(W ** 2) for W in W_list)\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * sum(np.sum(np.abs(W)) for W in W_list)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# ============================\n",
    "# Neural Network Class with Learning Rate Decay, Momentum, and Custom Weight Initialization\n",
    "# ============================\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.01, lambda_reg=0.001, reg_type=\"l2\",\n",
    "                 loss_function_name=None,\n",
    "                 activation_function_name=\"relu\",\n",
    "                 output_activation_function_name=None,\n",
    "                 activation_function_names=None,\n",
    "                 task=\"classification\",\n",
    "                 lr_decay_type=\"none\",  # Options: \"none\", \"exponential\", \"linear\"\n",
    "                 decay_rate=0.0,\n",
    "                 weight_init=\"base\",  # \"base\" (fan-in scaling) or \"glorot\"\n",
    "                 momentum_type=\"none\",  # Options: \"none\", \"momentum\", \"nesterov momentum\"\n",
    "                 momentum_alpha=0.9):    # momentum coefficient\n",
    "        \"\"\"\n",
    "        :param layers: List containing the size of each layer (input, hidden, output)\n",
    "        :param learning_rate: Initial learning rate\n",
    "        :param lambda_reg: Regularization coefficient\n",
    "        :param reg_type: Type of regularization (\"l2\", \"l1\", or other for none)\n",
    "        :param loss_function_name: Name of the loss function (if None, set based on task)\n",
    "        :param activation_function_name: Activation to use for hidden layers (if activation_function_names not provided)\n",
    "        :param output_activation_function_name: Activation for the output layer (if None, set based on task)\n",
    "        :param activation_function_names: List of activation function names for each layer (length = len(layers)-1)\n",
    "        :param task: \"classification\" or \"regression\"\n",
    "        :param lr_decay_type: Learning rate decay strategy (\"none\", \"exponential\", \"linear\")\n",
    "        :param decay_rate: Decay rate used in the learning rate schedule\n",
    "        :param weight_init: Weight initialization strategy (\"base\" uses fan-in scaling or \"glorot\")\n",
    "        :param momentum_type: Momentum strategy (\"none\", \"momentum\", \"nesterov momentum\")\n",
    "        :param momentum_alpha: Momentum coefficient (e.g., 0.9)\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.reg_type = reg_type\n",
    "        self.task = task\n",
    "        self.lr_decay_type = lr_decay_type\n",
    "        self.decay_rate = decay_rate\n",
    "        self.weight_init = weight_init  # Weight initialization strategy\n",
    "        \n",
    "        # Set momentum parameters\n",
    "        if momentum_type not in {\"none\", \"momentum\", \"nesterov momentum\"}:\n",
    "            raise ValueError(\"momentum_type must be 'none', 'momentum', or 'nesterov momentum'.\")\n",
    "        self.momentum_type = momentum_type\n",
    "        self.momentum_alpha = momentum_alpha if momentum_type != \"none\" else 0.0\n",
    "        \n",
    "        # Set defaults based on task\n",
    "        if self.task == \"regression\":\n",
    "            self.loss_function_name = loss_function_name or \"mse\"\n",
    "            output_activation_function_name = output_activation_function_name or \"linear\"\n",
    "        else:\n",
    "            self.loss_function_name = loss_function_name or \"binary_crossentropy\"\n",
    "            output_activation_function_name = output_activation_function_name or \"sigmoid\"\n",
    "        \n",
    "        # Set activation functions for layers\n",
    "        if activation_function_names is None:\n",
    "            self.activation_function_names = [activation_function_name] * (len(layers) - 1)\n",
    "            self.activation_function_names[-1] = output_activation_function_name\n",
    "        else:\n",
    "            if len(activation_function_names) != len(layers) - 1:\n",
    "                raise ValueError(\"activation_function_names must have length equal to len(layers)-1.\")\n",
    "            self.activation_function_names = activation_function_names\n",
    "        \n",
    "        self._initialize_parameters()\n",
    "        # Initialize momentum accumulators (even if not used, for consistency)\n",
    "        self.vW = [np.zeros_like(W) for W in self.W]\n",
    "        self.vb = [np.zeros_like(b) for b in self.b]\n",
    "        \n",
    "    def _initialize_parameters(self):\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            fan_in = self.layers[i]\n",
    "            fan_out = self.layers[i + 1]\n",
    "            if self.weight_init == \"base\":\n",
    "                # Standard random initialization scaled by fan-in\n",
    "                std = np.sqrt(1.0 / fan_in)\n",
    "            elif self.weight_init == \"glorot\":\n",
    "                # Glorot (Xavier) initialization\n",
    "                std = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported weight initialization strategy. Use 'base' or 'glorot'.\")\n",
    "            weight = np.random.randn(fan_in, fan_out) * std\n",
    "            self.W.append(weight)\n",
    "            self.b.append(np.zeros((1, fan_out)))\n",
    "    \n",
    "    def _apply_activation(self, x, func_name):\n",
    "        if func_name not in activation_functions:\n",
    "            raise ValueError(f\"Unsupported activation: {func_name}\")\n",
    "        return activation_functions[func_name](x)\n",
    "    \n",
    "    def _apply_activation_derivative(self, z, a, func_name):\n",
    "        if func_name not in activation_derivatives:\n",
    "            raise ValueError(f\"Unsupported activation derivative: {func_name}\")\n",
    "        return activation_derivatives[func_name](z, a)\n",
    "    \n",
    "    def _forward(self, X, weights=None, biases=None):\n",
    "        \"\"\"\n",
    "        Forward propagation. If weights and biases are provided, they are used;\n",
    "        otherwise the network's parameters are used.\n",
    "        Returns lists Z (pre-activations) and A (activations).\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.W\n",
    "        if biases is None:\n",
    "            biases = self.b\n",
    "            \n",
    "        A = [X]\n",
    "        Z = []\n",
    "        # Forward through hidden layers\n",
    "        for i in range(len(weights) - 1):\n",
    "            z_curr = np.dot(A[-1], weights[i]) + biases[i]\n",
    "            Z.append(z_curr)\n",
    "            a_curr = self._apply_activation(z_curr, self.activation_function_names[i])\n",
    "            A.append(a_curr)\n",
    "        # Forward through output layer\n",
    "        z_out = np.dot(A[-1], weights[-1]) + biases[-1]\n",
    "        Z.append(z_out)\n",
    "        a_out = self._apply_activation(z_out, self.activation_function_names[-1])\n",
    "        A.append(a_out)\n",
    "        return Z, A\n",
    "    \n",
    "    def _compute_gradients(self, X, y, Z, A, weights=None):\n",
    "        \"\"\"\n",
    "        Compute gradients dW and db given inputs X, target y, pre-activations Z and activations A.\n",
    "        Optionally, a custom set of weights (used in lookahead for Nesterov momentum) can be provided.\n",
    "        Returns lists dW and db.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.W\n",
    "        m = X.shape[0]\n",
    "        # Compute derivative of loss with respect to output activation\n",
    "        dA = loss_derivatives[self.loss_function_name](y, A[-1])\n",
    "        # Output layer\n",
    "        dZ = dA * self._apply_activation_derivative(Z[-1], A[-1], self.activation_function_names[-1])\n",
    "        reg_term = compute_reg_gradient(weights[-1], self.lambda_reg, self.reg_type, m)\n",
    "        dW = [np.dot(A[-2].T, dZ) / m + reg_term]\n",
    "        db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "        \n",
    "        # Backpropagate through hidden layers\n",
    "        for i in range(len(weights) - 2, -1, -1):\n",
    "            dA = np.dot(dZ, weights[i + 1].T)\n",
    "            dZ = dA * self._apply_activation_derivative(Z[i], A[i + 1], self.activation_function_names[i])\n",
    "            reg_term = compute_reg_gradient(weights[i], self.lambda_reg, self.reg_type, m)\n",
    "            dW.insert(0, np.dot(A[i].T, dZ) / m + reg_term)\n",
    "            db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "            \n",
    "        return dW, db\n",
    "    \n",
    "    def train(self, X, y, epochs=300, batch_size=32, verbose=True):\n",
    "        loss_history = []\n",
    "        n_samples = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            # Update learning rate based on decay schedule\n",
    "            if self.lr_decay_type == \"exponential\":\n",
    "                self.learning_rate = self.initial_learning_rate * np.exp(-self.decay_rate * epoch)\n",
    "            elif self.lr_decay_type == \"linear\":\n",
    "                self.learning_rate = self.initial_learning_rate * max(0, 1 - self.decay_rate * epoch)\n",
    "            # Otherwise (\"none\"), keep the initial learning rate.\n",
    "            \n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            \n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                \n",
    "                # Choose update strategy based on momentum type\n",
    "                if self.momentum_type == \"nesterov momentum\":\n",
    "                    # Compute lookahead parameters\n",
    "                    weights_lookahead = [self.W[j] - self.momentum_alpha * self.vW[j] for j in range(len(self.W))]\n",
    "                    biases_lookahead = [self.b[j] - self.momentum_alpha * self.vb[j] for j in range(len(self.b))]\n",
    "                    Z, A = self._forward(X_batch, weights=weights_lookahead, biases=biases_lookahead)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A, weights=weights_lookahead)\n",
    "                    \n",
    "                    # Update momentum accumulators and actual parameters\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.vW[j] = self.momentum_alpha * self.vW[j] + self.learning_rate * dW[j]\n",
    "                        self.vb[j] = self.momentum_alpha * self.vb[j] + self.learning_rate * db[j]\n",
    "                        self.W[j] -= self.vW[j]\n",
    "                        self.b[j] -= self.vb[j]\n",
    "                        \n",
    "                elif self.momentum_type == \"momentum\":\n",
    "                    # Standard momentum: compute gradients with current parameters\n",
    "                    Z, A = self._forward(X_batch)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.vW[j] = self.momentum_alpha * self.vW[j] + self.learning_rate * dW[j]\n",
    "                        self.vb[j] = self.momentum_alpha * self.vb[j] + self.learning_rate * db[j]\n",
    "                        self.W[j] -= self.vW[j]\n",
    "                        self.b[j] -= self.vb[j]\n",
    "                        \n",
    "                else:  # No momentum\n",
    "                    Z, A = self._forward(X_batch)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.W[j] -= self.learning_rate * dW[j]\n",
    "                        self.b[j] -= self.learning_rate * db[j]\n",
    "                        \n",
    "            # Log loss at checkpoints\n",
    "            if epoch % max(1, int(epochs / 20)) == 0:\n",
    "                _, A_full = self._forward(X)\n",
    "                loss = loss_functions[self.loss_function_name](y, A_full[-1])\n",
    "                reg_loss = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "                total_loss = loss + reg_loss\n",
    "                loss_history.append(total_loss)\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch:4d}, Loss: {total_loss:.4f}, Learning Rate: {self.learning_rate:.6f}\")\n",
    "                    \n",
    "        return loss_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, A = self._forward(X)\n",
    "        output = A[-1]\n",
    "        if self.task == \"classification\":\n",
    "            # For binary classification, threshold at 0.5\n",
    "            if output.shape[1] == 1:\n",
    "                return (output > 0.5).astype(int)\n",
    "            else:\n",
    "                return np.argmax(output, axis=1)\n",
    "        else:  # regression\n",
    "            return output\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        if self.task == \"regression\":\n",
    "            return mse_loss(y, predictions)\n",
    "        else:\n",
    "            # If y is one-hot encoded, convert to class labels\n",
    "            if y.ndim > 1 and y.shape[1] > 1:\n",
    "                y_true = np.argmax(y, axis=1)\n",
    "            else:\n",
    "                y_true = y\n",
    "            return np.mean(predictions == y_true)\n",
    "\n",
    "# ============================\n",
    "# Testing on a sklearn Classification Dataset\n",
    "# ============================\n",
    "\n",
    "# Load the breast cancer dataset (binary classification)\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target.reshape(-1, 1)  # reshape y to be a column vector\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the network architecture\n",
    "input_size = X_train.shape[1]\n",
    "hidden_units = 10\n",
    "output_size = 1  # binary classification\n",
    "layers = [input_size, hidden_units, output_size]\n",
    "\n",
    "# Define activation functions for hidden and output layers\n",
    "activation_funcs = [\"relu\", \"sigmoid\"]\n",
    "\n",
    "# Create the NeuralNetwork instance with learning rate decay, momentum, and custom weight initialization.\n",
    "nn_clf = NeuralNetwork(\n",
    "    layers=layers,\n",
    "    learning_rate=0.1,\n",
    "    lambda_reg=0.001,\n",
    "    reg_type=\"l2\",\n",
    "    loss_function_name=\"mse\",\n",
    "    activation_function_names=activation_funcs,\n",
    "    task=\"classification\",\n",
    "    lr_decay_type=\"exponential\",  # Try \"exponential\", \"linear\", or \"none\"\n",
    "    decay_rate=0.001,             # Adjust decay rate as needed\n",
    "    weight_init=\"base\",           # Choose between \"base\" or \"glorot\"\n",
    "    momentum_type=\"nesterov momentum\",  # \"none\", \"momentum\", or \"nesterov momentum\"\n",
    "    momentum_alpha=0.01            # Momentum coefficient\n",
    ")\n",
    "\n",
    "print(\"Training the neural network on the breast cancer dataset...\")\n",
    "loss_history = nn_clf.train(X_train, y_train, epochs=1000, batch_size=32, verbose=True)\n",
    "\n",
    "accuracy = nn_clf.evaluate(X_test, y_test)\n",
    "print(f\"\\nNeural Network Classification Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot the training loss history\n",
    "pd.Series(loss_history).plot(title=\"Training Loss History\")\n",
    "plt.xlabel(\"Checkpoint (Epochs)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-1-train.csv\n",
      "Using cached ../datasets/monks/monk-1-test.csv\n",
      "One-hot encoding MONK-1 dataset...\n",
      "Epoch    0, Loss: 0.2538, Learning Rate: 0.200000\n",
      "Epoch    5, Loss: 0.2384, Learning Rate: 0.199000\n",
      "Epoch   10, Loss: 0.2203, Learning Rate: 0.198000\n",
      "Epoch   15, Loss: 0.1969, Learning Rate: 0.197000\n",
      "Epoch   20, Loss: 0.1768, Learning Rate: 0.196000\n",
      "Epoch   25, Loss: 0.1612, Learning Rate: 0.195000\n",
      "Epoch   30, Loss: 0.1499, Learning Rate: 0.194000\n",
      "Epoch   35, Loss: 0.1412, Learning Rate: 0.193000\n",
      "Epoch   40, Loss: 0.1340, Learning Rate: 0.192000\n",
      "Epoch   45, Loss: 0.1279, Learning Rate: 0.191000\n",
      "Epoch   50, Loss: 0.1223, Learning Rate: 0.190000\n",
      "Epoch   55, Loss: 0.1173, Learning Rate: 0.189000\n",
      "Epoch   60, Loss: 0.1128, Learning Rate: 0.188000\n",
      "Epoch   65, Loss: 0.1085, Learning Rate: 0.187000\n",
      "Epoch   70, Loss: 0.1047, Learning Rate: 0.186000\n",
      "Epoch   75, Loss: 0.1008, Learning Rate: 0.185000\n",
      "Epoch   80, Loss: 0.0971, Learning Rate: 0.184000\n",
      "Epoch   85, Loss: 0.0936, Learning Rate: 0.183000\n",
      "Epoch   90, Loss: 0.0904, Learning Rate: 0.182000\n",
      "Epoch   95, Loss: 0.0873, Learning Rate: 0.181000\n",
      "\n",
      "Neural Network Classification Accuracy: 0.8287\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "# ============================\n",
    "# Activation functions and their derivatives\n",
    "# ============================\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(z, a):\n",
    "    # Note: z is not used here; kept for uniform signature.\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(z, a):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(z, a):\n",
    "    return np.ones_like(a)\n",
    "\n",
    "activation_functions = {\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"relu\": relu,\n",
    "    \"linear\": linear\n",
    "}\n",
    "\n",
    "activation_derivatives = {\n",
    "    \"sigmoid\": lambda z, a: sigmoid_derivative(z, a),\n",
    "    \"relu\": lambda z, a: relu_derivative(z, a),\n",
    "    \"linear\": lambda z, a: linear_derivative(z, a)\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Loss functions and their derivatives\n",
    "# ============================\n",
    "\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Binary crossentropy loss for binary classification.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_crossentropy_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the binary crossentropy loss.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return - (y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Squared Error (MSE) loss, typically used for regression.\n",
    "    \"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the MSE loss.\n",
    "    \"\"\"\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "def mee_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Euclidean Error (MEE) loss, defined as:\n",
    "        E_MEE = (1/N) * sum over i [ ||y_true[i] - y_pred[i]||_2 ].\n",
    "    \"\"\"\n",
    "    diff = y_true - y_pred  # shape: (N, d) or (N, 1)\n",
    "    # Euclidean distance for each sample\n",
    "    dist = np.sqrt(np.sum(diff ** 2, axis=1))  # shape: (N,)\n",
    "    return np.mean(dist)\n",
    "\n",
    "def mee_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the Mean Euclidean Error (MEE) loss.\n",
    "    For each sample i, derivative wrt y_pred[i] is:\n",
    "        (1/N) * ( (y_pred[i] - y_true[i]) / ||y_pred[i] - y_true[i]||_2 ).\n",
    "    We safely handle the case where the norm is zero.\n",
    "    \"\"\"\n",
    "    diff = (y_pred - y_true)  # shape: (N, d) or (N, 1)\n",
    "    dist = np.sqrt(np.sum(diff ** 2, axis=1, keepdims=True))  # shape: (N, 1)\n",
    "    # Avoid division by zero:\n",
    "    epsilon = 1e-8\n",
    "    dist_safe = np.where(dist == 0, epsilon, dist)\n",
    "    N = y_true.shape[0]\n",
    "    # Each sample's derivative is diff_i / dist_i, scaled by 1/N\n",
    "    derivative = diff / dist_safe / N  # shape: (N, d)\n",
    "    return derivative\n",
    "\n",
    "loss_functions = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_loss,\n",
    "    \"mse\": mse_loss,\n",
    "    \"mee\": mee_loss,  \n",
    "}\n",
    "\n",
    "loss_derivatives = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_derivative,\n",
    "    \"mse\": mse_derivative,\n",
    "    \"mee\": mee_derivative, \n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Regularization functions (modular)\n",
    "# ============================\n",
    "\n",
    "def compute_reg_gradient(W, lambda_reg, reg_type, m):\n",
    "    if reg_type == \"l2\":\n",
    "        return lambda_reg * W / m\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * np.sign(W) / m\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_reg_loss(W_list, lambda_reg, reg_type):\n",
    "    if reg_type == \"l2\":\n",
    "        return (lambda_reg / 2) * sum(np.sum(W ** 2) for W in W_list)\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * sum(np.sum(np.abs(W)) for W in W_list)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# ============================\n",
    "# Neural Network Class with Learning Rate Decay, Momentum, and Custom Weight Initialization\n",
    "# ============================\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.01, lambda_reg=0.001, reg_type=\"l2\",\n",
    "                 loss_function_name=None,\n",
    "                 activation_function_name=\"relu\",\n",
    "                 output_activation_function_name=None,\n",
    "                 activation_function_names=None,\n",
    "                 task=\"classification\",\n",
    "                 lr_decay_type=\"none\",  # Options: \"none\", \"exponential\", \"linear\"\n",
    "                 decay_rate=0.0,\n",
    "                 weight_init=\"base\",  # \"base\" (fan-in scaling) or \"glorot\"\n",
    "                 momentum_type=\"none\",  # Options: \"none\", \"momentum\", \"nesterov momentum\"\n",
    "                 momentum_alpha=0.9):    # momentum coefficient\n",
    "        \"\"\"\n",
    "        :param layers: List containing the size of each layer (input, hidden, output)\n",
    "        :param learning_rate: Initial learning rate\n",
    "        :param lambda_reg: Regularization coefficient\n",
    "        :param reg_type: Type of regularization (\"l2\", \"l1\", or other for none)\n",
    "        :param loss_function_name: Name of the loss function (if None, set based on task)\n",
    "        :param activation_function_name: Activation to use for hidden layers (if activation_function_names not provided)\n",
    "        :param output_activation_function_name: Activation for the output layer (if None, set based on task)\n",
    "        :param activation_function_names: List of activation function names for each layer (length = len(layers)-1)\n",
    "        :param task: \"classification\" or \"regression\"\n",
    "        :param lr_decay_type: Learning rate decay strategy (\"none\", \"exponential\", \"linear\")\n",
    "        :param decay_rate: Decay rate used in the learning rate schedule\n",
    "        :param weight_init: Weight initialization strategy (\"base\" uses fan-in scaling or \"glorot\")\n",
    "        :param momentum_type: Momentum strategy (\"none\", \"momentum\", \"nesterov momentum\")\n",
    "        :param momentum_alpha: Momentum coefficient (e.g., 0.9)\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.reg_type = reg_type\n",
    "        self.task = task\n",
    "        self.lr_decay_type = lr_decay_type\n",
    "        self.decay_rate = decay_rate\n",
    "        self.weight_init = weight_init  # Weight initialization strategy\n",
    "        \n",
    "        # Set momentum parameters\n",
    "        if momentum_type not in {\"none\", \"momentum\", \"nesterov momentum\"}:\n",
    "            raise ValueError(\"momentum_type must be 'none', 'momentum', or 'nesterov momentum'.\")\n",
    "        self.momentum_type = momentum_type\n",
    "        self.momentum_alpha = momentum_alpha if momentum_type != \"none\" else 0.0\n",
    "        \n",
    "        # Set defaults based on task\n",
    "        if self.task == \"regression\":\n",
    "            self.loss_function_name = loss_function_name or \"mse\"\n",
    "            output_activation_function_name = output_activation_function_name or \"linear\"\n",
    "        else:\n",
    "            # Classification\n",
    "            self.loss_function_name = loss_function_name or \"binary_crossentropy\"\n",
    "            output_activation_function_name = output_activation_function_name or \"sigmoid\"\n",
    "        \n",
    "        # Set activation functions for layers\n",
    "        if activation_function_names is None:\n",
    "            self.activation_function_names = [activation_function_name] * (len(layers) - 1)\n",
    "            self.activation_function_names[-1] = output_activation_function_name\n",
    "        else:\n",
    "            if len(activation_function_names) != len(layers) - 1:\n",
    "                raise ValueError(\"activation_function_names must have length equal to len(layers)-1.\")\n",
    "            self.activation_function_names = activation_function_names\n",
    "        \n",
    "        self._initialize_parameters()\n",
    "        # Initialize momentum accumulators (even if not used, for consistency)\n",
    "        self.vW = [np.zeros_like(W) for W in self.W]\n",
    "        self.vb = [np.zeros_like(b) for b in self.b]\n",
    "        \n",
    "    def _initialize_parameters(self):\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            fan_in = self.layers[i]\n",
    "            fan_out = self.layers[i + 1]\n",
    "            if self.weight_init == \"base\":\n",
    "                # Standard random initialization scaled by fan-in\n",
    "                std = np.sqrt(1.0 / fan_in)\n",
    "            elif self.weight_init == \"glorot\":\n",
    "                # Glorot (Xavier) initialization\n",
    "                std = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported weight initialization strategy. Use 'base' or 'glorot'.\")\n",
    "            weight = np.random.randn(fan_in, fan_out) * std\n",
    "            self.W.append(weight)\n",
    "            self.b.append(np.zeros((1, fan_out)))\n",
    "    \n",
    "    def _apply_activation(self, x, func_name):\n",
    "        if func_name not in activation_functions:\n",
    "            raise ValueError(f\"Unsupported activation: {func_name}\")\n",
    "        return activation_functions[func_name](x)\n",
    "    \n",
    "    def _apply_activation_derivative(self, z, a, func_name):\n",
    "        if func_name not in activation_derivatives:\n",
    "            raise ValueError(f\"Unsupported activation derivative: {func_name}\")\n",
    "        return activation_derivatives[func_name](z, a)\n",
    "    \n",
    "    def _forward(self, X, weights=None, biases=None):\n",
    "        \"\"\"\n",
    "        Forward propagation. If weights and biases are provided, they are used;\n",
    "        otherwise the network's parameters are used.\n",
    "        Returns lists Z (pre-activations) and A (activations).\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.W\n",
    "        if biases is None:\n",
    "            biases = self.b\n",
    "            \n",
    "        A = [X]\n",
    "        Z = []\n",
    "        # Forward through hidden layers\n",
    "        for i in range(len(weights) - 1):\n",
    "            z_curr = np.dot(A[-1], weights[i]) + biases[i]\n",
    "            Z.append(z_curr)\n",
    "            a_curr = self._apply_activation(z_curr, self.activation_function_names[i])\n",
    "            A.append(a_curr)\n",
    "        # Forward through output layer\n",
    "        z_out = np.dot(A[-1], weights[-1]) + biases[-1]\n",
    "        Z.append(z_out)\n",
    "        a_out = self._apply_activation(z_out, self.activation_function_names[-1])\n",
    "        A.append(a_out)\n",
    "        return Z, A\n",
    "    \n",
    "    def _compute_gradients(self, X, y, Z, A, weights=None):\n",
    "        \"\"\"\n",
    "        Compute gradients dW and db given inputs X, target y, pre-activations Z and activations A.\n",
    "        Optionally, a custom set of weights (used in lookahead for Nesterov momentum) can be provided.\n",
    "        Returns lists dW and db.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.W\n",
    "        m = X.shape[0]\n",
    "        # Compute derivative of loss with respect to output activation\n",
    "        dA = loss_derivatives[self.loss_function_name](y, A[-1])\n",
    "        # Output layer\n",
    "        dZ = dA * self._apply_activation_derivative(Z[-1], A[-1], self.activation_function_names[-1])\n",
    "        reg_term = compute_reg_gradient(weights[-1], self.lambda_reg, self.reg_type, m)\n",
    "        dW = [np.dot(A[-2].T, dZ) / m + reg_term]\n",
    "        db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "        \n",
    "        # Backpropagate through hidden layers\n",
    "        for i in range(len(weights) - 2, -1, -1):\n",
    "            dA = np.dot(dZ, weights[i + 1].T)\n",
    "            dZ = dA * self._apply_activation_derivative(Z[i], A[i + 1], self.activation_function_names[i])\n",
    "            reg_term = compute_reg_gradient(weights[i], self.lambda_reg, self.reg_type, m)\n",
    "            dW.insert(0, np.dot(A[i].T, dZ) / m + reg_term)\n",
    "            db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "            \n",
    "        return dW, db\n",
    "    \n",
    "    def train(self, X, y, epochs=300, batch_size=32, verbose=True):\n",
    "        loss_history = []\n",
    "        n_samples = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            # Update learning rate based on decay schedule\n",
    "            if self.lr_decay_type == \"exponential\":\n",
    "                self.learning_rate = self.initial_learning_rate * np.exp(-self.decay_rate * epoch)\n",
    "            elif self.lr_decay_type == \"linear\":\n",
    "                self.learning_rate = self.initial_learning_rate * max(0, 1 - self.decay_rate * epoch)\n",
    "            # Otherwise (\"none\"), keep the initial learning rate.\n",
    "            \n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            \n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                \n",
    "                # Choose update strategy based on momentum type\n",
    "                if self.momentum_type == \"nesterov momentum\":\n",
    "                    # Compute lookahead parameters\n",
    "                    weights_lookahead = [self.W[j] - self.momentum_alpha * self.vW[j] for j in range(len(self.W))]\n",
    "                    biases_lookahead = [self.b[j] - self.momentum_alpha * self.vb[j] for j in range(len(self.b))]\n",
    "                    Z, A = self._forward(X_batch, weights=weights_lookahead, biases=biases_lookahead)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A, weights=weights_lookahead)\n",
    "                    \n",
    "                    # Update momentum accumulators and actual parameters\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.vW[j] = self.momentum_alpha * self.vW[j] + self.learning_rate * dW[j]\n",
    "                        self.vb[j] = self.momentum_alpha * self.vb[j] + self.learning_rate * db[j]\n",
    "                        self.W[j] -= self.vW[j]\n",
    "                        self.b[j] -= self.vb[j]\n",
    "                        \n",
    "                elif self.momentum_type == \"momentum\":\n",
    "                    # Standard momentum: compute gradients with current parameters\n",
    "                    Z, A = self._forward(X_batch)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.vW[j] = self.momentum_alpha * self.vW[j] + self.learning_rate * dW[j]\n",
    "                        self.vb[j] = self.momentum_alpha * self.vb[j] + self.learning_rate * db[j]\n",
    "                        self.W[j] -= self.vW[j]\n",
    "                        self.b[j] -= self.vb[j]\n",
    "                        \n",
    "                else:  # No momentum\n",
    "                    Z, A = self._forward(X_batch)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.W[j] -= self.learning_rate * dW[j]\n",
    "                        self.b[j] -= self.learning_rate * db[j]\n",
    "                        \n",
    "            # Log loss at checkpoints\n",
    "            if epoch % max(1, int(epochs / 20)) == 0:\n",
    "                _, A_full = self._forward(X)\n",
    "                loss = loss_functions[self.loss_function_name](y, A_full[-1])\n",
    "                reg_loss = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "                total_loss = loss + reg_loss\n",
    "                loss_history.append(total_loss)\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch:4d}, Loss: {total_loss:.4f}, Learning Rate: {self.learning_rate:.6f}\")\n",
    "                    \n",
    "        return loss_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, A = self._forward(X)\n",
    "        output = A[-1]\n",
    "        if self.task == \"classification\":\n",
    "            # For binary classification, threshold at 0.5\n",
    "            if output.shape[1] == 1:\n",
    "                return (output > 0.5).astype(int)\n",
    "            else:\n",
    "                return np.argmax(output, axis=1)\n",
    "        else:  # regression\n",
    "            return output\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        if self.task == \"regression\":\n",
    "            return mse_loss(y, predictions)\n",
    "        else:\n",
    "            # If y is one-hot encoded, convert to class labels\n",
    "            if y.ndim > 1 and y.shape[1] > 1:\n",
    "                y_true = np.argmax(y, axis=1)\n",
    "            else:\n",
    "                y_true = y\n",
    "            return np.mean(predictions == y_true)\n",
    "\n",
    "# ============================\n",
    "# Testing on a monk's dataset\n",
    "# ============================\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(1, one_hot_encode=True)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_units = 10\n",
    "output_size = 1  # binary classification\n",
    "layers = [input_size, hidden_units, output_size]\n",
    "\n",
    "# Define activation functions for hidden and output layers\n",
    "activation_funcs = [\"relu\", \"sigmoid\"]\n",
    "\n",
    "nn_clf = NeuralNetwork(\n",
    "    layers=layers,\n",
    "    learning_rate=0.2,\n",
    "    lambda_reg=0.001,\n",
    "    reg_type=\"l2\",\n",
    "    loss_function_name=\"mse\",       \n",
    "    activation_function_names=activation_funcs,\n",
    "    task=\"classification\",\n",
    "    lr_decay_type=\"linear\",    # \"exponential\", \"linear\", or \"none\"\n",
    "    decay_rate=0.001,               # Adjust decay rate as needed\n",
    "    weight_init=\"base\",             # Choose between \"base\" or \"glorot\"å\n",
    ")\n",
    "\n",
    "\n",
    "loss_history = nn_clf.train(X_train, y_train, epochs=100, batch_size=32, verbose=True)\n",
    "\n",
    "accuracy = nn_clf.evaluate(X_test, y_test)\n",
    "print(f\"\\nNeural Network Classification Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "pd.Series(loss_history).plot(title=\"Training Loss History\")\n",
    "plt.xlabel(\"Checkpoint (Epochs)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-1-train.csv\n",
      "Using cached ../datasets/monks/monk-1-test.csv\n",
      "One-hot encoding MONK-1 dataset...\n",
      "Epoch    0, Training Loss: 0.2537, Validation Loss: 0.2570, Learning Rate: 0.200000\n",
      "Epoch    1, Training Loss: 0.2507, Validation Loss: 0.2551, Learning Rate: 0.199800\n",
      "Epoch    2, Training Loss: 0.2476, Validation Loss: 0.2531, Learning Rate: 0.199600\n",
      "Epoch    3, Training Loss: 0.2447, Validation Loss: 0.2513, Learning Rate: 0.199400\n",
      "Epoch    4, Training Loss: 0.2418, Validation Loss: 0.2494, Learning Rate: 0.199200\n",
      "Epoch    5, Training Loss: 0.2388, Validation Loss: 0.2474, Learning Rate: 0.199000\n",
      "Epoch    6, Training Loss: 0.2358, Validation Loss: 0.2454, Learning Rate: 0.198800\n",
      "Epoch    7, Training Loss: 0.2326, Validation Loss: 0.2431, Learning Rate: 0.198600\n",
      "Epoch    8, Training Loss: 0.2293, Validation Loss: 0.2408, Learning Rate: 0.198400\n",
      "Epoch    9, Training Loss: 0.2253, Validation Loss: 0.2381, Learning Rate: 0.198200\n",
      "Epoch   10, Training Loss: 0.2207, Validation Loss: 0.2348, Learning Rate: 0.198000\n",
      "Epoch   11, Training Loss: 0.2166, Validation Loss: 0.2319, Learning Rate: 0.197800\n",
      "Epoch   12, Training Loss: 0.2125, Validation Loss: 0.2289, Learning Rate: 0.197600\n",
      "Epoch   13, Training Loss: 0.2077, Validation Loss: 0.2254, Learning Rate: 0.197400\n",
      "Epoch   14, Training Loss: 0.2022, Validation Loss: 0.2216, Learning Rate: 0.197200\n",
      "Epoch   15, Training Loss: 0.1971, Validation Loss: 0.2183, Learning Rate: 0.197000\n",
      "Epoch   16, Training Loss: 0.1926, Validation Loss: 0.2154, Learning Rate: 0.196800\n",
      "Epoch   17, Training Loss: 0.1884, Validation Loss: 0.2125, Learning Rate: 0.196600\n",
      "Epoch   18, Training Loss: 0.1843, Validation Loss: 0.2097, Learning Rate: 0.196400\n",
      "Epoch   19, Training Loss: 0.1805, Validation Loss: 0.2068, Learning Rate: 0.196200\n",
      "Epoch   20, Training Loss: 0.1768, Validation Loss: 0.2042, Learning Rate: 0.196000\n",
      "Epoch   21, Training Loss: 0.1733, Validation Loss: 0.2016, Learning Rate: 0.195800\n",
      "Epoch   22, Training Loss: 0.1700, Validation Loss: 0.1993, Learning Rate: 0.195600\n",
      "Epoch   23, Training Loss: 0.1669, Validation Loss: 0.1971, Learning Rate: 0.195400\n",
      "Epoch   24, Training Loss: 0.1639, Validation Loss: 0.1950, Learning Rate: 0.195200\n",
      "Epoch   25, Training Loss: 0.1611, Validation Loss: 0.1930, Learning Rate: 0.195000\n",
      "Epoch   26, Training Loss: 0.1586, Validation Loss: 0.1911, Learning Rate: 0.194800\n",
      "Epoch   27, Training Loss: 0.1562, Validation Loss: 0.1895, Learning Rate: 0.194600\n",
      "Epoch   28, Training Loss: 0.1541, Validation Loss: 0.1876, Learning Rate: 0.194400\n",
      "Epoch   29, Training Loss: 0.1519, Validation Loss: 0.1861, Learning Rate: 0.194200\n",
      "Epoch   30, Training Loss: 0.1499, Validation Loss: 0.1847, Learning Rate: 0.194000\n",
      "Epoch   31, Training Loss: 0.1480, Validation Loss: 0.1831, Learning Rate: 0.193800\n",
      "Epoch   32, Training Loss: 0.1462, Validation Loss: 0.1818, Learning Rate: 0.193600\n",
      "Epoch   33, Training Loss: 0.1445, Validation Loss: 0.1802, Learning Rate: 0.193400\n",
      "Epoch   34, Training Loss: 0.1428, Validation Loss: 0.1791, Learning Rate: 0.193200\n",
      "Epoch   35, Training Loss: 0.1413, Validation Loss: 0.1775, Learning Rate: 0.193000\n",
      "Epoch   36, Training Loss: 0.1397, Validation Loss: 0.1765, Learning Rate: 0.192800\n",
      "Epoch   37, Training Loss: 0.1382, Validation Loss: 0.1757, Learning Rate: 0.192600\n",
      "Epoch   38, Training Loss: 0.1367, Validation Loss: 0.1743, Learning Rate: 0.192400\n",
      "Epoch   39, Training Loss: 0.1353, Validation Loss: 0.1733, Learning Rate: 0.192200\n",
      "Epoch   40, Training Loss: 0.1339, Validation Loss: 0.1722, Learning Rate: 0.192000\n",
      "Epoch   41, Training Loss: 0.1327, Validation Loss: 0.1716, Learning Rate: 0.191800\n",
      "Epoch   42, Training Loss: 0.1313, Validation Loss: 0.1703, Learning Rate: 0.191600\n",
      "Epoch   43, Training Loss: 0.1303, Validation Loss: 0.1688, Learning Rate: 0.191400\n",
      "Epoch   44, Training Loss: 0.1289, Validation Loss: 0.1683, Learning Rate: 0.191200\n",
      "Epoch   45, Training Loss: 0.1279, Validation Loss: 0.1682, Learning Rate: 0.191000\n",
      "Epoch   46, Training Loss: 0.1266, Validation Loss: 0.1664, Learning Rate: 0.190800\n",
      "Epoch   47, Training Loss: 0.1255, Validation Loss: 0.1660, Learning Rate: 0.190600\n",
      "Epoch   48, Training Loss: 0.1243, Validation Loss: 0.1650, Learning Rate: 0.190400\n",
      "Epoch   49, Training Loss: 0.1233, Validation Loss: 0.1648, Learning Rate: 0.190200\n",
      "Epoch   50, Training Loss: 0.1222, Validation Loss: 0.1629, Learning Rate: 0.190000\n",
      "Epoch   51, Training Loss: 0.1211, Validation Loss: 0.1622, Learning Rate: 0.189800\n",
      "Epoch   52, Training Loss: 0.1201, Validation Loss: 0.1610, Learning Rate: 0.189600\n",
      "Epoch   53, Training Loss: 0.1191, Validation Loss: 0.1605, Learning Rate: 0.189400\n",
      "Epoch   54, Training Loss: 0.1182, Validation Loss: 0.1592, Learning Rate: 0.189200\n",
      "Epoch   55, Training Loss: 0.1171, Validation Loss: 0.1593, Learning Rate: 0.189000\n",
      "Epoch   56, Training Loss: 0.1163, Validation Loss: 0.1576, Learning Rate: 0.188800\n",
      "Epoch   57, Training Loss: 0.1153, Validation Loss: 0.1570, Learning Rate: 0.188600\n",
      "Epoch   58, Training Loss: 0.1147, Validation Loss: 0.1581, Learning Rate: 0.188400\n",
      "Epoch   59, Training Loss: 0.1135, Validation Loss: 0.1560, Learning Rate: 0.188200\n",
      "Epoch   60, Training Loss: 0.1128, Validation Loss: 0.1543, Learning Rate: 0.188000\n",
      "Epoch   61, Training Loss: 0.1117, Validation Loss: 0.1543, Learning Rate: 0.187800\n",
      "Epoch   62, Training Loss: 0.1109, Validation Loss: 0.1542, Learning Rate: 0.187600\n",
      "Epoch   63, Training Loss: 0.1100, Validation Loss: 0.1531, Learning Rate: 0.187400\n",
      "Epoch   64, Training Loss: 0.1092, Validation Loss: 0.1527, Learning Rate: 0.187200\n",
      "Epoch   65, Training Loss: 0.1084, Validation Loss: 0.1523, Learning Rate: 0.187000\n",
      "Epoch   66, Training Loss: 0.1078, Validation Loss: 0.1521, Learning Rate: 0.186800\n",
      "Epoch   67, Training Loss: 0.1068, Validation Loss: 0.1505, Learning Rate: 0.186600\n",
      "Epoch   68, Training Loss: 0.1060, Validation Loss: 0.1493, Learning Rate: 0.186400\n",
      "Epoch   69, Training Loss: 0.1052, Validation Loss: 0.1487, Learning Rate: 0.186200\n",
      "Epoch   70, Training Loss: 0.1044, Validation Loss: 0.1479, Learning Rate: 0.186000\n",
      "Epoch   71, Training Loss: 0.1036, Validation Loss: 0.1475, Learning Rate: 0.185800\n",
      "Epoch   72, Training Loss: 0.1029, Validation Loss: 0.1464, Learning Rate: 0.185600\n",
      "Epoch   73, Training Loss: 0.1021, Validation Loss: 0.1463, Learning Rate: 0.185400\n",
      "Epoch   74, Training Loss: 0.1013, Validation Loss: 0.1453, Learning Rate: 0.185200\n",
      "Epoch   75, Training Loss: 0.1006, Validation Loss: 0.1450, Learning Rate: 0.185000\n",
      "Epoch   76, Training Loss: 0.1000, Validation Loss: 0.1447, Learning Rate: 0.184800\n",
      "Epoch   77, Training Loss: 0.0991, Validation Loss: 0.1434, Learning Rate: 0.184600\n",
      "Epoch   78, Training Loss: 0.0985, Validation Loss: 0.1420, Learning Rate: 0.184400\n",
      "Epoch   79, Training Loss: 0.0977, Validation Loss: 0.1419, Learning Rate: 0.184200\n",
      "Epoch   80, Training Loss: 0.0969, Validation Loss: 0.1414, Learning Rate: 0.184000\n",
      "Epoch   81, Training Loss: 0.0963, Validation Loss: 0.1408, Learning Rate: 0.183800\n",
      "Epoch   82, Training Loss: 0.0958, Validation Loss: 0.1409, Learning Rate: 0.183600\n",
      "Epoch   83, Training Loss: 0.0949, Validation Loss: 0.1397, Learning Rate: 0.183400\n",
      "Epoch   84, Training Loss: 0.0943, Validation Loss: 0.1380, Learning Rate: 0.183200\n",
      "Epoch   85, Training Loss: 0.0941, Validation Loss: 0.1367, Learning Rate: 0.183000\n",
      "Epoch   86, Training Loss: 0.0930, Validation Loss: 0.1367, Learning Rate: 0.182800\n",
      "Epoch   87, Training Loss: 0.0922, Validation Loss: 0.1370, Learning Rate: 0.182600\n",
      "Epoch   88, Training Loss: 0.0915, Validation Loss: 0.1360, Learning Rate: 0.182400\n",
      "Epoch   89, Training Loss: 0.0909, Validation Loss: 0.1348, Learning Rate: 0.182200\n",
      "Epoch   90, Training Loss: 0.0903, Validation Loss: 0.1345, Learning Rate: 0.182000\n",
      "Epoch   91, Training Loss: 0.0897, Validation Loss: 0.1341, Learning Rate: 0.181800\n",
      "Epoch   92, Training Loss: 0.0892, Validation Loss: 0.1342, Learning Rate: 0.181600\n",
      "Epoch   93, Training Loss: 0.0887, Validation Loss: 0.1318, Learning Rate: 0.181400\n",
      "Epoch   94, Training Loss: 0.0878, Validation Loss: 0.1320, Learning Rate: 0.181200\n",
      "Epoch   95, Training Loss: 0.0874, Validation Loss: 0.1327, Learning Rate: 0.181000\n",
      "Epoch   96, Training Loss: 0.0866, Validation Loss: 0.1307, Learning Rate: 0.180800\n",
      "Epoch   97, Training Loss: 0.0861, Validation Loss: 0.1308, Learning Rate: 0.180600\n",
      "Epoch   98, Training Loss: 0.0856, Validation Loss: 0.1298, Learning Rate: 0.180400\n",
      "Epoch   99, Training Loss: 0.0849, Validation Loss: 0.1292, Learning Rate: 0.180200\n",
      "Epoch  100, Training Loss: 0.0844, Validation Loss: 0.1290, Learning Rate: 0.180000\n",
      "Epoch  101, Training Loss: 0.0839, Validation Loss: 0.1274, Learning Rate: 0.179800\n",
      "Epoch  102, Training Loss: 0.0833, Validation Loss: 0.1278, Learning Rate: 0.179600\n",
      "Epoch  103, Training Loss: 0.0828, Validation Loss: 0.1278, Learning Rate: 0.179400\n",
      "Epoch  104, Training Loss: 0.0822, Validation Loss: 0.1268, Learning Rate: 0.179200\n",
      "Epoch  105, Training Loss: 0.0816, Validation Loss: 0.1255, Learning Rate: 0.179000\n",
      "Epoch  106, Training Loss: 0.0822, Validation Loss: 0.1235, Learning Rate: 0.178800\n",
      "Epoch  107, Training Loss: 0.0806, Validation Loss: 0.1245, Learning Rate: 0.178600\n",
      "Epoch  108, Training Loss: 0.0800, Validation Loss: 0.1242, Learning Rate: 0.178400\n",
      "Epoch  109, Training Loss: 0.0795, Validation Loss: 0.1237, Learning Rate: 0.178200\n",
      "Epoch  110, Training Loss: 0.0791, Validation Loss: 0.1223, Learning Rate: 0.178000\n",
      "Epoch  111, Training Loss: 0.0788, Validation Loss: 0.1214, Learning Rate: 0.177800\n",
      "Epoch  112, Training Loss: 0.0786, Validation Loss: 0.1242, Learning Rate: 0.177600\n",
      "Epoch  113, Training Loss: 0.0775, Validation Loss: 0.1215, Learning Rate: 0.177400\n",
      "Epoch  114, Training Loss: 0.0778, Validation Loss: 0.1236, Learning Rate: 0.177200\n",
      "Epoch  115, Training Loss: 0.0766, Validation Loss: 0.1212, Learning Rate: 0.177000\n",
      "Epoch  116, Training Loss: 0.0763, Validation Loss: 0.1213, Learning Rate: 0.176800\n",
      "Epoch  117, Training Loss: 0.0756, Validation Loss: 0.1199, Learning Rate: 0.176600\n",
      "Epoch  118, Training Loss: 0.0751, Validation Loss: 0.1183, Learning Rate: 0.176400\n",
      "Epoch  119, Training Loss: 0.0746, Validation Loss: 0.1180, Learning Rate: 0.176200\n",
      "Epoch  120, Training Loss: 0.0741, Validation Loss: 0.1173, Learning Rate: 0.176000\n",
      "Epoch  121, Training Loss: 0.0742, Validation Loss: 0.1161, Learning Rate: 0.175800\n",
      "Epoch  122, Training Loss: 0.0733, Validation Loss: 0.1174, Learning Rate: 0.175600\n",
      "Epoch  123, Training Loss: 0.0728, Validation Loss: 0.1169, Learning Rate: 0.175400\n",
      "Epoch  124, Training Loss: 0.0724, Validation Loss: 0.1162, Learning Rate: 0.175200\n",
      "Epoch  125, Training Loss: 0.0719, Validation Loss: 0.1151, Learning Rate: 0.175000\n",
      "Epoch  126, Training Loss: 0.0715, Validation Loss: 0.1138, Learning Rate: 0.174800\n",
      "Epoch  127, Training Loss: 0.0710, Validation Loss: 0.1142, Learning Rate: 0.174600\n",
      "Epoch  128, Training Loss: 0.0706, Validation Loss: 0.1142, Learning Rate: 0.174400\n",
      "Epoch  129, Training Loss: 0.0706, Validation Loss: 0.1150, Learning Rate: 0.174200\n",
      "Epoch  130, Training Loss: 0.0699, Validation Loss: 0.1140, Learning Rate: 0.174000\n",
      "Epoch  131, Training Loss: 0.0693, Validation Loss: 0.1118, Learning Rate: 0.173800\n",
      "Epoch  132, Training Loss: 0.0689, Validation Loss: 0.1112, Learning Rate: 0.173600\n",
      "Epoch  133, Training Loss: 0.0692, Validation Loss: 0.1098, Learning Rate: 0.173400\n",
      "Epoch  134, Training Loss: 0.0680, Validation Loss: 0.1106, Learning Rate: 0.173200\n",
      "Epoch  135, Training Loss: 0.0681, Validation Loss: 0.1123, Learning Rate: 0.173000\n",
      "Epoch  136, Training Loss: 0.0672, Validation Loss: 0.1100, Learning Rate: 0.172800\n",
      "Epoch  137, Training Loss: 0.0668, Validation Loss: 0.1092, Learning Rate: 0.172600\n",
      "Epoch  138, Training Loss: 0.0664, Validation Loss: 0.1093, Learning Rate: 0.172400\n",
      "Epoch  139, Training Loss: 0.0661, Validation Loss: 0.1091, Learning Rate: 0.172200\n",
      "Epoch  140, Training Loss: 0.0661, Validation Loss: 0.1068, Learning Rate: 0.172000\n",
      "Epoch  141, Training Loss: 0.0654, Validation Loss: 0.1066, Learning Rate: 0.171800\n",
      "Epoch  142, Training Loss: 0.0649, Validation Loss: 0.1073, Learning Rate: 0.171600\n",
      "Epoch  143, Training Loss: 0.0645, Validation Loss: 0.1058, Learning Rate: 0.171400\n",
      "Epoch  144, Training Loss: 0.0641, Validation Loss: 0.1057, Learning Rate: 0.171200\n",
      "Epoch  145, Training Loss: 0.0638, Validation Loss: 0.1056, Learning Rate: 0.171000\n",
      "Epoch  146, Training Loss: 0.0634, Validation Loss: 0.1053, Learning Rate: 0.170800\n",
      "Epoch  147, Training Loss: 0.0631, Validation Loss: 0.1042, Learning Rate: 0.170600\n",
      "Epoch  148, Training Loss: 0.0627, Validation Loss: 0.1045, Learning Rate: 0.170400\n",
      "Epoch  149, Training Loss: 0.0624, Validation Loss: 0.1029, Learning Rate: 0.170200\n",
      "Epoch  150, Training Loss: 0.0620, Validation Loss: 0.1029, Learning Rate: 0.170000\n",
      "Epoch  151, Training Loss: 0.0617, Validation Loss: 0.1033, Learning Rate: 0.169800\n",
      "Epoch  152, Training Loss: 0.0616, Validation Loss: 0.1012, Learning Rate: 0.169600\n",
      "Epoch  153, Training Loss: 0.0613, Validation Loss: 0.1008, Learning Rate: 0.169400\n",
      "Epoch  154, Training Loss: 0.0606, Validation Loss: 0.1015, Learning Rate: 0.169200\n",
      "Epoch  155, Training Loss: 0.0603, Validation Loss: 0.1006, Learning Rate: 0.169000\n",
      "Epoch  156, Training Loss: 0.0601, Validation Loss: 0.1015, Learning Rate: 0.168800\n",
      "Epoch  157, Training Loss: 0.0596, Validation Loss: 0.0998, Learning Rate: 0.168600\n",
      "Epoch  158, Training Loss: 0.0594, Validation Loss: 0.1003, Learning Rate: 0.168400\n",
      "Epoch  159, Training Loss: 0.0591, Validation Loss: 0.0983, Learning Rate: 0.168200\n",
      "Epoch  160, Training Loss: 0.0586, Validation Loss: 0.0988, Learning Rate: 0.168000\n",
      "Epoch  161, Training Loss: 0.0584, Validation Loss: 0.0992, Learning Rate: 0.167800\n",
      "Epoch  162, Training Loss: 0.0581, Validation Loss: 0.0972, Learning Rate: 0.167600\n",
      "Epoch  163, Training Loss: 0.0577, Validation Loss: 0.0979, Learning Rate: 0.167400\n",
      "Epoch  164, Training Loss: 0.0574, Validation Loss: 0.0965, Learning Rate: 0.167200\n",
      "Epoch  165, Training Loss: 0.0570, Validation Loss: 0.0966, Learning Rate: 0.167000\n",
      "Epoch  166, Training Loss: 0.0567, Validation Loss: 0.0964, Learning Rate: 0.166800\n",
      "Epoch  167, Training Loss: 0.0564, Validation Loss: 0.0960, Learning Rate: 0.166600\n",
      "Epoch  168, Training Loss: 0.0561, Validation Loss: 0.0953, Learning Rate: 0.166400\n",
      "Epoch  169, Training Loss: 0.0558, Validation Loss: 0.0946, Learning Rate: 0.166200\n",
      "Epoch  170, Training Loss: 0.0556, Validation Loss: 0.0948, Learning Rate: 0.166000\n",
      "Epoch  171, Training Loss: 0.0553, Validation Loss: 0.0934, Learning Rate: 0.165800\n",
      "Epoch  172, Training Loss: 0.0550, Validation Loss: 0.0937, Learning Rate: 0.165600\n",
      "Epoch  173, Training Loss: 0.0547, Validation Loss: 0.0932, Learning Rate: 0.165400\n",
      "Epoch  174, Training Loss: 0.0544, Validation Loss: 0.0931, Learning Rate: 0.165200\n",
      "Epoch  175, Training Loss: 0.0543, Validation Loss: 0.0933, Learning Rate: 0.165000\n",
      "Epoch  176, Training Loss: 0.0546, Validation Loss: 0.0947, Learning Rate: 0.164800\n",
      "Epoch  177, Training Loss: 0.0537, Validation Loss: 0.0909, Learning Rate: 0.164600\n",
      "Epoch  178, Training Loss: 0.0536, Validation Loss: 0.0924, Learning Rate: 0.164400\n",
      "Epoch  179, Training Loss: 0.0535, Validation Loss: 0.0925, Learning Rate: 0.164200\n",
      "Epoch  180, Training Loss: 0.0531, Validation Loss: 0.0921, Learning Rate: 0.164000\n",
      "Epoch  181, Training Loss: 0.0526, Validation Loss: 0.0897, Learning Rate: 0.163800\n",
      "Epoch  182, Training Loss: 0.0524, Validation Loss: 0.0906, Learning Rate: 0.163600\n",
      "Epoch  183, Training Loss: 0.0523, Validation Loss: 0.0884, Learning Rate: 0.163400\n",
      "Epoch  184, Training Loss: 0.0518, Validation Loss: 0.0890, Learning Rate: 0.163200\n",
      "Epoch  185, Training Loss: 0.0516, Validation Loss: 0.0880, Learning Rate: 0.163000\n",
      "Epoch  186, Training Loss: 0.0513, Validation Loss: 0.0879, Learning Rate: 0.162800\n",
      "Epoch  187, Training Loss: 0.0513, Validation Loss: 0.0893, Learning Rate: 0.162600\n",
      "Epoch  188, Training Loss: 0.0508, Validation Loss: 0.0876, Learning Rate: 0.162400\n",
      "Epoch  189, Training Loss: 0.0508, Validation Loss: 0.0887, Learning Rate: 0.162200\n",
      "Epoch  190, Training Loss: 0.0503, Validation Loss: 0.0870, Learning Rate: 0.162000\n",
      "Epoch  191, Training Loss: 0.0501, Validation Loss: 0.0865, Learning Rate: 0.161800\n",
      "Epoch  192, Training Loss: 0.0499, Validation Loss: 0.0858, Learning Rate: 0.161600\n",
      "Epoch  193, Training Loss: 0.0497, Validation Loss: 0.0859, Learning Rate: 0.161400\n",
      "Epoch  194, Training Loss: 0.0495, Validation Loss: 0.0852, Learning Rate: 0.161200\n",
      "Epoch  195, Training Loss: 0.0496, Validation Loss: 0.0842, Learning Rate: 0.161000\n",
      "Epoch  196, Training Loss: 0.0491, Validation Loss: 0.0856, Learning Rate: 0.160800\n",
      "Epoch  197, Training Loss: 0.0489, Validation Loss: 0.0853, Learning Rate: 0.160600\n",
      "Epoch  198, Training Loss: 0.0488, Validation Loss: 0.0833, Learning Rate: 0.160400\n",
      "Epoch  199, Training Loss: 0.0485, Validation Loss: 0.0848, Learning Rate: 0.160200\n",
      "Epoch  200, Training Loss: 0.0484, Validation Loss: 0.0825, Learning Rate: 0.160000\n",
      "Epoch  201, Training Loss: 0.0480, Validation Loss: 0.0834, Learning Rate: 0.159800\n",
      "Epoch  202, Training Loss: 0.0480, Validation Loss: 0.0824, Learning Rate: 0.159600\n",
      "Epoch  203, Training Loss: 0.0477, Validation Loss: 0.0833, Learning Rate: 0.159400\n",
      "Epoch  204, Training Loss: 0.0474, Validation Loss: 0.0824, Learning Rate: 0.159200\n",
      "Epoch  205, Training Loss: 0.0473, Validation Loss: 0.0811, Learning Rate: 0.159000\n",
      "Epoch  206, Training Loss: 0.0472, Validation Loss: 0.0825, Learning Rate: 0.158800\n",
      "Epoch  207, Training Loss: 0.0469, Validation Loss: 0.0819, Learning Rate: 0.158600\n",
      "Epoch  208, Training Loss: 0.0468, Validation Loss: 0.0804, Learning Rate: 0.158400\n",
      "Epoch  209, Training Loss: 0.0465, Validation Loss: 0.0812, Learning Rate: 0.158200\n",
      "Epoch  210, Training Loss: 0.0464, Validation Loss: 0.0812, Learning Rate: 0.158000\n",
      "Epoch  211, Training Loss: 0.0461, Validation Loss: 0.0800, Learning Rate: 0.157800\n",
      "Epoch  212, Training Loss: 0.0460, Validation Loss: 0.0791, Learning Rate: 0.157600\n",
      "Epoch  213, Training Loss: 0.0458, Validation Loss: 0.0799, Learning Rate: 0.157400\n",
      "Epoch  214, Training Loss: 0.0457, Validation Loss: 0.0785, Learning Rate: 0.157200\n",
      "Epoch  215, Training Loss: 0.0455, Validation Loss: 0.0794, Learning Rate: 0.157000\n",
      "Epoch  216, Training Loss: 0.0454, Validation Loss: 0.0795, Learning Rate: 0.156800\n",
      "Epoch  217, Training Loss: 0.0451, Validation Loss: 0.0784, Learning Rate: 0.156600\n",
      "Epoch  218, Training Loss: 0.0450, Validation Loss: 0.0789, Learning Rate: 0.156400\n",
      "Epoch  219, Training Loss: 0.0449, Validation Loss: 0.0787, Learning Rate: 0.156200\n",
      "Epoch  220, Training Loss: 0.0447, Validation Loss: 0.0777, Learning Rate: 0.156000\n",
      "Epoch  221, Training Loss: 0.0445, Validation Loss: 0.0773, Learning Rate: 0.155800\n",
      "Epoch  222, Training Loss: 0.0444, Validation Loss: 0.0772, Learning Rate: 0.155600\n",
      "Epoch  223, Training Loss: 0.0442, Validation Loss: 0.0767, Learning Rate: 0.155400\n",
      "Epoch  224, Training Loss: 0.0440, Validation Loss: 0.0761, Learning Rate: 0.155200\n",
      "Epoch  225, Training Loss: 0.0439, Validation Loss: 0.0763, Learning Rate: 0.155000\n",
      "Epoch  226, Training Loss: 0.0438, Validation Loss: 0.0755, Learning Rate: 0.154800\n",
      "Epoch  227, Training Loss: 0.0440, Validation Loss: 0.0747, Learning Rate: 0.154600\n",
      "Epoch  228, Training Loss: 0.0436, Validation Loss: 0.0763, Learning Rate: 0.154400\n",
      "Epoch  229, Training Loss: 0.0434, Validation Loss: 0.0759, Learning Rate: 0.154200\n",
      "Epoch  230, Training Loss: 0.0432, Validation Loss: 0.0747, Learning Rate: 0.154000\n",
      "Epoch  231, Training Loss: 0.0431, Validation Loss: 0.0746, Learning Rate: 0.153800\n",
      "Epoch  232, Training Loss: 0.0430, Validation Loss: 0.0739, Learning Rate: 0.153600\n",
      "Epoch  233, Training Loss: 0.0430, Validation Loss: 0.0734, Learning Rate: 0.153400\n",
      "Epoch  234, Training Loss: 0.0427, Validation Loss: 0.0737, Learning Rate: 0.153200\n",
      "Epoch  235, Training Loss: 0.0426, Validation Loss: 0.0733, Learning Rate: 0.153000\n",
      "Epoch  236, Training Loss: 0.0427, Validation Loss: 0.0748, Learning Rate: 0.152800\n",
      "Epoch  237, Training Loss: 0.0424, Validation Loss: 0.0737, Learning Rate: 0.152600\n",
      "Epoch  238, Training Loss: 0.0422, Validation Loss: 0.0727, Learning Rate: 0.152400\n",
      "Epoch  239, Training Loss: 0.0421, Validation Loss: 0.0723, Learning Rate: 0.152200\n",
      "Epoch  240, Training Loss: 0.0420, Validation Loss: 0.0731, Learning Rate: 0.152000\n",
      "Epoch  241, Training Loss: 0.0419, Validation Loss: 0.0727, Learning Rate: 0.151800\n",
      "Epoch  242, Training Loss: 0.0418, Validation Loss: 0.0718, Learning Rate: 0.151600\n",
      "Epoch  243, Training Loss: 0.0417, Validation Loss: 0.0723, Learning Rate: 0.151400\n",
      "Epoch  244, Training Loss: 0.0415, Validation Loss: 0.0716, Learning Rate: 0.151200\n",
      "Epoch  245, Training Loss: 0.0417, Validation Loss: 0.0704, Learning Rate: 0.151000\n",
      "Epoch  246, Training Loss: 0.0417, Validation Loss: 0.0700, Learning Rate: 0.150800\n",
      "Epoch  247, Training Loss: 0.0416, Validation Loss: 0.0697, Learning Rate: 0.150600\n",
      "Epoch  248, Training Loss: 0.0411, Validation Loss: 0.0701, Learning Rate: 0.150400\n",
      "Epoch  249, Training Loss: 0.0410, Validation Loss: 0.0708, Learning Rate: 0.150200\n",
      "Epoch  250, Training Loss: 0.0409, Validation Loss: 0.0706, Learning Rate: 0.150000\n",
      "Epoch  251, Training Loss: 0.0410, Validation Loss: 0.0715, Learning Rate: 0.149800\n",
      "Epoch  252, Training Loss: 0.0407, Validation Loss: 0.0704, Learning Rate: 0.149600\n",
      "Epoch  253, Training Loss: 0.0406, Validation Loss: 0.0697, Learning Rate: 0.149400\n",
      "Epoch  254, Training Loss: 0.0405, Validation Loss: 0.0690, Learning Rate: 0.149200\n",
      "Epoch  255, Training Loss: 0.0404, Validation Loss: 0.0689, Learning Rate: 0.149000\n",
      "Epoch  256, Training Loss: 0.0403, Validation Loss: 0.0692, Learning Rate: 0.148800\n",
      "Epoch  257, Training Loss: 0.0403, Validation Loss: 0.0682, Learning Rate: 0.148600\n",
      "Epoch  258, Training Loss: 0.0402, Validation Loss: 0.0682, Learning Rate: 0.148400\n",
      "Epoch  259, Training Loss: 0.0401, Validation Loss: 0.0694, Learning Rate: 0.148200\n",
      "Epoch  260, Training Loss: 0.0401, Validation Loss: 0.0690, Learning Rate: 0.148000\n",
      "Epoch  261, Training Loss: 0.0398, Validation Loss: 0.0679, Learning Rate: 0.147800\n",
      "Epoch  262, Training Loss: 0.0398, Validation Loss: 0.0675, Learning Rate: 0.147600\n",
      "Epoch  263, Training Loss: 0.0397, Validation Loss: 0.0683, Learning Rate: 0.147400\n",
      "Epoch  264, Training Loss: 0.0396, Validation Loss: 0.0673, Learning Rate: 0.147200\n",
      "Epoch  265, Training Loss: 0.0395, Validation Loss: 0.0676, Learning Rate: 0.147000\n",
      "Epoch  266, Training Loss: 0.0395, Validation Loss: 0.0677, Learning Rate: 0.146800\n",
      "Epoch  267, Training Loss: 0.0393, Validation Loss: 0.0669, Learning Rate: 0.146600\n",
      "Epoch  268, Training Loss: 0.0393, Validation Loss: 0.0670, Learning Rate: 0.146400\n",
      "Epoch  269, Training Loss: 0.0392, Validation Loss: 0.0665, Learning Rate: 0.146200\n",
      "Epoch  270, Training Loss: 0.0391, Validation Loss: 0.0668, Learning Rate: 0.146000\n",
      "Epoch  271, Training Loss: 0.0390, Validation Loss: 0.0659, Learning Rate: 0.145800\n",
      "Epoch  272, Training Loss: 0.0390, Validation Loss: 0.0658, Learning Rate: 0.145600\n",
      "Epoch  273, Training Loss: 0.0389, Validation Loss: 0.0663, Learning Rate: 0.145400\n",
      "Epoch  274, Training Loss: 0.0388, Validation Loss: 0.0657, Learning Rate: 0.145200\n",
      "Epoch  275, Training Loss: 0.0387, Validation Loss: 0.0653, Learning Rate: 0.145000\n",
      "Epoch  276, Training Loss: 0.0387, Validation Loss: 0.0651, Learning Rate: 0.144800\n",
      "Epoch  277, Training Loss: 0.0386, Validation Loss: 0.0657, Learning Rate: 0.144600\n",
      "Epoch  278, Training Loss: 0.0386, Validation Loss: 0.0656, Learning Rate: 0.144400\n",
      "Epoch  279, Training Loss: 0.0385, Validation Loss: 0.0655, Learning Rate: 0.144200\n",
      "Epoch  280, Training Loss: 0.0384, Validation Loss: 0.0645, Learning Rate: 0.144000\n",
      "Epoch  281, Training Loss: 0.0383, Validation Loss: 0.0646, Learning Rate: 0.143800\n",
      "Epoch  282, Training Loss: 0.0383, Validation Loss: 0.0652, Learning Rate: 0.143600\n",
      "Epoch  283, Training Loss: 0.0382, Validation Loss: 0.0645, Learning Rate: 0.143400\n",
      "Epoch  284, Training Loss: 0.0382, Validation Loss: 0.0637, Learning Rate: 0.143200\n",
      "Epoch  285, Training Loss: 0.0381, Validation Loss: 0.0635, Learning Rate: 0.143000\n",
      "Epoch  286, Training Loss: 0.0381, Validation Loss: 0.0645, Learning Rate: 0.142800\n",
      "Epoch  287, Training Loss: 0.0380, Validation Loss: 0.0643, Learning Rate: 0.142600\n",
      "Epoch  288, Training Loss: 0.0379, Validation Loss: 0.0635, Learning Rate: 0.142400\n",
      "Epoch  289, Training Loss: 0.0378, Validation Loss: 0.0633, Learning Rate: 0.142200\n",
      "Epoch  290, Training Loss: 0.0378, Validation Loss: 0.0627, Learning Rate: 0.142000\n",
      "Epoch  291, Training Loss: 0.0377, Validation Loss: 0.0628, Learning Rate: 0.141800\n",
      "Epoch  292, Training Loss: 0.0376, Validation Loss: 0.0629, Learning Rate: 0.141600\n",
      "Epoch  293, Training Loss: 0.0376, Validation Loss: 0.0630, Learning Rate: 0.141400\n",
      "Epoch  294, Training Loss: 0.0376, Validation Loss: 0.0632, Learning Rate: 0.141200\n",
      "Epoch  295, Training Loss: 0.0375, Validation Loss: 0.0622, Learning Rate: 0.141000\n",
      "Epoch  296, Training Loss: 0.0374, Validation Loss: 0.0626, Learning Rate: 0.140800\n",
      "Epoch  297, Training Loss: 0.0373, Validation Loss: 0.0621, Learning Rate: 0.140600\n",
      "Epoch  298, Training Loss: 0.0373, Validation Loss: 0.0618, Learning Rate: 0.140400\n",
      "Epoch  299, Training Loss: 0.0373, Validation Loss: 0.0612, Learning Rate: 0.140200\n",
      "Epoch  300, Training Loss: 0.0372, Validation Loss: 0.0616, Learning Rate: 0.140000\n",
      "Epoch  301, Training Loss: 0.0371, Validation Loss: 0.0617, Learning Rate: 0.139800\n",
      "Epoch  302, Training Loss: 0.0371, Validation Loss: 0.0616, Learning Rate: 0.139600\n",
      "Epoch  303, Training Loss: 0.0371, Validation Loss: 0.0620, Learning Rate: 0.139400\n",
      "Epoch  304, Training Loss: 0.0370, Validation Loss: 0.0611, Learning Rate: 0.139200\n",
      "Epoch  305, Training Loss: 0.0371, Validation Loss: 0.0621, Learning Rate: 0.139000\n",
      "Epoch  306, Training Loss: 0.0369, Validation Loss: 0.0607, Learning Rate: 0.138800\n",
      "Epoch  307, Training Loss: 0.0369, Validation Loss: 0.0607, Learning Rate: 0.138600\n",
      "Epoch  308, Training Loss: 0.0369, Validation Loss: 0.0614, Learning Rate: 0.138400\n",
      "Epoch  309, Training Loss: 0.0368, Validation Loss: 0.0601, Learning Rate: 0.138200\n",
      "Epoch  310, Training Loss: 0.0367, Validation Loss: 0.0602, Learning Rate: 0.138000\n",
      "Epoch  311, Training Loss: 0.0367, Validation Loss: 0.0600, Learning Rate: 0.137800\n",
      "Epoch  312, Training Loss: 0.0367, Validation Loss: 0.0599, Learning Rate: 0.137600\n",
      "Epoch  313, Training Loss: 0.0366, Validation Loss: 0.0603, Learning Rate: 0.137400\n",
      "Epoch  314, Training Loss: 0.0366, Validation Loss: 0.0602, Learning Rate: 0.137200\n",
      "Epoch  315, Training Loss: 0.0365, Validation Loss: 0.0601, Learning Rate: 0.137000\n",
      "Epoch  316, Training Loss: 0.0365, Validation Loss: 0.0595, Learning Rate: 0.136800\n",
      "Epoch  317, Training Loss: 0.0364, Validation Loss: 0.0594, Learning Rate: 0.136600\n",
      "Epoch  318, Training Loss: 0.0364, Validation Loss: 0.0594, Learning Rate: 0.136400\n",
      "Epoch  319, Training Loss: 0.0363, Validation Loss: 0.0593, Learning Rate: 0.136200\n",
      "Epoch  320, Training Loss: 0.0363, Validation Loss: 0.0595, Learning Rate: 0.136000\n",
      "Epoch  321, Training Loss: 0.0364, Validation Loss: 0.0599, Learning Rate: 0.135800\n",
      "Epoch  322, Training Loss: 0.0362, Validation Loss: 0.0591, Learning Rate: 0.135600\n",
      "Epoch  323, Training Loss: 0.0362, Validation Loss: 0.0588, Learning Rate: 0.135400\n",
      "Epoch  324, Training Loss: 0.0362, Validation Loss: 0.0583, Learning Rate: 0.135200\n",
      "Epoch  325, Training Loss: 0.0361, Validation Loss: 0.0587, Learning Rate: 0.135000\n",
      "Epoch  326, Training Loss: 0.0361, Validation Loss: 0.0588, Learning Rate: 0.134800\n",
      "Epoch  327, Training Loss: 0.0360, Validation Loss: 0.0586, Learning Rate: 0.134600\n",
      "Epoch  328, Training Loss: 0.0360, Validation Loss: 0.0580, Learning Rate: 0.134400\n",
      "Epoch  329, Training Loss: 0.0360, Validation Loss: 0.0586, Learning Rate: 0.134200\n",
      "Epoch  330, Training Loss: 0.0360, Validation Loss: 0.0576, Learning Rate: 0.134000\n",
      "Epoch  331, Training Loss: 0.0359, Validation Loss: 0.0577, Learning Rate: 0.133800\n",
      "Epoch  332, Training Loss: 0.0359, Validation Loss: 0.0579, Learning Rate: 0.133600\n",
      "Epoch  333, Training Loss: 0.0359, Validation Loss: 0.0575, Learning Rate: 0.133400\n",
      "Epoch  334, Training Loss: 0.0358, Validation Loss: 0.0578, Learning Rate: 0.133200\n",
      "Epoch  335, Training Loss: 0.0358, Validation Loss: 0.0578, Learning Rate: 0.133000\n",
      "Epoch  336, Training Loss: 0.0358, Validation Loss: 0.0580, Learning Rate: 0.132800\n",
      "Epoch  337, Training Loss: 0.0357, Validation Loss: 0.0577, Learning Rate: 0.132600\n",
      "Epoch  338, Training Loss: 0.0357, Validation Loss: 0.0576, Learning Rate: 0.132400\n",
      "Epoch  339, Training Loss: 0.0357, Validation Loss: 0.0574, Learning Rate: 0.132200\n",
      "Epoch  340, Training Loss: 0.0356, Validation Loss: 0.0573, Learning Rate: 0.132000\n",
      "Epoch  341, Training Loss: 0.0356, Validation Loss: 0.0574, Learning Rate: 0.131800\n",
      "Epoch  342, Training Loss: 0.0356, Validation Loss: 0.0567, Learning Rate: 0.131600\n",
      "Epoch  343, Training Loss: 0.0356, Validation Loss: 0.0567, Learning Rate: 0.131400\n",
      "Epoch  344, Training Loss: 0.0355, Validation Loss: 0.0568, Learning Rate: 0.131200\n",
      "Epoch  345, Training Loss: 0.0355, Validation Loss: 0.0564, Learning Rate: 0.131000\n",
      "Epoch  346, Training Loss: 0.0355, Validation Loss: 0.0567, Learning Rate: 0.130800\n",
      "Epoch  347, Training Loss: 0.0354, Validation Loss: 0.0565, Learning Rate: 0.130600\n",
      "Epoch  348, Training Loss: 0.0354, Validation Loss: 0.0566, Learning Rate: 0.130400\n",
      "Epoch  349, Training Loss: 0.0354, Validation Loss: 0.0566, Learning Rate: 0.130200\n",
      "Epoch  350, Training Loss: 0.0354, Validation Loss: 0.0562, Learning Rate: 0.130000\n",
      "Epoch  351, Training Loss: 0.0354, Validation Loss: 0.0560, Learning Rate: 0.129800\n",
      "Epoch  352, Training Loss: 0.0353, Validation Loss: 0.0560, Learning Rate: 0.129600\n",
      "Epoch  353, Training Loss: 0.0353, Validation Loss: 0.0563, Learning Rate: 0.129400\n",
      "Epoch  354, Training Loss: 0.0353, Validation Loss: 0.0560, Learning Rate: 0.129200\n",
      "Epoch  355, Training Loss: 0.0352, Validation Loss: 0.0560, Learning Rate: 0.129000\n",
      "Epoch  356, Training Loss: 0.0352, Validation Loss: 0.0560, Learning Rate: 0.128800\n",
      "Epoch  357, Training Loss: 0.0352, Validation Loss: 0.0561, Learning Rate: 0.128600\n",
      "Epoch  358, Training Loss: 0.0352, Validation Loss: 0.0558, Learning Rate: 0.128400\n",
      "Epoch  359, Training Loss: 0.0352, Validation Loss: 0.0556, Learning Rate: 0.128200\n",
      "Epoch  360, Training Loss: 0.0351, Validation Loss: 0.0556, Learning Rate: 0.128000\n",
      "Epoch  361, Training Loss: 0.0351, Validation Loss: 0.0555, Learning Rate: 0.127800\n",
      "Epoch  362, Training Loss: 0.0351, Validation Loss: 0.0556, Learning Rate: 0.127600\n",
      "Epoch  363, Training Loss: 0.0351, Validation Loss: 0.0555, Learning Rate: 0.127400\n",
      "Epoch  364, Training Loss: 0.0351, Validation Loss: 0.0555, Learning Rate: 0.127200\n",
      "Epoch  365, Training Loss: 0.0350, Validation Loss: 0.0550, Learning Rate: 0.127000\n",
      "Epoch  366, Training Loss: 0.0350, Validation Loss: 0.0552, Learning Rate: 0.126800\n",
      "Epoch  367, Training Loss: 0.0350, Validation Loss: 0.0550, Learning Rate: 0.126600\n",
      "Epoch  368, Training Loss: 0.0350, Validation Loss: 0.0546, Learning Rate: 0.126400\n",
      "Epoch  369, Training Loss: 0.0350, Validation Loss: 0.0545, Learning Rate: 0.126200\n",
      "Epoch  370, Training Loss: 0.0349, Validation Loss: 0.0545, Learning Rate: 0.126000\n",
      "Epoch  371, Training Loss: 0.0350, Validation Loss: 0.0542, Learning Rate: 0.125800\n",
      "Epoch  372, Training Loss: 0.0349, Validation Loss: 0.0544, Learning Rate: 0.125600\n",
      "Epoch  373, Training Loss: 0.0349, Validation Loss: 0.0543, Learning Rate: 0.125400\n",
      "Epoch  374, Training Loss: 0.0349, Validation Loss: 0.0541, Learning Rate: 0.125200\n",
      "Epoch  375, Training Loss: 0.0348, Validation Loss: 0.0541, Learning Rate: 0.125000\n",
      "Epoch  376, Training Loss: 0.0348, Validation Loss: 0.0543, Learning Rate: 0.124800\n",
      "Epoch  377, Training Loss: 0.0348, Validation Loss: 0.0538, Learning Rate: 0.124600\n",
      "Epoch  378, Training Loss: 0.0348, Validation Loss: 0.0539, Learning Rate: 0.124400\n",
      "Epoch  379, Training Loss: 0.0348, Validation Loss: 0.0540, Learning Rate: 0.124200\n",
      "Epoch  380, Training Loss: 0.0348, Validation Loss: 0.0542, Learning Rate: 0.124000\n",
      "Epoch  381, Training Loss: 0.0347, Validation Loss: 0.0539, Learning Rate: 0.123800\n",
      "Epoch  382, Training Loss: 0.0347, Validation Loss: 0.0540, Learning Rate: 0.123600\n",
      "Epoch  383, Training Loss: 0.0347, Validation Loss: 0.0539, Learning Rate: 0.123400\n",
      "Epoch  384, Training Loss: 0.0347, Validation Loss: 0.0535, Learning Rate: 0.123200\n",
      "Epoch  385, Training Loss: 0.0347, Validation Loss: 0.0535, Learning Rate: 0.123000\n",
      "Epoch  386, Training Loss: 0.0347, Validation Loss: 0.0536, Learning Rate: 0.122800\n",
      "Epoch  387, Training Loss: 0.0347, Validation Loss: 0.0535, Learning Rate: 0.122600\n",
      "Epoch  388, Training Loss: 0.0346, Validation Loss: 0.0534, Learning Rate: 0.122400\n",
      "Epoch  389, Training Loss: 0.0346, Validation Loss: 0.0534, Learning Rate: 0.122200\n",
      "Epoch  390, Training Loss: 0.0346, Validation Loss: 0.0530, Learning Rate: 0.122000\n",
      "Epoch  391, Training Loss: 0.0346, Validation Loss: 0.0530, Learning Rate: 0.121800\n",
      "Epoch  392, Training Loss: 0.0346, Validation Loss: 0.0530, Learning Rate: 0.121600\n",
      "Epoch  393, Training Loss: 0.0346, Validation Loss: 0.0530, Learning Rate: 0.121400\n",
      "Epoch  394, Training Loss: 0.0346, Validation Loss: 0.0531, Learning Rate: 0.121200\n",
      "Epoch  395, Training Loss: 0.0346, Validation Loss: 0.0527, Learning Rate: 0.121000\n",
      "Epoch  396, Training Loss: 0.0345, Validation Loss: 0.0530, Learning Rate: 0.120800\n",
      "Epoch  397, Training Loss: 0.0345, Validation Loss: 0.0528, Learning Rate: 0.120600\n",
      "Epoch  398, Training Loss: 0.0345, Validation Loss: 0.0529, Learning Rate: 0.120400\n",
      "Epoch  399, Training Loss: 0.0345, Validation Loss: 0.0527, Learning Rate: 0.120200\n",
      "Epoch  400, Training Loss: 0.0345, Validation Loss: 0.0529, Learning Rate: 0.120000\n",
      "Epoch  401, Training Loss: 0.0345, Validation Loss: 0.0527, Learning Rate: 0.119800\n",
      "Epoch  402, Training Loss: 0.0345, Validation Loss: 0.0525, Learning Rate: 0.119600\n",
      "Epoch  403, Training Loss: 0.0345, Validation Loss: 0.0525, Learning Rate: 0.119400\n",
      "Epoch  404, Training Loss: 0.0345, Validation Loss: 0.0522, Learning Rate: 0.119200\n",
      "Epoch  405, Training Loss: 0.0345, Validation Loss: 0.0521, Learning Rate: 0.119000\n",
      "Epoch  406, Training Loss: 0.0344, Validation Loss: 0.0522, Learning Rate: 0.118800\n",
      "Epoch  407, Training Loss: 0.0344, Validation Loss: 0.0523, Learning Rate: 0.118600\n",
      "Epoch  408, Training Loss: 0.0344, Validation Loss: 0.0525, Learning Rate: 0.118400\n",
      "Epoch  409, Training Loss: 0.0344, Validation Loss: 0.0522, Learning Rate: 0.118200\n",
      "Epoch  410, Training Loss: 0.0344, Validation Loss: 0.0524, Learning Rate: 0.118000\n",
      "Epoch  411, Training Loss: 0.0344, Validation Loss: 0.0522, Learning Rate: 0.117800\n",
      "Epoch  412, Training Loss: 0.0344, Validation Loss: 0.0522, Learning Rate: 0.117600\n",
      "Epoch  413, Training Loss: 0.0343, Validation Loss: 0.0519, Learning Rate: 0.117400\n",
      "Epoch  414, Training Loss: 0.0343, Validation Loss: 0.0519, Learning Rate: 0.117200\n",
      "Epoch  415, Training Loss: 0.0343, Validation Loss: 0.0519, Learning Rate: 0.117000\n",
      "Epoch  416, Training Loss: 0.0343, Validation Loss: 0.0516, Learning Rate: 0.116800\n",
      "Epoch  417, Training Loss: 0.0343, Validation Loss: 0.0517, Learning Rate: 0.116600\n",
      "Epoch  418, Training Loss: 0.0343, Validation Loss: 0.0516, Learning Rate: 0.116400\n",
      "Epoch  419, Training Loss: 0.0343, Validation Loss: 0.0518, Learning Rate: 0.116200\n",
      "Epoch  420, Training Loss: 0.0343, Validation Loss: 0.0515, Learning Rate: 0.116000\n",
      "Epoch  421, Training Loss: 0.0343, Validation Loss: 0.0516, Learning Rate: 0.115800\n",
      "Epoch  422, Training Loss: 0.0343, Validation Loss: 0.0516, Learning Rate: 0.115600\n",
      "Epoch  423, Training Loss: 0.0343, Validation Loss: 0.0517, Learning Rate: 0.115400\n",
      "Epoch  424, Training Loss: 0.0343, Validation Loss: 0.0515, Learning Rate: 0.115200\n",
      "Epoch  425, Training Loss: 0.0342, Validation Loss: 0.0513, Learning Rate: 0.115000\n",
      "Epoch  426, Training Loss: 0.0342, Validation Loss: 0.0512, Learning Rate: 0.114800\n",
      "Epoch  427, Training Loss: 0.0342, Validation Loss: 0.0514, Learning Rate: 0.114600\n",
      "Epoch  428, Training Loss: 0.0342, Validation Loss: 0.0511, Learning Rate: 0.114400\n",
      "Epoch  429, Training Loss: 0.0342, Validation Loss: 0.0511, Learning Rate: 0.114200\n",
      "Epoch  430, Training Loss: 0.0342, Validation Loss: 0.0510, Learning Rate: 0.114000\n",
      "Epoch  431, Training Loss: 0.0342, Validation Loss: 0.0511, Learning Rate: 0.113800\n",
      "Epoch  432, Training Loss: 0.0342, Validation Loss: 0.0512, Learning Rate: 0.113600\n",
      "Epoch  433, Training Loss: 0.0342, Validation Loss: 0.0511, Learning Rate: 0.113400\n",
      "Epoch  434, Training Loss: 0.0342, Validation Loss: 0.0511, Learning Rate: 0.113200\n",
      "Epoch  435, Training Loss: 0.0342, Validation Loss: 0.0508, Learning Rate: 0.113000\n",
      "Epoch  436, Training Loss: 0.0342, Validation Loss: 0.0508, Learning Rate: 0.112800\n",
      "Epoch  437, Training Loss: 0.0342, Validation Loss: 0.0509, Learning Rate: 0.112600\n",
      "Epoch  438, Training Loss: 0.0342, Validation Loss: 0.0506, Learning Rate: 0.112400\n",
      "Epoch  439, Training Loss: 0.0341, Validation Loss: 0.0508, Learning Rate: 0.112200\n",
      "Epoch  440, Training Loss: 0.0341, Validation Loss: 0.0508, Learning Rate: 0.112000\n",
      "Epoch  441, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.111800\n",
      "Epoch  442, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.111600\n",
      "Epoch  443, Training Loss: 0.0341, Validation Loss: 0.0507, Learning Rate: 0.111400\n",
      "Epoch  444, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.111200\n",
      "Epoch  445, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.111000\n",
      "Epoch  446, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.110800\n",
      "Epoch  447, Training Loss: 0.0341, Validation Loss: 0.0505, Learning Rate: 0.110600\n",
      "Epoch  448, Training Loss: 0.0341, Validation Loss: 0.0504, Learning Rate: 0.110400\n",
      "Epoch  449, Training Loss: 0.0341, Validation Loss: 0.0505, Learning Rate: 0.110200\n",
      "Epoch  450, Training Loss: 0.0341, Validation Loss: 0.0503, Learning Rate: 0.110000\n",
      "Epoch  451, Training Loss: 0.0341, Validation Loss: 0.0502, Learning Rate: 0.109800\n",
      "Epoch  452, Training Loss: 0.0341, Validation Loss: 0.0502, Learning Rate: 0.109600\n",
      "Epoch  453, Training Loss: 0.0341, Validation Loss: 0.0503, Learning Rate: 0.109400\n",
      "Epoch  454, Training Loss: 0.0341, Validation Loss: 0.0502, Learning Rate: 0.109200\n",
      "Epoch  455, Training Loss: 0.0341, Validation Loss: 0.0503, Learning Rate: 0.109000\n",
      "Epoch  456, Training Loss: 0.0341, Validation Loss: 0.0500, Learning Rate: 0.108800\n",
      "Epoch  457, Training Loss: 0.0341, Validation Loss: 0.0501, Learning Rate: 0.108600\n",
      "Epoch  458, Training Loss: 0.0341, Validation Loss: 0.0501, Learning Rate: 0.108400\n",
      "Epoch  459, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.108200\n",
      "Epoch  460, Training Loss: 0.0340, Validation Loss: 0.0500, Learning Rate: 0.108000\n",
      "Epoch  461, Training Loss: 0.0340, Validation Loss: 0.0500, Learning Rate: 0.107800\n",
      "Epoch  462, Training Loss: 0.0340, Validation Loss: 0.0498, Learning Rate: 0.107600\n",
      "Epoch  463, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.107400\n",
      "Epoch  464, Training Loss: 0.0340, Validation Loss: 0.0498, Learning Rate: 0.107200\n",
      "Epoch  465, Training Loss: 0.0340, Validation Loss: 0.0496, Learning Rate: 0.107000\n",
      "Epoch  466, Training Loss: 0.0340, Validation Loss: 0.0496, Learning Rate: 0.106800\n",
      "Epoch  467, Training Loss: 0.0340, Validation Loss: 0.0497, Learning Rate: 0.106600\n",
      "Epoch  468, Training Loss: 0.0340, Validation Loss: 0.0497, Learning Rate: 0.106400\n",
      "Epoch  469, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.106200\n",
      "Epoch  470, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.106000\n",
      "Epoch  471, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.105800\n",
      "Epoch  472, Training Loss: 0.0340, Validation Loss: 0.0496, Learning Rate: 0.105600\n",
      "Epoch  473, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.105400\n",
      "Epoch  474, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.105200\n",
      "Epoch  475, Training Loss: 0.0340, Validation Loss: 0.0493, Learning Rate: 0.105000\n",
      "Epoch  476, Training Loss: 0.0340, Validation Loss: 0.0492, Learning Rate: 0.104800\n",
      "Epoch  477, Training Loss: 0.0340, Validation Loss: 0.0493, Learning Rate: 0.104600\n",
      "Epoch  478, Training Loss: 0.0340, Validation Loss: 0.0494, Learning Rate: 0.104400\n",
      "Epoch  479, Training Loss: 0.0340, Validation Loss: 0.0494, Learning Rate: 0.104200\n",
      "Epoch  480, Training Loss: 0.0340, Validation Loss: 0.0494, Learning Rate: 0.104000\n",
      "Epoch  481, Training Loss: 0.0340, Validation Loss: 0.0492, Learning Rate: 0.103800\n",
      "Epoch  482, Training Loss: 0.0340, Validation Loss: 0.0493, Learning Rate: 0.103600\n",
      "Epoch  483, Training Loss: 0.0340, Validation Loss: 0.0492, Learning Rate: 0.103400\n",
      "Epoch  484, Training Loss: 0.0340, Validation Loss: 0.0491, Learning Rate: 0.103200\n",
      "Epoch  485, Training Loss: 0.0340, Validation Loss: 0.0492, Learning Rate: 0.103000\n",
      "Epoch  486, Training Loss: 0.0340, Validation Loss: 0.0491, Learning Rate: 0.102800\n",
      "Early stopping triggered at epoch 486. Restoring best model parameters.\n",
      "\n",
      "Neural Network Classification Accuracy: 0.9907\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### EARLY STOPPING #######\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "# ============================\n",
    "# Activation functions and their derivatives\n",
    "# ============================\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(z, a):\n",
    "    # Note: z is not used here; kept for uniform signature.\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(z, a):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(z, a):\n",
    "    return np.ones_like(a)\n",
    "\n",
    "activation_functions = {\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"relu\": relu,\n",
    "    \"linear\": linear\n",
    "}\n",
    "\n",
    "activation_derivatives = {\n",
    "    \"sigmoid\": lambda z, a: sigmoid_derivative(z, a),\n",
    "    \"relu\": lambda z, a: relu_derivative(z, a),\n",
    "    \"linear\": lambda z, a: linear_derivative(z, a)\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Loss functions and their derivatives\n",
    "# ============================\n",
    "\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Binary crossentropy loss for binary classification.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_crossentropy_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the binary crossentropy loss.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return - (y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Squared Error (MSE) loss, typically used for regression.\n",
    "    \"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the MSE loss.\n",
    "    \"\"\"\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "def mee_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Euclidean Error (MEE) loss, defined as:\n",
    "        E_MEE = (1/N) * sum over i [ ||y_true[i] - y_pred[i]||_2 ].\n",
    "    \"\"\"\n",
    "    diff = y_true - y_pred  # shape: (N, d) or (N, 1)\n",
    "    # Euclidean distance for each sample\n",
    "    dist = np.sqrt(np.sum(diff ** 2, axis=1))  # shape: (N,)\n",
    "    return np.mean(dist)\n",
    "\n",
    "def mee_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the Mean Euclidean Error (MEE) loss.\n",
    "    For each sample i, derivative wrt y_pred[i] is:\n",
    "        (1/N) * ( (y_pred[i] - y_true[i]) / ||y_pred[i] - y_true[i]||_2 ).\n",
    "    We safely handle the case where the norm is zero.\n",
    "    \"\"\"\n",
    "    diff = (y_pred - y_true)  # shape: (N, d) or (N, 1)\n",
    "    dist = np.sqrt(np.sum(diff ** 2, axis=1, keepdims=True))  # shape: (N, 1)\n",
    "    epsilon = 1e-8  # Avoid division by zero\n",
    "    dist_safe = np.where(dist == 0, epsilon, dist)\n",
    "    N = y_true.shape[0]\n",
    "    derivative = diff / dist_safe / N  # shape: (N, d)\n",
    "    return derivative\n",
    "\n",
    "loss_functions = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_loss,\n",
    "    \"mse\": mse_loss,\n",
    "    \"mee\": mee_loss,  \n",
    "}\n",
    "\n",
    "loss_derivatives = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_derivative,\n",
    "    \"mse\": mse_derivative,\n",
    "    \"mee\": mee_derivative, \n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Regularization functions (modular)\n",
    "# ============================\n",
    "\n",
    "def compute_reg_gradient(W, lambda_reg, reg_type, m):\n",
    "    if reg_type == \"l2\":\n",
    "        return lambda_reg * W / m\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * np.sign(W) / m\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_reg_loss(W_list, lambda_reg, reg_type):\n",
    "    if reg_type == \"l2\":\n",
    "        return (lambda_reg / 2) * sum(np.sum(W ** 2) for W in W_list)\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * sum(np.sum(np.abs(W)) for W in W_list)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# ============================\n",
    "# Neural Network Class with Learning Rate Decay, Momentum, Custom Weight Initialization, and Early Stopping\n",
    "# ============================\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.01, lambda_reg=0.001, reg_type=\"l2\",\n",
    "                 loss_function_name=None,\n",
    "                 activation_function_name=\"relu\",\n",
    "                 output_activation_function_name=None,\n",
    "                 activation_function_names=None,\n",
    "                 task=\"classification\",\n",
    "                 lr_decay_type=\"none\",  # Options: \"none\", \"exponential\", \"linear\"\n",
    "                 decay_rate=0.0,\n",
    "                 weight_init=\"base\",  # \"base\" (fan-in scaling) or \"glorot\"\n",
    "                 momentum_type=\"none\",  # Options: \"none\", \"momentum\", \"nesterov momentum\"\n",
    "                 momentum_alpha=0.9):    # momentum coefficient\n",
    "        \"\"\"\n",
    "        :param layers: List containing the size of each layer (input, hidden, output)\n",
    "        :param learning_rate: Initial learning rate\n",
    "        :param lambda_reg: Regularization coefficient\n",
    "        :param reg_type: Type of regularization (\"l2\", \"l1\", or other for none)\n",
    "        :param loss_function_name: Name of the loss function (if None, set based on task)\n",
    "        :param activation_function_name: Activation to use for hidden layers (if activation_function_names not provided)\n",
    "        :param output_activation_function_name: Activation for the output layer (if None, set based on task)\n",
    "        :param activation_function_names: List of activation function names for each layer (length = len(layers)-1)\n",
    "        :param task: \"classification\" or \"regression\"\n",
    "        :param lr_decay_type: Learning rate decay strategy (\"none\", \"exponential\", \"linear\")\n",
    "        :param decay_rate: Decay rate used in the learning rate schedule\n",
    "        :param weight_init: Weight initialization strategy (\"base\" uses fan-in scaling or \"glorot\")\n",
    "        :param momentum_type: Momentum strategy (\"none\", \"momentum\", \"nesterov momentum\")\n",
    "        :param momentum_alpha: Momentum coefficient (e.g., 0.9)\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.reg_type = reg_type\n",
    "        self.task = task\n",
    "        self.lr_decay_type = lr_decay_type\n",
    "        self.decay_rate = decay_rate\n",
    "        self.weight_init = weight_init  # Weight initialization strategy\n",
    "        \n",
    "        # Set momentum parameters\n",
    "        if momentum_type not in {\"none\", \"momentum\", \"nesterov momentum\"}:\n",
    "            raise ValueError(\"momentum_type must be 'none', 'momentum', or 'nesterov momentum'.\")\n",
    "        self.momentum_type = momentum_type\n",
    "        self.momentum_alpha = momentum_alpha if momentum_type != \"none\" else 0.0\n",
    "        \n",
    "        # Set defaults based on task\n",
    "        if self.task == \"regression\":\n",
    "            self.loss_function_name = loss_function_name or \"mse\"\n",
    "            output_activation_function_name = output_activation_function_name or \"linear\"\n",
    "        else:\n",
    "            # Classification\n",
    "            self.loss_function_name = loss_function_name or \"binary_crossentropy\"\n",
    "            output_activation_function_name = output_activation_function_name or \"sigmoid\"\n",
    "        \n",
    "        # Set activation functions for layers\n",
    "        if activation_function_names is None:\n",
    "            self.activation_function_names = [activation_function_name] * (len(layers) - 1)\n",
    "            self.activation_function_names[-1] = output_activation_function_name\n",
    "        else:\n",
    "            if len(activation_function_names) != len(layers) - 1:\n",
    "                raise ValueError(\"activation_function_names must have length equal to len(layers)-1.\")\n",
    "            self.activation_function_names = activation_function_names\n",
    "        \n",
    "        self._initialize_parameters()\n",
    "        # Initialize momentum accumulators (even if not used, for consistency)\n",
    "        self.vW = [np.zeros_like(W) for W in self.W]\n",
    "        self.vb = [np.zeros_like(b) for b in self.b]\n",
    "        \n",
    "    def _initialize_parameters(self):\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            fan_in = self.layers[i]\n",
    "            fan_out = self.layers[i + 1]\n",
    "            if self.weight_init == \"base\":\n",
    "                # Standard random initialization scaled by fan-in\n",
    "                std = np.sqrt(1.0 / fan_in)\n",
    "            elif self.weight_init == \"glorot\":\n",
    "                # Glorot (Xavier) initialization\n",
    "                std = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported weight initialization strategy. Use 'base' or 'glorot'.\")\n",
    "            weight = np.random.randn(fan_in, fan_out) * std\n",
    "            self.W.append(weight)\n",
    "            self.b.append(np.zeros((1, fan_out)))\n",
    "    \n",
    "    def _apply_activation(self, x, func_name):\n",
    "        if func_name not in activation_functions:\n",
    "            raise ValueError(f\"Unsupported activation: {func_name}\")\n",
    "        return activation_functions[func_name](x)\n",
    "    \n",
    "    def _apply_activation_derivative(self, z, a, func_name):\n",
    "        if func_name not in activation_derivatives:\n",
    "            raise ValueError(f\"Unsupported activation derivative: {func_name}\")\n",
    "        return activation_derivatives[func_name](z, a)\n",
    "    \n",
    "    def _forward(self, X, weights=None, biases=None):\n",
    "        \"\"\"\n",
    "        Forward propagation. If weights and biases are provided, they are used;\n",
    "        otherwise the network's parameters are used.\n",
    "        Returns lists Z (pre-activations) and A (activations).\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.W\n",
    "        if biases is None:\n",
    "            biases = self.b\n",
    "            \n",
    "        A = [X]\n",
    "        Z = []\n",
    "        # Forward through hidden layers\n",
    "        for i in range(len(weights) - 1):\n",
    "            z_curr = np.dot(A[-1], weights[i]) + biases[i]\n",
    "            Z.append(z_curr)\n",
    "            a_curr = self._apply_activation(z_curr, self.activation_function_names[i])\n",
    "            A.append(a_curr)\n",
    "        # Forward through output layer\n",
    "        z_out = np.dot(A[-1], weights[-1]) + biases[-1]\n",
    "        Z.append(z_out)\n",
    "        a_out = self._apply_activation(z_out, self.activation_function_names[-1])\n",
    "        A.append(a_out)\n",
    "        return Z, A\n",
    "    \n",
    "    def _compute_gradients(self, X, y, Z, A, weights=None):\n",
    "        \"\"\"\n",
    "        Compute gradients dW and db given inputs X, target y, pre-activations Z and activations A.\n",
    "        Optionally, a custom set of weights (used in lookahead for Nesterov momentum) can be provided.\n",
    "        Returns lists dW and db.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.W\n",
    "        m = X.shape[0]\n",
    "        # Compute derivative of loss with respect to output activation\n",
    "        dA = loss_derivatives[self.loss_function_name](y, A[-1])\n",
    "        # Output layer\n",
    "        dZ = dA * self._apply_activation_derivative(Z[-1], A[-1], self.activation_function_names[-1])\n",
    "        reg_term = compute_reg_gradient(weights[-1], self.lambda_reg, self.reg_type, m)\n",
    "        dW = [np.dot(A[-2].T, dZ) / m + reg_term]\n",
    "        db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "        \n",
    "        # Backpropagate through hidden layers\n",
    "        for i in range(len(weights) - 2, -1, -1):\n",
    "            dA = np.dot(dZ, weights[i + 1].T)\n",
    "            dZ = dA * self._apply_activation_derivative(Z[i], A[i + 1], self.activation_function_names[i])\n",
    "            reg_term = compute_reg_gradient(weights[i], self.lambda_reg, self.reg_type, m)\n",
    "            dW.insert(0, np.dot(A[i].T, dZ) / m + reg_term)\n",
    "            db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "            \n",
    "        return dW, db\n",
    "    \n",
    "    def train(self, X, y, epochs=300, batch_size=32, verbose=True,\n",
    "              early_stopping=False, validation_data=None, patience=10, min_delta=0.0):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        \n",
    "        :param X: Training data inputs.\n",
    "        :param y: Training data targets.\n",
    "        :param epochs: Maximum number of epochs to train.\n",
    "        :param batch_size: Mini-batch size.\n",
    "        :param verbose: Whether to print progress.\n",
    "        :param early_stopping: Boolean flag to enable early stopping.\n",
    "        :param validation_data: Tuple (X_val, y_val) for early stopping validation.\n",
    "        :param patience: Number of epochs with no improvement to wait before stopping.\n",
    "        :param min_delta: Minimum change in the monitored loss to qualify as an improvement.\n",
    "        :return: List containing loss history at checkpoints.\n",
    "        \"\"\"\n",
    "        if early_stopping and validation_data is None:\n",
    "            raise ValueError(\"Validation data must be provided if early stopping is enabled.\")\n",
    "        \n",
    "        loss_history = []\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        best_loss = np.inf\n",
    "        patience_counter = 0\n",
    "        best_weights = None\n",
    "        best_biases = None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Update learning rate based on decay schedule\n",
    "            if self.lr_decay_type == \"exponential\":\n",
    "                self.learning_rate = self.initial_learning_rate * np.exp(-self.decay_rate * epoch)\n",
    "            elif self.lr_decay_type == \"linear\":\n",
    "                self.learning_rate = self.initial_learning_rate * max(0, 1 - self.decay_rate * epoch)\n",
    "            # Otherwise (\"none\"), keep the initial learning rate.\n",
    "            \n",
    "            # Shuffle training data\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            \n",
    "            # Process mini-batches\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                \n",
    "                # Choose update strategy based on momentum type\n",
    "                if self.momentum_type == \"nesterov momentum\":\n",
    "                    # Compute lookahead parameters\n",
    "                    weights_lookahead = [self.W[j] - self.momentum_alpha * self.vW[j] for j in range(len(self.W))]\n",
    "                    biases_lookahead = [self.b[j] - self.momentum_alpha * self.vb[j] for j in range(len(self.b))]\n",
    "                    Z, A = self._forward(X_batch, weights=weights_lookahead, biases=biases_lookahead)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A, weights=weights_lookahead)\n",
    "                    \n",
    "                    # Update momentum accumulators and actual parameters\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.vW[j] = self.momentum_alpha * self.vW[j] + self.learning_rate * dW[j]\n",
    "                        self.vb[j] = self.momentum_alpha * self.vb[j] + self.learning_rate * db[j]\n",
    "                        self.W[j] -= self.vW[j]\n",
    "                        self.b[j] -= self.vb[j]\n",
    "                        \n",
    "                elif self.momentum_type == \"momentum\":\n",
    "                    # Standard momentum: compute gradients with current parameters\n",
    "                    Z, A = self._forward(X_batch)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.vW[j] = self.momentum_alpha * self.vW[j] + self.learning_rate * dW[j]\n",
    "                        self.vb[j] = self.momentum_alpha * self.vb[j] + self.learning_rate * db[j]\n",
    "                        self.W[j] -= self.vW[j]\n",
    "                        self.b[j] -= self.vb[j]\n",
    "                        \n",
    "                else:  # No momentum\n",
    "                    Z, A = self._forward(X_batch)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.W[j] -= self.learning_rate * dW[j]\n",
    "                        self.b[j] -= self.learning_rate * db[j]\n",
    "            \n",
    "            # Compute full training loss for logging purposes\n",
    "            _, A_full = self._forward(X)\n",
    "            train_loss = loss_functions[self.loss_function_name](y, A_full[-1])\n",
    "            reg_loss = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "            total_train_loss = train_loss + reg_loss\n",
    "            \n",
    "            if early_stopping:\n",
    "                # Compute validation loss\n",
    "                X_val, y_val = validation_data\n",
    "                _, A_val = self._forward(X_val)\n",
    "                val_loss = loss_functions[self.loss_function_name](y_val, A_val[-1])\n",
    "                reg_loss_val = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "                total_val_loss = val_loss + reg_loss_val\n",
    "\n",
    "                loss_history.append(total_val_loss)\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch:4d}, Training Loss: {total_train_loss:.4f}, Validation Loss: {total_val_loss:.4f}, Learning Rate: {self.learning_rate:.6f}\")\n",
    "                \n",
    "                # Check for improvement (by at least min_delta)\n",
    "                if total_val_loss < best_loss - min_delta:\n",
    "                    best_loss = total_val_loss\n",
    "                    patience_counter = 0\n",
    "                    # Save best model parameters\n",
    "                    best_weights = [w.copy() for w in self.W]\n",
    "                    best_biases = [b.copy() for b in self.b]\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        if verbose:\n",
    "                            print(f\"Early stopping triggered at epoch {epoch}. Restoring best model parameters.\")\n",
    "                        # Restore best model parameters\n",
    "                        if best_weights is not None:\n",
    "                            self.W = best_weights\n",
    "                            self.b = best_biases\n",
    "                        return loss_history\n",
    "            else:\n",
    "                # Without early stopping, log training loss at intervals\n",
    "                if epoch % max(1, int(epochs / 20)) == 0:\n",
    "                    loss_history.append(total_train_loss)\n",
    "                    if verbose:\n",
    "                        print(f\"Epoch {epoch:4d}, Loss: {total_train_loss:.4f}, Learning Rate: {self.learning_rate:.6f}\")\n",
    "                        \n",
    "        return loss_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, A = self._forward(X)\n",
    "        output = A[-1]\n",
    "        if self.task == \"classification\":\n",
    "            # For binary classification, threshold at 0.5\n",
    "            if output.shape[1] == 1:\n",
    "                return (output > 0.5).astype(int)\n",
    "            else:\n",
    "                return np.argmax(output, axis=1)\n",
    "        else:  # regression\n",
    "            return output\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        if self.task == \"regression\":\n",
    "            return mse_loss(y, predictions)\n",
    "        else:\n",
    "            # If y is one-hot encoded, convert to class labels\n",
    "            if y.ndim > 1 and y.shape[1] > 1:\n",
    "                y_true = np.argmax(y, axis=1)\n",
    "            else:\n",
    "                y_true = y\n",
    "            return np.mean(predictions == y_true)\n",
    "\n",
    "# ============================\n",
    "# Testing on a monk's dataset\n",
    "# ============================\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(1, one_hot_encode=True)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_units = 10\n",
    "output_size = 1  # binary classification\n",
    "layers = [input_size, hidden_units, output_size]\n",
    "\n",
    "# Define activation functions for hidden and output layers\n",
    "activation_funcs = [\"relu\", \"sigmoid\"]\n",
    "\n",
    "nn_clf = NeuralNetwork(\n",
    "    layers=layers,\n",
    "    learning_rate=0.2,\n",
    "    lambda_reg=0.001,\n",
    "    reg_type=\"l2\",\n",
    "    loss_function_name=\"mse\",       \n",
    "    activation_function_names=activation_funcs,\n",
    "    task=\"classification\",\n",
    "    lr_decay_type=\"linear\",    # Options: \"exponential\", \"linear\", or \"none\"\n",
    "    decay_rate=0.001,          # Adjust decay rate as needed\n",
    "    weight_init=\"base\",        # Choose between \"base\" or \"glorot\"\n",
    ")\n",
    "\n",
    "# For early stopping, we provide validation data.\n",
    "# (In practice, you might set aside a validation split from your training data.)\n",
    "loss_history = nn_clf.train(\n",
    "    X_train, y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    verbose=True,\n",
    "    early_stopping=True,\n",
    "    validation_data=(X_test, y_test),\n",
    "    patience=10,\n",
    "    min_delta=1e-4\n",
    ")\n",
    "\n",
    "accuracy = nn_clf.evaluate(X_test, y_test)\n",
    "print(f\"\\nNeural Network Classification Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "pd.Series(loss_history).plot(title=\"Training / Validation Loss History\")\n",
    "plt.xlabel(\"Checkpoint (Epochs)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-1-train.csv\n",
      "Using cached ../datasets/monks/monk-1-test.csv\n",
      "One-hot encoding MONK-1 dataset...\n",
      "Epoch    0, Training Loss: 0.2537, Validation Loss: 0.2570, Learning Rate: 0.200000\n",
      "Epoch    1, Training Loss: 0.2506, Validation Loss: 0.2550, Learning Rate: 0.199800\n",
      "Epoch    2, Training Loss: 0.2477, Validation Loss: 0.2531, Learning Rate: 0.199600\n",
      "Epoch    3, Training Loss: 0.2448, Validation Loss: 0.2512, Learning Rate: 0.199400\n",
      "Epoch    4, Training Loss: 0.2419, Validation Loss: 0.2494, Learning Rate: 0.199200\n",
      "Epoch    5, Training Loss: 0.2389, Validation Loss: 0.2474, Learning Rate: 0.199000\n",
      "Epoch    6, Training Loss: 0.2357, Validation Loss: 0.2453, Learning Rate: 0.198800\n",
      "Epoch    7, Training Loss: 0.2326, Validation Loss: 0.2431, Learning Rate: 0.198600\n",
      "Epoch    8, Training Loss: 0.2294, Validation Loss: 0.2410, Learning Rate: 0.198400\n",
      "Epoch    9, Training Loss: 0.2251, Validation Loss: 0.2380, Learning Rate: 0.198200\n",
      "Epoch   10, Training Loss: 0.2205, Validation Loss: 0.2347, Learning Rate: 0.198000\n",
      "Epoch   11, Training Loss: 0.2163, Validation Loss: 0.2316, Learning Rate: 0.197800\n",
      "Epoch   12, Training Loss: 0.2122, Validation Loss: 0.2288, Learning Rate: 0.197600\n",
      "Epoch   13, Training Loss: 0.2077, Validation Loss: 0.2256, Learning Rate: 0.197400\n",
      "Epoch   14, Training Loss: 0.2021, Validation Loss: 0.2218, Learning Rate: 0.197200\n",
      "Epoch   15, Training Loss: 0.1971, Validation Loss: 0.2185, Learning Rate: 0.197000\n",
      "Epoch   16, Training Loss: 0.1925, Validation Loss: 0.2154, Learning Rate: 0.196800\n",
      "Epoch   17, Training Loss: 0.1883, Validation Loss: 0.2124, Learning Rate: 0.196600\n",
      "Epoch   18, Training Loss: 0.1843, Validation Loss: 0.2095, Learning Rate: 0.196400\n",
      "Epoch   19, Training Loss: 0.1806, Validation Loss: 0.2067, Learning Rate: 0.196200\n",
      "Epoch   20, Training Loss: 0.1767, Validation Loss: 0.2041, Learning Rate: 0.196000\n",
      "Epoch   21, Training Loss: 0.1733, Validation Loss: 0.2016, Learning Rate: 0.195800\n",
      "Epoch   22, Training Loss: 0.1700, Validation Loss: 0.1993, Learning Rate: 0.195600\n",
      "Epoch   23, Training Loss: 0.1669, Validation Loss: 0.1970, Learning Rate: 0.195400\n",
      "Epoch   24, Training Loss: 0.1639, Validation Loss: 0.1949, Learning Rate: 0.195200\n",
      "Epoch   25, Training Loss: 0.1612, Validation Loss: 0.1931, Learning Rate: 0.195000\n",
      "Epoch   26, Training Loss: 0.1585, Validation Loss: 0.1911, Learning Rate: 0.194800\n",
      "Epoch   27, Training Loss: 0.1562, Validation Loss: 0.1895, Learning Rate: 0.194600\n",
      "Epoch   28, Training Loss: 0.1541, Validation Loss: 0.1878, Learning Rate: 0.194400\n",
      "Epoch   29, Training Loss: 0.1518, Validation Loss: 0.1857, Learning Rate: 0.194200\n",
      "Epoch   30, Training Loss: 0.1498, Validation Loss: 0.1844, Learning Rate: 0.194000\n",
      "Epoch   31, Training Loss: 0.1481, Validation Loss: 0.1833, Learning Rate: 0.193800\n",
      "Epoch   32, Training Loss: 0.1461, Validation Loss: 0.1815, Learning Rate: 0.193600\n",
      "Epoch   33, Training Loss: 0.1445, Validation Loss: 0.1799, Learning Rate: 0.193400\n",
      "Epoch   34, Training Loss: 0.1428, Validation Loss: 0.1786, Learning Rate: 0.193200\n",
      "Epoch   35, Training Loss: 0.1411, Validation Loss: 0.1777, Learning Rate: 0.193000\n",
      "Epoch   36, Training Loss: 0.1396, Validation Loss: 0.1765, Learning Rate: 0.192800\n",
      "Epoch   37, Training Loss: 0.1381, Validation Loss: 0.1756, Learning Rate: 0.192600\n",
      "Epoch   38, Training Loss: 0.1366, Validation Loss: 0.1741, Learning Rate: 0.192400\n",
      "Epoch   39, Training Loss: 0.1352, Validation Loss: 0.1729, Learning Rate: 0.192200\n",
      "Epoch   40, Training Loss: 0.1339, Validation Loss: 0.1717, Learning Rate: 0.192000\n",
      "Epoch   41, Training Loss: 0.1326, Validation Loss: 0.1707, Learning Rate: 0.191800\n",
      "Epoch   42, Training Loss: 0.1313, Validation Loss: 0.1696, Learning Rate: 0.191600\n",
      "Epoch   43, Training Loss: 0.1302, Validation Loss: 0.1684, Learning Rate: 0.191400\n",
      "Epoch   44, Training Loss: 0.1288, Validation Loss: 0.1679, Learning Rate: 0.191200\n",
      "Epoch   45, Training Loss: 0.1276, Validation Loss: 0.1672, Learning Rate: 0.191000\n",
      "Epoch   46, Training Loss: 0.1267, Validation Loss: 0.1656, Learning Rate: 0.190800\n",
      "Epoch   47, Training Loss: 0.1254, Validation Loss: 0.1651, Learning Rate: 0.190600\n",
      "Epoch   48, Training Loss: 0.1242, Validation Loss: 0.1641, Learning Rate: 0.190400\n",
      "Epoch   49, Training Loss: 0.1231, Validation Loss: 0.1642, Learning Rate: 0.190200\n",
      "Epoch   50, Training Loss: 0.1220, Validation Loss: 0.1630, Learning Rate: 0.190000\n",
      "Epoch   51, Training Loss: 0.1209, Validation Loss: 0.1620, Learning Rate: 0.189800\n",
      "Epoch   52, Training Loss: 0.1199, Validation Loss: 0.1614, Learning Rate: 0.189600\n",
      "Epoch   53, Training Loss: 0.1189, Validation Loss: 0.1600, Learning Rate: 0.189400\n",
      "Epoch   54, Training Loss: 0.1180, Validation Loss: 0.1588, Learning Rate: 0.189200\n",
      "Epoch   55, Training Loss: 0.1171, Validation Loss: 0.1580, Learning Rate: 0.189000\n",
      "Epoch   56, Training Loss: 0.1161, Validation Loss: 0.1575, Learning Rate: 0.188800\n",
      "Epoch   57, Training Loss: 0.1152, Validation Loss: 0.1565, Learning Rate: 0.188600\n",
      "Epoch   58, Training Loss: 0.1146, Validation Loss: 0.1553, Learning Rate: 0.188400\n",
      "Epoch   59, Training Loss: 0.1133, Validation Loss: 0.1555, Learning Rate: 0.188200\n",
      "Epoch   60, Training Loss: 0.1124, Validation Loss: 0.1547, Learning Rate: 0.188000\n",
      "Epoch   61, Training Loss: 0.1116, Validation Loss: 0.1542, Learning Rate: 0.187800\n",
      "Epoch   62, Training Loss: 0.1108, Validation Loss: 0.1531, Learning Rate: 0.187600\n",
      "Epoch   63, Training Loss: 0.1099, Validation Loss: 0.1525, Learning Rate: 0.187400\n",
      "Epoch   64, Training Loss: 0.1093, Validation Loss: 0.1512, Learning Rate: 0.187200\n",
      "Epoch   65, Training Loss: 0.1083, Validation Loss: 0.1506, Learning Rate: 0.187000\n",
      "Epoch   66, Training Loss: 0.1074, Validation Loss: 0.1506, Learning Rate: 0.186800\n",
      "Epoch   67, Training Loss: 0.1069, Validation Loss: 0.1512, Learning Rate: 0.186600\n",
      "Epoch   68, Training Loss: 0.1058, Validation Loss: 0.1490, Learning Rate: 0.186400\n",
      "Epoch   69, Training Loss: 0.1050, Validation Loss: 0.1489, Learning Rate: 0.186200\n",
      "Epoch   70, Training Loss: 0.1042, Validation Loss: 0.1477, Learning Rate: 0.186000\n",
      "Epoch   71, Training Loss: 0.1035, Validation Loss: 0.1473, Learning Rate: 0.185800\n",
      "Epoch   72, Training Loss: 0.1027, Validation Loss: 0.1464, Learning Rate: 0.185600\n",
      "Epoch   73, Training Loss: 0.1022, Validation Loss: 0.1447, Learning Rate: 0.185400\n",
      "Epoch   74, Training Loss: 0.1013, Validation Loss: 0.1443, Learning Rate: 0.185200\n",
      "Epoch   75, Training Loss: 0.1007, Validation Loss: 0.1455, Learning Rate: 0.185000\n",
      "Epoch   76, Training Loss: 0.0998, Validation Loss: 0.1440, Learning Rate: 0.184800\n",
      "Epoch   77, Training Loss: 0.0991, Validation Loss: 0.1423, Learning Rate: 0.184600\n",
      "Epoch   78, Training Loss: 0.0983, Validation Loss: 0.1423, Learning Rate: 0.184400\n",
      "Epoch   79, Training Loss: 0.0976, Validation Loss: 0.1416, Learning Rate: 0.184200\n",
      "Epoch   80, Training Loss: 0.0970, Validation Loss: 0.1415, Learning Rate: 0.184000\n",
      "Epoch   81, Training Loss: 0.0965, Validation Loss: 0.1392, Learning Rate: 0.183800\n",
      "Epoch   82, Training Loss: 0.0956, Validation Loss: 0.1386, Learning Rate: 0.183600\n",
      "Epoch   83, Training Loss: 0.0950, Validation Loss: 0.1379, Learning Rate: 0.183400\n",
      "Epoch   84, Training Loss: 0.0943, Validation Loss: 0.1375, Learning Rate: 0.183200\n",
      "Epoch   85, Training Loss: 0.0937, Validation Loss: 0.1386, Learning Rate: 0.183000\n",
      "Epoch   86, Training Loss: 0.0928, Validation Loss: 0.1368, Learning Rate: 0.182800\n",
      "Epoch   87, Training Loss: 0.0921, Validation Loss: 0.1362, Learning Rate: 0.182600\n",
      "Epoch   88, Training Loss: 0.0914, Validation Loss: 0.1359, Learning Rate: 0.182400\n",
      "Epoch   89, Training Loss: 0.0908, Validation Loss: 0.1356, Learning Rate: 0.182200\n",
      "Epoch   90, Training Loss: 0.0903, Validation Loss: 0.1351, Learning Rate: 0.182000\n",
      "Epoch   91, Training Loss: 0.0895, Validation Loss: 0.1341, Learning Rate: 0.181800\n",
      "Epoch   92, Training Loss: 0.0890, Validation Loss: 0.1335, Learning Rate: 0.181600\n",
      "Epoch   93, Training Loss: 0.0884, Validation Loss: 0.1321, Learning Rate: 0.181400\n",
      "Epoch   94, Training Loss: 0.0882, Validation Loss: 0.1340, Learning Rate: 0.181200\n",
      "Epoch   95, Training Loss: 0.0872, Validation Loss: 0.1310, Learning Rate: 0.181000\n",
      "Epoch   96, Training Loss: 0.0865, Validation Loss: 0.1309, Learning Rate: 0.180800\n",
      "Epoch   97, Training Loss: 0.0860, Validation Loss: 0.1299, Learning Rate: 0.180600\n",
      "Epoch   98, Training Loss: 0.0856, Validation Loss: 0.1290, Learning Rate: 0.180400\n",
      "Epoch   99, Training Loss: 0.0852, Validation Loss: 0.1282, Learning Rate: 0.180200\n",
      "Epoch  100, Training Loss: 0.0843, Validation Loss: 0.1284, Learning Rate: 0.180000\n",
      "Epoch  101, Training Loss: 0.0837, Validation Loss: 0.1279, Learning Rate: 0.179800\n",
      "Epoch  102, Training Loss: 0.0833, Validation Loss: 0.1284, Learning Rate: 0.179600\n",
      "Epoch  103, Training Loss: 0.0826, Validation Loss: 0.1273, Learning Rate: 0.179400\n",
      "Epoch  104, Training Loss: 0.0829, Validation Loss: 0.1252, Learning Rate: 0.179200\n",
      "Epoch  105, Training Loss: 0.0816, Validation Loss: 0.1256, Learning Rate: 0.179000\n",
      "Epoch  106, Training Loss: 0.0811, Validation Loss: 0.1257, Learning Rate: 0.178800\n",
      "Epoch  107, Training Loss: 0.0805, Validation Loss: 0.1246, Learning Rate: 0.178600\n",
      "Epoch  108, Training Loss: 0.0802, Validation Loss: 0.1256, Learning Rate: 0.178400\n",
      "Epoch  109, Training Loss: 0.0795, Validation Loss: 0.1241, Learning Rate: 0.178200\n",
      "Epoch  110, Training Loss: 0.0790, Validation Loss: 0.1234, Learning Rate: 0.178000\n",
      "Epoch  111, Training Loss: 0.0784, Validation Loss: 0.1221, Learning Rate: 0.177800\n",
      "Epoch  112, Training Loss: 0.0779, Validation Loss: 0.1216, Learning Rate: 0.177600\n",
      "Epoch  113, Training Loss: 0.0781, Validation Loss: 0.1203, Learning Rate: 0.177400\n",
      "Epoch  114, Training Loss: 0.0769, Validation Loss: 0.1208, Learning Rate: 0.177200\n",
      "Epoch  115, Training Loss: 0.0765, Validation Loss: 0.1207, Learning Rate: 0.177000\n",
      "Epoch  116, Training Loss: 0.0761, Validation Loss: 0.1204, Learning Rate: 0.176800\n",
      "Epoch  117, Training Loss: 0.0756, Validation Loss: 0.1187, Learning Rate: 0.176600\n",
      "Epoch  118, Training Loss: 0.0752, Validation Loss: 0.1179, Learning Rate: 0.176400\n",
      "Epoch  119, Training Loss: 0.0746, Validation Loss: 0.1185, Learning Rate: 0.176200\n",
      "Epoch  120, Training Loss: 0.0741, Validation Loss: 0.1174, Learning Rate: 0.176000\n",
      "Epoch  121, Training Loss: 0.0740, Validation Loss: 0.1160, Learning Rate: 0.175800\n",
      "Epoch  122, Training Loss: 0.0732, Validation Loss: 0.1169, Learning Rate: 0.175600\n",
      "Epoch  123, Training Loss: 0.0727, Validation Loss: 0.1161, Learning Rate: 0.175400\n",
      "Epoch  124, Training Loss: 0.0726, Validation Loss: 0.1144, Learning Rate: 0.175200\n",
      "Epoch  125, Training Loss: 0.0718, Validation Loss: 0.1149, Learning Rate: 0.175000\n",
      "Epoch  126, Training Loss: 0.0714, Validation Loss: 0.1149, Learning Rate: 0.174800\n",
      "Epoch  127, Training Loss: 0.0711, Validation Loss: 0.1152, Learning Rate: 0.174600\n",
      "Epoch  128, Training Loss: 0.0714, Validation Loss: 0.1164, Learning Rate: 0.174400\n",
      "Epoch  129, Training Loss: 0.0701, Validation Loss: 0.1128, Learning Rate: 0.174200\n",
      "Epoch  130, Training Loss: 0.0697, Validation Loss: 0.1122, Learning Rate: 0.174000\n",
      "Epoch  131, Training Loss: 0.0692, Validation Loss: 0.1115, Learning Rate: 0.173800\n",
      "Epoch  132, Training Loss: 0.0688, Validation Loss: 0.1120, Learning Rate: 0.173600\n",
      "Epoch  133, Training Loss: 0.0684, Validation Loss: 0.1107, Learning Rate: 0.173400\n",
      "Epoch  134, Training Loss: 0.0681, Validation Loss: 0.1098, Learning Rate: 0.173200\n",
      "Epoch  135, Training Loss: 0.0675, Validation Loss: 0.1099, Learning Rate: 0.173000\n",
      "Epoch  136, Training Loss: 0.0671, Validation Loss: 0.1096, Learning Rate: 0.172800\n",
      "Epoch  137, Training Loss: 0.0669, Validation Loss: 0.1087, Learning Rate: 0.172600\n",
      "Epoch  138, Training Loss: 0.0664, Validation Loss: 0.1093, Learning Rate: 0.172400\n",
      "Epoch  139, Training Loss: 0.0660, Validation Loss: 0.1087, Learning Rate: 0.172200\n",
      "Epoch  140, Training Loss: 0.0656, Validation Loss: 0.1076, Learning Rate: 0.172000\n",
      "Epoch  141, Training Loss: 0.0653, Validation Loss: 0.1079, Learning Rate: 0.171800\n",
      "Epoch  142, Training Loss: 0.0650, Validation Loss: 0.1080, Learning Rate: 0.171600\n",
      "Epoch  143, Training Loss: 0.0644, Validation Loss: 0.1063, Learning Rate: 0.171400\n",
      "Epoch  144, Training Loss: 0.0642, Validation Loss: 0.1067, Learning Rate: 0.171200\n",
      "Epoch  145, Training Loss: 0.0638, Validation Loss: 0.1061, Learning Rate: 0.171000\n",
      "Epoch  146, Training Loss: 0.0635, Validation Loss: 0.1059, Learning Rate: 0.170800\n",
      "Epoch  147, Training Loss: 0.0630, Validation Loss: 0.1039, Learning Rate: 0.170600\n",
      "Epoch  148, Training Loss: 0.0627, Validation Loss: 0.1045, Learning Rate: 0.170400\n",
      "Epoch  149, Training Loss: 0.0628, Validation Loss: 0.1022, Learning Rate: 0.170200\n",
      "Epoch  150, Training Loss: 0.0620, Validation Loss: 0.1037, Learning Rate: 0.170000\n",
      "Epoch  151, Training Loss: 0.0619, Validation Loss: 0.1014, Learning Rate: 0.169800\n",
      "Epoch  152, Training Loss: 0.0613, Validation Loss: 0.1027, Learning Rate: 0.169600\n",
      "Epoch  153, Training Loss: 0.0611, Validation Loss: 0.1008, Learning Rate: 0.169400\n",
      "Epoch  154, Training Loss: 0.0607, Validation Loss: 0.1006, Learning Rate: 0.169200\n",
      "Epoch  155, Training Loss: 0.0602, Validation Loss: 0.1006, Learning Rate: 0.169000\n",
      "Epoch  156, Training Loss: 0.0602, Validation Loss: 0.0995, Learning Rate: 0.168800\n",
      "Epoch  157, Training Loss: 0.0596, Validation Loss: 0.1001, Learning Rate: 0.168600\n",
      "Epoch  158, Training Loss: 0.0593, Validation Loss: 0.1001, Learning Rate: 0.168400\n",
      "Epoch  159, Training Loss: 0.0596, Validation Loss: 0.0978, Learning Rate: 0.168200\n",
      "Epoch  160, Training Loss: 0.0587, Validation Loss: 0.0979, Learning Rate: 0.168000\n",
      "Epoch  161, Training Loss: 0.0583, Validation Loss: 0.0990, Learning Rate: 0.167800\n",
      "Epoch  162, Training Loss: 0.0580, Validation Loss: 0.0983, Learning Rate: 0.167600\n",
      "Epoch  163, Training Loss: 0.0582, Validation Loss: 0.0962, Learning Rate: 0.167400\n",
      "Epoch  164, Training Loss: 0.0573, Validation Loss: 0.0972, Learning Rate: 0.167200\n",
      "Epoch  165, Training Loss: 0.0571, Validation Loss: 0.0960, Learning Rate: 0.167000\n",
      "Epoch  166, Training Loss: 0.0583, Validation Loss: 0.1005, Learning Rate: 0.166800\n",
      "Epoch  167, Training Loss: 0.0565, Validation Loss: 0.0961, Learning Rate: 0.166600\n",
      "Epoch  168, Training Loss: 0.0562, Validation Loss: 0.0947, Learning Rate: 0.166400\n",
      "Epoch  169, Training Loss: 0.0558, Validation Loss: 0.0953, Learning Rate: 0.166200\n",
      "Epoch  170, Training Loss: 0.0558, Validation Loss: 0.0937, Learning Rate: 0.166000\n",
      "Epoch  171, Training Loss: 0.0553, Validation Loss: 0.0936, Learning Rate: 0.165800\n",
      "Epoch  172, Training Loss: 0.0550, Validation Loss: 0.0934, Learning Rate: 0.165600\n",
      "Epoch  173, Training Loss: 0.0550, Validation Loss: 0.0922, Learning Rate: 0.165400\n",
      "Epoch  174, Training Loss: 0.0544, Validation Loss: 0.0928, Learning Rate: 0.165200\n",
      "Epoch  175, Training Loss: 0.0541, Validation Loss: 0.0923, Learning Rate: 0.165000\n",
      "Epoch  176, Training Loss: 0.0538, Validation Loss: 0.0925, Learning Rate: 0.164800\n",
      "Epoch  177, Training Loss: 0.0536, Validation Loss: 0.0925, Learning Rate: 0.164600\n",
      "Epoch  178, Training Loss: 0.0537, Validation Loss: 0.0933, Learning Rate: 0.164400\n",
      "Epoch  179, Training Loss: 0.0533, Validation Loss: 0.0926, Learning Rate: 0.164200\n",
      "Epoch  180, Training Loss: 0.0529, Validation Loss: 0.0904, Learning Rate: 0.164000\n",
      "Epoch  181, Training Loss: 0.0525, Validation Loss: 0.0909, Learning Rate: 0.163800\n",
      "Epoch  182, Training Loss: 0.0523, Validation Loss: 0.0907, Learning Rate: 0.163600\n",
      "Epoch  183, Training Loss: 0.0525, Validation Loss: 0.0917, Learning Rate: 0.163400\n",
      "Epoch  184, Training Loss: 0.0518, Validation Loss: 0.0894, Learning Rate: 0.163200\n",
      "Epoch  185, Training Loss: 0.0516, Validation Loss: 0.0885, Learning Rate: 0.163000\n",
      "Epoch  186, Training Loss: 0.0513, Validation Loss: 0.0888, Learning Rate: 0.162800\n",
      "Epoch  187, Training Loss: 0.0511, Validation Loss: 0.0875, Learning Rate: 0.162600\n",
      "Epoch  188, Training Loss: 0.0511, Validation Loss: 0.0890, Learning Rate: 0.162400\n",
      "Epoch  189, Training Loss: 0.0506, Validation Loss: 0.0865, Learning Rate: 0.162200\n",
      "Epoch  190, Training Loss: 0.0505, Validation Loss: 0.0877, Learning Rate: 0.162000\n",
      "Epoch  191, Training Loss: 0.0501, Validation Loss: 0.0861, Learning Rate: 0.161800\n",
      "Epoch  192, Training Loss: 0.0499, Validation Loss: 0.0857, Learning Rate: 0.161600\n",
      "Epoch  193, Training Loss: 0.0497, Validation Loss: 0.0855, Learning Rate: 0.161400\n",
      "Epoch  194, Training Loss: 0.0501, Validation Loss: 0.0876, Learning Rate: 0.161200\n",
      "Epoch  195, Training Loss: 0.0492, Validation Loss: 0.0852, Learning Rate: 0.161000\n",
      "Epoch  196, Training Loss: 0.0490, Validation Loss: 0.0844, Learning Rate: 0.160800\n",
      "Epoch  197, Training Loss: 0.0490, Validation Loss: 0.0833, Learning Rate: 0.160600\n",
      "Epoch  198, Training Loss: 0.0486, Validation Loss: 0.0842, Learning Rate: 0.160400\n",
      "Epoch  199, Training Loss: 0.0484, Validation Loss: 0.0834, Learning Rate: 0.160200\n",
      "Epoch  200, Training Loss: 0.0483, Validation Loss: 0.0838, Learning Rate: 0.160000\n",
      "Epoch  201, Training Loss: 0.0480, Validation Loss: 0.0823, Learning Rate: 0.159800\n",
      "Epoch  202, Training Loss: 0.0478, Validation Loss: 0.0819, Learning Rate: 0.159600\n",
      "Epoch  203, Training Loss: 0.0475, Validation Loss: 0.0822, Learning Rate: 0.159400\n",
      "Epoch  204, Training Loss: 0.0474, Validation Loss: 0.0821, Learning Rate: 0.159200\n",
      "Epoch  205, Training Loss: 0.0474, Validation Loss: 0.0805, Learning Rate: 0.159000\n",
      "Epoch  206, Training Loss: 0.0472, Validation Loss: 0.0823, Learning Rate: 0.158800\n",
      "Epoch  207, Training Loss: 0.0468, Validation Loss: 0.0809, Learning Rate: 0.158600\n",
      "Epoch  208, Training Loss: 0.0466, Validation Loss: 0.0803, Learning Rate: 0.158400\n",
      "Epoch  209, Training Loss: 0.0464, Validation Loss: 0.0801, Learning Rate: 0.158200\n",
      "Epoch  210, Training Loss: 0.0463, Validation Loss: 0.0798, Learning Rate: 0.158000\n",
      "Epoch  211, Training Loss: 0.0461, Validation Loss: 0.0795, Learning Rate: 0.157800\n",
      "Epoch  212, Training Loss: 0.0459, Validation Loss: 0.0800, Learning Rate: 0.157600\n",
      "Epoch  213, Training Loss: 0.0457, Validation Loss: 0.0795, Learning Rate: 0.157400\n",
      "Epoch  214, Training Loss: 0.0456, Validation Loss: 0.0787, Learning Rate: 0.157200\n",
      "Epoch  215, Training Loss: 0.0456, Validation Loss: 0.0775, Learning Rate: 0.157000\n",
      "Epoch  216, Training Loss: 0.0452, Validation Loss: 0.0783, Learning Rate: 0.156800\n",
      "Epoch  217, Training Loss: 0.0451, Validation Loss: 0.0780, Learning Rate: 0.156600\n",
      "Epoch  218, Training Loss: 0.0450, Validation Loss: 0.0783, Learning Rate: 0.156400\n",
      "Epoch  219, Training Loss: 0.0447, Validation Loss: 0.0772, Learning Rate: 0.156200\n",
      "Epoch  220, Training Loss: 0.0448, Validation Loss: 0.0763, Learning Rate: 0.156000\n",
      "Epoch  221, Training Loss: 0.0444, Validation Loss: 0.0770, Learning Rate: 0.155800\n",
      "Epoch  222, Training Loss: 0.0443, Validation Loss: 0.0765, Learning Rate: 0.155600\n",
      "Epoch  223, Training Loss: 0.0442, Validation Loss: 0.0770, Learning Rate: 0.155400\n",
      "Epoch  224, Training Loss: 0.0442, Validation Loss: 0.0775, Learning Rate: 0.155200\n",
      "Epoch  225, Training Loss: 0.0439, Validation Loss: 0.0761, Learning Rate: 0.155000\n",
      "Epoch  226, Training Loss: 0.0438, Validation Loss: 0.0752, Learning Rate: 0.154800\n",
      "Epoch  227, Training Loss: 0.0436, Validation Loss: 0.0757, Learning Rate: 0.154600\n",
      "Epoch  228, Training Loss: 0.0434, Validation Loss: 0.0751, Learning Rate: 0.154400\n",
      "Epoch  229, Training Loss: 0.0433, Validation Loss: 0.0746, Learning Rate: 0.154200\n",
      "Epoch  230, Training Loss: 0.0432, Validation Loss: 0.0747, Learning Rate: 0.154000\n",
      "Epoch  231, Training Loss: 0.0430, Validation Loss: 0.0741, Learning Rate: 0.153800\n",
      "Epoch  232, Training Loss: 0.0429, Validation Loss: 0.0744, Learning Rate: 0.153600\n",
      "Epoch  233, Training Loss: 0.0433, Validation Loss: 0.0759, Learning Rate: 0.153400\n",
      "Epoch  234, Training Loss: 0.0427, Validation Loss: 0.0739, Learning Rate: 0.153200\n",
      "Epoch  235, Training Loss: 0.0425, Validation Loss: 0.0734, Learning Rate: 0.153000\n",
      "Epoch  236, Training Loss: 0.0424, Validation Loss: 0.0732, Learning Rate: 0.152800\n",
      "Epoch  237, Training Loss: 0.0423, Validation Loss: 0.0724, Learning Rate: 0.152600\n",
      "Epoch  238, Training Loss: 0.0422, Validation Loss: 0.0722, Learning Rate: 0.152400\n",
      "Epoch  239, Training Loss: 0.0420, Validation Loss: 0.0724, Learning Rate: 0.152200\n",
      "Epoch  240, Training Loss: 0.0419, Validation Loss: 0.0719, Learning Rate: 0.152000\n",
      "Epoch  241, Training Loss: 0.0418, Validation Loss: 0.0724, Learning Rate: 0.151800\n",
      "Epoch  242, Training Loss: 0.0417, Validation Loss: 0.0722, Learning Rate: 0.151600\n",
      "Epoch  243, Training Loss: 0.0416, Validation Loss: 0.0712, Learning Rate: 0.151400\n",
      "Epoch  244, Training Loss: 0.0417, Validation Loss: 0.0706, Learning Rate: 0.151200\n",
      "Epoch  245, Training Loss: 0.0414, Validation Loss: 0.0708, Learning Rate: 0.151000\n",
      "Epoch  246, Training Loss: 0.0413, Validation Loss: 0.0705, Learning Rate: 0.150800\n",
      "Epoch  247, Training Loss: 0.0411, Validation Loss: 0.0703, Learning Rate: 0.150600\n",
      "Epoch  248, Training Loss: 0.0411, Validation Loss: 0.0700, Learning Rate: 0.150400\n",
      "Epoch  249, Training Loss: 0.0409, Validation Loss: 0.0700, Learning Rate: 0.150200\n",
      "Epoch  250, Training Loss: 0.0408, Validation Loss: 0.0706, Learning Rate: 0.150000\n",
      "Epoch  251, Training Loss: 0.0408, Validation Loss: 0.0707, Learning Rate: 0.149800\n",
      "Epoch  252, Training Loss: 0.0406, Validation Loss: 0.0697, Learning Rate: 0.149600\n",
      "Epoch  253, Training Loss: 0.0407, Validation Loss: 0.0687, Learning Rate: 0.149400\n",
      "Epoch  254, Training Loss: 0.0404, Validation Loss: 0.0694, Learning Rate: 0.149200\n",
      "Epoch  255, Training Loss: 0.0404, Validation Loss: 0.0688, Learning Rate: 0.149000\n",
      "Epoch  256, Training Loss: 0.0403, Validation Loss: 0.0686, Learning Rate: 0.148800\n",
      "Epoch  257, Training Loss: 0.0401, Validation Loss: 0.0683, Learning Rate: 0.148600\n",
      "Epoch  258, Training Loss: 0.0402, Validation Loss: 0.0694, Learning Rate: 0.148400\n",
      "Epoch  259, Training Loss: 0.0400, Validation Loss: 0.0680, Learning Rate: 0.148200\n",
      "Epoch  260, Training Loss: 0.0399, Validation Loss: 0.0682, Learning Rate: 0.148000\n",
      "Epoch  261, Training Loss: 0.0398, Validation Loss: 0.0679, Learning Rate: 0.147800\n",
      "Epoch  262, Training Loss: 0.0397, Validation Loss: 0.0676, Learning Rate: 0.147600\n",
      "Epoch  263, Training Loss: 0.0396, Validation Loss: 0.0676, Learning Rate: 0.147400\n",
      "Epoch  264, Training Loss: 0.0396, Validation Loss: 0.0680, Learning Rate: 0.147200\n",
      "Epoch  265, Training Loss: 0.0395, Validation Loss: 0.0680, Learning Rate: 0.147000\n",
      "Epoch  266, Training Loss: 0.0394, Validation Loss: 0.0669, Learning Rate: 0.146800\n",
      "Epoch  267, Training Loss: 0.0393, Validation Loss: 0.0666, Learning Rate: 0.146600\n",
      "Epoch  268, Training Loss: 0.0392, Validation Loss: 0.0662, Learning Rate: 0.146400\n",
      "Epoch  269, Training Loss: 0.0391, Validation Loss: 0.0660, Learning Rate: 0.146200\n",
      "Epoch  270, Training Loss: 0.0390, Validation Loss: 0.0661, Learning Rate: 0.146000\n",
      "Epoch  271, Training Loss: 0.0391, Validation Loss: 0.0653, Learning Rate: 0.145800\n",
      "Epoch  272, Training Loss: 0.0389, Validation Loss: 0.0655, Learning Rate: 0.145600\n",
      "Epoch  273, Training Loss: 0.0388, Validation Loss: 0.0657, Learning Rate: 0.145400\n",
      "Epoch  274, Training Loss: 0.0387, Validation Loss: 0.0657, Learning Rate: 0.145200\n",
      "Epoch  275, Training Loss: 0.0387, Validation Loss: 0.0653, Learning Rate: 0.145000\n",
      "Epoch  276, Training Loss: 0.0386, Validation Loss: 0.0651, Learning Rate: 0.144800\n",
      "Epoch  277, Training Loss: 0.0385, Validation Loss: 0.0651, Learning Rate: 0.144600\n",
      "Epoch  278, Training Loss: 0.0385, Validation Loss: 0.0653, Learning Rate: 0.144400\n",
      "Epoch  279, Training Loss: 0.0384, Validation Loss: 0.0649, Learning Rate: 0.144200\n",
      "Epoch  280, Training Loss: 0.0383, Validation Loss: 0.0644, Learning Rate: 0.144000\n",
      "Epoch  281, Training Loss: 0.0383, Validation Loss: 0.0644, Learning Rate: 0.143800\n",
      "Epoch  282, Training Loss: 0.0382, Validation Loss: 0.0643, Learning Rate: 0.143600\n",
      "Epoch  283, Training Loss: 0.0381, Validation Loss: 0.0637, Learning Rate: 0.143400\n",
      "Epoch  284, Training Loss: 0.0381, Validation Loss: 0.0641, Learning Rate: 0.143200\n",
      "Epoch  285, Training Loss: 0.0380, Validation Loss: 0.0642, Learning Rate: 0.143000\n",
      "Epoch  286, Training Loss: 0.0380, Validation Loss: 0.0640, Learning Rate: 0.142800\n",
      "Epoch  287, Training Loss: 0.0380, Validation Loss: 0.0628, Learning Rate: 0.142600\n",
      "Epoch  288, Training Loss: 0.0378, Validation Loss: 0.0631, Learning Rate: 0.142400\n",
      "Epoch  289, Training Loss: 0.0377, Validation Loss: 0.0630, Learning Rate: 0.142200\n",
      "Epoch  290, Training Loss: 0.0378, Validation Loss: 0.0637, Learning Rate: 0.142000\n",
      "Epoch  291, Training Loss: 0.0376, Validation Loss: 0.0628, Learning Rate: 0.141800\n",
      "Epoch  292, Training Loss: 0.0376, Validation Loss: 0.0627, Learning Rate: 0.141600\n",
      "Epoch  293, Training Loss: 0.0375, Validation Loss: 0.0624, Learning Rate: 0.141400\n",
      "Epoch  294, Training Loss: 0.0375, Validation Loss: 0.0624, Learning Rate: 0.141200\n",
      "Epoch  295, Training Loss: 0.0374, Validation Loss: 0.0625, Learning Rate: 0.141000\n",
      "Epoch  296, Training Loss: 0.0374, Validation Loss: 0.0622, Learning Rate: 0.140800\n",
      "Epoch  297, Training Loss: 0.0373, Validation Loss: 0.0616, Learning Rate: 0.140600\n",
      "Epoch  298, Training Loss: 0.0373, Validation Loss: 0.0618, Learning Rate: 0.140400\n",
      "Epoch  299, Training Loss: 0.0372, Validation Loss: 0.0615, Learning Rate: 0.140200\n",
      "Epoch  300, Training Loss: 0.0372, Validation Loss: 0.0612, Learning Rate: 0.140000\n",
      "Epoch  301, Training Loss: 0.0371, Validation Loss: 0.0612, Learning Rate: 0.139800\n",
      "Epoch  302, Training Loss: 0.0371, Validation Loss: 0.0616, Learning Rate: 0.139600\n",
      "Epoch  303, Training Loss: 0.0370, Validation Loss: 0.0615, Learning Rate: 0.139400\n",
      "Epoch  304, Training Loss: 0.0370, Validation Loss: 0.0609, Learning Rate: 0.139200\n",
      "Epoch  305, Training Loss: 0.0369, Validation Loss: 0.0609, Learning Rate: 0.139000\n",
      "Epoch  306, Training Loss: 0.0369, Validation Loss: 0.0612, Learning Rate: 0.138800\n",
      "Epoch  307, Training Loss: 0.0368, Validation Loss: 0.0604, Learning Rate: 0.138600\n",
      "Epoch  308, Training Loss: 0.0368, Validation Loss: 0.0605, Learning Rate: 0.138400\n",
      "Epoch  309, Training Loss: 0.0368, Validation Loss: 0.0600, Learning Rate: 0.138200\n",
      "Epoch  310, Training Loss: 0.0367, Validation Loss: 0.0600, Learning Rate: 0.138000\n",
      "Epoch  311, Training Loss: 0.0366, Validation Loss: 0.0602, Learning Rate: 0.137800\n",
      "Epoch  312, Training Loss: 0.0366, Validation Loss: 0.0601, Learning Rate: 0.137600\n",
      "Epoch  313, Training Loss: 0.0365, Validation Loss: 0.0599, Learning Rate: 0.137400\n",
      "Epoch  314, Training Loss: 0.0365, Validation Loss: 0.0599, Learning Rate: 0.137200\n",
      "Epoch  315, Training Loss: 0.0365, Validation Loss: 0.0599, Learning Rate: 0.137000\n",
      "Epoch  316, Training Loss: 0.0365, Validation Loss: 0.0600, Learning Rate: 0.136800\n",
      "Epoch  317, Training Loss: 0.0364, Validation Loss: 0.0599, Learning Rate: 0.136600\n",
      "Epoch  318, Training Loss: 0.0364, Validation Loss: 0.0597, Learning Rate: 0.136400\n",
      "Epoch  319, Training Loss: 0.0363, Validation Loss: 0.0595, Learning Rate: 0.136200\n",
      "Epoch  320, Training Loss: 0.0363, Validation Loss: 0.0590, Learning Rate: 0.136000\n",
      "Epoch  321, Training Loss: 0.0362, Validation Loss: 0.0589, Learning Rate: 0.135800\n",
      "Epoch  322, Training Loss: 0.0362, Validation Loss: 0.0585, Learning Rate: 0.135600\n",
      "Epoch  323, Training Loss: 0.0362, Validation Loss: 0.0583, Learning Rate: 0.135400\n",
      "Epoch  324, Training Loss: 0.0361, Validation Loss: 0.0589, Learning Rate: 0.135200\n",
      "Epoch  325, Training Loss: 0.0361, Validation Loss: 0.0589, Learning Rate: 0.135000\n",
      "Epoch  326, Training Loss: 0.0360, Validation Loss: 0.0584, Learning Rate: 0.134800\n",
      "Epoch  327, Training Loss: 0.0360, Validation Loss: 0.0586, Learning Rate: 0.134600\n",
      "Epoch  328, Training Loss: 0.0360, Validation Loss: 0.0582, Learning Rate: 0.134400\n",
      "Epoch  329, Training Loss: 0.0359, Validation Loss: 0.0581, Learning Rate: 0.134200\n",
      "Epoch  330, Training Loss: 0.0359, Validation Loss: 0.0580, Learning Rate: 0.134000\n",
      "Epoch  331, Training Loss: 0.0359, Validation Loss: 0.0577, Learning Rate: 0.133800\n",
      "Epoch  332, Training Loss: 0.0358, Validation Loss: 0.0579, Learning Rate: 0.133600\n",
      "Epoch  333, Training Loss: 0.0358, Validation Loss: 0.0577, Learning Rate: 0.133400\n",
      "Epoch  334, Training Loss: 0.0358, Validation Loss: 0.0578, Learning Rate: 0.133200\n",
      "Epoch  335, Training Loss: 0.0358, Validation Loss: 0.0572, Learning Rate: 0.133000\n",
      "Epoch  336, Training Loss: 0.0357, Validation Loss: 0.0575, Learning Rate: 0.132800\n",
      "Epoch  337, Training Loss: 0.0357, Validation Loss: 0.0574, Learning Rate: 0.132600\n",
      "Epoch  338, Training Loss: 0.0357, Validation Loss: 0.0570, Learning Rate: 0.132400\n",
      "Epoch  339, Training Loss: 0.0356, Validation Loss: 0.0573, Learning Rate: 0.132200\n",
      "Epoch  340, Training Loss: 0.0356, Validation Loss: 0.0569, Learning Rate: 0.132000\n",
      "Epoch  341, Training Loss: 0.0356, Validation Loss: 0.0576, Learning Rate: 0.131800\n",
      "Epoch  342, Training Loss: 0.0356, Validation Loss: 0.0568, Learning Rate: 0.131600\n",
      "Epoch  343, Training Loss: 0.0355, Validation Loss: 0.0566, Learning Rate: 0.131400\n",
      "Epoch  344, Training Loss: 0.0355, Validation Loss: 0.0568, Learning Rate: 0.131200\n",
      "Epoch  345, Training Loss: 0.0355, Validation Loss: 0.0565, Learning Rate: 0.131000\n",
      "Epoch  346, Training Loss: 0.0354, Validation Loss: 0.0564, Learning Rate: 0.130800\n",
      "Epoch  347, Training Loss: 0.0354, Validation Loss: 0.0565, Learning Rate: 0.130600\n",
      "Epoch  348, Training Loss: 0.0354, Validation Loss: 0.0562, Learning Rate: 0.130400\n",
      "Epoch  349, Training Loss: 0.0354, Validation Loss: 0.0564, Learning Rate: 0.130200\n",
      "Epoch  350, Training Loss: 0.0353, Validation Loss: 0.0560, Learning Rate: 0.130000\n",
      "Epoch  351, Training Loss: 0.0353, Validation Loss: 0.0562, Learning Rate: 0.129800\n",
      "Epoch  352, Training Loss: 0.0353, Validation Loss: 0.0557, Learning Rate: 0.129600\n",
      "Epoch  353, Training Loss: 0.0353, Validation Loss: 0.0560, Learning Rate: 0.129400\n",
      "Epoch  354, Training Loss: 0.0352, Validation Loss: 0.0558, Learning Rate: 0.129200\n",
      "Epoch  355, Training Loss: 0.0352, Validation Loss: 0.0560, Learning Rate: 0.129000\n",
      "Epoch  356, Training Loss: 0.0352, Validation Loss: 0.0554, Learning Rate: 0.128800\n",
      "Epoch  357, Training Loss: 0.0353, Validation Loss: 0.0550, Learning Rate: 0.128600\n",
      "Epoch  358, Training Loss: 0.0352, Validation Loss: 0.0553, Learning Rate: 0.128400\n",
      "Epoch  359, Training Loss: 0.0351, Validation Loss: 0.0551, Learning Rate: 0.128200\n",
      "Epoch  360, Training Loss: 0.0351, Validation Loss: 0.0551, Learning Rate: 0.128000\n",
      "Epoch  361, Training Loss: 0.0351, Validation Loss: 0.0549, Learning Rate: 0.127800\n",
      "Epoch  362, Training Loss: 0.0351, Validation Loss: 0.0551, Learning Rate: 0.127600\n",
      "Epoch  363, Training Loss: 0.0351, Validation Loss: 0.0554, Learning Rate: 0.127400\n",
      "Epoch  364, Training Loss: 0.0350, Validation Loss: 0.0549, Learning Rate: 0.127200\n",
      "Epoch  365, Training Loss: 0.0350, Validation Loss: 0.0551, Learning Rate: 0.127000\n",
      "Epoch  366, Training Loss: 0.0350, Validation Loss: 0.0548, Learning Rate: 0.126800\n",
      "Epoch  367, Training Loss: 0.0350, Validation Loss: 0.0551, Learning Rate: 0.126600\n",
      "Epoch  368, Training Loss: 0.0350, Validation Loss: 0.0549, Learning Rate: 0.126400\n",
      "Epoch  369, Training Loss: 0.0349, Validation Loss: 0.0549, Learning Rate: 0.126200\n",
      "Epoch  370, Training Loss: 0.0349, Validation Loss: 0.0548, Learning Rate: 0.126000\n",
      "Epoch  371, Training Loss: 0.0349, Validation Loss: 0.0545, Learning Rate: 0.125800\n",
      "Epoch  372, Training Loss: 0.0349, Validation Loss: 0.0543, Learning Rate: 0.125600\n",
      "Epoch  373, Training Loss: 0.0349, Validation Loss: 0.0544, Learning Rate: 0.125400\n",
      "Epoch  374, Training Loss: 0.0348, Validation Loss: 0.0541, Learning Rate: 0.125200\n",
      "Epoch  375, Training Loss: 0.0348, Validation Loss: 0.0545, Learning Rate: 0.125000\n",
      "Epoch  376, Training Loss: 0.0348, Validation Loss: 0.0542, Learning Rate: 0.124800\n",
      "Epoch  377, Training Loss: 0.0348, Validation Loss: 0.0542, Learning Rate: 0.124600\n",
      "Epoch  378, Training Loss: 0.0348, Validation Loss: 0.0541, Learning Rate: 0.124400\n",
      "Epoch  379, Training Loss: 0.0348, Validation Loss: 0.0540, Learning Rate: 0.124200\n",
      "Epoch  380, Training Loss: 0.0348, Validation Loss: 0.0536, Learning Rate: 0.124000\n",
      "Epoch  381, Training Loss: 0.0347, Validation Loss: 0.0535, Learning Rate: 0.123800\n",
      "Epoch  382, Training Loss: 0.0347, Validation Loss: 0.0535, Learning Rate: 0.123600\n",
      "Epoch  383, Training Loss: 0.0347, Validation Loss: 0.0536, Learning Rate: 0.123400\n",
      "Epoch  384, Training Loss: 0.0347, Validation Loss: 0.0539, Learning Rate: 0.123200\n",
      "Epoch  385, Training Loss: 0.0347, Validation Loss: 0.0535, Learning Rate: 0.123000\n",
      "Epoch  386, Training Loss: 0.0347, Validation Loss: 0.0536, Learning Rate: 0.122800\n",
      "Epoch  387, Training Loss: 0.0347, Validation Loss: 0.0538, Learning Rate: 0.122600\n",
      "Epoch  388, Training Loss: 0.0346, Validation Loss: 0.0537, Learning Rate: 0.122400\n",
      "Epoch  389, Training Loss: 0.0346, Validation Loss: 0.0535, Learning Rate: 0.122200\n",
      "Epoch  390, Training Loss: 0.0346, Validation Loss: 0.0532, Learning Rate: 0.122000\n",
      "Epoch  391, Training Loss: 0.0346, Validation Loss: 0.0534, Learning Rate: 0.121800\n",
      "Epoch  392, Training Loss: 0.0346, Validation Loss: 0.0529, Learning Rate: 0.121600\n",
      "Epoch  393, Training Loss: 0.0346, Validation Loss: 0.0531, Learning Rate: 0.121400\n",
      "Epoch  394, Training Loss: 0.0345, Validation Loss: 0.0530, Learning Rate: 0.121200\n",
      "Epoch  395, Training Loss: 0.0345, Validation Loss: 0.0530, Learning Rate: 0.121000\n",
      "Epoch  396, Training Loss: 0.0345, Validation Loss: 0.0530, Learning Rate: 0.120800\n",
      "Epoch  397, Training Loss: 0.0345, Validation Loss: 0.0530, Learning Rate: 0.120600\n",
      "Epoch  398, Training Loss: 0.0345, Validation Loss: 0.0528, Learning Rate: 0.120400\n",
      "Epoch  399, Training Loss: 0.0345, Validation Loss: 0.0526, Learning Rate: 0.120200\n",
      "Epoch  400, Training Loss: 0.0345, Validation Loss: 0.0526, Learning Rate: 0.120000\n",
      "Epoch  401, Training Loss: 0.0345, Validation Loss: 0.0527, Learning Rate: 0.119800\n",
      "Epoch  402, Training Loss: 0.0344, Validation Loss: 0.0526, Learning Rate: 0.119600\n",
      "Epoch  403, Training Loss: 0.0344, Validation Loss: 0.0527, Learning Rate: 0.119400\n",
      "Epoch  404, Training Loss: 0.0344, Validation Loss: 0.0523, Learning Rate: 0.119200\n",
      "Epoch  405, Training Loss: 0.0344, Validation Loss: 0.0522, Learning Rate: 0.119000\n",
      "Epoch  406, Training Loss: 0.0344, Validation Loss: 0.0521, Learning Rate: 0.118800\n",
      "Epoch  407, Training Loss: 0.0344, Validation Loss: 0.0520, Learning Rate: 0.118600\n",
      "Epoch  408, Training Loss: 0.0344, Validation Loss: 0.0521, Learning Rate: 0.118400\n",
      "Epoch  409, Training Loss: 0.0344, Validation Loss: 0.0520, Learning Rate: 0.118200\n",
      "Epoch  410, Training Loss: 0.0344, Validation Loss: 0.0520, Learning Rate: 0.118000\n",
      "Epoch  411, Training Loss: 0.0343, Validation Loss: 0.0520, Learning Rate: 0.117800\n",
      "Epoch  412, Training Loss: 0.0344, Validation Loss: 0.0517, Learning Rate: 0.117600\n",
      "Epoch  413, Training Loss: 0.0343, Validation Loss: 0.0518, Learning Rate: 0.117400\n",
      "Epoch  414, Training Loss: 0.0343, Validation Loss: 0.0519, Learning Rate: 0.117200\n",
      "Epoch  415, Training Loss: 0.0343, Validation Loss: 0.0518, Learning Rate: 0.117000\n",
      "Epoch  416, Training Loss: 0.0343, Validation Loss: 0.0517, Learning Rate: 0.116800\n",
      "Epoch  417, Training Loss: 0.0343, Validation Loss: 0.0518, Learning Rate: 0.116600\n",
      "Epoch  418, Training Loss: 0.0343, Validation Loss: 0.0515, Learning Rate: 0.116400\n",
      "Epoch  419, Training Loss: 0.0343, Validation Loss: 0.0515, Learning Rate: 0.116200\n",
      "Epoch  420, Training Loss: 0.0343, Validation Loss: 0.0514, Learning Rate: 0.116000\n",
      "Epoch  421, Training Loss: 0.0343, Validation Loss: 0.0514, Learning Rate: 0.115800\n",
      "Epoch  422, Training Loss: 0.0343, Validation Loss: 0.0513, Learning Rate: 0.115600\n",
      "Epoch  423, Training Loss: 0.0342, Validation Loss: 0.0512, Learning Rate: 0.115400\n",
      "Epoch  424, Training Loss: 0.0342, Validation Loss: 0.0512, Learning Rate: 0.115200\n",
      "Epoch  425, Training Loss: 0.0342, Validation Loss: 0.0513, Learning Rate: 0.115000\n",
      "Epoch  426, Training Loss: 0.0342, Validation Loss: 0.0512, Learning Rate: 0.114800\n",
      "Epoch  427, Training Loss: 0.0342, Validation Loss: 0.0510, Learning Rate: 0.114600\n",
      "Epoch  428, Training Loss: 0.0342, Validation Loss: 0.0511, Learning Rate: 0.114400\n",
      "Epoch  429, Training Loss: 0.0342, Validation Loss: 0.0511, Learning Rate: 0.114200\n",
      "Epoch  430, Training Loss: 0.0342, Validation Loss: 0.0509, Learning Rate: 0.114000\n",
      "Epoch  431, Training Loss: 0.0342, Validation Loss: 0.0510, Learning Rate: 0.113800\n",
      "Epoch  432, Training Loss: 0.0342, Validation Loss: 0.0509, Learning Rate: 0.113600\n",
      "Epoch  433, Training Loss: 0.0342, Validation Loss: 0.0509, Learning Rate: 0.113400\n",
      "Epoch  434, Training Loss: 0.0342, Validation Loss: 0.0508, Learning Rate: 0.113200\n",
      "Epoch  435, Training Loss: 0.0342, Validation Loss: 0.0509, Learning Rate: 0.113000\n",
      "Epoch  436, Training Loss: 0.0341, Validation Loss: 0.0508, Learning Rate: 0.112800\n",
      "Epoch  437, Training Loss: 0.0341, Validation Loss: 0.0509, Learning Rate: 0.112600\n",
      "Epoch  438, Training Loss: 0.0341, Validation Loss: 0.0507, Learning Rate: 0.112400\n",
      "Epoch  439, Training Loss: 0.0341, Validation Loss: 0.0508, Learning Rate: 0.112200\n",
      "Epoch  440, Training Loss: 0.0341, Validation Loss: 0.0508, Learning Rate: 0.112000\n",
      "Epoch  441, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.111800\n",
      "Epoch  442, Training Loss: 0.0341, Validation Loss: 0.0508, Learning Rate: 0.111600\n",
      "Epoch  443, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.111400\n",
      "Epoch  444, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.111200\n",
      "Epoch  445, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.111000\n",
      "Epoch  446, Training Loss: 0.0341, Validation Loss: 0.0504, Learning Rate: 0.110800\n",
      "Epoch  447, Training Loss: 0.0341, Validation Loss: 0.0503, Learning Rate: 0.110600\n",
      "Epoch  448, Training Loss: 0.0341, Validation Loss: 0.0502, Learning Rate: 0.110400\n",
      "Epoch  449, Training Loss: 0.0341, Validation Loss: 0.0502, Learning Rate: 0.110200\n",
      "Epoch  450, Training Loss: 0.0341, Validation Loss: 0.0503, Learning Rate: 0.110000\n",
      "Epoch  451, Training Loss: 0.0341, Validation Loss: 0.0502, Learning Rate: 0.109800\n",
      "Epoch  452, Training Loss: 0.0341, Validation Loss: 0.0502, Learning Rate: 0.109600\n",
      "Epoch  453, Training Loss: 0.0341, Validation Loss: 0.0500, Learning Rate: 0.109400\n",
      "Epoch  454, Training Loss: 0.0341, Validation Loss: 0.0499, Learning Rate: 0.109200\n",
      "Epoch  455, Training Loss: 0.0340, Validation Loss: 0.0500, Learning Rate: 0.109000\n",
      "Epoch  456, Training Loss: 0.0340, Validation Loss: 0.0500, Learning Rate: 0.108800\n",
      "Epoch  457, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.108600\n",
      "Epoch  458, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.108400\n",
      "Epoch  459, Training Loss: 0.0340, Validation Loss: 0.0500, Learning Rate: 0.108200\n",
      "Epoch  460, Training Loss: 0.0340, Validation Loss: 0.0498, Learning Rate: 0.108000\n",
      "Epoch  461, Training Loss: 0.0340, Validation Loss: 0.0497, Learning Rate: 0.107800\n",
      "Epoch  462, Training Loss: 0.0340, Validation Loss: 0.0497, Learning Rate: 0.107600\n",
      "Epoch  463, Training Loss: 0.0340, Validation Loss: 0.0498, Learning Rate: 0.107400\n",
      "Epoch  464, Training Loss: 0.0340, Validation Loss: 0.0496, Learning Rate: 0.107200\n",
      "Epoch  465, Training Loss: 0.0340, Validation Loss: 0.0497, Learning Rate: 0.107000\n",
      "Epoch  466, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.106800\n",
      "Epoch  467, Training Loss: 0.0340, Validation Loss: 0.0494, Learning Rate: 0.106600\n",
      "Epoch  468, Training Loss: 0.0340, Validation Loss: 0.0496, Learning Rate: 0.106400\n",
      "Epoch  469, Training Loss: 0.0340, Validation Loss: 0.0496, Learning Rate: 0.106200\n",
      "Epoch  470, Training Loss: 0.0340, Validation Loss: 0.0496, Learning Rate: 0.106000\n",
      "Epoch  471, Training Loss: 0.0340, Validation Loss: 0.0494, Learning Rate: 0.105800\n",
      "Epoch  472, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.105600\n",
      "Epoch  473, Training Loss: 0.0340, Validation Loss: 0.0494, Learning Rate: 0.105400\n",
      "Epoch  474, Training Loss: 0.0340, Validation Loss: 0.0493, Learning Rate: 0.105200\n",
      "Epoch  475, Training Loss: 0.0340, Validation Loss: 0.0493, Learning Rate: 0.105000\n",
      "Epoch  476, Training Loss: 0.0340, Validation Loss: 0.0491, Learning Rate: 0.104800\n",
      "Epoch  477, Training Loss: 0.0340, Validation Loss: 0.0491, Learning Rate: 0.104600\n",
      "Epoch  478, Training Loss: 0.0340, Validation Loss: 0.0491, Learning Rate: 0.104400\n",
      "Epoch  479, Training Loss: 0.0340, Validation Loss: 0.0489, Learning Rate: 0.104200\n",
      "Epoch  480, Training Loss: 0.0340, Validation Loss: 0.0490, Learning Rate: 0.104000\n",
      "Epoch  481, Training Loss: 0.0340, Validation Loss: 0.0490, Learning Rate: 0.103800\n",
      "Epoch  482, Training Loss: 0.0340, Validation Loss: 0.0491, Learning Rate: 0.103600\n",
      "Epoch  483, Training Loss: 0.0340, Validation Loss: 0.0491, Learning Rate: 0.103400\n",
      "Epoch  484, Training Loss: 0.0339, Validation Loss: 0.0490, Learning Rate: 0.103200\n",
      "Epoch  485, Training Loss: 0.0340, Validation Loss: 0.0490, Learning Rate: 0.103000\n",
      "Epoch  486, Training Loss: 0.0339, Validation Loss: 0.0489, Learning Rate: 0.102800\n",
      "Epoch  487, Training Loss: 0.0339, Validation Loss: 0.0488, Learning Rate: 0.102600\n",
      "Epoch  488, Training Loss: 0.0340, Validation Loss: 0.0490, Learning Rate: 0.102400\n",
      "Epoch  489, Training Loss: 0.0339, Validation Loss: 0.0489, Learning Rate: 0.102200\n",
      "Epoch  490, Training Loss: 0.0339, Validation Loss: 0.0487, Learning Rate: 0.102000\n",
      "Epoch  491, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.101800\n",
      "Epoch  492, Training Loss: 0.0339, Validation Loss: 0.0487, Learning Rate: 0.101600\n",
      "Epoch  493, Training Loss: 0.0339, Validation Loss: 0.0487, Learning Rate: 0.101400\n",
      "Epoch  494, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.101200\n",
      "Epoch  495, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.101000\n",
      "Epoch  496, Training Loss: 0.0339, Validation Loss: 0.0485, Learning Rate: 0.100800\n",
      "Epoch  497, Training Loss: 0.0339, Validation Loss: 0.0485, Learning Rate: 0.100600\n",
      "Epoch  498, Training Loss: 0.0339, Validation Loss: 0.0485, Learning Rate: 0.100400\n",
      "Epoch  499, Training Loss: 0.0339, Validation Loss: 0.0485, Learning Rate: 0.100200\n",
      "Epoch  500, Training Loss: 0.0339, Validation Loss: 0.0485, Learning Rate: 0.100000\n",
      "Epoch  501, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.099800\n",
      "Epoch  502, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.099600\n",
      "Epoch  503, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.099400\n",
      "Epoch  504, Training Loss: 0.0339, Validation Loss: 0.0484, Learning Rate: 0.099200\n",
      "Epoch  505, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.099000\n",
      "Epoch  506, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.098800\n",
      "Epoch  507, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.098600\n",
      "Epoch  508, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.098400\n",
      "Epoch  509, Training Loss: 0.0339, Validation Loss: 0.0482, Learning Rate: 0.098200\n",
      "Epoch  510, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.098000\n",
      "Epoch  511, Training Loss: 0.0339, Validation Loss: 0.0484, Learning Rate: 0.097800\n",
      "Epoch  512, Training Loss: 0.0339, Validation Loss: 0.0482, Learning Rate: 0.097600\n",
      "Epoch  513, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.097400\n",
      "Epoch  514, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.097200\n",
      "Epoch  515, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.097000\n",
      "Epoch  516, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.096800\n",
      "Epoch  517, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.096600\n",
      "Epoch  518, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.096400\n",
      "Epoch  519, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.096200\n",
      "Epoch  520, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.096000\n",
      "Epoch  521, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.095800\n",
      "Epoch  522, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.095600\n",
      "Epoch  523, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.095400\n",
      "Epoch  524, Training Loss: 0.0339, Validation Loss: 0.0479, Learning Rate: 0.095200\n",
      "Epoch  525, Training Loss: 0.0339, Validation Loss: 0.0479, Learning Rate: 0.095000\n",
      "Epoch  526, Training Loss: 0.0339, Validation Loss: 0.0479, Learning Rate: 0.094800\n",
      "Epoch  527, Training Loss: 0.0339, Validation Loss: 0.0477, Learning Rate: 0.094600\n",
      "Epoch  528, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.094400\n",
      "Epoch  529, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.094200\n",
      "Epoch  530, Training Loss: 0.0339, Validation Loss: 0.0477, Learning Rate: 0.094000\n",
      "Epoch  531, Training Loss: 0.0339, Validation Loss: 0.0477, Learning Rate: 0.093800\n",
      "Epoch  532, Training Loss: 0.0339, Validation Loss: 0.0477, Learning Rate: 0.093600\n",
      "Epoch  533, Training Loss: 0.0339, Validation Loss: 0.0477, Learning Rate: 0.093400\n",
      "Epoch  534, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.093200\n",
      "Epoch  535, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.093000\n",
      "Epoch  536, Training Loss: 0.0339, Validation Loss: 0.0475, Learning Rate: 0.092800\n",
      "Epoch  537, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.092600\n",
      "Epoch  538, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.092400\n",
      "Epoch  539, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.092200\n",
      "Epoch  540, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.092000\n",
      "Epoch  541, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.091800\n",
      "Epoch  542, Training Loss: 0.0339, Validation Loss: 0.0475, Learning Rate: 0.091600\n",
      "Epoch  543, Training Loss: 0.0339, Validation Loss: 0.0475, Learning Rate: 0.091400\n",
      "Epoch  544, Training Loss: 0.0339, Validation Loss: 0.0474, Learning Rate: 0.091200\n",
      "Epoch  545, Training Loss: 0.0339, Validation Loss: 0.0474, Learning Rate: 0.091000\n",
      "Epoch  546, Training Loss: 0.0339, Validation Loss: 0.0474, Learning Rate: 0.090800\n",
      "Epoch  547, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.090600\n",
      "Epoch  548, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.090400\n",
      "Epoch  549, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.090200\n",
      "Epoch  550, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.090000\n",
      "Epoch  551, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.089800\n",
      "Epoch  552, Training Loss: 0.0339, Validation Loss: 0.0472, Learning Rate: 0.089600\n",
      "Epoch  553, Training Loss: 0.0339, Validation Loss: 0.0472, Learning Rate: 0.089400\n",
      "Epoch  554, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.089200\n",
      "Epoch  555, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.089000\n",
      "Epoch  556, Training Loss: 0.0339, Validation Loss: 0.0472, Learning Rate: 0.088800\n",
      "Epoch  557, Training Loss: 0.0339, Validation Loss: 0.0472, Learning Rate: 0.088600\n",
      "Epoch  558, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.088400\n",
      "Epoch  559, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.088200\n",
      "Epoch  560, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.088000\n",
      "Epoch  561, Training Loss: 0.0339, Validation Loss: 0.0470, Learning Rate: 0.087800\n",
      "Epoch  562, Training Loss: 0.0339, Validation Loss: 0.0470, Learning Rate: 0.087600\n",
      "Epoch  563, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.087400\n",
      "Epoch  564, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.087200\n",
      "Epoch  565, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.087000\n",
      "Epoch  566, Training Loss: 0.0339, Validation Loss: 0.0470, Learning Rate: 0.086800\n",
      "Epoch  567, Training Loss: 0.0339, Validation Loss: 0.0469, Learning Rate: 0.086600\n",
      "Epoch  568, Training Loss: 0.0339, Validation Loss: 0.0469, Learning Rate: 0.086400\n",
      "Epoch  569, Training Loss: 0.0339, Validation Loss: 0.0469, Learning Rate: 0.086200\n",
      "Epoch  570, Training Loss: 0.0339, Validation Loss: 0.0469, Learning Rate: 0.086000\n",
      "Epoch  571, Training Loss: 0.0339, Validation Loss: 0.0469, Learning Rate: 0.085800\n",
      "Epoch  572, Training Loss: 0.0339, Validation Loss: 0.0469, Learning Rate: 0.085600\n",
      "Epoch  573, Training Loss: 0.0339, Validation Loss: 0.0468, Learning Rate: 0.085400\n",
      "Epoch  574, Training Loss: 0.0339, Validation Loss: 0.0468, Learning Rate: 0.085200\n",
      "Epoch  575, Training Loss: 0.0339, Validation Loss: 0.0468, Learning Rate: 0.085000\n",
      "Epoch  576, Training Loss: 0.0339, Validation Loss: 0.0467, Learning Rate: 0.084800\n",
      "Epoch  577, Training Loss: 0.0339, Validation Loss: 0.0467, Learning Rate: 0.084600\n",
      "Epoch  578, Training Loss: 0.0339, Validation Loss: 0.0467, Learning Rate: 0.084400\n",
      "Epoch  579, Training Loss: 0.0339, Validation Loss: 0.0467, Learning Rate: 0.084200\n",
      "Epoch  580, Training Loss: 0.0339, Validation Loss: 0.0467, Learning Rate: 0.084000\n",
      "Epoch  581, Training Loss: 0.0339, Validation Loss: 0.0468, Learning Rate: 0.083800\n",
      "Epoch  582, Training Loss: 0.0339, Validation Loss: 0.0468, Learning Rate: 0.083600\n",
      "Epoch  583, Training Loss: 0.0339, Validation Loss: 0.0468, Learning Rate: 0.083400\n",
      "Epoch  584, Training Loss: 0.0339, Validation Loss: 0.0467, Learning Rate: 0.083200\n",
      "Epoch  585, Training Loss: 0.0339, Validation Loss: 0.0467, Learning Rate: 0.083000\n",
      "Epoch  586, Training Loss: 0.0339, Validation Loss: 0.0466, Learning Rate: 0.082800\n",
      "Early stopping triggered at epoch 586. Restoring best model parameters.\n",
      "\n",
      "Neural Network Classification Accuracy: 0.9907\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "# ============================\n",
    "# Activation functions and their derivatives\n",
    "# ============================\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(z, a):\n",
    "    # Note: z is not used here; kept for uniform signature.\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(z, a):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(z, a):\n",
    "    return np.ones_like(a)\n",
    "\n",
    "activation_functions = {\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"relu\": relu,\n",
    "    \"linear\": linear\n",
    "}\n",
    "\n",
    "activation_derivatives = {\n",
    "    \"sigmoid\": lambda z, a: sigmoid_derivative(z, a),\n",
    "    \"relu\": lambda z, a: relu_derivative(z, a),\n",
    "    \"linear\": lambda z, a: linear_derivative(z, a)\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Loss functions and their derivatives\n",
    "# ============================\n",
    "\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Binary crossentropy loss for binary classification.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_crossentropy_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the binary crossentropy loss.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return - (y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Squared Error (MSE) loss, typically used for regression.\n",
    "    \"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the MSE loss.\n",
    "    \"\"\"\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "def mee_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Euclidean Error (MEE) loss, defined as:\n",
    "        E_MEE = (1/N) * sum over i [ ||y_true[i] - y_pred[i]||_2 ].\n",
    "    \"\"\"\n",
    "    diff = y_true - y_pred  # shape: (N, d) or (N, 1)\n",
    "    # Euclidean distance for each sample\n",
    "    dist = np.sqrt(np.sum(diff ** 2, axis=1))\n",
    "    return np.mean(dist)\n",
    "\n",
    "def mee_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the Mean Euclidean Error (MEE) loss.\n",
    "    For each sample i, derivative wrt y_pred[i] is:\n",
    "        (1/N) * ( (y_pred[i] - y_true[i]) / ||y_pred[i] - y_true[i]||_2 ).\n",
    "    We safely handle the case where the norm is zero.\n",
    "    \"\"\"\n",
    "    diff = y_pred - y_true\n",
    "    dist = np.sqrt(np.sum(diff ** 2, axis=1, keepdims=True))\n",
    "    epsilon = 1e-8  # Avoid division by zero\n",
    "    dist_safe = np.where(dist == 0, epsilon, dist)\n",
    "    N = y_true.shape[0]\n",
    "    derivative = diff / dist_safe / N\n",
    "    return derivative\n",
    "\n",
    "loss_functions = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_loss,\n",
    "    \"mse\": mse_loss,\n",
    "    \"mee\": mee_loss,  \n",
    "}\n",
    "\n",
    "loss_derivatives = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_derivative,\n",
    "    \"mse\": mse_derivative,\n",
    "    \"mee\": mee_derivative, \n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Regularization functions (modular)\n",
    "# ============================\n",
    "\n",
    "def compute_reg_gradient(W, lambda_reg, reg_type, m):\n",
    "    if reg_type == \"l2\":\n",
    "        return lambda_reg * W / m\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * np.sign(W) / m\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_reg_loss(W_list, lambda_reg, reg_type):\n",
    "    if reg_type == \"l2\":\n",
    "        return (lambda_reg / 2) * sum(np.sum(W ** 2) for W in W_list)\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * sum(np.sum(np.abs(W)) for W in W_list)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# ============================\n",
    "# Neural Network Class with Learning Rate Decay, Momentum, Custom Weight Initialization, and Early Stopping\n",
    "# ============================\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.01, lambda_reg=0.001, reg_type=\"l2\",\n",
    "                 loss_function_name=None,\n",
    "                 activation_function_name=\"relu\",\n",
    "                 output_activation_function_name=None,\n",
    "                 activation_function_names=None,\n",
    "                 task=\"classification\",\n",
    "                 lr_decay_type=\"none\",  # Options: \"none\", \"exponential\", \"linear\"\n",
    "                 decay_rate=0.0,\n",
    "                 weight_init=\"base\",  # \"base\" (fan-in scaling) or \"glorot\"\n",
    "                 momentum_type=\"none\",  # Options: \"none\", \"momentum\", \"nesterov momentum\"\n",
    "                 momentum_alpha=0.9):\n",
    "        \"\"\"\n",
    "        :param layers: List containing the size of each layer (input, hidden, output)\n",
    "        :param learning_rate: Initial learning rate\n",
    "        :param lambda_reg: Regularization coefficient\n",
    "        :param reg_type: Type of regularization (\"l2\", \"l1\", or other for none)\n",
    "        :param loss_function_name: Name of the loss function (if None, set based on task)\n",
    "        :param activation_function_name: Activation to use for hidden layers (if activation_function_names not provided)\n",
    "        :param output_activation_function_name: Activation for the output layer (if None, set based on task)\n",
    "        :param activation_function_names: List of activation function names for each layer (length = len(layers)-1)\n",
    "        :param task: \"classification\" or \"regression\"\n",
    "        :param lr_decay_type: Learning rate decay strategy (\"none\", \"exponential\", \"linear\")\n",
    "        :param decay_rate: Decay rate used in the learning rate schedule\n",
    "        :param weight_init: Weight initialization strategy (\"base\" uses fan-in scaling or \"glorot\")\n",
    "        :param momentum_type: Momentum strategy (\"none\", \"momentum\", \"nesterov momentum\")\n",
    "        :param momentum_alpha: Momentum coefficient (e.g., 0.9)\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.reg_type = reg_type\n",
    "        self.task = task\n",
    "        self.lr_decay_type = lr_decay_type\n",
    "        self.decay_rate = decay_rate\n",
    "        self.weight_init = weight_init\n",
    "        \n",
    "        # Set momentum parameters\n",
    "        if momentum_type not in {\"none\", \"momentum\", \"nesterov momentum\"}:\n",
    "            raise ValueError(\"momentum_type must be 'none', 'momentum', or 'nesterov momentum'.\")\n",
    "        self.momentum_type = momentum_type\n",
    "        self.momentum_alpha = momentum_alpha if momentum_type != \"none\" else 0.0\n",
    "        \n",
    "        # Set defaults based on task\n",
    "        if self.task == \"regression\":\n",
    "            self.loss_function_name = loss_function_name or \"mse\"\n",
    "            output_activation_function_name = output_activation_function_name or \"linear\"\n",
    "        else:\n",
    "            # Classification\n",
    "            self.loss_function_name = loss_function_name or \"binary_crossentropy\"\n",
    "            output_activation_function_name = output_activation_function_name or \"sigmoid\"\n",
    "        \n",
    "        # Set activation functions for layers\n",
    "        if activation_function_names is None:\n",
    "            self.activation_function_names = [activation_function_name] * (len(layers) - 1)\n",
    "            self.activation_function_names[-1] = output_activation_function_name\n",
    "        else:\n",
    "            if len(activation_function_names) != len(layers) - 1:\n",
    "                raise ValueError(\"activation_function_names must have length equal to len(layers)-1.\")\n",
    "            self.activation_function_names = activation_function_names\n",
    "        \n",
    "        self._initialize_parameters()\n",
    "        # Initialize momentum accumulators (even if not used, for consistency)\n",
    "        self.vW = [np.zeros_like(W) for W in self.W]\n",
    "        self.vb = [np.zeros_like(b) for b in self.b]\n",
    "        \n",
    "        # Initialize loss history lists (will be (re)initialized in train())\n",
    "        self.train_loss_history = []\n",
    "        self.val_loss_history = None\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            fan_in = self.layers[i]\n",
    "            fan_out = self.layers[i + 1]\n",
    "            if self.weight_init == \"base\":\n",
    "                std = np.sqrt(1.0 / fan_in)\n",
    "            elif self.weight_init == \"glorot\":\n",
    "                std = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported weight initialization strategy. Use 'base' or 'glorot'.\")\n",
    "            weight = np.random.randn(fan_in, fan_out) * std\n",
    "            self.W.append(weight)\n",
    "            self.b.append(np.zeros((1, fan_out)))\n",
    "    \n",
    "    def _apply_activation(self, x, func_name):\n",
    "        if func_name not in activation_functions:\n",
    "            raise ValueError(f\"Unsupported activation: {func_name}\")\n",
    "        return activation_functions[func_name](x)\n",
    "    \n",
    "    def _apply_activation_derivative(self, z, a, func_name):\n",
    "        if func_name not in activation_derivatives:\n",
    "            raise ValueError(f\"Unsupported activation derivative: {func_name}\")\n",
    "        return activation_derivatives[func_name](z, a)\n",
    "    \n",
    "    def _forward(self, X, weights=None, biases=None):\n",
    "        \"\"\"\n",
    "        Forward propagation. If weights and biases are provided, they are used;\n",
    "        otherwise the network's parameters are used.\n",
    "        Returns lists Z (pre-activations) and A (activations).\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.W\n",
    "        if biases is None:\n",
    "            biases = self.b\n",
    "            \n",
    "        A = [X]\n",
    "        Z = []\n",
    "        # Forward through hidden layers\n",
    "        for i in range(len(weights) - 1):\n",
    "            z_curr = np.dot(A[-1], weights[i]) + biases[i]\n",
    "            Z.append(z_curr)\n",
    "            a_curr = self._apply_activation(z_curr, self.activation_function_names[i])\n",
    "            A.append(a_curr)\n",
    "        # Forward through output layer\n",
    "        z_out = np.dot(A[-1], weights[-1]) + biases[-1]\n",
    "        Z.append(z_out)\n",
    "        a_out = self._apply_activation(z_out, self.activation_function_names[-1])\n",
    "        A.append(a_out)\n",
    "        return Z, A\n",
    "    \n",
    "    def _compute_gradients(self, X, y, Z, A, weights=None):\n",
    "        \"\"\"\n",
    "        Compute gradients dW and db given inputs X, target y, pre-activations Z and activations A.\n",
    "        Optionally, a custom set of weights (used in lookahead for Nesterov momentum) can be provided.\n",
    "        Returns lists dW and db.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.W\n",
    "        m = X.shape[0]\n",
    "        dA = loss_derivatives[self.loss_function_name](y, A[-1])\n",
    "        # Output layer\n",
    "        dZ = dA * self._apply_activation_derivative(Z[-1], A[-1], self.activation_function_names[-1])\n",
    "        reg_term = compute_reg_gradient(weights[-1], self.lambda_reg, self.reg_type, m)\n",
    "        dW = [np.dot(A[-2].T, dZ) / m + reg_term]\n",
    "        db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "        \n",
    "        # Backpropagate through hidden layers\n",
    "        for i in range(len(weights) - 2, -1, -1):\n",
    "            dA = np.dot(dZ, weights[i + 1].T)\n",
    "            dZ = dA * self._apply_activation_derivative(Z[i], A[i + 1], self.activation_function_names[i])\n",
    "            reg_term = compute_reg_gradient(weights[i], self.lambda_reg, self.reg_type, m)\n",
    "            dW.insert(0, np.dot(A[i].T, dZ) / m + reg_term)\n",
    "            db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "            \n",
    "        return dW, db\n",
    "    \n",
    "    def train(self, X, y, epochs=300, batch_size=32, verbose=True,\n",
    "              early_stopping=False, validation_data=None, patience=10, min_delta=0.0):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "\n",
    "        The loss histories for training and validation (if provided) are stored in:\n",
    "            self.train_loss_history and self.val_loss_history\n",
    "\n",
    "        :param X: Training data inputs.\n",
    "        :param y: Training data targets.\n",
    "        :param epochs: Maximum number of epochs to train.\n",
    "        :param batch_size: Mini-batch size.\n",
    "        :param verbose: Whether to print progress.\n",
    "        :param early_stopping: Enable early stopping if True.\n",
    "        :param validation_data: Tuple (X_val, y_val) for early stopping and validation loss logging.\n",
    "        :param patience: Number of epochs with no improvement to wait before stopping.\n",
    "        :param min_delta: Minimum change in the monitored loss to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        # Reinitialize loss histories\n",
    "        self.train_loss_history = []\n",
    "        if validation_data is not None:\n",
    "            self.val_loss_history = []\n",
    "        else:\n",
    "            self.val_loss_history = None\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        best_loss = np.inf\n",
    "        patience_counter = 0\n",
    "        best_weights = None\n",
    "        best_biases = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Update learning rate based on decay schedule\n",
    "            if self.lr_decay_type == \"exponential\":\n",
    "                self.learning_rate = self.initial_learning_rate * np.exp(-self.decay_rate * epoch)\n",
    "            elif self.lr_decay_type == \"linear\":\n",
    "                self.learning_rate = self.initial_learning_rate * max(0, 1 - self.decay_rate * epoch)\n",
    "            # Otherwise (\"none\"), keep the initial learning rate.\n",
    "            \n",
    "            # Shuffle training data\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            \n",
    "            # Process mini-batches\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                \n",
    "                if self.momentum_type == \"nesterov momentum\":\n",
    "                    weights_lookahead = [self.W[j] - self.momentum_alpha * self.vW[j] for j in range(len(self.W))]\n",
    "                    biases_lookahead = [self.b[j] - self.momentum_alpha * self.vb[j] for j in range(len(self.b))]\n",
    "                    Z, A = self._forward(X_batch, weights=weights_lookahead, biases=biases_lookahead)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A, weights=weights_lookahead)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.vW[j] = self.momentum_alpha * self.vW[j] + self.learning_rate * dW[j]\n",
    "                        self.vb[j] = self.momentum_alpha * self.vb[j] + self.learning_rate * db[j]\n",
    "                        self.W[j] -= self.vW[j]\n",
    "                        self.b[j] -= self.vb[j]\n",
    "                        \n",
    "                elif self.momentum_type == \"momentum\":\n",
    "                    Z, A = self._forward(X_batch)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.vW[j] = self.momentum_alpha * self.vW[j] + self.learning_rate * dW[j]\n",
    "                        self.vb[j] = self.momentum_alpha * self.vb[j] + self.learning_rate * db[j]\n",
    "                        self.W[j] -= self.vW[j]\n",
    "                        self.b[j] -= self.vb[j]\n",
    "                        \n",
    "                else:  # No momentum\n",
    "                    Z, A = self._forward(X_batch)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.W[j] -= self.learning_rate * dW[j]\n",
    "                        self.b[j] -= self.learning_rate * db[j]\n",
    "            \n",
    "            # Compute training loss\n",
    "            _, A_full = self._forward(X)\n",
    "            train_loss = loss_functions[self.loss_function_name](y, A_full[-1])\n",
    "            reg_loss = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "            total_train_loss = train_loss + reg_loss\n",
    "            self.train_loss_history.append(total_train_loss)\n",
    "            \n",
    "            # Compute validation loss if validation data is provided\n",
    "            if validation_data is not None:\n",
    "                X_val, y_val = validation_data\n",
    "                _, A_val = self._forward(X_val)\n",
    "                val_loss = loss_functions[self.loss_function_name](y_val, A_val[-1])\n",
    "                reg_loss_val = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "                total_val_loss = val_loss + reg_loss_val\n",
    "                self.val_loss_history.append(total_val_loss)\n",
    "            else:\n",
    "                total_val_loss = None\n",
    "\n",
    "            # Verbose logging\n",
    "            if verbose:\n",
    "                if total_val_loss is not None:\n",
    "                    print(f\"Epoch {epoch:4d}, Training Loss: {total_train_loss:.4f}, \"\n",
    "                          f\"Validation Loss: {total_val_loss:.4f}, Learning Rate: {self.learning_rate:.6f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch:4d}, Training Loss: {total_train_loss:.4f}, \"\n",
    "                          f\"Learning Rate: {self.learning_rate:.6f}\")\n",
    "            \n",
    "            # Early stopping check (only if validation data is provided)\n",
    "            if early_stopping and (validation_data is not None):\n",
    "                if total_val_loss < best_loss - min_delta:\n",
    "                    best_loss = total_val_loss\n",
    "                    patience_counter = 0\n",
    "                    best_weights = [w.copy() for w in self.W]\n",
    "                    best_biases = [b.copy() for b in self.b]\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        if verbose:\n",
    "                            print(f\"Early stopping triggered at epoch {epoch}. Restoring best model parameters.\")\n",
    "                        if best_weights is not None:\n",
    "                            self.W = best_weights\n",
    "                            self.b = best_biases\n",
    "                        break\n",
    "\n",
    "    def plot_loss_history(self):\n",
    "        \"\"\"\n",
    "        Plot the training loss history and, if available, the validation loss history.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.train_loss_history, label=\"Training Loss\")\n",
    "        if self.val_loss_history is not None and len(self.val_loss_history) > 0:\n",
    "            plt.plot(self.val_loss_history, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss History\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, A = self._forward(X)\n",
    "        output = A[-1]\n",
    "        if self.task == \"classification\":\n",
    "            if output.shape[1] == 1:\n",
    "                return (output > 0.5).astype(int)\n",
    "            else:\n",
    "                return np.argmax(output, axis=1)\n",
    "        else:\n",
    "            return output\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        if self.task == \"regression\":\n",
    "            return mse_loss(y, predictions)\n",
    "        else:\n",
    "            if y.ndim > 1 and y.shape[1] > 1:\n",
    "                y_true = np.argmax(y, axis=1)\n",
    "            else:\n",
    "                y_true = y\n",
    "            return np.mean(predictions == y_true)\n",
    "\n",
    "# ============================\n",
    "# Testing on a monk's dataset\n",
    "# ============================\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(1, one_hot_encode=True)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_units = 10\n",
    "output_size = 1  # binary classification\n",
    "layers = [input_size, hidden_units, output_size]\n",
    "\n",
    "# Define activation functions for hidden and output layers\n",
    "activation_funcs = [\"relu\", \"sigmoid\"]\n",
    "\n",
    "nn_clf = NeuralNetwork(\n",
    "    layers=layers,\n",
    "    learning_rate=0.2,\n",
    "    lambda_reg=0.001,\n",
    "    reg_type=\"l2\",\n",
    "    loss_function_name=\"mse\",       \n",
    "    activation_function_names=activation_funcs,\n",
    "    task=\"classification\",\n",
    "    lr_decay_type=\"linear\",    # Options: \"exponential\", \"linear\", or \"none\"\n",
    "    decay_rate=0.001,\n",
    "    weight_init=\"base\",        # \"base\" or \"glorot\"\n",
    ")\n",
    "\n",
    "# For early stopping, we provide validation data.\n",
    "nn_clf.train(\n",
    "    X_train, y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    verbose=True,\n",
    "    early_stopping=True,\n",
    "    validation_data=(X_test, y_test),\n",
    "    patience=10,\n",
    "    min_delta=1e-4\n",
    ")\n",
    "\n",
    "accuracy = nn_clf.evaluate(X_test, y_test)\n",
    "print(f\"\\nNeural Network Classification Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot the loss histories\n",
    "nn_clf.plot_loss_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-1-train.csv\n",
      "Using cached ../datasets/monks/monk-1-test.csv\n",
      "One-hot encoding MONK-1 dataset...\n",
      "Epoch    0, Training Loss: 0.2536, Validation Loss: 0.2569, Learning Rate: 0.200000\n",
      "Epoch    1, Training Loss: 0.2503, Validation Loss: 0.2548, Learning Rate: 0.199800\n",
      "Epoch    2, Training Loss: 0.2474, Validation Loss: 0.2530, Learning Rate: 0.199600\n",
      "Epoch    3, Training Loss: 0.2445, Validation Loss: 0.2511, Learning Rate: 0.199400\n",
      "Epoch    4, Training Loss: 0.2414, Validation Loss: 0.2491, Learning Rate: 0.199200\n",
      "Epoch    5, Training Loss: 0.2383, Validation Loss: 0.2471, Learning Rate: 0.199000\n",
      "Epoch    6, Training Loss: 0.2352, Validation Loss: 0.2450, Learning Rate: 0.198800\n",
      "Epoch    7, Training Loss: 0.2321, Validation Loss: 0.2427, Learning Rate: 0.198600\n",
      "Epoch    8, Training Loss: 0.2286, Validation Loss: 0.2403, Learning Rate: 0.198400\n",
      "Epoch    9, Training Loss: 0.2240, Validation Loss: 0.2369, Learning Rate: 0.198200\n",
      "Epoch   10, Training Loss: 0.2196, Validation Loss: 0.2339, Learning Rate: 0.198000\n",
      "Epoch   11, Training Loss: 0.2156, Validation Loss: 0.2310, Learning Rate: 0.197800\n",
      "Epoch   12, Training Loss: 0.2115, Validation Loss: 0.2280, Learning Rate: 0.197600\n",
      "Epoch   13, Training Loss: 0.2058, Validation Loss: 0.2241, Learning Rate: 0.197400\n",
      "Epoch   14, Training Loss: 0.2007, Validation Loss: 0.2208, Learning Rate: 0.197200\n",
      "Epoch   15, Training Loss: 0.1959, Validation Loss: 0.2176, Learning Rate: 0.197000\n",
      "Epoch   16, Training Loss: 0.1916, Validation Loss: 0.2147, Learning Rate: 0.196800\n",
      "Epoch   17, Training Loss: 0.1875, Validation Loss: 0.2117, Learning Rate: 0.196600\n",
      "Epoch   18, Training Loss: 0.1835, Validation Loss: 0.2087, Learning Rate: 0.196400\n",
      "Epoch   19, Training Loss: 0.1797, Validation Loss: 0.2061, Learning Rate: 0.196200\n",
      "Epoch   20, Training Loss: 0.1761, Validation Loss: 0.2034, Learning Rate: 0.196000\n",
      "Epoch   21, Training Loss: 0.1726, Validation Loss: 0.2008, Learning Rate: 0.195800\n",
      "Epoch   22, Training Loss: 0.1694, Validation Loss: 0.1984, Learning Rate: 0.195600\n",
      "Epoch   23, Training Loss: 0.1664, Validation Loss: 0.1963, Learning Rate: 0.195400\n",
      "Epoch   24, Training Loss: 0.1637, Validation Loss: 0.1944, Learning Rate: 0.195200\n",
      "Epoch   25, Training Loss: 0.1607, Validation Loss: 0.1921, Learning Rate: 0.195000\n",
      "Epoch   26, Training Loss: 0.1581, Validation Loss: 0.1902, Learning Rate: 0.194800\n",
      "Epoch   27, Training Loss: 0.1558, Validation Loss: 0.1884, Learning Rate: 0.194600\n",
      "Epoch   28, Training Loss: 0.1536, Validation Loss: 0.1869, Learning Rate: 0.194400\n",
      "Epoch   29, Training Loss: 0.1515, Validation Loss: 0.1851, Learning Rate: 0.194200\n",
      "Epoch   30, Training Loss: 0.1498, Validation Loss: 0.1835, Learning Rate: 0.194000\n",
      "Epoch   31, Training Loss: 0.1478, Validation Loss: 0.1819, Learning Rate: 0.193800\n",
      "Epoch   32, Training Loss: 0.1459, Validation Loss: 0.1806, Learning Rate: 0.193600\n",
      "Epoch   33, Training Loss: 0.1441, Validation Loss: 0.1796, Learning Rate: 0.193400\n",
      "Epoch   34, Training Loss: 0.1424, Validation Loss: 0.1781, Learning Rate: 0.193200\n",
      "Epoch   35, Training Loss: 0.1411, Validation Loss: 0.1765, Learning Rate: 0.193000\n",
      "Epoch   36, Training Loss: 0.1393, Validation Loss: 0.1756, Learning Rate: 0.192800\n",
      "Epoch   37, Training Loss: 0.1379, Validation Loss: 0.1743, Learning Rate: 0.192600\n",
      "Epoch   38, Training Loss: 0.1363, Validation Loss: 0.1735, Learning Rate: 0.192400\n",
      "Epoch   39, Training Loss: 0.1351, Validation Loss: 0.1723, Learning Rate: 0.192200\n",
      "Epoch   40, Training Loss: 0.1337, Validation Loss: 0.1713, Learning Rate: 0.192000\n",
      "Epoch   41, Training Loss: 0.1323, Validation Loss: 0.1709, Learning Rate: 0.191800\n",
      "Epoch   42, Training Loss: 0.1311, Validation Loss: 0.1701, Learning Rate: 0.191600\n",
      "Epoch   43, Training Loss: 0.1298, Validation Loss: 0.1691, Learning Rate: 0.191400\n",
      "Epoch   44, Training Loss: 0.1286, Validation Loss: 0.1675, Learning Rate: 0.191200\n",
      "Epoch   45, Training Loss: 0.1275, Validation Loss: 0.1665, Learning Rate: 0.191000\n",
      "Epoch   46, Training Loss: 0.1262, Validation Loss: 0.1657, Learning Rate: 0.190800\n",
      "Epoch   47, Training Loss: 0.1251, Validation Loss: 0.1650, Learning Rate: 0.190600\n",
      "Epoch   48, Training Loss: 0.1239, Validation Loss: 0.1642, Learning Rate: 0.190400\n",
      "Epoch   49, Training Loss: 0.1229, Validation Loss: 0.1634, Learning Rate: 0.190200\n",
      "Epoch   50, Training Loss: 0.1218, Validation Loss: 0.1629, Learning Rate: 0.190000\n",
      "Epoch   51, Training Loss: 0.1208, Validation Loss: 0.1614, Learning Rate: 0.189800\n",
      "Epoch   52, Training Loss: 0.1198, Validation Loss: 0.1604, Learning Rate: 0.189600\n",
      "Epoch   53, Training Loss: 0.1191, Validation Loss: 0.1591, Learning Rate: 0.189400\n",
      "Epoch   54, Training Loss: 0.1178, Validation Loss: 0.1595, Learning Rate: 0.189200\n",
      "Epoch   55, Training Loss: 0.1168, Validation Loss: 0.1584, Learning Rate: 0.189000\n",
      "Epoch   56, Training Loss: 0.1159, Validation Loss: 0.1577, Learning Rate: 0.188800\n",
      "Epoch   57, Training Loss: 0.1150, Validation Loss: 0.1575, Learning Rate: 0.188600\n",
      "Epoch   58, Training Loss: 0.1141, Validation Loss: 0.1569, Learning Rate: 0.188400\n",
      "Epoch   59, Training Loss: 0.1132, Validation Loss: 0.1558, Learning Rate: 0.188200\n",
      "Epoch   60, Training Loss: 0.1123, Validation Loss: 0.1545, Learning Rate: 0.188000\n",
      "Epoch   61, Training Loss: 0.1115, Validation Loss: 0.1547, Learning Rate: 0.187800\n",
      "Epoch   62, Training Loss: 0.1106, Validation Loss: 0.1539, Learning Rate: 0.187600\n",
      "Epoch   63, Training Loss: 0.1097, Validation Loss: 0.1527, Learning Rate: 0.187400\n",
      "Epoch   64, Training Loss: 0.1089, Validation Loss: 0.1521, Learning Rate: 0.187200\n",
      "Epoch   65, Training Loss: 0.1083, Validation Loss: 0.1526, Learning Rate: 0.187000\n",
      "Epoch   66, Training Loss: 0.1074, Validation Loss: 0.1501, Learning Rate: 0.186800\n",
      "Epoch   67, Training Loss: 0.1066, Validation Loss: 0.1492, Learning Rate: 0.186600\n",
      "Epoch   68, Training Loss: 0.1060, Validation Loss: 0.1504, Learning Rate: 0.186400\n",
      "Epoch   69, Training Loss: 0.1050, Validation Loss: 0.1484, Learning Rate: 0.186200\n",
      "Epoch   70, Training Loss: 0.1042, Validation Loss: 0.1476, Learning Rate: 0.186000\n",
      "Epoch   71, Training Loss: 0.1034, Validation Loss: 0.1468, Learning Rate: 0.185800\n",
      "Epoch   72, Training Loss: 0.1026, Validation Loss: 0.1468, Learning Rate: 0.185600\n",
      "Epoch   73, Training Loss: 0.1019, Validation Loss: 0.1455, Learning Rate: 0.185400\n",
      "Epoch   74, Training Loss: 0.1012, Validation Loss: 0.1455, Learning Rate: 0.185200\n",
      "Epoch   75, Training Loss: 0.1004, Validation Loss: 0.1442, Learning Rate: 0.185000\n",
      "Epoch   76, Training Loss: 0.0998, Validation Loss: 0.1432, Learning Rate: 0.184800\n",
      "Epoch   77, Training Loss: 0.0989, Validation Loss: 0.1429, Learning Rate: 0.184600\n",
      "Epoch   78, Training Loss: 0.0985, Validation Loss: 0.1436, Learning Rate: 0.184400\n",
      "Epoch   79, Training Loss: 0.0975, Validation Loss: 0.1418, Learning Rate: 0.184200\n",
      "Epoch   80, Training Loss: 0.0968, Validation Loss: 0.1410, Learning Rate: 0.184000\n",
      "Epoch   81, Training Loss: 0.0961, Validation Loss: 0.1405, Learning Rate: 0.183800\n",
      "Epoch   82, Training Loss: 0.0956, Validation Loss: 0.1406, Learning Rate: 0.183600\n",
      "Epoch   83, Training Loss: 0.0948, Validation Loss: 0.1387, Learning Rate: 0.183400\n",
      "Epoch   84, Training Loss: 0.0940, Validation Loss: 0.1386, Learning Rate: 0.183200\n",
      "Epoch   85, Training Loss: 0.0934, Validation Loss: 0.1380, Learning Rate: 0.183000\n",
      "Epoch   86, Training Loss: 0.0928, Validation Loss: 0.1370, Learning Rate: 0.182800\n",
      "Epoch   87, Training Loss: 0.0923, Validation Loss: 0.1360, Learning Rate: 0.182600\n",
      "Epoch   88, Training Loss: 0.0914, Validation Loss: 0.1357, Learning Rate: 0.182400\n",
      "Epoch   89, Training Loss: 0.0909, Validation Loss: 0.1362, Learning Rate: 0.182200\n",
      "Epoch   90, Training Loss: 0.0904, Validation Loss: 0.1360, Learning Rate: 0.182000\n",
      "Epoch   91, Training Loss: 0.0896, Validation Loss: 0.1334, Learning Rate: 0.181800\n",
      "Epoch   92, Training Loss: 0.0889, Validation Loss: 0.1332, Learning Rate: 0.181600\n",
      "Epoch   93, Training Loss: 0.0884, Validation Loss: 0.1323, Learning Rate: 0.181400\n",
      "Epoch   94, Training Loss: 0.0878, Validation Loss: 0.1330, Learning Rate: 0.181200\n",
      "Epoch   95, Training Loss: 0.0871, Validation Loss: 0.1319, Learning Rate: 0.181000\n",
      "Epoch   96, Training Loss: 0.0865, Validation Loss: 0.1312, Learning Rate: 0.180800\n",
      "Epoch   97, Training Loss: 0.0863, Validation Loss: 0.1318, Learning Rate: 0.180600\n",
      "Epoch   98, Training Loss: 0.0856, Validation Loss: 0.1310, Learning Rate: 0.180400\n",
      "Epoch   99, Training Loss: 0.0848, Validation Loss: 0.1291, Learning Rate: 0.180200\n",
      "Epoch  100, Training Loss: 0.0843, Validation Loss: 0.1284, Learning Rate: 0.180000\n",
      "Epoch  101, Training Loss: 0.0837, Validation Loss: 0.1279, Learning Rate: 0.179800\n",
      "Epoch  102, Training Loss: 0.0831, Validation Loss: 0.1278, Learning Rate: 0.179600\n",
      "Epoch  103, Training Loss: 0.0829, Validation Loss: 0.1262, Learning Rate: 0.179400\n",
      "Epoch  104, Training Loss: 0.0823, Validation Loss: 0.1276, Learning Rate: 0.179200\n",
      "Epoch  105, Training Loss: 0.0816, Validation Loss: 0.1267, Learning Rate: 0.179000\n",
      "Epoch  106, Training Loss: 0.0811, Validation Loss: 0.1262, Learning Rate: 0.178800\n",
      "Epoch  107, Training Loss: 0.0805, Validation Loss: 0.1251, Learning Rate: 0.178600\n",
      "Epoch  108, Training Loss: 0.0801, Validation Loss: 0.1232, Learning Rate: 0.178400\n",
      "Epoch  109, Training Loss: 0.0795, Validation Loss: 0.1242, Learning Rate: 0.178200\n",
      "Epoch  110, Training Loss: 0.0790, Validation Loss: 0.1236, Learning Rate: 0.178000\n",
      "Epoch  111, Training Loss: 0.0784, Validation Loss: 0.1229, Learning Rate: 0.177800\n",
      "Epoch  112, Training Loss: 0.0782, Validation Loss: 0.1233, Learning Rate: 0.177600\n",
      "Epoch  113, Training Loss: 0.0776, Validation Loss: 0.1208, Learning Rate: 0.177400\n",
      "Epoch  114, Training Loss: 0.0772, Validation Loss: 0.1198, Learning Rate: 0.177200\n",
      "Epoch  115, Training Loss: 0.0764, Validation Loss: 0.1207, Learning Rate: 0.177000\n",
      "Epoch  116, Training Loss: 0.0759, Validation Loss: 0.1194, Learning Rate: 0.176800\n",
      "Epoch  117, Training Loss: 0.0755, Validation Loss: 0.1198, Learning Rate: 0.176600\n",
      "Epoch  118, Training Loss: 0.0749, Validation Loss: 0.1187, Learning Rate: 0.176400\n",
      "Epoch  119, Training Loss: 0.0753, Validation Loss: 0.1168, Learning Rate: 0.176200\n",
      "Epoch  120, Training Loss: 0.0745, Validation Loss: 0.1164, Learning Rate: 0.176000\n",
      "Epoch  121, Training Loss: 0.0737, Validation Loss: 0.1173, Learning Rate: 0.175800\n",
      "Epoch  122, Training Loss: 0.0732, Validation Loss: 0.1165, Learning Rate: 0.175600\n",
      "Epoch  123, Training Loss: 0.0728, Validation Loss: 0.1153, Learning Rate: 0.175400\n",
      "Epoch  124, Training Loss: 0.0727, Validation Loss: 0.1144, Learning Rate: 0.175200\n",
      "Epoch  125, Training Loss: 0.0718, Validation Loss: 0.1149, Learning Rate: 0.175000\n",
      "Epoch  126, Training Loss: 0.0715, Validation Loss: 0.1136, Learning Rate: 0.174800\n",
      "Epoch  127, Training Loss: 0.0719, Validation Loss: 0.1122, Learning Rate: 0.174600\n",
      "Epoch  128, Training Loss: 0.0706, Validation Loss: 0.1125, Learning Rate: 0.174400\n",
      "Epoch  129, Training Loss: 0.0702, Validation Loss: 0.1137, Learning Rate: 0.174200\n",
      "Epoch  130, Training Loss: 0.0707, Validation Loss: 0.1156, Learning Rate: 0.174000\n",
      "Epoch  131, Training Loss: 0.0692, Validation Loss: 0.1115, Learning Rate: 0.173800\n",
      "Epoch  132, Training Loss: 0.0689, Validation Loss: 0.1108, Learning Rate: 0.173600\n",
      "Epoch  133, Training Loss: 0.0684, Validation Loss: 0.1112, Learning Rate: 0.173400\n",
      "Epoch  134, Training Loss: 0.0685, Validation Loss: 0.1127, Learning Rate: 0.173200\n",
      "Epoch  135, Training Loss: 0.0676, Validation Loss: 0.1110, Learning Rate: 0.173000\n",
      "Epoch  136, Training Loss: 0.0676, Validation Loss: 0.1086, Learning Rate: 0.172800\n",
      "Epoch  137, Training Loss: 0.0670, Validation Loss: 0.1102, Learning Rate: 0.172600\n",
      "Epoch  138, Training Loss: 0.0664, Validation Loss: 0.1083, Learning Rate: 0.172400\n",
      "Epoch  139, Training Loss: 0.0660, Validation Loss: 0.1086, Learning Rate: 0.172200\n",
      "Epoch  140, Training Loss: 0.0656, Validation Loss: 0.1078, Learning Rate: 0.172000\n",
      "Epoch  141, Training Loss: 0.0652, Validation Loss: 0.1071, Learning Rate: 0.171800\n",
      "Epoch  142, Training Loss: 0.0648, Validation Loss: 0.1067, Learning Rate: 0.171600\n",
      "Epoch  143, Training Loss: 0.0645, Validation Loss: 0.1067, Learning Rate: 0.171400\n",
      "Epoch  144, Training Loss: 0.0645, Validation Loss: 0.1079, Learning Rate: 0.171200\n",
      "Epoch  145, Training Loss: 0.0638, Validation Loss: 0.1056, Learning Rate: 0.171000\n",
      "Epoch  146, Training Loss: 0.0634, Validation Loss: 0.1051, Learning Rate: 0.170800\n",
      "Epoch  147, Training Loss: 0.0632, Validation Loss: 0.1038, Learning Rate: 0.170600\n",
      "Epoch  148, Training Loss: 0.0631, Validation Loss: 0.1031, Learning Rate: 0.170400\n",
      "Epoch  149, Training Loss: 0.0624, Validation Loss: 0.1049, Learning Rate: 0.170200\n",
      "Epoch  150, Training Loss: 0.0620, Validation Loss: 0.1028, Learning Rate: 0.170000\n",
      "Epoch  151, Training Loss: 0.0616, Validation Loss: 0.1023, Learning Rate: 0.169800\n",
      "Epoch  152, Training Loss: 0.0612, Validation Loss: 0.1022, Learning Rate: 0.169600\n",
      "Epoch  153, Training Loss: 0.0614, Validation Loss: 0.1039, Learning Rate: 0.169400\n",
      "Epoch  154, Training Loss: 0.0606, Validation Loss: 0.1016, Learning Rate: 0.169200\n",
      "Epoch  155, Training Loss: 0.0603, Validation Loss: 0.1014, Learning Rate: 0.169000\n",
      "Epoch  156, Training Loss: 0.0599, Validation Loss: 0.1002, Learning Rate: 0.168800\n",
      "Epoch  157, Training Loss: 0.0601, Validation Loss: 0.1020, Learning Rate: 0.168600\n",
      "Epoch  158, Training Loss: 0.0593, Validation Loss: 0.1001, Learning Rate: 0.168400\n",
      "Epoch  159, Training Loss: 0.0590, Validation Loss: 0.0999, Learning Rate: 0.168200\n",
      "Epoch  160, Training Loss: 0.0595, Validation Loss: 0.1016, Learning Rate: 0.168000\n",
      "Epoch  161, Training Loss: 0.0587, Validation Loss: 0.0971, Learning Rate: 0.167800\n",
      "Epoch  162, Training Loss: 0.0580, Validation Loss: 0.0975, Learning Rate: 0.167600\n",
      "Epoch  163, Training Loss: 0.0580, Validation Loss: 0.0962, Learning Rate: 0.167400\n",
      "Epoch  164, Training Loss: 0.0574, Validation Loss: 0.0966, Learning Rate: 0.167200\n",
      "Epoch  165, Training Loss: 0.0572, Validation Loss: 0.0973, Learning Rate: 0.167000\n",
      "Epoch  166, Training Loss: 0.0567, Validation Loss: 0.0956, Learning Rate: 0.166800\n",
      "Epoch  167, Training Loss: 0.0566, Validation Loss: 0.0965, Learning Rate: 0.166600\n",
      "Epoch  168, Training Loss: 0.0562, Validation Loss: 0.0951, Learning Rate: 0.166400\n",
      "Epoch  169, Training Loss: 0.0559, Validation Loss: 0.0942, Learning Rate: 0.166200\n",
      "Epoch  170, Training Loss: 0.0556, Validation Loss: 0.0951, Learning Rate: 0.166000\n",
      "Epoch  171, Training Loss: 0.0554, Validation Loss: 0.0948, Learning Rate: 0.165800\n",
      "Epoch  172, Training Loss: 0.0551, Validation Loss: 0.0946, Learning Rate: 0.165600\n",
      "Epoch  173, Training Loss: 0.0547, Validation Loss: 0.0930, Learning Rate: 0.165400\n",
      "Epoch  174, Training Loss: 0.0544, Validation Loss: 0.0931, Learning Rate: 0.165200\n",
      "Epoch  175, Training Loss: 0.0541, Validation Loss: 0.0923, Learning Rate: 0.165000\n",
      "Epoch  176, Training Loss: 0.0539, Validation Loss: 0.0920, Learning Rate: 0.164800\n",
      "Epoch  177, Training Loss: 0.0537, Validation Loss: 0.0928, Learning Rate: 0.164600\n",
      "Epoch  178, Training Loss: 0.0533, Validation Loss: 0.0918, Learning Rate: 0.164400\n",
      "Epoch  179, Training Loss: 0.0533, Validation Loss: 0.0902, Learning Rate: 0.164200\n",
      "Epoch  180, Training Loss: 0.0528, Validation Loss: 0.0903, Learning Rate: 0.164000\n",
      "Epoch  181, Training Loss: 0.0527, Validation Loss: 0.0894, Learning Rate: 0.163800\n",
      "Epoch  182, Training Loss: 0.0523, Validation Loss: 0.0895, Learning Rate: 0.163600\n",
      "Epoch  183, Training Loss: 0.0521, Validation Loss: 0.0896, Learning Rate: 0.163400\n",
      "Epoch  184, Training Loss: 0.0518, Validation Loss: 0.0885, Learning Rate: 0.163200\n",
      "Epoch  185, Training Loss: 0.0516, Validation Loss: 0.0878, Learning Rate: 0.163000\n",
      "Epoch  186, Training Loss: 0.0514, Validation Loss: 0.0876, Learning Rate: 0.162800\n",
      "Epoch  187, Training Loss: 0.0510, Validation Loss: 0.0876, Learning Rate: 0.162600\n",
      "Epoch  188, Training Loss: 0.0508, Validation Loss: 0.0871, Learning Rate: 0.162400\n",
      "Epoch  189, Training Loss: 0.0505, Validation Loss: 0.0870, Learning Rate: 0.162200\n",
      "Epoch  190, Training Loss: 0.0504, Validation Loss: 0.0861, Learning Rate: 0.162000\n",
      "Epoch  191, Training Loss: 0.0503, Validation Loss: 0.0874, Learning Rate: 0.161800\n",
      "Epoch  192, Training Loss: 0.0500, Validation Loss: 0.0852, Learning Rate: 0.161600\n",
      "Epoch  193, Training Loss: 0.0497, Validation Loss: 0.0858, Learning Rate: 0.161400\n",
      "Epoch  194, Training Loss: 0.0494, Validation Loss: 0.0850, Learning Rate: 0.161200\n",
      "Epoch  195, Training Loss: 0.0494, Validation Loss: 0.0860, Learning Rate: 0.161000\n",
      "Epoch  196, Training Loss: 0.0493, Validation Loss: 0.0837, Learning Rate: 0.160800\n",
      "Epoch  197, Training Loss: 0.0490, Validation Loss: 0.0856, Learning Rate: 0.160600\n",
      "Epoch  198, Training Loss: 0.0487, Validation Loss: 0.0846, Learning Rate: 0.160400\n",
      "Epoch  199, Training Loss: 0.0484, Validation Loss: 0.0836, Learning Rate: 0.160200\n",
      "Epoch  200, Training Loss: 0.0482, Validation Loss: 0.0829, Learning Rate: 0.160000\n",
      "Epoch  201, Training Loss: 0.0480, Validation Loss: 0.0830, Learning Rate: 0.159800\n",
      "Epoch  202, Training Loss: 0.0478, Validation Loss: 0.0824, Learning Rate: 0.159600\n",
      "Epoch  203, Training Loss: 0.0476, Validation Loss: 0.0822, Learning Rate: 0.159400\n",
      "Epoch  204, Training Loss: 0.0474, Validation Loss: 0.0816, Learning Rate: 0.159200\n",
      "Epoch  205, Training Loss: 0.0476, Validation Loss: 0.0805, Learning Rate: 0.159000\n",
      "Epoch  206, Training Loss: 0.0470, Validation Loss: 0.0809, Learning Rate: 0.158800\n",
      "Epoch  207, Training Loss: 0.0469, Validation Loss: 0.0804, Learning Rate: 0.158600\n",
      "Epoch  208, Training Loss: 0.0467, Validation Loss: 0.0802, Learning Rate: 0.158400\n",
      "Epoch  209, Training Loss: 0.0465, Validation Loss: 0.0806, Learning Rate: 0.158200\n",
      "Epoch  210, Training Loss: 0.0463, Validation Loss: 0.0799, Learning Rate: 0.158000\n",
      "Epoch  211, Training Loss: 0.0462, Validation Loss: 0.0802, Learning Rate: 0.157800\n",
      "Epoch  212, Training Loss: 0.0459, Validation Loss: 0.0791, Learning Rate: 0.157600\n",
      "Epoch  213, Training Loss: 0.0458, Validation Loss: 0.0797, Learning Rate: 0.157400\n",
      "Epoch  214, Training Loss: 0.0456, Validation Loss: 0.0791, Learning Rate: 0.157200\n",
      "Epoch  215, Training Loss: 0.0458, Validation Loss: 0.0777, Learning Rate: 0.157000\n",
      "Epoch  216, Training Loss: 0.0454, Validation Loss: 0.0779, Learning Rate: 0.156800\n",
      "Epoch  217, Training Loss: 0.0452, Validation Loss: 0.0774, Learning Rate: 0.156600\n",
      "Epoch  218, Training Loss: 0.0450, Validation Loss: 0.0783, Learning Rate: 0.156400\n",
      "Epoch  219, Training Loss: 0.0448, Validation Loss: 0.0773, Learning Rate: 0.156200\n",
      "Epoch  220, Training Loss: 0.0446, Validation Loss: 0.0772, Learning Rate: 0.156000\n",
      "Epoch  221, Training Loss: 0.0446, Validation Loss: 0.0781, Learning Rate: 0.155800\n",
      "Epoch  222, Training Loss: 0.0444, Validation Loss: 0.0763, Learning Rate: 0.155600\n",
      "Epoch  223, Training Loss: 0.0442, Validation Loss: 0.0764, Learning Rate: 0.155400\n",
      "Epoch  224, Training Loss: 0.0440, Validation Loss: 0.0763, Learning Rate: 0.155200\n",
      "Epoch  225, Training Loss: 0.0439, Validation Loss: 0.0757, Learning Rate: 0.155000\n",
      "Epoch  226, Training Loss: 0.0437, Validation Loss: 0.0756, Learning Rate: 0.154800\n",
      "Epoch  227, Training Loss: 0.0436, Validation Loss: 0.0750, Learning Rate: 0.154600\n",
      "Epoch  228, Training Loss: 0.0434, Validation Loss: 0.0751, Learning Rate: 0.154400\n",
      "Epoch  229, Training Loss: 0.0433, Validation Loss: 0.0752, Learning Rate: 0.154200\n",
      "Epoch  230, Training Loss: 0.0432, Validation Loss: 0.0747, Learning Rate: 0.154000\n",
      "Epoch  231, Training Loss: 0.0431, Validation Loss: 0.0741, Learning Rate: 0.153800\n",
      "Epoch  232, Training Loss: 0.0430, Validation Loss: 0.0737, Learning Rate: 0.153600\n",
      "Epoch  233, Training Loss: 0.0428, Validation Loss: 0.0740, Learning Rate: 0.153400\n",
      "Epoch  234, Training Loss: 0.0427, Validation Loss: 0.0737, Learning Rate: 0.153200\n",
      "Epoch  235, Training Loss: 0.0427, Validation Loss: 0.0728, Learning Rate: 0.153000\n",
      "Epoch  236, Training Loss: 0.0424, Validation Loss: 0.0730, Learning Rate: 0.152800\n",
      "Epoch  237, Training Loss: 0.0424, Validation Loss: 0.0723, Learning Rate: 0.152600\n",
      "Epoch  238, Training Loss: 0.0422, Validation Loss: 0.0724, Learning Rate: 0.152400\n",
      "Epoch  239, Training Loss: 0.0421, Validation Loss: 0.0721, Learning Rate: 0.152200\n",
      "Epoch  240, Training Loss: 0.0419, Validation Loss: 0.0725, Learning Rate: 0.152000\n",
      "Epoch  241, Training Loss: 0.0418, Validation Loss: 0.0720, Learning Rate: 0.151800\n",
      "Epoch  242, Training Loss: 0.0418, Validation Loss: 0.0727, Learning Rate: 0.151600\n",
      "Epoch  243, Training Loss: 0.0416, Validation Loss: 0.0717, Learning Rate: 0.151400\n",
      "Epoch  244, Training Loss: 0.0415, Validation Loss: 0.0712, Learning Rate: 0.151200\n",
      "Epoch  245, Training Loss: 0.0415, Validation Loss: 0.0720, Learning Rate: 0.151000\n",
      "Epoch  246, Training Loss: 0.0413, Validation Loss: 0.0706, Learning Rate: 0.150800\n",
      "Epoch  247, Training Loss: 0.0412, Validation Loss: 0.0716, Learning Rate: 0.150600\n",
      "Epoch  248, Training Loss: 0.0411, Validation Loss: 0.0713, Learning Rate: 0.150400\n",
      "Epoch  249, Training Loss: 0.0410, Validation Loss: 0.0708, Learning Rate: 0.150200\n",
      "Epoch  250, Training Loss: 0.0408, Validation Loss: 0.0700, Learning Rate: 0.150000\n",
      "Epoch  251, Training Loss: 0.0408, Validation Loss: 0.0698, Learning Rate: 0.149800\n",
      "Epoch  252, Training Loss: 0.0408, Validation Loss: 0.0690, Learning Rate: 0.149600\n",
      "Epoch  253, Training Loss: 0.0408, Validation Loss: 0.0686, Learning Rate: 0.149400\n",
      "Epoch  254, Training Loss: 0.0405, Validation Loss: 0.0696, Learning Rate: 0.149200\n",
      "Epoch  255, Training Loss: 0.0404, Validation Loss: 0.0689, Learning Rate: 0.149000\n",
      "Epoch  256, Training Loss: 0.0403, Validation Loss: 0.0690, Learning Rate: 0.148800\n",
      "Epoch  257, Training Loss: 0.0402, Validation Loss: 0.0692, Learning Rate: 0.148600\n",
      "Epoch  258, Training Loss: 0.0402, Validation Loss: 0.0680, Learning Rate: 0.148400\n",
      "Epoch  259, Training Loss: 0.0400, Validation Loss: 0.0687, Learning Rate: 0.148200\n",
      "Epoch  260, Training Loss: 0.0399, Validation Loss: 0.0680, Learning Rate: 0.148000\n",
      "Epoch  261, Training Loss: 0.0398, Validation Loss: 0.0677, Learning Rate: 0.147800\n",
      "Epoch  262, Training Loss: 0.0398, Validation Loss: 0.0673, Learning Rate: 0.147600\n",
      "Epoch  263, Training Loss: 0.0397, Validation Loss: 0.0673, Learning Rate: 0.147400\n",
      "Epoch  264, Training Loss: 0.0396, Validation Loss: 0.0678, Learning Rate: 0.147200\n",
      "Epoch  265, Training Loss: 0.0396, Validation Loss: 0.0666, Learning Rate: 0.147000\n",
      "Epoch  266, Training Loss: 0.0394, Validation Loss: 0.0672, Learning Rate: 0.146800\n",
      "Epoch  267, Training Loss: 0.0393, Validation Loss: 0.0671, Learning Rate: 0.146600\n",
      "Epoch  268, Training Loss: 0.0394, Validation Loss: 0.0677, Learning Rate: 0.146400\n",
      "Epoch  269, Training Loss: 0.0392, Validation Loss: 0.0662, Learning Rate: 0.146200\n",
      "Epoch  270, Training Loss: 0.0391, Validation Loss: 0.0662, Learning Rate: 0.146000\n",
      "Epoch  271, Training Loss: 0.0390, Validation Loss: 0.0667, Learning Rate: 0.145800\n",
      "Epoch  272, Training Loss: 0.0390, Validation Loss: 0.0667, Learning Rate: 0.145600\n",
      "Epoch  273, Training Loss: 0.0389, Validation Loss: 0.0665, Learning Rate: 0.145400\n",
      "Epoch  274, Training Loss: 0.0388, Validation Loss: 0.0652, Learning Rate: 0.145200\n",
      "Epoch  275, Training Loss: 0.0387, Validation Loss: 0.0653, Learning Rate: 0.145000\n",
      "Epoch  276, Training Loss: 0.0388, Validation Loss: 0.0645, Learning Rate: 0.144800\n",
      "Epoch  277, Training Loss: 0.0386, Validation Loss: 0.0647, Learning Rate: 0.144600\n",
      "Epoch  278, Training Loss: 0.0386, Validation Loss: 0.0645, Learning Rate: 0.144400\n",
      "Epoch  279, Training Loss: 0.0384, Validation Loss: 0.0649, Learning Rate: 0.144200\n",
      "Epoch  280, Training Loss: 0.0384, Validation Loss: 0.0646, Learning Rate: 0.144000\n",
      "Epoch  281, Training Loss: 0.0383, Validation Loss: 0.0648, Learning Rate: 0.143800\n",
      "Epoch  282, Training Loss: 0.0382, Validation Loss: 0.0647, Learning Rate: 0.143600\n",
      "Epoch  283, Training Loss: 0.0382, Validation Loss: 0.0640, Learning Rate: 0.143400\n",
      "Epoch  284, Training Loss: 0.0381, Validation Loss: 0.0637, Learning Rate: 0.143200\n",
      "Epoch  285, Training Loss: 0.0380, Validation Loss: 0.0641, Learning Rate: 0.143000\n",
      "Epoch  286, Training Loss: 0.0380, Validation Loss: 0.0633, Learning Rate: 0.142800\n",
      "Epoch  287, Training Loss: 0.0379, Validation Loss: 0.0640, Learning Rate: 0.142600\n",
      "Epoch  288, Training Loss: 0.0378, Validation Loss: 0.0633, Learning Rate: 0.142400\n",
      "Epoch  289, Training Loss: 0.0378, Validation Loss: 0.0635, Learning Rate: 0.142200\n",
      "Epoch  290, Training Loss: 0.0378, Validation Loss: 0.0626, Learning Rate: 0.142000\n",
      "Epoch  291, Training Loss: 0.0377, Validation Loss: 0.0631, Learning Rate: 0.141800\n",
      "Epoch  292, Training Loss: 0.0376, Validation Loss: 0.0631, Learning Rate: 0.141600\n",
      "Epoch  293, Training Loss: 0.0376, Validation Loss: 0.0631, Learning Rate: 0.141400\n",
      "Epoch  294, Training Loss: 0.0375, Validation Loss: 0.0631, Learning Rate: 0.141200\n",
      "Epoch  295, Training Loss: 0.0375, Validation Loss: 0.0622, Learning Rate: 0.141000\n",
      "Epoch  296, Training Loss: 0.0374, Validation Loss: 0.0620, Learning Rate: 0.140800\n",
      "Epoch  297, Training Loss: 0.0373, Validation Loss: 0.0620, Learning Rate: 0.140600\n",
      "Epoch  298, Training Loss: 0.0373, Validation Loss: 0.0615, Learning Rate: 0.140400\n",
      "Epoch  299, Training Loss: 0.0372, Validation Loss: 0.0617, Learning Rate: 0.140200\n",
      "Epoch  300, Training Loss: 0.0372, Validation Loss: 0.0615, Learning Rate: 0.140000\n",
      "Epoch  301, Training Loss: 0.0372, Validation Loss: 0.0620, Learning Rate: 0.139800\n",
      "Epoch  302, Training Loss: 0.0371, Validation Loss: 0.0609, Learning Rate: 0.139600\n",
      "Epoch  303, Training Loss: 0.0371, Validation Loss: 0.0606, Learning Rate: 0.139400\n",
      "Epoch  304, Training Loss: 0.0370, Validation Loss: 0.0609, Learning Rate: 0.139200\n",
      "Epoch  305, Training Loss: 0.0369, Validation Loss: 0.0611, Learning Rate: 0.139000\n",
      "Epoch  306, Training Loss: 0.0372, Validation Loss: 0.0623, Learning Rate: 0.138800\n",
      "Epoch  307, Training Loss: 0.0368, Validation Loss: 0.0608, Learning Rate: 0.138600\n",
      "Epoch  308, Training Loss: 0.0368, Validation Loss: 0.0609, Learning Rate: 0.138400\n",
      "Epoch  309, Training Loss: 0.0367, Validation Loss: 0.0604, Learning Rate: 0.138200\n",
      "Epoch  310, Training Loss: 0.0367, Validation Loss: 0.0607, Learning Rate: 0.138000\n",
      "Epoch  311, Training Loss: 0.0367, Validation Loss: 0.0607, Learning Rate: 0.137800\n",
      "Epoch  312, Training Loss: 0.0366, Validation Loss: 0.0599, Learning Rate: 0.137600\n",
      "Epoch  313, Training Loss: 0.0366, Validation Loss: 0.0601, Learning Rate: 0.137400\n",
      "Epoch  314, Training Loss: 0.0365, Validation Loss: 0.0600, Learning Rate: 0.137200\n",
      "Epoch  315, Training Loss: 0.0365, Validation Loss: 0.0596, Learning Rate: 0.137000\n",
      "Epoch  316, Training Loss: 0.0364, Validation Loss: 0.0596, Learning Rate: 0.136800\n",
      "Epoch  317, Training Loss: 0.0364, Validation Loss: 0.0599, Learning Rate: 0.136600\n",
      "Epoch  318, Training Loss: 0.0364, Validation Loss: 0.0594, Learning Rate: 0.136400\n",
      "Epoch  319, Training Loss: 0.0363, Validation Loss: 0.0594, Learning Rate: 0.136200\n",
      "Epoch  320, Training Loss: 0.0364, Validation Loss: 0.0601, Learning Rate: 0.136000\n",
      "Epoch  321, Training Loss: 0.0362, Validation Loss: 0.0591, Learning Rate: 0.135800\n",
      "Epoch  322, Training Loss: 0.0362, Validation Loss: 0.0592, Learning Rate: 0.135600\n",
      "Epoch  323, Training Loss: 0.0362, Validation Loss: 0.0590, Learning Rate: 0.135400\n",
      "Epoch  324, Training Loss: 0.0362, Validation Loss: 0.0593, Learning Rate: 0.135200\n",
      "Epoch  325, Training Loss: 0.0361, Validation Loss: 0.0586, Learning Rate: 0.135000\n",
      "Epoch  326, Training Loss: 0.0361, Validation Loss: 0.0582, Learning Rate: 0.134800\n",
      "Epoch  327, Training Loss: 0.0360, Validation Loss: 0.0582, Learning Rate: 0.134600\n",
      "Epoch  328, Training Loss: 0.0360, Validation Loss: 0.0587, Learning Rate: 0.134400\n",
      "Epoch  329, Training Loss: 0.0360, Validation Loss: 0.0579, Learning Rate: 0.134200\n",
      "Epoch  330, Training Loss: 0.0359, Validation Loss: 0.0579, Learning Rate: 0.134000\n",
      "Epoch  331, Training Loss: 0.0359, Validation Loss: 0.0578, Learning Rate: 0.133800\n",
      "Epoch  332, Training Loss: 0.0359, Validation Loss: 0.0580, Learning Rate: 0.133600\n",
      "Epoch  333, Training Loss: 0.0358, Validation Loss: 0.0579, Learning Rate: 0.133400\n",
      "Epoch  334, Training Loss: 0.0358, Validation Loss: 0.0580, Learning Rate: 0.133200\n",
      "Epoch  335, Training Loss: 0.0358, Validation Loss: 0.0579, Learning Rate: 0.133000\n",
      "Epoch  336, Training Loss: 0.0358, Validation Loss: 0.0573, Learning Rate: 0.132800\n",
      "Epoch  337, Training Loss: 0.0357, Validation Loss: 0.0571, Learning Rate: 0.132600\n",
      "Epoch  338, Training Loss: 0.0357, Validation Loss: 0.0571, Learning Rate: 0.132400\n",
      "Epoch  339, Training Loss: 0.0356, Validation Loss: 0.0574, Learning Rate: 0.132200\n",
      "Epoch  340, Training Loss: 0.0356, Validation Loss: 0.0574, Learning Rate: 0.132000\n",
      "Epoch  341, Training Loss: 0.0356, Validation Loss: 0.0570, Learning Rate: 0.131800\n",
      "Epoch  342, Training Loss: 0.0356, Validation Loss: 0.0570, Learning Rate: 0.131600\n",
      "Epoch  343, Training Loss: 0.0356, Validation Loss: 0.0567, Learning Rate: 0.131400\n",
      "Epoch  344, Training Loss: 0.0355, Validation Loss: 0.0565, Learning Rate: 0.131200\n",
      "Epoch  345, Training Loss: 0.0355, Validation Loss: 0.0568, Learning Rate: 0.131000\n",
      "Epoch  346, Training Loss: 0.0355, Validation Loss: 0.0565, Learning Rate: 0.130800\n",
      "Epoch  347, Training Loss: 0.0354, Validation Loss: 0.0564, Learning Rate: 0.130600\n",
      "Epoch  348, Training Loss: 0.0354, Validation Loss: 0.0566, Learning Rate: 0.130400\n",
      "Epoch  349, Training Loss: 0.0354, Validation Loss: 0.0560, Learning Rate: 0.130200\n",
      "Epoch  350, Training Loss: 0.0354, Validation Loss: 0.0562, Learning Rate: 0.130000\n",
      "Epoch  351, Training Loss: 0.0353, Validation Loss: 0.0564, Learning Rate: 0.129800\n",
      "Epoch  352, Training Loss: 0.0353, Validation Loss: 0.0558, Learning Rate: 0.129600\n",
      "Epoch  353, Training Loss: 0.0353, Validation Loss: 0.0558, Learning Rate: 0.129400\n",
      "Epoch  354, Training Loss: 0.0353, Validation Loss: 0.0558, Learning Rate: 0.129200\n",
      "Epoch  355, Training Loss: 0.0352, Validation Loss: 0.0557, Learning Rate: 0.129000\n",
      "Epoch  356, Training Loss: 0.0352, Validation Loss: 0.0554, Learning Rate: 0.128800\n",
      "Epoch  357, Training Loss: 0.0352, Validation Loss: 0.0553, Learning Rate: 0.128600\n",
      "Epoch  358, Training Loss: 0.0352, Validation Loss: 0.0556, Learning Rate: 0.128400\n",
      "Epoch  359, Training Loss: 0.0352, Validation Loss: 0.0558, Learning Rate: 0.128200\n",
      "Epoch  360, Training Loss: 0.0351, Validation Loss: 0.0554, Learning Rate: 0.128000\n",
      "Epoch  361, Training Loss: 0.0351, Validation Loss: 0.0551, Learning Rate: 0.127800\n",
      "Epoch  362, Training Loss: 0.0351, Validation Loss: 0.0554, Learning Rate: 0.127600\n",
      "Epoch  363, Training Loss: 0.0351, Validation Loss: 0.0552, Learning Rate: 0.127400\n",
      "Epoch  364, Training Loss: 0.0350, Validation Loss: 0.0551, Learning Rate: 0.127200\n",
      "Epoch  365, Training Loss: 0.0350, Validation Loss: 0.0551, Learning Rate: 0.127000\n",
      "Epoch  366, Training Loss: 0.0350, Validation Loss: 0.0552, Learning Rate: 0.126800\n",
      "Epoch  367, Training Loss: 0.0350, Validation Loss: 0.0550, Learning Rate: 0.126600\n",
      "Epoch  368, Training Loss: 0.0350, Validation Loss: 0.0549, Learning Rate: 0.126400\n",
      "Epoch  369, Training Loss: 0.0350, Validation Loss: 0.0545, Learning Rate: 0.126200\n",
      "Epoch  370, Training Loss: 0.0349, Validation Loss: 0.0544, Learning Rate: 0.126000\n",
      "Epoch  371, Training Loss: 0.0349, Validation Loss: 0.0545, Learning Rate: 0.125800\n",
      "Epoch  372, Training Loss: 0.0349, Validation Loss: 0.0546, Learning Rate: 0.125600\n",
      "Epoch  373, Training Loss: 0.0349, Validation Loss: 0.0543, Learning Rate: 0.125400\n",
      "Epoch  374, Training Loss: 0.0349, Validation Loss: 0.0545, Learning Rate: 0.125200\n",
      "Epoch  375, Training Loss: 0.0348, Validation Loss: 0.0543, Learning Rate: 0.125000\n",
      "Epoch  376, Training Loss: 0.0348, Validation Loss: 0.0543, Learning Rate: 0.124800\n",
      "Epoch  377, Training Loss: 0.0348, Validation Loss: 0.0541, Learning Rate: 0.124600\n",
      "Epoch  378, Training Loss: 0.0348, Validation Loss: 0.0543, Learning Rate: 0.124400\n",
      "Epoch  379, Training Loss: 0.0348, Validation Loss: 0.0538, Learning Rate: 0.124200\n",
      "Epoch  380, Training Loss: 0.0348, Validation Loss: 0.0537, Learning Rate: 0.124000\n",
      "Epoch  381, Training Loss: 0.0348, Validation Loss: 0.0536, Learning Rate: 0.123800\n",
      "Epoch  382, Training Loss: 0.0347, Validation Loss: 0.0536, Learning Rate: 0.123600\n",
      "Epoch  383, Training Loss: 0.0347, Validation Loss: 0.0538, Learning Rate: 0.123400\n",
      "Epoch  384, Training Loss: 0.0347, Validation Loss: 0.0535, Learning Rate: 0.123200\n",
      "Epoch  385, Training Loss: 0.0347, Validation Loss: 0.0534, Learning Rate: 0.123000\n",
      "Epoch  386, Training Loss: 0.0347, Validation Loss: 0.0536, Learning Rate: 0.122800\n",
      "Epoch  387, Training Loss: 0.0346, Validation Loss: 0.0534, Learning Rate: 0.122600\n",
      "Epoch  388, Training Loss: 0.0346, Validation Loss: 0.0532, Learning Rate: 0.122400\n",
      "Epoch  389, Training Loss: 0.0346, Validation Loss: 0.0531, Learning Rate: 0.122200\n",
      "Epoch  390, Training Loss: 0.0346, Validation Loss: 0.0533, Learning Rate: 0.122000\n",
      "Epoch  391, Training Loss: 0.0346, Validation Loss: 0.0534, Learning Rate: 0.121800\n",
      "Epoch  392, Training Loss: 0.0346, Validation Loss: 0.0534, Learning Rate: 0.121600\n",
      "Epoch  393, Training Loss: 0.0346, Validation Loss: 0.0531, Learning Rate: 0.121400\n",
      "Epoch  394, Training Loss: 0.0346, Validation Loss: 0.0531, Learning Rate: 0.121200\n",
      "Epoch  395, Training Loss: 0.0345, Validation Loss: 0.0530, Learning Rate: 0.121000\n",
      "Epoch  396, Training Loss: 0.0345, Validation Loss: 0.0530, Learning Rate: 0.120800\n",
      "Epoch  397, Training Loss: 0.0345, Validation Loss: 0.0530, Learning Rate: 0.120600\n",
      "Epoch  398, Training Loss: 0.0345, Validation Loss: 0.0530, Learning Rate: 0.120400\n",
      "Epoch  399, Training Loss: 0.0345, Validation Loss: 0.0527, Learning Rate: 0.120200\n",
      "Epoch  400, Training Loss: 0.0345, Validation Loss: 0.0527, Learning Rate: 0.120000\n",
      "Epoch  401, Training Loss: 0.0345, Validation Loss: 0.0526, Learning Rate: 0.119800\n",
      "Epoch  402, Training Loss: 0.0345, Validation Loss: 0.0527, Learning Rate: 0.119600\n",
      "Epoch  403, Training Loss: 0.0344, Validation Loss: 0.0526, Learning Rate: 0.119400\n",
      "Epoch  404, Training Loss: 0.0344, Validation Loss: 0.0524, Learning Rate: 0.119200\n",
      "Epoch  405, Training Loss: 0.0344, Validation Loss: 0.0524, Learning Rate: 0.119000\n",
      "Epoch  406, Training Loss: 0.0344, Validation Loss: 0.0523, Learning Rate: 0.118800\n",
      "Epoch  407, Training Loss: 0.0344, Validation Loss: 0.0522, Learning Rate: 0.118600\n",
      "Epoch  408, Training Loss: 0.0344, Validation Loss: 0.0523, Learning Rate: 0.118400\n",
      "Epoch  409, Training Loss: 0.0344, Validation Loss: 0.0524, Learning Rate: 0.118200\n",
      "Epoch  410, Training Loss: 0.0344, Validation Loss: 0.0525, Learning Rate: 0.118000\n",
      "Epoch  411, Training Loss: 0.0344, Validation Loss: 0.0521, Learning Rate: 0.117800\n",
      "Epoch  412, Training Loss: 0.0344, Validation Loss: 0.0521, Learning Rate: 0.117600\n",
      "Epoch  413, Training Loss: 0.0344, Validation Loss: 0.0518, Learning Rate: 0.117400\n",
      "Epoch  414, Training Loss: 0.0343, Validation Loss: 0.0519, Learning Rate: 0.117200\n",
      "Epoch  415, Training Loss: 0.0343, Validation Loss: 0.0519, Learning Rate: 0.117000\n",
      "Epoch  416, Training Loss: 0.0343, Validation Loss: 0.0520, Learning Rate: 0.116800\n",
      "Epoch  417, Training Loss: 0.0343, Validation Loss: 0.0515, Learning Rate: 0.116600\n",
      "Epoch  418, Training Loss: 0.0343, Validation Loss: 0.0518, Learning Rate: 0.116400\n",
      "Epoch  419, Training Loss: 0.0343, Validation Loss: 0.0515, Learning Rate: 0.116200\n",
      "Epoch  420, Training Loss: 0.0343, Validation Loss: 0.0515, Learning Rate: 0.116000\n",
      "Epoch  421, Training Loss: 0.0343, Validation Loss: 0.0516, Learning Rate: 0.115800\n",
      "Epoch  422, Training Loss: 0.0343, Validation Loss: 0.0513, Learning Rate: 0.115600\n",
      "Epoch  423, Training Loss: 0.0343, Validation Loss: 0.0511, Learning Rate: 0.115400\n",
      "Epoch  424, Training Loss: 0.0343, Validation Loss: 0.0512, Learning Rate: 0.115200\n",
      "Epoch  425, Training Loss: 0.0342, Validation Loss: 0.0513, Learning Rate: 0.115000\n",
      "Epoch  426, Training Loss: 0.0342, Validation Loss: 0.0513, Learning Rate: 0.114800\n",
      "Epoch  427, Training Loss: 0.0342, Validation Loss: 0.0514, Learning Rate: 0.114600\n",
      "Epoch  428, Training Loss: 0.0342, Validation Loss: 0.0512, Learning Rate: 0.114400\n",
      "Epoch  429, Training Loss: 0.0342, Validation Loss: 0.0512, Learning Rate: 0.114200\n",
      "Epoch  430, Training Loss: 0.0342, Validation Loss: 0.0509, Learning Rate: 0.114000\n",
      "Epoch  431, Training Loss: 0.0342, Validation Loss: 0.0512, Learning Rate: 0.113800\n",
      "Epoch  432, Training Loss: 0.0342, Validation Loss: 0.0509, Learning Rate: 0.113600\n",
      "Epoch  433, Training Loss: 0.0342, Validation Loss: 0.0509, Learning Rate: 0.113400\n",
      "Epoch  434, Training Loss: 0.0342, Validation Loss: 0.0510, Learning Rate: 0.113200\n",
      "Epoch  435, Training Loss: 0.0342, Validation Loss: 0.0510, Learning Rate: 0.113000\n",
      "Epoch  436, Training Loss: 0.0342, Validation Loss: 0.0508, Learning Rate: 0.112800\n",
      "Epoch  437, Training Loss: 0.0342, Validation Loss: 0.0506, Learning Rate: 0.112600\n",
      "Epoch  438, Training Loss: 0.0342, Validation Loss: 0.0509, Learning Rate: 0.112400\n",
      "Epoch  439, Training Loss: 0.0341, Validation Loss: 0.0507, Learning Rate: 0.112200\n",
      "Epoch  440, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.112000\n",
      "Epoch  441, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.111800\n",
      "Epoch  442, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.111600\n",
      "Epoch  443, Training Loss: 0.0341, Validation Loss: 0.0505, Learning Rate: 0.111400\n",
      "Epoch  444, Training Loss: 0.0341, Validation Loss: 0.0505, Learning Rate: 0.111200\n",
      "Epoch  445, Training Loss: 0.0341, Validation Loss: 0.0505, Learning Rate: 0.111000\n",
      "Epoch  446, Training Loss: 0.0341, Validation Loss: 0.0504, Learning Rate: 0.110800\n",
      "Epoch  447, Training Loss: 0.0341, Validation Loss: 0.0503, Learning Rate: 0.110600\n",
      "Epoch  448, Training Loss: 0.0341, Validation Loss: 0.0503, Learning Rate: 0.110400\n",
      "Epoch  449, Training Loss: 0.0341, Validation Loss: 0.0503, Learning Rate: 0.110200\n",
      "Epoch  450, Training Loss: 0.0341, Validation Loss: 0.0502, Learning Rate: 0.110000\n",
      "Epoch  451, Training Loss: 0.0341, Validation Loss: 0.0502, Learning Rate: 0.109800\n",
      "Epoch  452, Training Loss: 0.0341, Validation Loss: 0.0503, Learning Rate: 0.109600\n",
      "Epoch  453, Training Loss: 0.0341, Validation Loss: 0.0503, Learning Rate: 0.109400\n",
      "Epoch  454, Training Loss: 0.0341, Validation Loss: 0.0502, Learning Rate: 0.109200\n",
      "Epoch  455, Training Loss: 0.0341, Validation Loss: 0.0500, Learning Rate: 0.109000\n",
      "Epoch  456, Training Loss: 0.0341, Validation Loss: 0.0500, Learning Rate: 0.108800\n",
      "Epoch  457, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.108600\n",
      "Epoch  458, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.108400\n",
      "Epoch  459, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.108200\n",
      "Epoch  460, Training Loss: 0.0340, Validation Loss: 0.0498, Learning Rate: 0.108000\n",
      "Epoch  461, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.107800\n",
      "Epoch  462, Training Loss: 0.0340, Validation Loss: 0.0497, Learning Rate: 0.107600\n",
      "Epoch  463, Training Loss: 0.0340, Validation Loss: 0.0498, Learning Rate: 0.107400\n",
      "Epoch  464, Training Loss: 0.0340, Validation Loss: 0.0497, Learning Rate: 0.107200\n",
      "Epoch  465, Training Loss: 0.0340, Validation Loss: 0.0496, Learning Rate: 0.107000\n",
      "Epoch  466, Training Loss: 0.0340, Validation Loss: 0.0496, Learning Rate: 0.106800\n",
      "Epoch  467, Training Loss: 0.0340, Validation Loss: 0.0497, Learning Rate: 0.106600\n",
      "Epoch  468, Training Loss: 0.0340, Validation Loss: 0.0496, Learning Rate: 0.106400\n",
      "Epoch  469, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.106200\n",
      "Epoch  470, Training Loss: 0.0340, Validation Loss: 0.0494, Learning Rate: 0.106000\n",
      "Epoch  471, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.105800\n",
      "Epoch  472, Training Loss: 0.0340, Validation Loss: 0.0494, Learning Rate: 0.105600\n",
      "Epoch  473, Training Loss: 0.0340, Validation Loss: 0.0494, Learning Rate: 0.105400\n",
      "Epoch  474, Training Loss: 0.0340, Validation Loss: 0.0494, Learning Rate: 0.105200\n",
      "Epoch  475, Training Loss: 0.0340, Validation Loss: 0.0492, Learning Rate: 0.105000\n",
      "Epoch  476, Training Loss: 0.0340, Validation Loss: 0.0492, Learning Rate: 0.104800\n",
      "Epoch  477, Training Loss: 0.0340, Validation Loss: 0.0494, Learning Rate: 0.104600\n",
      "Epoch  478, Training Loss: 0.0340, Validation Loss: 0.0493, Learning Rate: 0.104400\n",
      "Epoch  479, Training Loss: 0.0340, Validation Loss: 0.0493, Learning Rate: 0.104200\n",
      "Epoch  480, Training Loss: 0.0340, Validation Loss: 0.0493, Learning Rate: 0.104000\n",
      "Epoch  481, Training Loss: 0.0340, Validation Loss: 0.0493, Learning Rate: 0.103800\n",
      "Epoch  482, Training Loss: 0.0340, Validation Loss: 0.0491, Learning Rate: 0.103600\n",
      "Epoch  483, Training Loss: 0.0340, Validation Loss: 0.0491, Learning Rate: 0.103400\n",
      "Epoch  484, Training Loss: 0.0340, Validation Loss: 0.0490, Learning Rate: 0.103200\n",
      "Epoch  485, Training Loss: 0.0340, Validation Loss: 0.0490, Learning Rate: 0.103000\n",
      "Epoch  486, Training Loss: 0.0340, Validation Loss: 0.0490, Learning Rate: 0.102800\n",
      "Epoch  487, Training Loss: 0.0340, Validation Loss: 0.0490, Learning Rate: 0.102600\n",
      "Epoch  488, Training Loss: 0.0340, Validation Loss: 0.0489, Learning Rate: 0.102400\n",
      "Epoch  489, Training Loss: 0.0340, Validation Loss: 0.0489, Learning Rate: 0.102200\n",
      "Epoch  490, Training Loss: 0.0340, Validation Loss: 0.0488, Learning Rate: 0.102000\n",
      "Epoch  491, Training Loss: 0.0339, Validation Loss: 0.0488, Learning Rate: 0.101800\n",
      "Epoch  492, Training Loss: 0.0339, Validation Loss: 0.0488, Learning Rate: 0.101600\n",
      "Epoch  493, Training Loss: 0.0339, Validation Loss: 0.0488, Learning Rate: 0.101400\n",
      "Epoch  494, Training Loss: 0.0339, Validation Loss: 0.0487, Learning Rate: 0.101200\n",
      "Epoch  495, Training Loss: 0.0339, Validation Loss: 0.0487, Learning Rate: 0.101000\n",
      "Epoch  496, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.100800\n",
      "Epoch  497, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.100600\n",
      "Epoch  498, Training Loss: 0.0339, Validation Loss: 0.0487, Learning Rate: 0.100400\n",
      "Epoch  499, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.100200\n",
      "Epoch  500, Training Loss: 0.0339, Validation Loss: 0.0484, Learning Rate: 0.100000\n",
      "Epoch  501, Training Loss: 0.0339, Validation Loss: 0.0485, Learning Rate: 0.099800\n",
      "Epoch  502, Training Loss: 0.0339, Validation Loss: 0.0485, Learning Rate: 0.099600\n",
      "Epoch  503, Training Loss: 0.0339, Validation Loss: 0.0485, Learning Rate: 0.099400\n",
      "Epoch  504, Training Loss: 0.0339, Validation Loss: 0.0484, Learning Rate: 0.099200\n",
      "Epoch  505, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.099000\n",
      "Epoch  506, Training Loss: 0.0339, Validation Loss: 0.0484, Learning Rate: 0.098800\n",
      "Epoch  507, Training Loss: 0.0339, Validation Loss: 0.0484, Learning Rate: 0.098600\n",
      "Epoch  508, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.098400\n",
      "Epoch  509, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.098200\n",
      "Epoch  510, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.098000\n",
      "Epoch  511, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.097800\n",
      "Epoch  512, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.097600\n",
      "Epoch  513, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.097400\n",
      "Epoch  514, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.097200\n",
      "Epoch  515, Training Loss: 0.0339, Validation Loss: 0.0482, Learning Rate: 0.097000\n",
      "Epoch  516, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.096800\n",
      "Epoch  517, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.096600\n",
      "Epoch  518, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.096400\n",
      "Epoch  519, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.096200\n",
      "Epoch  520, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.096000\n",
      "Epoch  521, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.095800\n",
      "Epoch  522, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.095600\n",
      "Epoch  523, Training Loss: 0.0339, Validation Loss: 0.0479, Learning Rate: 0.095400\n",
      "Epoch  524, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.095200\n",
      "Epoch  525, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.095000\n",
      "Epoch  526, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.094800\n",
      "Epoch  527, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.094600\n",
      "Epoch  528, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.094400\n",
      "Epoch  529, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.094200\n",
      "Epoch  530, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.094000\n",
      "Epoch  531, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.093800\n",
      "Epoch  532, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.093600\n",
      "Epoch  533, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.093400\n",
      "Epoch  534, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.093200\n",
      "Early stopping triggered at epoch 534. Restoring best model parameters.\n",
      "\n",
      "Neural Network Classification Accuracy: 0.9907\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "# ============================\n",
    "# Activation functions and their derivatives\n",
    "# ============================\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(z, a):\n",
    "    # Note: z is not used here; kept for uniform signature.\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(z, a):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(z, a):\n",
    "    return np.ones_like(a)\n",
    "\n",
    "activation_functions = {\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"relu\": relu,\n",
    "    \"linear\": linear\n",
    "}\n",
    "\n",
    "activation_derivatives = {\n",
    "    \"sigmoid\": lambda z, a: sigmoid_derivative(z, a),\n",
    "    \"relu\": lambda z, a: relu_derivative(z, a),\n",
    "    \"linear\": lambda z, a: linear_derivative(z, a)\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Loss functions and their derivatives\n",
    "# ============================\n",
    "\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Binary crossentropy loss for binary classification.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_crossentropy_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the binary crossentropy loss.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return - (y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Squared Error (MSE) loss, typically used for regression.\n",
    "    \"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the MSE loss.\n",
    "    \"\"\"\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "def mee_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Euclidean Error (MEE) loss, defined as:\n",
    "        E_MEE = (1/N) * sum over i [ ||y_true[i] - y_pred[i]||_2 ].\n",
    "    \"\"\"\n",
    "    diff = y_true - y_pred  # shape: (N, d) or (N, 1)\n",
    "    # Euclidean distance for each sample\n",
    "    dist = np.sqrt(np.sum(diff ** 2, axis=1))\n",
    "    return np.mean(dist)\n",
    "\n",
    "def mee_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the Mean Euclidean Error (MEE) loss.\n",
    "    For each sample i, derivative wrt y_pred[i] is:\n",
    "        (1/N) * ( (y_pred[i] - y_true[i]) / ||y_pred[i] - y_true[i]||_2 ).\n",
    "    We safely handle the case where the norm is zero.\n",
    "    \"\"\"\n",
    "    diff = y_pred - y_true\n",
    "    dist = np.sqrt(np.sum(diff ** 2, axis=1, keepdims=True))\n",
    "    epsilon = 1e-8  # Avoid division by zero\n",
    "    dist_safe = np.where(dist == 0, epsilon, dist)\n",
    "    N = y_true.shape[0]\n",
    "    derivative = diff / dist_safe / N\n",
    "    return derivative\n",
    "\n",
    "loss_functions = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_loss,\n",
    "    \"mse\": mse_loss,\n",
    "    \"mee\": mee_loss,  \n",
    "}\n",
    "\n",
    "loss_derivatives = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_derivative,\n",
    "    \"mse\": mse_derivative,\n",
    "    \"mee\": mee_derivative, \n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Regularization functions (modular)\n",
    "# ============================\n",
    "\n",
    "def compute_reg_gradient(W, lambda_reg, reg_type, m):\n",
    "    if reg_type == \"l2\":\n",
    "        return lambda_reg * W / m\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * np.sign(W) / m\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_reg_loss(W_list, lambda_reg, reg_type):\n",
    "    if reg_type == \"l2\":\n",
    "        return (lambda_reg / 2) * sum(np.sum(W ** 2) for W in W_list)\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * sum(np.sum(np.abs(W)) for W in W_list)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# ============================\n",
    "# Neural Network Class with Learning Rate Decay, Momentum, Custom Weight Initialization, and Early Stopping\n",
    "# ============================\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.01, lambda_reg=0.001, reg_type=\"l2\",\n",
    "                 loss_function_name=None,\n",
    "                 activation_function_name=\"relu\",\n",
    "                 output_activation_function_name=None,\n",
    "                 activation_function_names=None,\n",
    "                 task=\"classification\",\n",
    "                 lr_decay_type=\"none\",  # Options: \"none\", \"exponential\", \"linear\"\n",
    "                 decay_rate=0.0,\n",
    "                 weight_init=\"base\",  # \"base\" (fan-in scaling) or \"glorot\"\n",
    "                 momentum_type=\"none\",  # Options: \"none\", \"momentum\", \"nesterov momentum\"\n",
    "                 momentum_alpha=0.9):\n",
    "        \"\"\"\n",
    "        :param layers: List containing the size of each layer (input, hidden, output)\n",
    "        :param learning_rate: Initial learning rate\n",
    "        :param lambda_reg: Regularization coefficient\n",
    "        :param reg_type: Type of regularization (\"l2\", \"l1\", or other for none)\n",
    "        :param loss_function_name: Name of the loss function (if None, set based on task)\n",
    "        :param activation_function_name: Activation to use for hidden layers (if activation_function_names not provided)\n",
    "        :param output_activation_function_name: Activation for the output layer (if None, set based on task)\n",
    "        :param activation_function_names: List of activation function names for each layer (length = len(layers)-1)\n",
    "        :param task: \"classification\" or \"regression\"\n",
    "        :param lr_decay_type: Learning rate decay strategy (\"none\", \"exponential\", \"linear\")\n",
    "        :param decay_rate: Decay rate used in the learning rate schedule\n",
    "        :param weight_init: Weight initialization strategy (\"base\" uses fan-in scaling or \"glorot\")\n",
    "        :param momentum_type: Momentum strategy (\"none\", \"momentum\", \"nesterov momentum\")\n",
    "        :param momentum_alpha: Momentum coefficient (e.g., 0.9)\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.reg_type = reg_type\n",
    "        self.task = task\n",
    "        self.lr_decay_type = lr_decay_type\n",
    "        self.decay_rate = decay_rate\n",
    "        self.weight_init = weight_init\n",
    "        \n",
    "        # Set momentum parameters\n",
    "        if momentum_type not in {\"none\", \"momentum\", \"nesterov momentum\"}:\n",
    "            raise ValueError(\"momentum_type must be 'none', 'momentum', or 'nesterov momentum'.\")\n",
    "        self.momentum_type = momentum_type\n",
    "        self.momentum_alpha = momentum_alpha if momentum_type != \"none\" else 0.0\n",
    "        \n",
    "        # Set defaults based on task\n",
    "        if self.task == \"regression\":\n",
    "            self.loss_function_name = loss_function_name or \"mse\"\n",
    "            output_activation_function_name = output_activation_function_name or \"linear\"\n",
    "        else:\n",
    "            # Classification\n",
    "            self.loss_function_name = loss_function_name or \"binary_crossentropy\"\n",
    "            output_activation_function_name = output_activation_function_name or \"sigmoid\"\n",
    "        \n",
    "        # Set activation functions for layers\n",
    "        if activation_function_names is None:\n",
    "            self.activation_function_names = [activation_function_name] * (len(layers) - 1)\n",
    "            self.activation_function_names[-1] = output_activation_function_name\n",
    "        else:\n",
    "            if len(activation_function_names) != len(layers) - 1:\n",
    "                raise ValueError(\"activation_function_names must have length equal to len(layers)-1.\")\n",
    "            self.activation_function_names = activation_function_names\n",
    "        \n",
    "        self._initialize_parameters()\n",
    "        # Initialize momentum accumulators (even if not used, for consistency)\n",
    "        self.vW = [np.zeros_like(W) for W in self.W]\n",
    "        self.vb = [np.zeros_like(b) for b in self.b]\n",
    "        \n",
    "        # Initialize loss history lists (will be (re)initialized in train())\n",
    "        self.train_loss_history = []\n",
    "        self.val_loss_history = None\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            fan_in = self.layers[i]\n",
    "            fan_out = self.layers[i + 1]\n",
    "            if self.weight_init == \"base\":\n",
    "                std = np.sqrt(1.0 / fan_in)\n",
    "            elif self.weight_init == \"glorot\":\n",
    "                std = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported weight initialization strategy. Use 'base' or 'glorot'.\")\n",
    "            weight = np.random.randn(fan_in, fan_out) * std\n",
    "            self.W.append(weight)\n",
    "            self.b.append(np.zeros((1, fan_out)))\n",
    "    \n",
    "    def _apply_activation(self, x, func_name):\n",
    "        if func_name not in activation_functions:\n",
    "            raise ValueError(f\"Unsupported activation: {func_name}\")\n",
    "        return activation_functions[func_name](x)\n",
    "    \n",
    "    def _apply_activation_derivative(self, z, a, func_name):\n",
    "        if func_name not in activation_derivatives:\n",
    "            raise ValueError(f\"Unsupported activation derivative: {func_name}\")\n",
    "        return activation_derivatives[func_name](z, a)\n",
    "    \n",
    "    def _forward(self, X, weights=None, biases=None):\n",
    "        \"\"\"\n",
    "        Forward propagation. If weights and biases are provided, they are used;\n",
    "        otherwise the network's parameters are used.\n",
    "        Returns lists Z (pre-activations) and A (activations).\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.W\n",
    "        if biases is None:\n",
    "            biases = self.b\n",
    "            \n",
    "        A = [X]\n",
    "        Z = []\n",
    "        # Forward through hidden layers\n",
    "        for i in range(len(weights) - 1):\n",
    "            z_curr = np.dot(A[-1], weights[i]) + biases[i]\n",
    "            Z.append(z_curr)\n",
    "            a_curr = self._apply_activation(z_curr, self.activation_function_names[i])\n",
    "            A.append(a_curr)\n",
    "        # Forward through output layer\n",
    "        z_out = np.dot(A[-1], weights[-1]) + biases[-1]\n",
    "        Z.append(z_out)\n",
    "        a_out = self._apply_activation(z_out, self.activation_function_names[-1])\n",
    "        A.append(a_out)\n",
    "        return Z, A\n",
    "    \n",
    "    def _compute_gradients(self, X, y, Z, A, weights=None):\n",
    "        \"\"\"\n",
    "        Compute gradients dW and db given inputs X, target y, pre-activations Z and activations A.\n",
    "        Optionally, a custom set of weights (used in lookahead for Nesterov momentum) can be provided.\n",
    "        Returns lists dW and db.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.W\n",
    "        m = X.shape[0]\n",
    "        dA = loss_derivatives[self.loss_function_name](y, A[-1])\n",
    "        # Output layer\n",
    "        dZ = dA * self._apply_activation_derivative(Z[-1], A[-1], self.activation_function_names[-1])\n",
    "        reg_term = compute_reg_gradient(weights[-1], self.lambda_reg, self.reg_type, m)\n",
    "        dW = [np.dot(A[-2].T, dZ) / m + reg_term]\n",
    "        db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "        \n",
    "        # Backpropagate through hidden layers\n",
    "        for i in range(len(weights) - 2, -1, -1):\n",
    "            dA = np.dot(dZ, weights[i + 1].T)\n",
    "            dZ = dA * self._apply_activation_derivative(Z[i], A[i + 1], self.activation_function_names[i])\n",
    "            reg_term = compute_reg_gradient(weights[i], self.lambda_reg, self.reg_type, m)\n",
    "            dW.insert(0, np.dot(A[i].T, dZ) / m + reg_term)\n",
    "            db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "            \n",
    "        return dW, db\n",
    "    \n",
    "    def train(self, X, y, epochs=300, batch_size=32, verbose=True,\n",
    "              early_stopping=False, validation_data=None, patience=10, min_delta=0.0):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "\n",
    "        The loss histories for training and validation (if provided) are stored in:\n",
    "            self.train_loss_history and self.val_loss_history\n",
    "\n",
    "        :param X: Training data inputs.\n",
    "        :param y: Training data targets.\n",
    "        :param epochs: Maximum number of epochs to train.\n",
    "        :param batch_size: Mini-batch size.\n",
    "        :param verbose: Whether to print progress.\n",
    "        :param early_stopping: Enable early stopping if True.\n",
    "        :param validation_data: Tuple (X_val, y_val) for early stopping and validation loss logging.\n",
    "        :param patience: Number of epochs with no improvement to wait before stopping.\n",
    "        :param min_delta: Minimum change in the monitored loss to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        # Reinitialize loss histories\n",
    "        self.train_loss_history = []\n",
    "        if validation_data is not None:\n",
    "            self.val_loss_history = []\n",
    "        else:\n",
    "            self.val_loss_history = None\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        best_loss = np.inf\n",
    "        patience_counter = 0\n",
    "        best_weights = None\n",
    "        best_biases = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Update learning rate based on decay schedule\n",
    "            if self.lr_decay_type == \"exponential\":\n",
    "                self.learning_rate = self.initial_learning_rate * np.exp(-self.decay_rate * epoch)\n",
    "            elif self.lr_decay_type == \"linear\":\n",
    "                self.learning_rate = self.initial_learning_rate * max(0, 1 - self.decay_rate * epoch)\n",
    "            # Otherwise (\"none\"), keep the initial learning rate.\n",
    "            \n",
    "            # Shuffle training data\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            \n",
    "            # Process mini-batches\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                \n",
    "                if self.momentum_type == \"nesterov momentum\":\n",
    "                    weights_lookahead = [self.W[j] - self.momentum_alpha * self.vW[j] for j in range(len(self.W))]\n",
    "                    biases_lookahead = [self.b[j] - self.momentum_alpha * self.vb[j] for j in range(len(self.b))]\n",
    "                    Z, A = self._forward(X_batch, weights=weights_lookahead, biases=biases_lookahead)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A, weights=weights_lookahead)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.vW[j] = self.momentum_alpha * self.vW[j] + self.learning_rate * dW[j]\n",
    "                        self.vb[j] = self.momentum_alpha * self.vb[j] + self.learning_rate * db[j]\n",
    "                        self.W[j] -= self.vW[j]\n",
    "                        self.b[j] -= self.vb[j]\n",
    "                        \n",
    "                elif self.momentum_type == \"momentum\":\n",
    "                    Z, A = self._forward(X_batch)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.vW[j] = self.momentum_alpha * self.vW[j] + self.learning_rate * dW[j]\n",
    "                        self.vb[j] = self.momentum_alpha * self.vb[j] + self.learning_rate * db[j]\n",
    "                        self.W[j] -= self.vW[j]\n",
    "                        self.b[j] -= self.vb[j]\n",
    "                        \n",
    "                else:  # No momentum\n",
    "                    Z, A = self._forward(X_batch)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.W[j] -= self.learning_rate * dW[j]\n",
    "                        self.b[j] -= self.learning_rate * db[j]\n",
    "            \n",
    "            # Compute training loss\n",
    "            _, A_full = self._forward(X)\n",
    "            train_loss = loss_functions[self.loss_function_name](y, A_full[-1])\n",
    "            reg_loss = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "            total_train_loss = train_loss + reg_loss\n",
    "            self.train_loss_history.append(total_train_loss)\n",
    "            \n",
    "            # Compute validation loss if validation data is provided\n",
    "            if validation_data is not None:\n",
    "                X_val, y_val = validation_data\n",
    "                _, A_val = self._forward(X_val)\n",
    "                val_loss = loss_functions[self.loss_function_name](y_val, A_val[-1])\n",
    "                reg_loss_val = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "                total_val_loss = val_loss + reg_loss_val\n",
    "                self.val_loss_history.append(total_val_loss)\n",
    "            else:\n",
    "                total_val_loss = None\n",
    "\n",
    "            # Verbose logging\n",
    "            if verbose:\n",
    "                if total_val_loss is not None:\n",
    "                    print(f\"Epoch {epoch:4d}, Training Loss: {total_train_loss:.4f}, \"\n",
    "                          f\"Validation Loss: {total_val_loss:.4f}, Learning Rate: {self.learning_rate:.6f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch:4d}, Training Loss: {total_train_loss:.4f}, \"\n",
    "                          f\"Learning Rate: {self.learning_rate:.6f}\")\n",
    "            \n",
    "            # Early stopping check (only if validation data is provided)\n",
    "            if early_stopping and (validation_data is not None):\n",
    "                if total_val_loss < best_loss - min_delta:\n",
    "                    best_loss = total_val_loss\n",
    "                    patience_counter = 0\n",
    "                    best_weights = [w.copy() for w in self.W]\n",
    "                    best_biases = [b.copy() for b in self.b]\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        if verbose:\n",
    "                            print(f\"Early stopping triggered at epoch {epoch}. Restoring best model parameters.\")\n",
    "                        if best_weights is not None:\n",
    "                            self.W = best_weights\n",
    "                            self.b = best_biases\n",
    "                        break\n",
    "\n",
    "    def plot_loss_history(self):\n",
    "        \"\"\"\n",
    "        Plot the training loss history and, if available, the validation loss history.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.train_loss_history, label=\"Training Loss\")\n",
    "        if self.val_loss_history is not None and len(self.val_loss_history) > 0:\n",
    "            plt.plot(self.val_loss_history, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss History\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, A = self._forward(X)\n",
    "        output = A[-1]\n",
    "        if self.task == \"classification\":\n",
    "            if output.shape[1] == 1:\n",
    "                return (output > 0.5).astype(int)\n",
    "            else:\n",
    "                return np.argmax(output, axis=1)\n",
    "        else:\n",
    "            return output\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        if self.task == \"regression\":\n",
    "            return mse_loss(y, predictions)\n",
    "        else:\n",
    "            if y.ndim > 1 and y.shape[1] > 1:\n",
    "                y_true = np.argmax(y, axis=1)\n",
    "            else:\n",
    "                y_true = y\n",
    "            return np.mean(predictions == y_true)\n",
    "\n",
    "# ============================\n",
    "# Testing on a monk's dataset\n",
    "# ============================\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(1, one_hot_encode=True)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_units = 10\n",
    "output_size = 1  # binary classification\n",
    "layers = [input_size, hidden_units, output_size]\n",
    "\n",
    "# Define activation functions for hidden and output layers\n",
    "activation_funcs = [\"relu\", \"sigmoid\"]\n",
    "\n",
    "nn_clf = NeuralNetwork(\n",
    "    layers=layers,\n",
    "    learning_rate=0.2,\n",
    "    lambda_reg=0.001,\n",
    "    reg_type=\"l2\",\n",
    "    loss_function_name=\"mse\",       \n",
    "    activation_function_names=activation_funcs,\n",
    "    task=\"classification\",\n",
    "    lr_decay_type=\"linear\",    # Options: \"exponential\", \"linear\", or \"none\"\n",
    "    decay_rate=0.001,\n",
    "    weight_init=\"base\",        # \"base\" or \"glorot\"\n",
    ")\n",
    "\n",
    "# For early stopping, we provide validation data.\n",
    "nn_clf.train(\n",
    "    X_train, y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    verbose=True,\n",
    "    early_stopping=True,\n",
    "    validation_data=(X_test, y_test),\n",
    "    patience=10,\n",
    "    min_delta=1e-4\n",
    ")\n",
    "\n",
    "accuracy = nn_clf.evaluate(X_test, y_test)\n",
    "print(f\"\\nNeural Network Classification Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot the loss histories\n",
    "nn_clf.plot_loss_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-1-train.csv\n",
      "Using cached ../datasets/monks/monk-1-test.csv\n",
      "One-hot encoding MONK-1 dataset...\n",
      "Epoch    0, Training Loss: 0.2537, Validation Loss: 0.2570, Learning Rate: 0.200000\n",
      "Epoch    1, Training Loss: 0.2504, Validation Loss: 0.2549, Learning Rate: 0.199800\n",
      "Epoch    2, Training Loss: 0.2473, Validation Loss: 0.2529, Learning Rate: 0.199600\n",
      "Epoch    3, Training Loss: 0.2444, Validation Loss: 0.2511, Learning Rate: 0.199400\n",
      "Epoch    4, Training Loss: 0.2416, Validation Loss: 0.2493, Learning Rate: 0.199200\n",
      "Epoch    5, Training Loss: 0.2386, Validation Loss: 0.2474, Learning Rate: 0.199000\n",
      "Epoch    6, Training Loss: 0.2355, Validation Loss: 0.2453, Learning Rate: 0.198800\n",
      "Epoch    7, Training Loss: 0.2324, Validation Loss: 0.2430, Learning Rate: 0.198600\n",
      "Epoch    8, Training Loss: 0.2291, Validation Loss: 0.2407, Learning Rate: 0.198400\n",
      "Epoch    9, Training Loss: 0.2251, Validation Loss: 0.2380, Learning Rate: 0.198200\n",
      "Epoch   10, Training Loss: 0.2206, Validation Loss: 0.2348, Learning Rate: 0.198000\n",
      "Epoch   11, Training Loss: 0.2164, Validation Loss: 0.2318, Learning Rate: 0.197800\n",
      "Epoch   12, Training Loss: 0.2124, Validation Loss: 0.2289, Learning Rate: 0.197600\n",
      "Epoch   13, Training Loss: 0.2074, Validation Loss: 0.2253, Learning Rate: 0.197400\n",
      "Epoch   14, Training Loss: 0.2019, Validation Loss: 0.2216, Learning Rate: 0.197200\n",
      "Epoch   15, Training Loss: 0.1970, Validation Loss: 0.2185, Learning Rate: 0.197000\n",
      "Epoch   16, Training Loss: 0.1925, Validation Loss: 0.2154, Learning Rate: 0.196800\n",
      "Epoch   17, Training Loss: 0.1882, Validation Loss: 0.2124, Learning Rate: 0.196600\n",
      "Epoch   18, Training Loss: 0.1843, Validation Loss: 0.2096, Learning Rate: 0.196400\n",
      "Epoch   19, Training Loss: 0.1805, Validation Loss: 0.2068, Learning Rate: 0.196200\n",
      "Epoch   20, Training Loss: 0.1769, Validation Loss: 0.2043, Learning Rate: 0.196000\n",
      "Epoch   21, Training Loss: 0.1734, Validation Loss: 0.2017, Learning Rate: 0.195800\n",
      "Epoch   22, Training Loss: 0.1703, Validation Loss: 0.1994, Learning Rate: 0.195600\n",
      "Epoch   23, Training Loss: 0.1670, Validation Loss: 0.1969, Learning Rate: 0.195400\n",
      "Epoch   24, Training Loss: 0.1641, Validation Loss: 0.1949, Learning Rate: 0.195200\n",
      "Epoch   25, Training Loss: 0.1614, Validation Loss: 0.1927, Learning Rate: 0.195000\n",
      "Epoch   26, Training Loss: 0.1587, Validation Loss: 0.1907, Learning Rate: 0.194800\n",
      "Epoch   27, Training Loss: 0.1564, Validation Loss: 0.1889, Learning Rate: 0.194600\n",
      "Epoch   28, Training Loss: 0.1541, Validation Loss: 0.1871, Learning Rate: 0.194400\n",
      "Epoch   29, Training Loss: 0.1519, Validation Loss: 0.1856, Learning Rate: 0.194200\n",
      "Epoch   30, Training Loss: 0.1501, Validation Loss: 0.1840, Learning Rate: 0.194000\n",
      "Epoch   31, Training Loss: 0.1480, Validation Loss: 0.1828, Learning Rate: 0.193800\n",
      "Epoch   32, Training Loss: 0.1463, Validation Loss: 0.1812, Learning Rate: 0.193600\n",
      "Epoch   33, Training Loss: 0.1445, Validation Loss: 0.1802, Learning Rate: 0.193400\n",
      "Epoch   34, Training Loss: 0.1428, Validation Loss: 0.1787, Learning Rate: 0.193200\n",
      "Epoch   35, Training Loss: 0.1414, Validation Loss: 0.1780, Learning Rate: 0.193000\n",
      "Epoch   36, Training Loss: 0.1397, Validation Loss: 0.1766, Learning Rate: 0.192800\n",
      "Epoch   37, Training Loss: 0.1381, Validation Loss: 0.1753, Learning Rate: 0.192600\n",
      "Epoch   38, Training Loss: 0.1367, Validation Loss: 0.1743, Learning Rate: 0.192400\n",
      "Epoch   39, Training Loss: 0.1354, Validation Loss: 0.1737, Learning Rate: 0.192200\n",
      "Epoch   40, Training Loss: 0.1340, Validation Loss: 0.1726, Learning Rate: 0.192000\n",
      "Epoch   41, Training Loss: 0.1327, Validation Loss: 0.1712, Learning Rate: 0.191800\n",
      "Epoch   42, Training Loss: 0.1314, Validation Loss: 0.1703, Learning Rate: 0.191600\n",
      "Epoch   43, Training Loss: 0.1301, Validation Loss: 0.1696, Learning Rate: 0.191400\n",
      "Epoch   44, Training Loss: 0.1289, Validation Loss: 0.1688, Learning Rate: 0.191200\n",
      "Epoch   45, Training Loss: 0.1278, Validation Loss: 0.1670, Learning Rate: 0.191000\n",
      "Epoch   46, Training Loss: 0.1266, Validation Loss: 0.1669, Learning Rate: 0.190800\n",
      "Epoch   47, Training Loss: 0.1254, Validation Loss: 0.1658, Learning Rate: 0.190600\n",
      "Epoch   48, Training Loss: 0.1243, Validation Loss: 0.1651, Learning Rate: 0.190400\n",
      "Epoch   49, Training Loss: 0.1233, Validation Loss: 0.1633, Learning Rate: 0.190200\n",
      "Epoch   50, Training Loss: 0.1221, Validation Loss: 0.1630, Learning Rate: 0.190000\n",
      "Epoch   51, Training Loss: 0.1211, Validation Loss: 0.1625, Learning Rate: 0.189800\n",
      "Epoch   52, Training Loss: 0.1201, Validation Loss: 0.1613, Learning Rate: 0.189600\n",
      "Epoch   53, Training Loss: 0.1191, Validation Loss: 0.1610, Learning Rate: 0.189400\n",
      "Epoch   54, Training Loss: 0.1182, Validation Loss: 0.1608, Learning Rate: 0.189200\n",
      "Epoch   55, Training Loss: 0.1171, Validation Loss: 0.1590, Learning Rate: 0.189000\n",
      "Epoch   56, Training Loss: 0.1162, Validation Loss: 0.1583, Learning Rate: 0.188800\n",
      "Epoch   57, Training Loss: 0.1153, Validation Loss: 0.1572, Learning Rate: 0.188600\n",
      "Epoch   58, Training Loss: 0.1144, Validation Loss: 0.1567, Learning Rate: 0.188400\n",
      "Epoch   59, Training Loss: 0.1135, Validation Loss: 0.1564, Learning Rate: 0.188200\n",
      "Epoch   60, Training Loss: 0.1127, Validation Loss: 0.1559, Learning Rate: 0.188000\n",
      "Epoch   61, Training Loss: 0.1117, Validation Loss: 0.1541, Learning Rate: 0.187800\n",
      "Epoch   62, Training Loss: 0.1109, Validation Loss: 0.1534, Learning Rate: 0.187600\n",
      "Epoch   63, Training Loss: 0.1100, Validation Loss: 0.1534, Learning Rate: 0.187400\n",
      "Epoch   64, Training Loss: 0.1092, Validation Loss: 0.1527, Learning Rate: 0.187200\n",
      "Epoch   65, Training Loss: 0.1083, Validation Loss: 0.1517, Learning Rate: 0.187000\n",
      "Epoch   66, Training Loss: 0.1075, Validation Loss: 0.1511, Learning Rate: 0.186800\n",
      "Epoch   67, Training Loss: 0.1067, Validation Loss: 0.1500, Learning Rate: 0.186600\n",
      "Epoch   68, Training Loss: 0.1059, Validation Loss: 0.1496, Learning Rate: 0.186400\n",
      "Epoch   69, Training Loss: 0.1051, Validation Loss: 0.1485, Learning Rate: 0.186200\n",
      "Epoch   70, Training Loss: 0.1044, Validation Loss: 0.1478, Learning Rate: 0.186000\n",
      "Epoch   71, Training Loss: 0.1037, Validation Loss: 0.1467, Learning Rate: 0.185800\n",
      "Epoch   72, Training Loss: 0.1029, Validation Loss: 0.1471, Learning Rate: 0.185600\n",
      "Epoch   73, Training Loss: 0.1023, Validation Loss: 0.1473, Learning Rate: 0.185400\n",
      "Epoch   74, Training Loss: 0.1013, Validation Loss: 0.1454, Learning Rate: 0.185200\n",
      "Epoch   75, Training Loss: 0.1006, Validation Loss: 0.1448, Learning Rate: 0.185000\n",
      "Epoch   76, Training Loss: 0.1000, Validation Loss: 0.1434, Learning Rate: 0.184800\n",
      "Epoch   77, Training Loss: 0.0992, Validation Loss: 0.1429, Learning Rate: 0.184600\n",
      "Epoch   78, Training Loss: 0.0984, Validation Loss: 0.1428, Learning Rate: 0.184400\n",
      "Epoch   79, Training Loss: 0.0977, Validation Loss: 0.1418, Learning Rate: 0.184200\n",
      "Epoch   80, Training Loss: 0.0970, Validation Loss: 0.1417, Learning Rate: 0.184000\n",
      "Epoch   81, Training Loss: 0.0967, Validation Loss: 0.1425, Learning Rate: 0.183800\n",
      "Epoch   82, Training Loss: 0.0956, Validation Loss: 0.1405, Learning Rate: 0.183600\n",
      "Epoch   83, Training Loss: 0.0949, Validation Loss: 0.1393, Learning Rate: 0.183400\n",
      "Epoch   84, Training Loss: 0.0943, Validation Loss: 0.1395, Learning Rate: 0.183200\n",
      "Epoch   85, Training Loss: 0.0937, Validation Loss: 0.1387, Learning Rate: 0.183000\n",
      "Epoch   86, Training Loss: 0.0928, Validation Loss: 0.1371, Learning Rate: 0.182800\n",
      "Epoch   87, Training Loss: 0.0925, Validation Loss: 0.1382, Learning Rate: 0.182600\n",
      "Epoch   88, Training Loss: 0.0915, Validation Loss: 0.1359, Learning Rate: 0.182400\n",
      "Epoch   89, Training Loss: 0.0912, Validation Loss: 0.1345, Learning Rate: 0.182200\n",
      "Epoch   90, Training Loss: 0.0904, Validation Loss: 0.1341, Learning Rate: 0.182000\n",
      "Epoch   91, Training Loss: 0.0897, Validation Loss: 0.1347, Learning Rate: 0.181800\n",
      "Epoch   92, Training Loss: 0.0895, Validation Loss: 0.1354, Learning Rate: 0.181600\n",
      "Epoch   93, Training Loss: 0.0885, Validation Loss: 0.1332, Learning Rate: 0.181400\n",
      "Epoch   94, Training Loss: 0.0879, Validation Loss: 0.1319, Learning Rate: 0.181200\n",
      "Epoch   95, Training Loss: 0.0872, Validation Loss: 0.1318, Learning Rate: 0.181000\n",
      "Epoch   96, Training Loss: 0.0871, Validation Loss: 0.1328, Learning Rate: 0.180800\n",
      "Epoch   97, Training Loss: 0.0863, Validation Loss: 0.1318, Learning Rate: 0.180600\n",
      "Epoch   98, Training Loss: 0.0856, Validation Loss: 0.1306, Learning Rate: 0.180400\n",
      "Epoch   99, Training Loss: 0.0850, Validation Loss: 0.1290, Learning Rate: 0.180200\n",
      "Epoch  100, Training Loss: 0.0846, Validation Loss: 0.1279, Learning Rate: 0.180000\n",
      "Epoch  101, Training Loss: 0.0839, Validation Loss: 0.1289, Learning Rate: 0.179800\n",
      "Epoch  102, Training Loss: 0.0834, Validation Loss: 0.1268, Learning Rate: 0.179600\n",
      "Epoch  103, Training Loss: 0.0829, Validation Loss: 0.1278, Learning Rate: 0.179400\n",
      "Epoch  104, Training Loss: 0.0823, Validation Loss: 0.1257, Learning Rate: 0.179200\n",
      "Epoch  105, Training Loss: 0.0816, Validation Loss: 0.1260, Learning Rate: 0.179000\n",
      "Epoch  106, Training Loss: 0.0811, Validation Loss: 0.1249, Learning Rate: 0.178800\n",
      "Epoch  107, Training Loss: 0.0805, Validation Loss: 0.1246, Learning Rate: 0.178600\n",
      "Epoch  108, Training Loss: 0.0800, Validation Loss: 0.1240, Learning Rate: 0.178400\n",
      "Epoch  109, Training Loss: 0.0795, Validation Loss: 0.1232, Learning Rate: 0.178200\n",
      "Epoch  110, Training Loss: 0.0791, Validation Loss: 0.1237, Learning Rate: 0.178000\n",
      "Epoch  111, Training Loss: 0.0785, Validation Loss: 0.1224, Learning Rate: 0.177800\n",
      "Epoch  112, Training Loss: 0.0781, Validation Loss: 0.1217, Learning Rate: 0.177600\n",
      "Epoch  113, Training Loss: 0.0776, Validation Loss: 0.1224, Learning Rate: 0.177400\n",
      "Epoch  114, Training Loss: 0.0770, Validation Loss: 0.1208, Learning Rate: 0.177200\n",
      "Epoch  115, Training Loss: 0.0766, Validation Loss: 0.1210, Learning Rate: 0.177000\n",
      "Epoch  116, Training Loss: 0.0760, Validation Loss: 0.1198, Learning Rate: 0.176800\n",
      "Epoch  117, Training Loss: 0.0756, Validation Loss: 0.1188, Learning Rate: 0.176600\n",
      "Epoch  118, Training Loss: 0.0754, Validation Loss: 0.1179, Learning Rate: 0.176400\n",
      "Epoch  119, Training Loss: 0.0747, Validation Loss: 0.1192, Learning Rate: 0.176200\n",
      "Epoch  120, Training Loss: 0.0743, Validation Loss: 0.1172, Learning Rate: 0.176000\n",
      "Epoch  121, Training Loss: 0.0738, Validation Loss: 0.1168, Learning Rate: 0.175800\n",
      "Epoch  122, Training Loss: 0.0734, Validation Loss: 0.1181, Learning Rate: 0.175600\n",
      "Epoch  123, Training Loss: 0.0728, Validation Loss: 0.1161, Learning Rate: 0.175400\n",
      "Epoch  124, Training Loss: 0.0724, Validation Loss: 0.1160, Learning Rate: 0.175200\n",
      "Epoch  125, Training Loss: 0.0721, Validation Loss: 0.1167, Learning Rate: 0.175000\n",
      "Epoch  126, Training Loss: 0.0716, Validation Loss: 0.1139, Learning Rate: 0.174800\n",
      "Epoch  127, Training Loss: 0.0710, Validation Loss: 0.1140, Learning Rate: 0.174600\n",
      "Epoch  128, Training Loss: 0.0706, Validation Loss: 0.1129, Learning Rate: 0.174400\n",
      "Epoch  129, Training Loss: 0.0701, Validation Loss: 0.1133, Learning Rate: 0.174200\n",
      "Epoch  130, Training Loss: 0.0704, Validation Loss: 0.1150, Learning Rate: 0.174000\n",
      "Epoch  131, Training Loss: 0.0693, Validation Loss: 0.1126, Learning Rate: 0.173800\n",
      "Epoch  132, Training Loss: 0.0689, Validation Loss: 0.1126, Learning Rate: 0.173600\n",
      "Epoch  133, Training Loss: 0.0686, Validation Loss: 0.1104, Learning Rate: 0.173400\n",
      "Epoch  134, Training Loss: 0.0680, Validation Loss: 0.1107, Learning Rate: 0.173200\n",
      "Epoch  135, Training Loss: 0.0676, Validation Loss: 0.1102, Learning Rate: 0.173000\n",
      "Epoch  136, Training Loss: 0.0672, Validation Loss: 0.1096, Learning Rate: 0.172800\n",
      "Epoch  137, Training Loss: 0.0671, Validation Loss: 0.1080, Learning Rate: 0.172600\n",
      "Epoch  138, Training Loss: 0.0664, Validation Loss: 0.1086, Learning Rate: 0.172400\n",
      "Epoch  139, Training Loss: 0.0662, Validation Loss: 0.1093, Learning Rate: 0.172200\n",
      "Epoch  140, Training Loss: 0.0656, Validation Loss: 0.1077, Learning Rate: 0.172000\n",
      "Epoch  141, Training Loss: 0.0653, Validation Loss: 0.1067, Learning Rate: 0.171800\n",
      "Epoch  142, Training Loss: 0.0650, Validation Loss: 0.1071, Learning Rate: 0.171600\n",
      "Epoch  143, Training Loss: 0.0646, Validation Loss: 0.1066, Learning Rate: 0.171400\n",
      "Epoch  144, Training Loss: 0.0641, Validation Loss: 0.1056, Learning Rate: 0.171200\n",
      "Epoch  145, Training Loss: 0.0637, Validation Loss: 0.1052, Learning Rate: 0.171000\n",
      "Epoch  146, Training Loss: 0.0634, Validation Loss: 0.1050, Learning Rate: 0.170800\n",
      "Epoch  147, Training Loss: 0.0633, Validation Loss: 0.1032, Learning Rate: 0.170600\n",
      "Epoch  148, Training Loss: 0.0627, Validation Loss: 0.1043, Learning Rate: 0.170400\n",
      "Epoch  149, Training Loss: 0.0623, Validation Loss: 0.1030, Learning Rate: 0.170200\n",
      "Epoch  150, Training Loss: 0.0620, Validation Loss: 0.1026, Learning Rate: 0.170000\n",
      "Epoch  151, Training Loss: 0.0616, Validation Loss: 0.1022, Learning Rate: 0.169800\n",
      "Epoch  152, Training Loss: 0.0613, Validation Loss: 0.1016, Learning Rate: 0.169600\n",
      "Epoch  153, Training Loss: 0.0609, Validation Loss: 0.1014, Learning Rate: 0.169400\n",
      "Epoch  154, Training Loss: 0.0606, Validation Loss: 0.1016, Learning Rate: 0.169200\n",
      "Epoch  155, Training Loss: 0.0602, Validation Loss: 0.1006, Learning Rate: 0.169000\n",
      "Epoch  156, Training Loss: 0.0599, Validation Loss: 0.1004, Learning Rate: 0.168800\n",
      "Epoch  157, Training Loss: 0.0596, Validation Loss: 0.0998, Learning Rate: 0.168600\n",
      "Epoch  158, Training Loss: 0.0592, Validation Loss: 0.0989, Learning Rate: 0.168400\n",
      "Epoch  159, Training Loss: 0.0590, Validation Loss: 0.0994, Learning Rate: 0.168200\n",
      "Epoch  160, Training Loss: 0.0587, Validation Loss: 0.0993, Learning Rate: 0.168000\n",
      "Epoch  161, Training Loss: 0.0590, Validation Loss: 0.0966, Learning Rate: 0.167800\n",
      "Epoch  162, Training Loss: 0.0582, Validation Loss: 0.0966, Learning Rate: 0.167600\n",
      "Epoch  163, Training Loss: 0.0576, Validation Loss: 0.0970, Learning Rate: 0.167400\n",
      "Epoch  164, Training Loss: 0.0573, Validation Loss: 0.0965, Learning Rate: 0.167200\n",
      "Epoch  165, Training Loss: 0.0571, Validation Loss: 0.0969, Learning Rate: 0.167000\n",
      "Epoch  166, Training Loss: 0.0568, Validation Loss: 0.0965, Learning Rate: 0.166800\n",
      "Epoch  167, Training Loss: 0.0565, Validation Loss: 0.0961, Learning Rate: 0.166600\n",
      "Epoch  168, Training Loss: 0.0562, Validation Loss: 0.0946, Learning Rate: 0.166400\n",
      "Epoch  169, Training Loss: 0.0558, Validation Loss: 0.0944, Learning Rate: 0.166200\n",
      "Epoch  170, Training Loss: 0.0556, Validation Loss: 0.0949, Learning Rate: 0.166000\n",
      "Epoch  171, Training Loss: 0.0553, Validation Loss: 0.0944, Learning Rate: 0.165800\n",
      "Epoch  172, Training Loss: 0.0552, Validation Loss: 0.0945, Learning Rate: 0.165600\n",
      "Epoch  173, Training Loss: 0.0547, Validation Loss: 0.0929, Learning Rate: 0.165400\n",
      "Epoch  174, Training Loss: 0.0544, Validation Loss: 0.0932, Learning Rate: 0.165200\n",
      "Epoch  175, Training Loss: 0.0542, Validation Loss: 0.0931, Learning Rate: 0.165000\n",
      "Epoch  176, Training Loss: 0.0539, Validation Loss: 0.0916, Learning Rate: 0.164800\n",
      "Epoch  177, Training Loss: 0.0536, Validation Loss: 0.0910, Learning Rate: 0.164600\n",
      "Epoch  178, Training Loss: 0.0533, Validation Loss: 0.0910, Learning Rate: 0.164400\n",
      "Epoch  179, Training Loss: 0.0530, Validation Loss: 0.0905, Learning Rate: 0.164200\n",
      "Epoch  180, Training Loss: 0.0528, Validation Loss: 0.0899, Learning Rate: 0.164000\n",
      "Epoch  181, Training Loss: 0.0526, Validation Loss: 0.0906, Learning Rate: 0.163800\n",
      "Epoch  182, Training Loss: 0.0523, Validation Loss: 0.0891, Learning Rate: 0.163600\n",
      "Epoch  183, Training Loss: 0.0522, Validation Loss: 0.0883, Learning Rate: 0.163400\n",
      "Epoch  184, Training Loss: 0.0518, Validation Loss: 0.0896, Learning Rate: 0.163200\n",
      "Epoch  185, Training Loss: 0.0515, Validation Loss: 0.0888, Learning Rate: 0.163000\n",
      "Epoch  186, Training Loss: 0.0514, Validation Loss: 0.0875, Learning Rate: 0.162800\n",
      "Epoch  187, Training Loss: 0.0510, Validation Loss: 0.0873, Learning Rate: 0.162600\n",
      "Epoch  188, Training Loss: 0.0508, Validation Loss: 0.0869, Learning Rate: 0.162400\n",
      "Epoch  189, Training Loss: 0.0509, Validation Loss: 0.0884, Learning Rate: 0.162200\n",
      "Epoch  190, Training Loss: 0.0503, Validation Loss: 0.0860, Learning Rate: 0.162000\n",
      "Epoch  191, Training Loss: 0.0505, Validation Loss: 0.0851, Learning Rate: 0.161800\n",
      "Epoch  192, Training Loss: 0.0499, Validation Loss: 0.0863, Learning Rate: 0.161600\n",
      "Epoch  193, Training Loss: 0.0499, Validation Loss: 0.0846, Learning Rate: 0.161400\n",
      "Epoch  194, Training Loss: 0.0494, Validation Loss: 0.0850, Learning Rate: 0.161200\n",
      "Epoch  195, Training Loss: 0.0492, Validation Loss: 0.0845, Learning Rate: 0.161000\n",
      "Epoch  196, Training Loss: 0.0492, Validation Loss: 0.0857, Learning Rate: 0.160800\n",
      "Epoch  197, Training Loss: 0.0491, Validation Loss: 0.0855, Learning Rate: 0.160600\n",
      "Epoch  198, Training Loss: 0.0486, Validation Loss: 0.0842, Learning Rate: 0.160400\n",
      "Epoch  199, Training Loss: 0.0484, Validation Loss: 0.0839, Learning Rate: 0.160200\n",
      "Epoch  200, Training Loss: 0.0484, Validation Loss: 0.0822, Learning Rate: 0.160000\n",
      "Epoch  201, Training Loss: 0.0479, Validation Loss: 0.0828, Learning Rate: 0.159800\n",
      "Epoch  202, Training Loss: 0.0480, Validation Loss: 0.0813, Learning Rate: 0.159600\n",
      "Epoch  203, Training Loss: 0.0476, Validation Loss: 0.0815, Learning Rate: 0.159400\n",
      "Epoch  204, Training Loss: 0.0476, Validation Loss: 0.0829, Learning Rate: 0.159200\n",
      "Epoch  205, Training Loss: 0.0474, Validation Loss: 0.0808, Learning Rate: 0.159000\n",
      "Epoch  206, Training Loss: 0.0471, Validation Loss: 0.0819, Learning Rate: 0.158800\n",
      "Epoch  207, Training Loss: 0.0468, Validation Loss: 0.0808, Learning Rate: 0.158600\n",
      "Epoch  208, Training Loss: 0.0469, Validation Loss: 0.0795, Learning Rate: 0.158400\n",
      "Epoch  209, Training Loss: 0.0465, Validation Loss: 0.0806, Learning Rate: 0.158200\n",
      "Epoch  210, Training Loss: 0.0468, Validation Loss: 0.0789, Learning Rate: 0.158000\n",
      "Epoch  211, Training Loss: 0.0461, Validation Loss: 0.0795, Learning Rate: 0.157800\n",
      "Epoch  212, Training Loss: 0.0460, Validation Loss: 0.0790, Learning Rate: 0.157600\n",
      "Epoch  213, Training Loss: 0.0458, Validation Loss: 0.0797, Learning Rate: 0.157400\n",
      "Epoch  214, Training Loss: 0.0456, Validation Loss: 0.0790, Learning Rate: 0.157200\n",
      "Epoch  215, Training Loss: 0.0454, Validation Loss: 0.0785, Learning Rate: 0.157000\n",
      "Epoch  216, Training Loss: 0.0453, Validation Loss: 0.0779, Learning Rate: 0.156800\n",
      "Epoch  217, Training Loss: 0.0451, Validation Loss: 0.0780, Learning Rate: 0.156600\n",
      "Epoch  218, Training Loss: 0.0451, Validation Loss: 0.0787, Learning Rate: 0.156400\n",
      "Epoch  219, Training Loss: 0.0455, Validation Loss: 0.0800, Learning Rate: 0.156200\n",
      "Epoch  220, Training Loss: 0.0446, Validation Loss: 0.0769, Learning Rate: 0.156000\n",
      "Epoch  221, Training Loss: 0.0445, Validation Loss: 0.0777, Learning Rate: 0.155800\n",
      "Epoch  222, Training Loss: 0.0444, Validation Loss: 0.0760, Learning Rate: 0.155600\n",
      "Epoch  223, Training Loss: 0.0442, Validation Loss: 0.0766, Learning Rate: 0.155400\n",
      "Epoch  224, Training Loss: 0.0440, Validation Loss: 0.0758, Learning Rate: 0.155200\n",
      "Epoch  225, Training Loss: 0.0444, Validation Loss: 0.0779, Learning Rate: 0.155000\n",
      "Epoch  226, Training Loss: 0.0437, Validation Loss: 0.0755, Learning Rate: 0.154800\n",
      "Epoch  227, Training Loss: 0.0436, Validation Loss: 0.0758, Learning Rate: 0.154600\n",
      "Epoch  228, Training Loss: 0.0435, Validation Loss: 0.0745, Learning Rate: 0.154400\n",
      "Epoch  229, Training Loss: 0.0434, Validation Loss: 0.0753, Learning Rate: 0.154200\n",
      "Epoch  230, Training Loss: 0.0433, Validation Loss: 0.0754, Learning Rate: 0.154000\n",
      "Epoch  231, Training Loss: 0.0432, Validation Loss: 0.0737, Learning Rate: 0.153800\n",
      "Epoch  232, Training Loss: 0.0430, Validation Loss: 0.0737, Learning Rate: 0.153600\n",
      "Epoch  233, Training Loss: 0.0428, Validation Loss: 0.0738, Learning Rate: 0.153400\n",
      "Epoch  234, Training Loss: 0.0427, Validation Loss: 0.0734, Learning Rate: 0.153200\n",
      "Epoch  235, Training Loss: 0.0426, Validation Loss: 0.0727, Learning Rate: 0.153000\n",
      "Epoch  236, Training Loss: 0.0424, Validation Loss: 0.0733, Learning Rate: 0.152800\n",
      "Epoch  237, Training Loss: 0.0427, Validation Loss: 0.0746, Learning Rate: 0.152600\n",
      "Epoch  238, Training Loss: 0.0422, Validation Loss: 0.0733, Learning Rate: 0.152400\n",
      "Epoch  239, Training Loss: 0.0421, Validation Loss: 0.0721, Learning Rate: 0.152200\n",
      "Epoch  240, Training Loss: 0.0420, Validation Loss: 0.0728, Learning Rate: 0.152000\n",
      "Epoch  241, Training Loss: 0.0418, Validation Loss: 0.0721, Learning Rate: 0.151800\n",
      "Epoch  242, Training Loss: 0.0417, Validation Loss: 0.0719, Learning Rate: 0.151600\n",
      "Epoch  243, Training Loss: 0.0417, Validation Loss: 0.0726, Learning Rate: 0.151400\n",
      "Epoch  244, Training Loss: 0.0415, Validation Loss: 0.0712, Learning Rate: 0.151200\n",
      "Epoch  245, Training Loss: 0.0414, Validation Loss: 0.0706, Learning Rate: 0.151000\n",
      "Epoch  246, Training Loss: 0.0413, Validation Loss: 0.0711, Learning Rate: 0.150800\n",
      "Epoch  247, Training Loss: 0.0412, Validation Loss: 0.0712, Learning Rate: 0.150600\n",
      "Epoch  248, Training Loss: 0.0411, Validation Loss: 0.0712, Learning Rate: 0.150400\n",
      "Epoch  249, Training Loss: 0.0410, Validation Loss: 0.0698, Learning Rate: 0.150200\n",
      "Epoch  250, Training Loss: 0.0409, Validation Loss: 0.0696, Learning Rate: 0.150000\n",
      "Epoch  251, Training Loss: 0.0408, Validation Loss: 0.0701, Learning Rate: 0.149800\n",
      "Epoch  252, Training Loss: 0.0406, Validation Loss: 0.0697, Learning Rate: 0.149600\n",
      "Epoch  253, Training Loss: 0.0406, Validation Loss: 0.0691, Learning Rate: 0.149400\n",
      "Epoch  254, Training Loss: 0.0405, Validation Loss: 0.0690, Learning Rate: 0.149200\n",
      "Epoch  255, Training Loss: 0.0404, Validation Loss: 0.0689, Learning Rate: 0.149000\n",
      "Epoch  256, Training Loss: 0.0403, Validation Loss: 0.0684, Learning Rate: 0.148800\n",
      "Epoch  257, Training Loss: 0.0402, Validation Loss: 0.0690, Learning Rate: 0.148600\n",
      "Epoch  258, Training Loss: 0.0401, Validation Loss: 0.0686, Learning Rate: 0.148400\n",
      "Epoch  259, Training Loss: 0.0400, Validation Loss: 0.0685, Learning Rate: 0.148200\n",
      "Epoch  260, Training Loss: 0.0400, Validation Loss: 0.0687, Learning Rate: 0.148000\n",
      "Epoch  261, Training Loss: 0.0399, Validation Loss: 0.0687, Learning Rate: 0.147800\n",
      "Epoch  262, Training Loss: 0.0398, Validation Loss: 0.0671, Learning Rate: 0.147600\n",
      "Epoch  263, Training Loss: 0.0397, Validation Loss: 0.0672, Learning Rate: 0.147400\n",
      "Epoch  264, Training Loss: 0.0396, Validation Loss: 0.0679, Learning Rate: 0.147200\n",
      "Epoch  265, Training Loss: 0.0395, Validation Loss: 0.0676, Learning Rate: 0.147000\n",
      "Epoch  266, Training Loss: 0.0394, Validation Loss: 0.0674, Learning Rate: 0.146800\n",
      "Epoch  267, Training Loss: 0.0393, Validation Loss: 0.0673, Learning Rate: 0.146600\n",
      "Epoch  268, Training Loss: 0.0394, Validation Loss: 0.0677, Learning Rate: 0.146400\n",
      "Epoch  269, Training Loss: 0.0392, Validation Loss: 0.0662, Learning Rate: 0.146200\n",
      "Epoch  270, Training Loss: 0.0391, Validation Loss: 0.0668, Learning Rate: 0.146000\n",
      "Epoch  271, Training Loss: 0.0391, Validation Loss: 0.0670, Learning Rate: 0.145800\n",
      "Epoch  272, Training Loss: 0.0389, Validation Loss: 0.0664, Learning Rate: 0.145600\n",
      "Epoch  273, Training Loss: 0.0389, Validation Loss: 0.0664, Learning Rate: 0.145400\n",
      "Epoch  274, Training Loss: 0.0388, Validation Loss: 0.0651, Learning Rate: 0.145200\n",
      "Epoch  275, Training Loss: 0.0387, Validation Loss: 0.0656, Learning Rate: 0.145000\n",
      "Epoch  276, Training Loss: 0.0387, Validation Loss: 0.0658, Learning Rate: 0.144800\n",
      "Epoch  277, Training Loss: 0.0386, Validation Loss: 0.0657, Learning Rate: 0.144600\n",
      "Epoch  278, Training Loss: 0.0385, Validation Loss: 0.0651, Learning Rate: 0.144400\n",
      "Epoch  279, Training Loss: 0.0384, Validation Loss: 0.0649, Learning Rate: 0.144200\n",
      "Epoch  280, Training Loss: 0.0384, Validation Loss: 0.0646, Learning Rate: 0.144000\n",
      "Epoch  281, Training Loss: 0.0383, Validation Loss: 0.0641, Learning Rate: 0.143800\n",
      "Epoch  282, Training Loss: 0.0383, Validation Loss: 0.0636, Learning Rate: 0.143600\n",
      "Epoch  283, Training Loss: 0.0383, Validation Loss: 0.0636, Learning Rate: 0.143400\n",
      "Epoch  284, Training Loss: 0.0383, Validation Loss: 0.0633, Learning Rate: 0.143200\n",
      "Epoch  285, Training Loss: 0.0381, Validation Loss: 0.0634, Learning Rate: 0.143000\n",
      "Epoch  286, Training Loss: 0.0381, Validation Loss: 0.0630, Learning Rate: 0.142800\n",
      "Epoch  287, Training Loss: 0.0380, Validation Loss: 0.0628, Learning Rate: 0.142600\n",
      "Epoch  288, Training Loss: 0.0379, Validation Loss: 0.0633, Learning Rate: 0.142400\n",
      "Epoch  289, Training Loss: 0.0378, Validation Loss: 0.0634, Learning Rate: 0.142200\n",
      "Epoch  290, Training Loss: 0.0378, Validation Loss: 0.0625, Learning Rate: 0.142000\n",
      "Epoch  291, Training Loss: 0.0377, Validation Loss: 0.0631, Learning Rate: 0.141800\n",
      "Epoch  292, Training Loss: 0.0376, Validation Loss: 0.0629, Learning Rate: 0.141600\n",
      "Epoch  293, Training Loss: 0.0376, Validation Loss: 0.0619, Learning Rate: 0.141400\n",
      "Epoch  294, Training Loss: 0.0375, Validation Loss: 0.0624, Learning Rate: 0.141200\n",
      "Epoch  295, Training Loss: 0.0375, Validation Loss: 0.0620, Learning Rate: 0.141000\n",
      "Epoch  296, Training Loss: 0.0374, Validation Loss: 0.0620, Learning Rate: 0.140800\n",
      "Epoch  297, Training Loss: 0.0373, Validation Loss: 0.0619, Learning Rate: 0.140600\n",
      "Epoch  298, Training Loss: 0.0373, Validation Loss: 0.0620, Learning Rate: 0.140400\n",
      "Epoch  299, Training Loss: 0.0372, Validation Loss: 0.0618, Learning Rate: 0.140200\n",
      "Epoch  300, Training Loss: 0.0372, Validation Loss: 0.0619, Learning Rate: 0.140000\n",
      "Epoch  301, Training Loss: 0.0371, Validation Loss: 0.0615, Learning Rate: 0.139800\n",
      "Epoch  302, Training Loss: 0.0371, Validation Loss: 0.0611, Learning Rate: 0.139600\n",
      "Epoch  303, Training Loss: 0.0371, Validation Loss: 0.0618, Learning Rate: 0.139400\n",
      "Epoch  304, Training Loss: 0.0370, Validation Loss: 0.0616, Learning Rate: 0.139200\n",
      "Epoch  305, Training Loss: 0.0370, Validation Loss: 0.0608, Learning Rate: 0.139000\n",
      "Epoch  306, Training Loss: 0.0369, Validation Loss: 0.0608, Learning Rate: 0.138800\n",
      "Epoch  307, Training Loss: 0.0369, Validation Loss: 0.0611, Learning Rate: 0.138600\n",
      "Epoch  308, Training Loss: 0.0368, Validation Loss: 0.0608, Learning Rate: 0.138400\n",
      "Epoch  309, Training Loss: 0.0368, Validation Loss: 0.0607, Learning Rate: 0.138200\n",
      "Epoch  310, Training Loss: 0.0367, Validation Loss: 0.0608, Learning Rate: 0.138000\n",
      "Epoch  311, Training Loss: 0.0367, Validation Loss: 0.0599, Learning Rate: 0.137800\n",
      "Epoch  312, Training Loss: 0.0366, Validation Loss: 0.0601, Learning Rate: 0.137600\n",
      "Epoch  313, Training Loss: 0.0366, Validation Loss: 0.0602, Learning Rate: 0.137400\n",
      "Epoch  314, Training Loss: 0.0365, Validation Loss: 0.0601, Learning Rate: 0.137200\n",
      "Epoch  315, Training Loss: 0.0365, Validation Loss: 0.0597, Learning Rate: 0.137000\n",
      "Epoch  316, Training Loss: 0.0365, Validation Loss: 0.0596, Learning Rate: 0.136800\n",
      "Epoch  317, Training Loss: 0.0364, Validation Loss: 0.0595, Learning Rate: 0.136600\n",
      "Epoch  318, Training Loss: 0.0364, Validation Loss: 0.0589, Learning Rate: 0.136400\n",
      "Epoch  319, Training Loss: 0.0363, Validation Loss: 0.0594, Learning Rate: 0.136200\n",
      "Epoch  320, Training Loss: 0.0363, Validation Loss: 0.0594, Learning Rate: 0.136000\n",
      "Epoch  321, Training Loss: 0.0363, Validation Loss: 0.0590, Learning Rate: 0.135800\n",
      "Epoch  322, Training Loss: 0.0362, Validation Loss: 0.0592, Learning Rate: 0.135600\n",
      "Epoch  323, Training Loss: 0.0362, Validation Loss: 0.0589, Learning Rate: 0.135400\n",
      "Epoch  324, Training Loss: 0.0362, Validation Loss: 0.0591, Learning Rate: 0.135200\n",
      "Epoch  325, Training Loss: 0.0361, Validation Loss: 0.0583, Learning Rate: 0.135000\n",
      "Epoch  326, Training Loss: 0.0361, Validation Loss: 0.0585, Learning Rate: 0.134800\n",
      "Epoch  327, Training Loss: 0.0361, Validation Loss: 0.0591, Learning Rate: 0.134600\n",
      "Epoch  328, Training Loss: 0.0361, Validation Loss: 0.0590, Learning Rate: 0.134400\n",
      "Epoch  329, Training Loss: 0.0360, Validation Loss: 0.0580, Learning Rate: 0.134200\n",
      "Epoch  330, Training Loss: 0.0359, Validation Loss: 0.0581, Learning Rate: 0.134000\n",
      "Epoch  331, Training Loss: 0.0359, Validation Loss: 0.0578, Learning Rate: 0.133800\n",
      "Epoch  332, Training Loss: 0.0359, Validation Loss: 0.0577, Learning Rate: 0.133600\n",
      "Epoch  333, Training Loss: 0.0358, Validation Loss: 0.0579, Learning Rate: 0.133400\n",
      "Epoch  334, Training Loss: 0.0358, Validation Loss: 0.0577, Learning Rate: 0.133200\n",
      "Epoch  335, Training Loss: 0.0358, Validation Loss: 0.0576, Learning Rate: 0.133000\n",
      "Epoch  336, Training Loss: 0.0358, Validation Loss: 0.0573, Learning Rate: 0.132800\n",
      "Epoch  337, Training Loss: 0.0357, Validation Loss: 0.0574, Learning Rate: 0.132600\n",
      "Epoch  338, Training Loss: 0.0357, Validation Loss: 0.0575, Learning Rate: 0.132400\n",
      "Epoch  339, Training Loss: 0.0357, Validation Loss: 0.0570, Learning Rate: 0.132200\n",
      "Epoch  340, Training Loss: 0.0357, Validation Loss: 0.0569, Learning Rate: 0.132000\n",
      "Epoch  341, Training Loss: 0.0356, Validation Loss: 0.0569, Learning Rate: 0.131800\n",
      "Epoch  342, Training Loss: 0.0356, Validation Loss: 0.0569, Learning Rate: 0.131600\n",
      "Epoch  343, Training Loss: 0.0356, Validation Loss: 0.0565, Learning Rate: 0.131400\n",
      "Epoch  344, Training Loss: 0.0355, Validation Loss: 0.0566, Learning Rate: 0.131200\n",
      "Epoch  345, Training Loss: 0.0355, Validation Loss: 0.0569, Learning Rate: 0.131000\n",
      "Epoch  346, Training Loss: 0.0355, Validation Loss: 0.0566, Learning Rate: 0.130800\n",
      "Epoch  347, Training Loss: 0.0355, Validation Loss: 0.0568, Learning Rate: 0.130600\n",
      "Epoch  348, Training Loss: 0.0354, Validation Loss: 0.0563, Learning Rate: 0.130400\n",
      "Epoch  349, Training Loss: 0.0354, Validation Loss: 0.0561, Learning Rate: 0.130200\n",
      "Epoch  350, Training Loss: 0.0354, Validation Loss: 0.0563, Learning Rate: 0.130000\n",
      "Epoch  351, Training Loss: 0.0353, Validation Loss: 0.0562, Learning Rate: 0.129800\n",
      "Epoch  352, Training Loss: 0.0353, Validation Loss: 0.0558, Learning Rate: 0.129600\n",
      "Epoch  353, Training Loss: 0.0353, Validation Loss: 0.0559, Learning Rate: 0.129400\n",
      "Epoch  354, Training Loss: 0.0353, Validation Loss: 0.0559, Learning Rate: 0.129200\n",
      "Epoch  355, Training Loss: 0.0352, Validation Loss: 0.0558, Learning Rate: 0.129000\n",
      "Epoch  356, Training Loss: 0.0352, Validation Loss: 0.0559, Learning Rate: 0.128800\n",
      "Epoch  357, Training Loss: 0.0352, Validation Loss: 0.0558, Learning Rate: 0.128600\n",
      "Epoch  358, Training Loss: 0.0352, Validation Loss: 0.0556, Learning Rate: 0.128400\n",
      "Epoch  359, Training Loss: 0.0352, Validation Loss: 0.0555, Learning Rate: 0.128200\n",
      "Epoch  360, Training Loss: 0.0351, Validation Loss: 0.0556, Learning Rate: 0.128000\n",
      "Epoch  361, Training Loss: 0.0351, Validation Loss: 0.0553, Learning Rate: 0.127800\n",
      "Epoch  362, Training Loss: 0.0351, Validation Loss: 0.0554, Learning Rate: 0.127600\n",
      "Epoch  363, Training Loss: 0.0351, Validation Loss: 0.0551, Learning Rate: 0.127400\n",
      "Epoch  364, Training Loss: 0.0351, Validation Loss: 0.0551, Learning Rate: 0.127200\n",
      "Epoch  365, Training Loss: 0.0350, Validation Loss: 0.0552, Learning Rate: 0.127000\n",
      "Epoch  366, Training Loss: 0.0350, Validation Loss: 0.0552, Learning Rate: 0.126800\n",
      "Epoch  367, Training Loss: 0.0350, Validation Loss: 0.0551, Learning Rate: 0.126600\n",
      "Epoch  368, Training Loss: 0.0350, Validation Loss: 0.0549, Learning Rate: 0.126400\n",
      "Epoch  369, Training Loss: 0.0350, Validation Loss: 0.0546, Learning Rate: 0.126200\n",
      "Epoch  370, Training Loss: 0.0349, Validation Loss: 0.0547, Learning Rate: 0.126000\n",
      "Epoch  371, Training Loss: 0.0349, Validation Loss: 0.0546, Learning Rate: 0.125800\n",
      "Epoch  372, Training Loss: 0.0349, Validation Loss: 0.0543, Learning Rate: 0.125600\n",
      "Epoch  373, Training Loss: 0.0349, Validation Loss: 0.0544, Learning Rate: 0.125400\n",
      "Epoch  374, Training Loss: 0.0349, Validation Loss: 0.0541, Learning Rate: 0.125200\n",
      "Epoch  375, Training Loss: 0.0349, Validation Loss: 0.0540, Learning Rate: 0.125000\n",
      "Epoch  376, Training Loss: 0.0348, Validation Loss: 0.0541, Learning Rate: 0.124800\n",
      "Epoch  377, Training Loss: 0.0348, Validation Loss: 0.0543, Learning Rate: 0.124600\n",
      "Epoch  378, Training Loss: 0.0348, Validation Loss: 0.0539, Learning Rate: 0.124400\n",
      "Epoch  379, Training Loss: 0.0348, Validation Loss: 0.0538, Learning Rate: 0.124200\n",
      "Epoch  380, Training Loss: 0.0348, Validation Loss: 0.0541, Learning Rate: 0.124000\n",
      "Epoch  381, Training Loss: 0.0347, Validation Loss: 0.0539, Learning Rate: 0.123800\n",
      "Epoch  382, Training Loss: 0.0347, Validation Loss: 0.0538, Learning Rate: 0.123600\n",
      "Epoch  383, Training Loss: 0.0347, Validation Loss: 0.0539, Learning Rate: 0.123400\n",
      "Epoch  384, Training Loss: 0.0347, Validation Loss: 0.0536, Learning Rate: 0.123200\n",
      "Epoch  385, Training Loss: 0.0347, Validation Loss: 0.0538, Learning Rate: 0.123000\n",
      "Epoch  386, Training Loss: 0.0347, Validation Loss: 0.0538, Learning Rate: 0.122800\n",
      "Epoch  387, Training Loss: 0.0347, Validation Loss: 0.0537, Learning Rate: 0.122600\n",
      "Epoch  388, Training Loss: 0.0346, Validation Loss: 0.0536, Learning Rate: 0.122400\n",
      "Epoch  389, Training Loss: 0.0346, Validation Loss: 0.0532, Learning Rate: 0.122200\n",
      "Epoch  390, Training Loss: 0.0346, Validation Loss: 0.0532, Learning Rate: 0.122000\n",
      "Epoch  391, Training Loss: 0.0346, Validation Loss: 0.0535, Learning Rate: 0.121800\n",
      "Epoch  392, Training Loss: 0.0346, Validation Loss: 0.0533, Learning Rate: 0.121600\n",
      "Epoch  393, Training Loss: 0.0346, Validation Loss: 0.0531, Learning Rate: 0.121400\n",
      "Epoch  394, Training Loss: 0.0346, Validation Loss: 0.0531, Learning Rate: 0.121200\n",
      "Epoch  395, Training Loss: 0.0346, Validation Loss: 0.0531, Learning Rate: 0.121000\n",
      "Epoch  396, Training Loss: 0.0345, Validation Loss: 0.0528, Learning Rate: 0.120800\n",
      "Epoch  397, Training Loss: 0.0345, Validation Loss: 0.0526, Learning Rate: 0.120600\n",
      "Epoch  398, Training Loss: 0.0345, Validation Loss: 0.0530, Learning Rate: 0.120400\n",
      "Epoch  399, Training Loss: 0.0345, Validation Loss: 0.0528, Learning Rate: 0.120200\n",
      "Epoch  400, Training Loss: 0.0345, Validation Loss: 0.0529, Learning Rate: 0.120000\n",
      "Epoch  401, Training Loss: 0.0345, Validation Loss: 0.0526, Learning Rate: 0.119800\n",
      "Epoch  402, Training Loss: 0.0345, Validation Loss: 0.0526, Learning Rate: 0.119600\n",
      "Epoch  403, Training Loss: 0.0345, Validation Loss: 0.0525, Learning Rate: 0.119400\n",
      "Epoch  404, Training Loss: 0.0344, Validation Loss: 0.0526, Learning Rate: 0.119200\n",
      "Epoch  405, Training Loss: 0.0344, Validation Loss: 0.0525, Learning Rate: 0.119000\n",
      "Epoch  406, Training Loss: 0.0344, Validation Loss: 0.0526, Learning Rate: 0.118800\n",
      "Epoch  407, Training Loss: 0.0344, Validation Loss: 0.0524, Learning Rate: 0.118600\n",
      "Epoch  408, Training Loss: 0.0344, Validation Loss: 0.0522, Learning Rate: 0.118400\n",
      "Epoch  409, Training Loss: 0.0344, Validation Loss: 0.0521, Learning Rate: 0.118200\n",
      "Epoch  410, Training Loss: 0.0344, Validation Loss: 0.0522, Learning Rate: 0.118000\n",
      "Epoch  411, Training Loss: 0.0344, Validation Loss: 0.0521, Learning Rate: 0.117800\n",
      "Epoch  412, Training Loss: 0.0344, Validation Loss: 0.0522, Learning Rate: 0.117600\n",
      "Epoch  413, Training Loss: 0.0344, Validation Loss: 0.0521, Learning Rate: 0.117400\n",
      "Epoch  414, Training Loss: 0.0343, Validation Loss: 0.0521, Learning Rate: 0.117200\n",
      "Epoch  415, Training Loss: 0.0343, Validation Loss: 0.0519, Learning Rate: 0.117000\n",
      "Epoch  416, Training Loss: 0.0343, Validation Loss: 0.0519, Learning Rate: 0.116800\n",
      "Epoch  417, Training Loss: 0.0343, Validation Loss: 0.0518, Learning Rate: 0.116600\n",
      "Epoch  418, Training Loss: 0.0343, Validation Loss: 0.0520, Learning Rate: 0.116400\n",
      "Epoch  419, Training Loss: 0.0343, Validation Loss: 0.0519, Learning Rate: 0.116200\n",
      "Epoch  420, Training Loss: 0.0343, Validation Loss: 0.0517, Learning Rate: 0.116000\n",
      "Epoch  421, Training Loss: 0.0343, Validation Loss: 0.0515, Learning Rate: 0.115800\n",
      "Epoch  422, Training Loss: 0.0343, Validation Loss: 0.0514, Learning Rate: 0.115600\n",
      "Epoch  423, Training Loss: 0.0343, Validation Loss: 0.0514, Learning Rate: 0.115400\n",
      "Epoch  424, Training Loss: 0.0343, Validation Loss: 0.0515, Learning Rate: 0.115200\n",
      "Epoch  425, Training Loss: 0.0342, Validation Loss: 0.0513, Learning Rate: 0.115000\n",
      "Epoch  426, Training Loss: 0.0342, Validation Loss: 0.0513, Learning Rate: 0.114800\n",
      "Epoch  427, Training Loss: 0.0342, Validation Loss: 0.0513, Learning Rate: 0.114600\n",
      "Epoch  428, Training Loss: 0.0342, Validation Loss: 0.0512, Learning Rate: 0.114400\n",
      "Epoch  429, Training Loss: 0.0342, Validation Loss: 0.0512, Learning Rate: 0.114200\n",
      "Epoch  430, Training Loss: 0.0342, Validation Loss: 0.0511, Learning Rate: 0.114000\n",
      "Epoch  431, Training Loss: 0.0342, Validation Loss: 0.0511, Learning Rate: 0.113800\n",
      "Epoch  432, Training Loss: 0.0342, Validation Loss: 0.0510, Learning Rate: 0.113600\n",
      "Epoch  433, Training Loss: 0.0342, Validation Loss: 0.0510, Learning Rate: 0.113400\n",
      "Epoch  434, Training Loss: 0.0342, Validation Loss: 0.0511, Learning Rate: 0.113200\n",
      "Epoch  435, Training Loss: 0.0342, Validation Loss: 0.0511, Learning Rate: 0.113000\n",
      "Epoch  436, Training Loss: 0.0342, Validation Loss: 0.0507, Learning Rate: 0.112800\n",
      "Epoch  437, Training Loss: 0.0342, Validation Loss: 0.0506, Learning Rate: 0.112600\n",
      "Epoch  438, Training Loss: 0.0342, Validation Loss: 0.0506, Learning Rate: 0.112400\n",
      "Epoch  439, Training Loss: 0.0341, Validation Loss: 0.0507, Learning Rate: 0.112200\n",
      "Epoch  440, Training Loss: 0.0341, Validation Loss: 0.0507, Learning Rate: 0.112000\n",
      "Epoch  441, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.111800\n",
      "Epoch  442, Training Loss: 0.0341, Validation Loss: 0.0507, Learning Rate: 0.111600\n",
      "Epoch  443, Training Loss: 0.0341, Validation Loss: 0.0505, Learning Rate: 0.111400\n",
      "Epoch  444, Training Loss: 0.0341, Validation Loss: 0.0505, Learning Rate: 0.111200\n",
      "Epoch  445, Training Loss: 0.0341, Validation Loss: 0.0505, Learning Rate: 0.111000\n",
      "Epoch  446, Training Loss: 0.0341, Validation Loss: 0.0505, Learning Rate: 0.110800\n",
      "Epoch  447, Training Loss: 0.0341, Validation Loss: 0.0504, Learning Rate: 0.110600\n",
      "Epoch  448, Training Loss: 0.0341, Validation Loss: 0.0502, Learning Rate: 0.110400\n",
      "Epoch  449, Training Loss: 0.0341, Validation Loss: 0.0504, Learning Rate: 0.110200\n",
      "Epoch  450, Training Loss: 0.0341, Validation Loss: 0.0501, Learning Rate: 0.110000\n",
      "Epoch  451, Training Loss: 0.0341, Validation Loss: 0.0502, Learning Rate: 0.109800\n",
      "Epoch  452, Training Loss: 0.0341, Validation Loss: 0.0501, Learning Rate: 0.109600\n",
      "Epoch  453, Training Loss: 0.0341, Validation Loss: 0.0501, Learning Rate: 0.109400\n",
      "Epoch  454, Training Loss: 0.0341, Validation Loss: 0.0502, Learning Rate: 0.109200\n",
      "Epoch  455, Training Loss: 0.0341, Validation Loss: 0.0500, Learning Rate: 0.109000\n",
      "Epoch  456, Training Loss: 0.0341, Validation Loss: 0.0500, Learning Rate: 0.108800\n",
      "Epoch  457, Training Loss: 0.0341, Validation Loss: 0.0499, Learning Rate: 0.108600\n",
      "Epoch  458, Training Loss: 0.0341, Validation Loss: 0.0500, Learning Rate: 0.108400\n",
      "Epoch  459, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.108200\n",
      "Epoch  460, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.108000\n",
      "Epoch  461, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.107800\n",
      "Epoch  462, Training Loss: 0.0340, Validation Loss: 0.0498, Learning Rate: 0.107600\n",
      "Epoch  463, Training Loss: 0.0340, Validation Loss: 0.0497, Learning Rate: 0.107400\n",
      "Epoch  464, Training Loss: 0.0340, Validation Loss: 0.0497, Learning Rate: 0.107200\n",
      "Epoch  465, Training Loss: 0.0340, Validation Loss: 0.0498, Learning Rate: 0.107000\n",
      "Epoch  466, Training Loss: 0.0340, Validation Loss: 0.0497, Learning Rate: 0.106800\n",
      "Epoch  467, Training Loss: 0.0340, Validation Loss: 0.0497, Learning Rate: 0.106600\n",
      "Epoch  468, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.106400\n",
      "Epoch  469, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.106200\n",
      "Epoch  470, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.106000\n",
      "Epoch  471, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.105800\n",
      "Epoch  472, Training Loss: 0.0340, Validation Loss: 0.0493, Learning Rate: 0.105600\n",
      "Epoch  473, Training Loss: 0.0340, Validation Loss: 0.0494, Learning Rate: 0.105400\n",
      "Epoch  474, Training Loss: 0.0340, Validation Loss: 0.0492, Learning Rate: 0.105200\n",
      "Epoch  475, Training Loss: 0.0340, Validation Loss: 0.0494, Learning Rate: 0.105000\n",
      "Epoch  476, Training Loss: 0.0340, Validation Loss: 0.0493, Learning Rate: 0.104800\n",
      "Epoch  477, Training Loss: 0.0340, Validation Loss: 0.0493, Learning Rate: 0.104600\n",
      "Epoch  478, Training Loss: 0.0340, Validation Loss: 0.0493, Learning Rate: 0.104400\n",
      "Epoch  479, Training Loss: 0.0340, Validation Loss: 0.0492, Learning Rate: 0.104200\n",
      "Epoch  480, Training Loss: 0.0340, Validation Loss: 0.0491, Learning Rate: 0.104000\n",
      "Epoch  481, Training Loss: 0.0340, Validation Loss: 0.0491, Learning Rate: 0.103800\n",
      "Epoch  482, Training Loss: 0.0340, Validation Loss: 0.0490, Learning Rate: 0.103600\n",
      "Epoch  483, Training Loss: 0.0340, Validation Loss: 0.0490, Learning Rate: 0.103400\n",
      "Epoch  484, Training Loss: 0.0340, Validation Loss: 0.0490, Learning Rate: 0.103200\n",
      "Epoch  485, Training Loss: 0.0340, Validation Loss: 0.0491, Learning Rate: 0.103000\n",
      "Epoch  486, Training Loss: 0.0340, Validation Loss: 0.0490, Learning Rate: 0.102800\n",
      "Epoch  487, Training Loss: 0.0340, Validation Loss: 0.0489, Learning Rate: 0.102600\n",
      "Epoch  488, Training Loss: 0.0340, Validation Loss: 0.0489, Learning Rate: 0.102400\n",
      "Epoch  489, Training Loss: 0.0340, Validation Loss: 0.0489, Learning Rate: 0.102200\n",
      "Epoch  490, Training Loss: 0.0340, Validation Loss: 0.0489, Learning Rate: 0.102000\n",
      "Epoch  491, Training Loss: 0.0340, Validation Loss: 0.0490, Learning Rate: 0.101800\n",
      "Epoch  492, Training Loss: 0.0340, Validation Loss: 0.0489, Learning Rate: 0.101600\n",
      "Epoch  493, Training Loss: 0.0339, Validation Loss: 0.0488, Learning Rate: 0.101400\n",
      "Epoch  494, Training Loss: 0.0339, Validation Loss: 0.0488, Learning Rate: 0.101200\n",
      "Epoch  495, Training Loss: 0.0339, Validation Loss: 0.0487, Learning Rate: 0.101000\n",
      "Epoch  496, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.100800\n",
      "Epoch  497, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.100600\n",
      "Epoch  498, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.100400\n",
      "Epoch  499, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.100200\n",
      "Epoch  500, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.100000\n",
      "Epoch  501, Training Loss: 0.0339, Validation Loss: 0.0484, Learning Rate: 0.099800\n",
      "Epoch  502, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.099600\n",
      "Epoch  503, Training Loss: 0.0339, Validation Loss: 0.0484, Learning Rate: 0.099400\n",
      "Epoch  504, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.099200\n",
      "Epoch  505, Training Loss: 0.0339, Validation Loss: 0.0484, Learning Rate: 0.099000\n",
      "Epoch  506, Training Loss: 0.0339, Validation Loss: 0.0484, Learning Rate: 0.098800\n",
      "Epoch  507, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.098600\n",
      "Epoch  508, Training Loss: 0.0339, Validation Loss: 0.0482, Learning Rate: 0.098400\n",
      "Epoch  509, Training Loss: 0.0339, Validation Loss: 0.0482, Learning Rate: 0.098200\n",
      "Epoch  510, Training Loss: 0.0339, Validation Loss: 0.0482, Learning Rate: 0.098000\n",
      "Epoch  511, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.097800\n",
      "Epoch  512, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.097600\n",
      "Epoch  513, Training Loss: 0.0339, Validation Loss: 0.0482, Learning Rate: 0.097400\n",
      "Epoch  514, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.097200\n",
      "Epoch  515, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.097000\n",
      "Epoch  516, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.096800\n",
      "Epoch  517, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.096600\n",
      "Epoch  518, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.096400\n",
      "Epoch  519, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.096200\n",
      "Epoch  520, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.096000\n",
      "Epoch  521, Training Loss: 0.0339, Validation Loss: 0.0479, Learning Rate: 0.095800\n",
      "Epoch  522, Training Loss: 0.0339, Validation Loss: 0.0479, Learning Rate: 0.095600\n",
      "Epoch  523, Training Loss: 0.0339, Validation Loss: 0.0479, Learning Rate: 0.095400\n",
      "Epoch  524, Training Loss: 0.0339, Validation Loss: 0.0479, Learning Rate: 0.095200\n",
      "Epoch  525, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.095000\n",
      "Epoch  526, Training Loss: 0.0339, Validation Loss: 0.0479, Learning Rate: 0.094800\n",
      "Epoch  527, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.094600\n",
      "Epoch  528, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.094400\n",
      "Epoch  529, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.094200\n",
      "Epoch  530, Training Loss: 0.0339, Validation Loss: 0.0479, Learning Rate: 0.094000\n",
      "Epoch  531, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.093800\n",
      "Epoch  532, Training Loss: 0.0339, Validation Loss: 0.0477, Learning Rate: 0.093600\n",
      "Epoch  533, Training Loss: 0.0339, Validation Loss: 0.0477, Learning Rate: 0.093400\n",
      "Epoch  534, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.093200\n",
      "Epoch  535, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.093000\n",
      "Epoch  536, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.092800\n",
      "Epoch  537, Training Loss: 0.0339, Validation Loss: 0.0475, Learning Rate: 0.092600\n",
      "Epoch  538, Training Loss: 0.0339, Validation Loss: 0.0475, Learning Rate: 0.092400\n",
      "Epoch  539, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.092200\n",
      "Epoch  540, Training Loss: 0.0339, Validation Loss: 0.0475, Learning Rate: 0.092000\n",
      "Epoch  541, Training Loss: 0.0339, Validation Loss: 0.0475, Learning Rate: 0.091800\n",
      "Epoch  542, Training Loss: 0.0339, Validation Loss: 0.0475, Learning Rate: 0.091600\n",
      "Epoch  543, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.091400\n",
      "Epoch  544, Training Loss: 0.0339, Validation Loss: 0.0475, Learning Rate: 0.091200\n",
      "Epoch  545, Training Loss: 0.0339, Validation Loss: 0.0474, Learning Rate: 0.091000\n",
      "Epoch  546, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.090800\n",
      "Epoch  547, Training Loss: 0.0339, Validation Loss: 0.0474, Learning Rate: 0.090600\n",
      "Epoch  548, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.090400\n",
      "Epoch  549, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.090200\n",
      "Epoch  550, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.090000\n",
      "Epoch  551, Training Loss: 0.0339, Validation Loss: 0.0472, Learning Rate: 0.089800\n",
      "Epoch  552, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.089600\n",
      "Epoch  553, Training Loss: 0.0339, Validation Loss: 0.0472, Learning Rate: 0.089400\n",
      "Epoch  554, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.089200\n",
      "Epoch  555, Training Loss: 0.0339, Validation Loss: 0.0472, Learning Rate: 0.089000\n",
      "Epoch  556, Training Loss: 0.0339, Validation Loss: 0.0472, Learning Rate: 0.088800\n",
      "Epoch  557, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.088600\n",
      "Epoch  558, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.088400\n",
      "Epoch  559, Training Loss: 0.0339, Validation Loss: 0.0472, Learning Rate: 0.088200\n",
      "Epoch  560, Training Loss: 0.0339, Validation Loss: 0.0472, Learning Rate: 0.088000\n",
      "Epoch  561, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.087800\n",
      "Epoch  562, Training Loss: 0.0339, Validation Loss: 0.0472, Learning Rate: 0.087600\n",
      "Epoch  563, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.087400\n",
      "Epoch  564, Training Loss: 0.0339, Validation Loss: 0.0470, Learning Rate: 0.087200\n",
      "Epoch  565, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.087000\n",
      "Epoch  566, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.086800\n",
      "Epoch  567, Training Loss: 0.0339, Validation Loss: 0.0470, Learning Rate: 0.086600\n",
      "Epoch  568, Training Loss: 0.0339, Validation Loss: 0.0470, Learning Rate: 0.086400\n",
      "Early stopping triggered at epoch 568. Restoring best model parameters.\n",
      "\n",
      "Neural Network Classification Accuracy: 0.9907\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAIjCAYAAADvBuGTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAhIhJREFUeJzt3Qd4VFX+xvE3PYEUeu+9gxQRFVFBEHvvXdfedV3d/dtd+9q7rmIXuy52VCyIVKnSew+hJIRA+v/5nZsJCQQEUu7M5Pt5nvHeuffOnTMMYF7OOb8TUVhYWCgAAAAAQLlElu/lAAAAAABDuAIAAACACkC4AgAAAIAKQLgCAAAAgApAuAIAAACACkC4AgAAAIAKQLgCAAAAgApAuAIAAACACkC4AgAAAIAKQLgCAGAvRURE6K677vK7GQCAIEO4AgBUihEjRrgQMmnSJAUzC0nWzrS0tDLPt2rVSsccc0y53+edd97RE088Ue77AACCV7TfDQAAINRs3bpV0dHRex2uZs6cqeuvv77S2gUA8BfhCgCAvRQfH69gkJeXp4KCAsXGxvrdFAAAwwIBAH77448/NHz4cCUnJysxMVGDBw/W77//Xuqa3Nxc3X333Wrfvr0LNnXr1tXBBx+s7777rviaNWvW6MILL1SzZs0UFxenxo0b6/jjj9eSJUsqfc7V5s2bXY+UDSG0927QoIGOOOIITZkyxZ0/9NBD9cUXX2jp0qXutfawawNSU1N18cUXq2HDhu7z9ezZU6+//nqp97TPYa979NFH3fDCtm3buveaMGGCatasqeuuu26ndq5YsUJRUVF64IEHKvzXAACwM3quAAC+mTVrlgYOHOiC1S233KKYmBi9+OKLLoz89NNP6t+/v7vOgowFhEsuuUT777+/MjIy3FwuCy8WYszJJ5/s7nfNNde44GKBxcLXsmXLSgWZXdmwYUOZx61n6K9cfvnl+vDDD3X11VerS5cuWr9+vX799VfNnj1bvXv31r/+9S+lp6e7sPP444+711iQDAwxtM+7YMEC9/rWrVvrgw8+0AUXXKBNmzbtFJpee+01bdu2TZdeeqkLVy1atNCJJ56okSNH6rHHHnNhKuDdd99VYWGhzj777L/8DACAClAIAEAleO211wrtfzMTJ07c5TUnnHBCYWxsbOHChQuLj61ataowKSmp8JBDDik+1rNnz8Kjjz56l/fZuHGje69HHnlkr9t55513utfu7rHje9sxe11ASkpK4VVXXbXb97F7tGzZcqfjTzzxhLvfW2+9VXwsJyencMCAAYWJiYmFGRkZ7tjixYvddcnJyYWpqaml7vHNN9+4c1999VWp4z169CgcNGjQXv6KAAD2FcMCAQC+yM/P17fffqsTTjhBbdq0KT5uw/nOOuss1/NjPVSmVq1arldq/vz5Zd4rISHBzTsaM2aMNm7cuE/t+eijj1xP144PG6r3V6x948eP16pVq/b6fb/88ks1atRIZ555ZvEx68G79tprlZmZ6XrwSrIeuvr165c6NmTIEDVp0kRvv/128TErnjF9+nSdc845e90mAMC+IVwBAHyxbt06ZWVlqWPHjjud69y5sxuOt3z5cvf8nnvucUPkOnTooO7du+vvf/+7Cw4BNjzuoYce0ldffeXC0CGHHKKHH37YzcPaU/YaCyk7PvakeIW9l4WZ5s2bu2GLNoxx0aJFe/S+Ng/L5pJFRkbu9GsQOF+SDRvckb3Whv59+umn7tfUWNCytp966ql71A4AQPkRrgAAQc+Cz8KFC/Xqq6+qW7dueuWVV9xcJtsGWEGJefPmublZFipuv/12F1CsYEZlO+2001yYevrpp10P0iOPPKKuXbu6sFfRrJeuLOedd57r6bKAZSMXrfS7rc+VkpJS4W0AAJSNcAUA8IUNbatRo4bmzp2707k5c+a43hjrCQqoU6eOqwZoRRqsR6tHjx6lKvYZq6B30003ueGG1pOUk5Oj//znP1XyeWw445VXXunCzeLFi11Fw3//+9/F563SX1latmzphjvuWDjDfg0C5/eEhc799tvP9Vj98ssvrpDHueeeW67PBADYO4QrAIAvrKrd0KFD9dlnn5Uql7527VrX62Kl1q2KoLHqeyVZpb127dopOzvbPbehcFZBb8eglZSUVHxNZc4ds0qAJVkpduvBKvneVi59x+vMUUcd5YYvWrW/kutXWS+Yfc5BgwbtcVssTFmwtFLtFu6sxD0AoOpQih0AUKlsKN/XX3+903ErMX7fffe5ohEWpKzXJzo62pVit1Bi85gCrLy5lSvv06eP68GyMuyB0ufGhgPa+lg2PM+utft88sknLqidccYZlfr5bI0rW1vrlFNOcetTWSAaPXq0Jk6cWKrXzNpuAerGG29Uv3793HXHHnusK6lun9lKr0+ePNmVjbfPNnbsWBeSLCDuKSsEYiXt7bNfccUVrjAGAKDqEK4AAJXq+eefL/O4hQmbl2RD2G677TY3V8qGxtnaVm+99VbxGlfGKud9/vnnrlfGgpcNlbNgZoUtjA0ftGp733//vd58800Xrjp16qT333/fVderTDa00YKhte3jjz92n8F61Z577jkXcALsmqlTp7p1qmytK/sMFq5sDpVVObz11lvdwsFWIdGKfNh19mu0N6yYh/UGWgVChgQCQNWLsHrsPrwvAACoBLag8IwZM9yixACAqsWcKwAAwsTq1av1xRdf0GsFAD5hWCAAACHOqhPaHC0rTW/zrC677DK/mwQA1RI9VwAAhLiffvrJ9VZZyLJ5W40aNfK7SQBQLTHnCgAAAAAqAD1XAAAAAFABCFcAAAAAUAEoaFEGW6Nk1apVbuHGiIgIv5sDAAAAwCc2i8oWjG/SpIkiI3ffN0W4KoMFK1uQEgAAAADM8uXL1axZM+0O4aoM1mMV+AVMTk72uzkAAAAAfJKRkeE6XgIZYXcIV2UIDAW0YEW4AgAAABCxB9OFKGgBAAAAABWAcAUAAAAAFYBwBQAAAAAVgDlXAAAACAn5+fnKzc31uxkIM1FRUYqOjq6QJZgIVwAAAAh6mZmZWrFihVtzCKhoNWrUUOPGjRUbG1uu+xCuAAAAEPQ9Vhas7Afg+vXrV0gPA2AsrOfk5GjdunVavHix2rdv/5cLBe8O4QoAAABBzYYC2g/BFqwSEhL8bg7CTEJCgmJiYrR06VIXtOLj4/f5XhS0AAAAQEigxwqVpTy9VaXuUyF3AQAAAIBqjnAFAAAAABWAcAUAAACEiFatWumJJ57Y4+vHjBnjhlNu2rSpUtsFD+EKAAAAqGAWaHb3uOuuu/bpvhMnTtSll166x9cfeOCBWr16tVJSUlSZCHEeqgUCAAAAFcwCTcDIkSN1xx13aO7cucXHEhMTi/etEqKVm7eFbP+KVUzcG7ZuU6NGjfbqNdh39FwBAAAgpFgYycrJ8+Wxp4sYW6AJPKzXyHp1As/nzJmjpKQkffXVV+rTp4/i4uL066+/auHChTr++OPVsGFDF7769eun0aNH73ZYoN33lVde0YknnujWAbN1mj7//PNd9iiNGDFCtWrV0jfffKPOnTu79znyyCNLhcG8vDxde+217rq6devqH//4h84//3ydcMIJ+/ydbdy4Ueedd55q167t2jl8+HDNnz+/+LyVQT/22GPd+Zo1a6pr16768ssvi1979tlnF5fit8/42muvKRjRcwUAAICQsjU3X13u+MaX9/7znmGqEVsxP0LfeuutevTRR9WmTRsXKpYvX66jjjpK//73v13geuONN1zgsB6vFi1a7PI+d999tx5++GE98sgjevrpp10QsbBSp06dMq/Pyspy7/vmm2+6EuTnnHOObr75Zr399tvu/EMPPeT2LcBYAHvyySf16aef6rDDDtvnz3rBBRe4MGXBLzk52QU2+6x//vmnW2PqqquucmtM/fzzzy5c2fFA797tt9/unlsYrVevnhYsWKCtW7cqGBGuAAAAAB/cc889OuKII4qfWxjq2bNn8fN7771Xn3zyiQskV1999W6Dy5lnnun277//fj311FOaMGGC65Ha1aLML7zwgtq2beue272tLQEW0G677TbXG2aeeeaZ4l6kfTG/KFSNHTvWzQEzFt6aN2/uQtupp56qZcuW6eSTT1b37t3deQucAXZuv/32U9++fYt774IV4SrYbVgszflC2v9SKTrW79YAAAD4LiEmyvUg+fXeFSUQFgIyMzNdoYsvvvjCDdOz4XnWQ2PhYnd69OhRvG+9PtYzlJqausvrbVheIFiZxo0bF1+fnp6utWvXav/99y8+HxUV5YYvFhQU7NPnnD17tptP1r9//+JjNtywY8eO7pyxYYhXXHGFvv32Ww0ZMsQFrcDnsuP2fMqUKRo6dKgbnhgIacGGOVfBzH4DvzpM+vZf0pKf/W4NAABAULA5RDY0z4+HvXdFsSBUkg3Ns54q63365ZdfNHXqVNeTY8PldseG1e3467O7IFTW9Xs6l6yyXHLJJVq0aJHOPfdczZgxwwVP60EzNj/LhjnecMMNWrVqlQYPHux+rYIR4SqYRUZKnY729v/cPjERAAAA4ceGzdkQPxuOZ6HKil8sWbKkSttgxTesoIaVfA+wSobWa7SvOnfu7Hrhxo8fX3xs/fr1bi5Zly5dio/ZMMHLL79cH3/8sW666Sa9/PLLxeesmIUV1XjrrbdcQY+XXnpJwYhhgcGuy/HSpFelOaOkox+TovjKAAAAwpFVwbNgYUUsrDfJCjns61C88rjmmmv0wAMPqF27durUqZPrQbKKfXvSazdjxgxXCTHAXmPzyKwK4t/+9je9+OKL7rwV82jatKk7bq6//nrXQ9WhQwf3Xj/++KMLZcbK2NuwRKsgmJ2drVGjRhWfCzb8pB7sWh4sJdSRstZLS8dKbQb53SIAAABUgscee0wXXXSRm09kVfGsol5GRkaVt8Ped82aNa50us23skWLhw0b5vb/yiGHHFLqub3Geq2s8uB1112nY445xg1ztOusSEZgiKL1jlnFwBUrVrg5Y1aM4/HHHy9eq8sKbFgvnpViHzhwoN577z0Fo4hCvwdYBiH7TWxdojahz75c3312tfTHm1K/S6Sj/+N3awAAAKrUtm3btHjxYrVu3Vrx8fF+N6fasd4z6yk67bTTXAXD6vZ7LGMvsgFzroJcXn6BljUqKtE5+39ekQsAAACgkljxCJvvNG/ePDfMz6r1WfA466yz/G5a0CNcBbnBj/2kwZ9IebFJUuZaafn2iYAAAABARbOFhUeMGKF+/frpoIMOcgFr9OjRQTvPKZgw5yrItW+QqKXrs7S47iC1Xz1K+vMzqeUAv5sFAACAMGVV+6xyIfYePVdBrkezWm77Y2TRQml/fsrQQAAAACAIEa6CXM/mXrj6cFMHKS5F2rxaWjbO72YBAAAACMZw9eyzz6pVq1auMkf//v01YcKEXV5rk+us/GLt2rXdY8iQITtdb4uvWU39kg8r5xiKejRNcdt563OU06FoQeGZH/nbKAAAAADBF65GjhypG2+8UXfeeadb+dkWGbM6+qmpqWVeP2bMGJ155pluYbFx48a5MaFDhw7VypUrS11nYWr16tXFj3fffVehqHbNWLWoU8Ptz6s/dPvQwPw8fxsGAAAAILjClS2WZqs1X3jhherSpYteeOEF1ahRQ6+++mqZ17/99tu68sor1atXL7di9CuvvOJq73///felrouLi1OjRo2KH9bLFap6NPN6r37O6yzVqOstKLz4J7+bBQAAACBYwpWtzjx58mQ3tK+4QZGR7rn1Su2JrKws5ebmqk6dOjv1cDVo0EAdO3Z0tfnXr1+/y3tkZ2e7xcFKPoJJr6J5V1NXZEpdTvAOzvzY30YBAAAACJ5wlZaWpvz8fDVs2LDUcXu+Zs2aPbrHP/7xDzVp0qRUQLMhgW+88YbrzXrooYf0008/afjw4e69yvLAAw+4VZcDDxtqGIwVA6evSJe6nbx9QeG8bH8bBgAAgEp16KGH6vrrry9+bnUKnnjiid2+xuoNfPrpp+V+74q6T3Xi+7DA8njwwQf13nvv6ZNPPnHFMALOOOMMHXfccerevbtOOOEEjRo1ShMnTnS9WWW57bbblJ6eXvxYvny5gkm3psmKjJDWZGzT2tr7SUmNpex0aUHpoZAAAAAIDscee+wuC6r98ssvLrhMnz59r+9rP9Neeumlqkh33XWXm3KzI6tbYB0UlWnEiBGqVcvrSAgHvoarevXqKSoqSmvXri113J7bPKndefTRR124+vbbb9WjR4/dXtumTRv3XgsWLCjzvM3PSk5OLvUIJjVio9W+QZLbn7YiQ+p6kneCqoEAAABB6eKLL9Z3332nFStW7HTutddeU9++ff/yZ9iy1K9f39UnqAr287j9nIwQCVexsbHq06dPqWIUgeIUAwYM2OXrHn74Yd177736+uuv3W/Mv2K/qW3OVePGjRWqejZP2Xlo4NwvpZwt/jYMAACgqhUWej8D+fGw994DxxxzjAtC1jNTUmZmpj744AMXvuznU6uC3bRpUxeYbNTVX1W43nFY4Pz583XIIYe4UVxWHM4CXVnTaDp06ODewzodbr/9dlezwFj77r77bk2bNq14CaNAm3ccFjhjxgwdfvjhSkhIUN26dV0Pmn2eksshnXDCCa4TxH7utmuuuuqq4vfaF8uWLdPxxx+vxMRE1wFy2mmnleqYsXYfdthhSkpKcuctW0yaNMmdW7p0qetBtMJ2NWvWVNeuXfXll1+qMkXLZ1aG/fzzz3chaf/993e/WbZs2eKqB5rzzjvP/YazeVHG5lDdcccdeuedd9xvrsDcLPsFt4d9wfYb5OSTT3Zpe+HChbrlllvUrl07V+I9VNm8q/cnrdC0FZukoftLtVpKm5ZK876RuhX1ZAEAAFQHuVnS/U38ee9/rpJia/7lZdHR0e7nWAsq//rXv1xQMRasrA6AhSr7udXCgIUfCwZffPGFzj33XLVt29b9XPxXrFPipJNOcvUKxo8f76a3lJyfFWDBw9phdQosIFmlbjtmPyOffvrpmjlzpuu0GD16tLveahDsyH4+t5+lrQPEhibaskmXXHKJrr766lIB8scff3TByrY2aszub0MO7T33ln2+QLCyGgp5eXkurNk9A9N9zj77bO233356/vnn3Yi4qVOnKiYmxp2za62A3s8//+zC1Z9//unuFdbhyn5x1q1b5wKTBSX7xbcvN1DkwtKqVRAMsF84+0U65ZRTSt3H1smy8aL2i2rjV19//XVt2rTJ/SaydbCspyuUuzWLKwYu26SCQinSeq9+fcwbGki4AgAACDoXXXSRHnnkERcMrDBFYEigdQIECqndfPPNxddfc801+uabb/T+++/vUbiyMDRnzhz3GvuZ19x///07zZP6v//7v+J965yw97S6BRaurBfKAoeFwd1Ny7GOjW3btrmicRZUzDPPPON6hqzzI/Cze+3atd1x+5nclk06+uij3ai0fQlX9joLg4sXLy4uOGfvbz1QFvD69evnssLf//53916mffv2xa+3c/ZrbT2CxnrtKpvv4cpY4rVHWXYsQrFkyZLd3st+g9hvsHDTqVGSasRGaXN2nuanZqpjIFzN/07ali7F7/wvDAAAAGEppobXg+TXe+8h+4H/wAMPdOu3WriynhwrZnHPPfe489aDZWHIwtTKlStdB4ItEbSnc6pmz57tQkcgWJmyptaMHDlSTz31lBvRZb1l1gO0tzUG7L169uxZHKzMQQcd5HqX5s6dWxyuunbt6oJVgPViWUDaF4HPV7KStw19tAIYds7ClY2Csx60N99801UPP/XUU13Pn7n22mvdkkxWo8HOWdDal3lu1aZaYHUSHRVZ3Hs1ZdlGqWFXqV5HKT9bmvOF380DAACoOjbEzobm+fEoGt63p2xu1UcffaTNmze7Xiv7wX/QoEHunPVqPfnkk25YoA2jsyFtNvTOQlZFsbVjbejcUUcd5Spo//HHH26YYkW+R0kxRUPyAmw4pAWwymIj12bNmuV6yH744QcXvqySuLHQtWjRIjfU0gKeTUN6+umnVZkIVyGkd4vabjt56UbvD3agsMUs7zcQAAAAgosVYLApLjaszoa02VDBwPyrsWPHujlF55xzjusVsmFr8+bN2+N7d+7c2S0hZCXTA37//fdS1/z2229q2bKlC1QWLmzYnBV62LHI3K7Wgy35XlY8wuZeBVj77bN17NhRlaFz0ecruUySzZuyqT8WogKsWMcNN9zgeqhsDpqF2ADr9br88sv18ccf66abbtLLL7+sykS4CiF9Wtbe3nNlOh/rbRf/LOVu9bFlAAAAKIvNZ7IaA7auqoUgq6gXYEHHqvtZALJhbpdddtlOSxTtjg11s2BhxeEs+NiQQwtRJdl72Nwjm2NlwwJteGCgZ6fkPCyb12Q9Z2lpaW5o4o6s98sqEtp7WQEM62mzOWLWKxQYErivLNjZe5d82K+HfT6bL2XvPWXKFE2YMMEVCbGePwuKW7dudVOLbBqRBUYLezYXy0KZseIeNl3IPpu93tocOFdZCFchZL8W3rDAReu2aMOWHKlBZym5mZS3TVryq9/NAwAAwC6GBm7cuNEN+Ss5P8oKTfTu3dsdtzlZVlDCSpnvKes1sqBkIcMKYNgwuH//+9+lrjnuuONcr46FECscZ0HOSrGXZHORbMFjK2lu5ePLKgdv88AsqGzYsMHNdbLicoMHD3bFK8orMzPTVfwr+bBCGdbD99lnn7kiGVZu3sKW9e7ZHDJjc7usnL0FLguZ1ktoxTyscnggtFnFQAtU9vnsmueee06VKaKwcA+L9VcjGRkZrnqLlbMMtgWFB/9njBau26L/nt9Xgzs3lP53vTT5NWn/S6WjHvG7eQAAABXOqtRZ70Pr1q1d7wlQlb/H9iYb0HMV6kMD2x/hbed/u8eL2gEAAACoeISrEA1XrqiFaT1IioyRNi6R1i/0t3EAAABANUa4CtGKgdOWpysvv0CKS5RaHbS99woAAACALwhXIaZt/UQlx0dra26+5qzZ7B1sP9TbEq4AAAAA3xCuQkxkZIT2K7nelWlXNO9q6VgpZ/vaAwAAAOGEOmwI9t9bhKtwKGpRr71Uq4WUnyMt/c3fxgEAAFQwK7ltcnJy/G4KwlRWVpbbxsTElOs+0RXUHvhZ1MJW+W51iDT1LS9cBSoIAgAAhIHo6Gi3ztK6devcD7+2vhNQUT1WFqxSU1NVq1at4iC/rwhXIahn81qKjJBWbNyq1IxtapAcL7U8sChcjfW7eQAAABXKFpNt3LixW4do6dKlfjcHYahWrVpuEefyIlyFoMS4aHVslKzZqzPc0MAjuzX2wpVZOUXKyZJia/jdTAAAgAoTGxur9u3bMzQQFc56Q8vbYxVAuApRfVrWKgpXm7xwVbuVlNRE2rxKWjlJan2I300EAACoUDYcMD4+3u9mALvEgNUQ1au5N+9q6rJN2+ddBXqvlo7zsWUAAABA9US4ClG9mqe47YyVRYsJm2b9vO3KyT62DAAAAKieCFchqk29RCUVLSY8b22md7BZ3+3hinUgAAAAgCpFuArhxYR7Nqvl9qcuLxoa2LCbFBkjZaVJm6ikAwAAAFQlwlUI61k0NHDq8qL1rmLipUbdvX2GBgIAAABVinAVDkUtAj1Xpmkfb7uCcAUAAABUJcJVGPRczU/NVGZ23s7zrgAAAABUGcJVCGuQFK+mtRJc7YrpKzaV7rlaPVXKz/W1fQAAAEB1QrgKcb2a71DUok5bKS5Fytsmpf7pb+MAAACAaoRwFS7hKrCYcGSk1LS3t8/QQAAAAKDKEK5CXM8de64MRS0AAACAKke4CnHdm6YoMkJK3ZyttRnbdihqMcnXtgEAAADVCeEqxCXERql9gyS3P2NFeumeq3VzpW1FxwAAAABUKsJVGOjW1CvJPn1lUZBKbCDVbiWpUFpB7xUAAABQFQhXYaBHMy9czQiUYzfN+3vb5RN8ahUAAABQvRCuwqjnasbKDBXaolem+f7edvl4H1sGAAAAVB+EqzDQpXGyoiIjlJaZrTWBohaBnisrx16Q72v7AAAAgOqAcBUmRS3a1q/p9ues3uwdbNBFik2UsjO8whYAAAAAKhXhKky0b+hVDJyfWhSuIqOkRj28/TXTfWwZAAAAUD0QrsJE+waJbjt/beb2g42LwtVqwhUAAABQ2QhXYaJDUc/VvNQS4apRd29LzxUAAABQ6QhXYdZztWDt5u0VA0sOCwwcAwAAAFApCFdhomXdmoqOjNCWnHytTi+qGFi/kxQZI21LlzYt87uJAAAAQFgjXIWJ2OhItarnVQycHxgaGB0rNejk7a+Z4WPrAAAAgPBHuArLohZFFQMNFQMBAACAKkG4CiNt63vhalHalp3DFRUDAQAAgEpFuAojbYoWEl60roxy7AwLBAAAACoV4SqMtC6ac7VoXYmeq4bdvG3GCilrg08tAwAAAMIf4SqMtCkaFpi6OVuZ2XnewfhkqXZrb3/1NB9bBwAAAIQ3wlUYSUmIUb3EWLe/uGTvVfFiwgwNBAAAACoL4SpchwamlTXviqIWAAAAQGUhXIWZNvWKKgaW6rnq6W2pGAgAAABUGsJVuFYMTCtjWOD6+VJOlk8tAwAAAMIb4SpsKwaWGBaY1EiqWV8qLJBS//SvcQAAAEAYI1yFacXAxWlbVFhY6B2MiChR1IKhgQAAAEBlIFyFmRZ1aigqMkJZOflam5G9/USjoqIWzLsCAAAAKgXhKszERkeqee2EnYcGUjEQAAAAqFSEqzAeGliqqEXjXt52zUwpL8enlgEAAADhi3AVhtoUF7UoEa7qtJES6kj52fReAQAAAJWAcBWGWheXYy8xLNCKWjTr5+0vn+BTywAAAIDwRbiqLgsJm+ZF4WoF4QoAAACoaISrMNS2gddztWJjlrbl5m8/0Wx/b7t8ok8tAwAAAMIX4SoM1U+MU0pCjAoKd+i9atpHioiUMlZIGav8bCIAAAAQdghXYSgiIkIdGnpDA+enbt5+Ii5RatDV22feFQAAAFChCFdhql2DJLedv7ZEUQvTPDA0kHAFAAAAVCTCVZhq36CMnquS4YqiFgAAAECFIlyFqQ4Nd9FzFSjHvnqalJftQ8sAAACA8ES4ClPti+ZcLVm/Rdl5+aUXE65RV8rP8QIWAAAAgApBuApTDZLilBQf7SoGLk7bssNiwsy7AgAAACoa4SqMKwa2ruetd7VsfVbpkywmDAAAAFQ4wlUYa16nhtsu27BDuCrZc1VY6EPLAAAAgPBDuApjzWt74Wr5juGqaW8pIkravFpKX+FP4wAAAIAwQ7gKYy121XMVW1Nq1M3bZ2ggAAAAUCEIV9UgXC3fuHXnkxS1AAAAACoU4ao6hKsNWSqwsoGlTh7gbZf+5kPLAAAAgPBDuApjjWvFKyoyQtl5BVqXucOCwS0P8rZrZkhbN/nSPgAAACCcEK7CWExUpJrUii973lVyY29BYRVKy373p4EAAABAGCFcVZeiFjuudWVaHextl/5axa0CAAAAwg/hKsw1q+WFq5Wbyihq0bIoXC0ZW8WtAgAAAMIP4SrMNa2d4LYry6oY2Kpo3tXqaVL25ipuGQAAABBeCFdhrkktL1ytSi8jXKU0k2q1lArzpWXjq75xAAAAQBghXIW5QEGLMnuuDPOuAAAAgApBuKpGc64KC3dY66pkSfYlhCsAAACgPAhXYa5hSpwiIuTWutqwJWfXPVer/pBytlR5+wAAAIBwQbgKc3HRUaqfGLfrioG1W0opLaSCPNa7AgAAAMqBcFWNKgauKitcmdYDve3in6uwVQAAAEB4IVxVo4qBK3ZV1KL1Id6WcAUAAADsM8JVNdA0UI5907ayL2hV1HO1eqq0Lb0KWwYAAACEj6AIV88++6xatWql+Ph49e/fXxMmTNjltS+//LIGDhyo2rVru8eQIUN2ut6q4t1xxx1q3LixEhIS3DXz589XdQ9XKzZmlX1BSlOpThupsICqgQAAAECohquRI0fqxhtv1J133qkpU6aoZ8+eGjZsmFJTU8u8fsyYMTrzzDP1448/aty4cWrevLmGDh2qlStXFl/z8MMP66mnntILL7yg8ePHq2bNmu6e27btoucmzLWo65VjX7ZhF+HKtD3c287/ropaBQAAAISXiMIyFz+qOtZT1a9fPz3zzDPueUFBgQtM11xzjW699da/fH1+fr7rwbLXn3feea7XqkmTJrrpppt08803u2vS09PVsGFDjRgxQmecccZf3jMjI0MpKSnudcnJyQp1i9O26LBHxyg+JlKz7zlSEVabfUfzvpHeOU1KbibdMFOufjsAAABQzWXsRTbwtecqJydHkydPdsP2ihsUGemeW6/UnsjKylJubq7q1Knjni9evFhr1qwpdU/7xbAQt6t7Zmdnu1+0ko9wGxYYFRmhbbkFSt2cvet5V9HxUsYKKfXPqm4iAAAAEPJ8DVdpaWmu58l6lUqy5xaQ9sQ//vEP11MVCFOB1+3NPR944AEXwAIP6zkLJ7HRkWpSK97tL12/i6GBsTW2Vw20XiwAAAAAoTXnqjwefPBBvffee/rkk09cMYx9ddttt7luvsBj+fLlCjet6tZ02yXrt+z6ovZDvS3zrgAAAIDQClf16tVTVFSU1q5dW+q4PW/UqNFuX/voo4+6cPXtt9+qR48exccDr9ube8bFxbnxkyUf4aZlUVGLpXsSrpaPl7ZurKKWAQAAAOHB13AVGxurPn366Pvvvy8+ZgUt7PmAAQN2+TqrBnjvvffq66+/Vt++fUuda926tQtRJe9pc6isauDu7hnutvdc7aZiYO2WUv1OUmG+tGD7rx8AAACAEBgWaGXYbe2q119/XbNnz9YVV1yhLVu26MILL3TnrQKgDdsLeOihh3T77bfr1VdfdWtj2Twqe2RmZrrzVgnv+uuv13333afPP/9cM2bMcPeweVknnHCCqqsWdYrKse8uXBmGBgIAAAD7JFo+O/3007Vu3Tq36K+FpF69erkeqUBBimXLlrkKggHPP/+8qzJ4yimnlLqPrZN11113uf1bbrnFBbRLL71UmzZt0sEHH+zuWZ55WaGuVb3tc66sXH2Z5dhNh2HSb09JC76TCvKlyKiqbSgAAAAQonxf5yoYhds6V2Zbbr463f61259y+xGqUzO27Avzc6WH20rZ6dLFo6Xm/aq2oQAAAEAQCZl1rlB14mOi1Cg5/q8rBkbFSG0P8/bnU5IdAAAA2FOEq2okUDHwL+dd2dBAM//bKmgVAAAAEB4IV9XIHq11Zdod4W1XT5M279lizgAAAEB1R7iqRlrWC6x19Rc9V4n1pSa9vX16rwAAAIA9QriqRlrW2cOeK8PQQAAAAGCvEK6qkT2ec1VyvauFY6S8nEpuGQAAABD6CFfVMFyt35KjjG25u7+4cS+pZgMpZ7O09NeqaSAAAAAQwghX1UhSfIzqJXrrWy1e9xdDA23h5k5HefszP66C1gEAAAChjXBVzbRrkOi2C1Iz//ribqd429mfS3nZldwyAAAAILQRrqppuJq/J+Gq5YFSUmNpW7q0YHTlNw4AAAAIYYSraqZ9g6Q977mKjJK6nuTtz/iwklsGAAAAhDbCVTXTvnhY4OY9e0H3k73t3K+k7D0IZAAAAEA1RbiqpsMCl23I0rbc/L9+gS0mXLu1lLfVC1gAAAAAykS4qmbqJ8UpOT5aBYXS4rQ9WEw4IkLqXlTYYsb7ld4+AAAAIFQRrqqZiIiIvStqYbqf5m0XfC9lplZi6wAAAIDQRbiqhvaqqIWp30Fq1k8qzJem03sFAAAAlIVwVQ21b7iXRS1Mr7O87dS3pcLCSmoZAAAAELoIV9VQ271ZSDjASrJHJ0ipf0pLf6u8xgEAAAAhinBVjcuxW0GL3PyCPXtRQi2p15ne/rhnKrF1AAAAQGgiXFVDTVISlBATpdz8Qi1dn7XnLzzgSm9rJdnXL6y09gEAAAChiHBVDUVGbq8YuFdDA+u1l9oPlVQoTXm98hoIAAAAhCDCVTUfGrhXRS1M7/O97dR3pPzcSmgZAAAAEJoIV9VUu6KKgfPW7kXPlekwTKrZQNqyTpr7ZeU0DgAAAAhBhKtqqmNDb62ruWv2sucqKkbqfa63/+MDUn5eJbQOAAAACD2Eq2qqU+Nkt124LlM5eXtYMTDgwGukhNrSutnMvQIAAACKEK6qqSYp8UqKj1ZeQaELWHvFgtWht3n7vz4uFeRXShsBAACAUEK4qqYiIiLUqdE+Dg00vc/zQlb6cmn+dxXfQAAAACDEEK6qsY5F4Wr2moy9f3FMgtTrbG9/4ssV3DIAAAAg9BCuqrFOjZL3vefK9L3I+sCkBaPpvQIAAEC1R7iqxgLDAues3sdwVbetdMCV3v7n10jb9qEHDAAAAAgThKtqrENRuFqTsU3pWfu4IPDg26U6baTNq6kcCAAAgGqNcFWNJcfHqGmtBLc/Z1/mXQXmXh18g7c//kXWvQIAAEC1Rbiq5jo3LhoauK/zrkz306Qa9bzKgZNfq7jGAQAAACGEcFXNBSoGlitcxcRLB13r7X91izR7VAW1DgAAAAgdhKtqLlAxcJ+HBQYceK3U+3ypsED6+jaGBwIAAKDaIVxVc4GKgfPWbFZBQeG+3ygiQhr+UNHwwGXS7M8rrpEAAABACCBcVXOt69VUbHSktuTka9mGrPLdzIpb9LvE2//taamwHGENAAAACDGEq2ouOipSnYt6r2auSi//DS1cRSdIq6ZIc78s//0AAACAEEG4gro2TXHbmSsrYBHgxPrSAVd4+6PvZu4VAAAAqg3CFdS9OFxVQM+VOeg6KaG2lDZXmvhKxdwTAAAACHKEK6hbk6JwtSpdhRUxTyqhljT4Dm//h/ukjUvKf08AAAAgyBGuoA6NEhUTFaFNWblasXFrxdy09wVSs35SzmbplSOkVVMr5r4AAABAkCJcQXHRUcWLCc+qiKIWJjJSOnWE1LC7tCVV+ugSKT+3Yu4NAAAABCHCFUoNDZxRUfOuTEoz6YJR3tpX6+dLk16tuHsDAAAAQYZwBadbRVYM3HH+1eH/8vZ/ekjK2VKx9wcAAACCBOEKO4SrCipqUdJ+50m1W0lZ66Upb1bsvQEAAIAgQbiC06lRkqIiI7R+S47WZGyr2JtHRXvl2c1vT0u5FXx/AAAAIAgQruDEx0SpfYNEtz9jRQXOuwroeZaU1FjKWCF9fWvF3x8AAADwGeEKlbeYcEkx8dLxz0qKkCa/Jk1+veLfAwAAAPAR4QrFejTzwtW0yui5Mu0GS4P+4e2Pul6a+1XlvA8AAADgA8IVivVsXsttp63YVPFFLQIOvVXqdbZUWCB9cKG0fELlvA8AAABQxQhXKNapUbJioyO1KStXS9ZnVc6bRERIxz4ptR8q5W2VPv6bVFlBDgAAAKhChCsUs2DVrUmy25+6fGPlvVFUjHTKq1JsorRxCb1XAAAACAuEK5TSq3ltt526bFPlvlFcktTpGG9/xgeV+14AAABAFSBcoZReLbx5V1OXV3K4Mt1P9bZT35ZGniNlrqv89wQAAAAqCeEKpfRq5oWrP1dnaFtufuW+WZtDpZQWUm6WNPt/rH8FAACAkEa4QinN6ySoTs1Y5eYXavbqjMp9s6ho6dIx0kkvSxGR0swPpQWjK/c9AQAAgEpCuEIpERER6tW8CocG1qwr9ThN6neJ9/z9C6QVkyr/fQEAAIAKRrjCTqo0XAUccY/UaqCUs1n65DLKswMAACDkEK4QHOEqJkE64x0ppoa0foG0akrVvTcAAABQAQhX2EnPonC1dH2W0jKzq+6N45OljsO9/RkfVt37AgAAABWAcIWdpCTEqGPDJLc/cfGGqn3zQHn2ae9K09+XCgqq9v0BAACAfUS4Qpn6t6njtuOrOly1HSzVaStt3Sh9/Dfp3TOkrVU4PBEAAADYR4QrlKl/67pu+/ui9VX7xtGx0iWjpcP+JUXFSfO/kb64sWrbAAAAAOwDwhXKtH9rr+dq7trN2pSVU7VvXqOONOgW6fz/WXF4aeZH0vIJVdsGAAAAYC8RrlCm+klxalO/pquIPnHJRn8a0aK/1Otsb/+tk6Xv7pTWL/SnLQAAAMBfIFzhL4cGjq/qoYElDblTatRdys6Qxj4hPX+QtHaWf+0BAAAAdoFwhV06wK+iFiUlNpAu/Vk6/W0vZOVtlSa+4l97AAAAgF0gXOEve65mrUpXxrZc/xoSGSl1PkYaep/3fMZHUu5W/9oDAAAAlIFwhV1qlBKvlnVrqKBQmrzUp3lXJbU6REppIWWnS9Pe87s1AAAAQCmEK+xW/6KqgeMW+jjvqmQPVr+LvP2vbpGW/Op3iwAAAIBihCvs1sHt67vtT3PXKSgceK3U+VgpP0f6+DIpJ8vvFgEAAAAO4Qq7dUj7eoqM8Na7WrkpCOY5RUZJJ73sDQ/MWCH99pTfLQIAAAAcwhV2q1aNWPVuUdvt/zgnVUEhJkE64m5vf8yD0ui7pK1BMCcMAAAA1RrhCn/psE4N3HbM3CAJV6briVJfm39VKP36uPRYV2n8i3KrHgMAAAA+IFzhLx3W0QtXYxes17bcfAWFiAjpmMel096UGnSVcrd4RS6+L+rRAgAAAKoY4Qp/qXPjJDVMjtPW3HxN8HNB4bJ0OU66Yqx0xL3e8wmvUOQCAAAAviBc4S9FREQU9179GExDA0v2Yg24WqrVQsrZLM35wu8WAQAAoBoiXGGPHFoUrsYES0n2stbA6nmmt//HG8y9AgAAQJUjXGGPHNSurmKiIrQ4bYt7BCULVxGR0uKfpS9ukhaMlgqCZI4YAAAAwh7hCnskKT5G/VrVCa6S7Duq01oa/rC3P+m/0lsnS59dJRUU+N0yAAAAVAOEK+yxoJ53FbD/36QTX5LaD5MioqRp70pjHvC7VQAAAKgGCFfYY4d1qu+24xdtUFZOnoJWz9Ols9+Xjn/Ge/7Lo9Ky8X63CgAAAGHO93D17LPPqlWrVoqPj1f//v01YcKEXV47a9YsnXzyye56q2D3xBNP7HTNXXfd5c6VfHTq1KmSP0X10LZ+oprVTlBOfkHwFrYoqddZUo8zpMIC6cOLpA2L/G4RAAAAwtg+havly5drxYoVxc8tEF1//fV66aWX9uo+I0eO1I033qg777xTU6ZMUc+ePTVs2DClppY97CwrK0tt2rTRgw8+qEaNGu3yvl27dtXq1auLH7/++utetQtls6B6dI/Gbv/TP1YqJAx/SKrbTspYIb12tJQVZOt0AQAAoHqHq7POOks//vij21+zZo2OOOIIF7D+9a9/6Z577tnj+zz22GP629/+pgsvvFBdunTRCy+8oBo1aujVV18t8/p+/frpkUce0RlnnKG4uLhd3jc6OtqFr8CjXr16+/ApUZaT9mtWPO9qU1aOgl5CLemCL72AtXmV9OP9frcIAAAAYWqfwtXMmTO1//77u/33339f3bp102+//aa3335bI0aM2KN75OTkaPLkyRoyZMj2xkRGuufjxo1TecyfP19NmjRxvVxnn322li1bttvrs7OzlZGRUeqBsnVslKTOjZOVm1+oUdNXKyQkNZSOedzbn/SqNO09KggCAAAgOMJVbm5ucc/R6NGjddxxx7l9m9tkw/D2RFpamvLz89WwYcNSx+259YbtK5u3ZQHv66+/1vPPP6/Fixdr4MCB2rx58y5f88ADDyglJaX40bx5831+/+rgxP2ahNbQQNP6EKnLCVJhvvTJZdJ3t/vdIgAAAISZfQpXNqfJhvD98ssv+u6773TkkUe646tWrVLdunXlp+HDh+vUU09Vjx493PytL7/8Ups2bXI9bLty2223KT09vfhhc8qwa8f1bKqICGnS0o1atj5LIePEF6RD/+nt//6ctHq63y0CAABAdQ9XDz30kF588UUdeuihOvPMM10hCvP5558XDxf8KzYPKioqSmvXri113J7vrljF3qpVq5Y6dOigBQsW7PIa64VLTk4u9cCuNUqJ10FtvXlsn04Nod6rmATp0H9I3U72Kgh+dqW0dZPfrQIAAEB1DlcWqmxYnz1KFp+49NJLXY/WnoiNjVWfPn30/fffFx8rKChwzwcMGKCKkpmZqYULF6pxY6/KHSrGCfs1LR4aWFhYqJAy9D4poY60Zob05olS7ja/WwQAAIDqGq62bt3qikDUrl3bPV+6dKlbc2ru3Llq0KDBHt/HyrC//PLLev311zV79mxdccUV2rJli6seaM477zw3ZK9kEYypU6e6h+2vXLnS7Zfslbr55pv1008/acmSJa7Ixoknnuh6yKyHDRXnyG6NFB8TqUVpWzR9RbpCSnIT6fz/eQFr1RRp9F3SmAellwdLv+68dhoAAACwJ6K1D44//niddNJJuvzyy918JisiERMT43qyrLy6haQ9cfrpp2vdunW64447XBGLXr16uUIUgSIXVuXPKggG2Jyu/fbbr/j5o48+6h6DBg3SmDFj3DFbf8uC1Pr161W/fn0dfPDB+v33390+Kk5iXLSGdmmkz6et0id/rFTP5rUUUhp1k4Y/LH18iTT++e3HrTer70VSPENDAQAAsHciCvdhTJfNl7LeISts8corr+jpp5/WH3/8oY8++sgFJeuFCmVWit2qBlpxC+Zf7dqPc1J14YiJqlszVr//c7BiovapI9Q/9lv/jeOlxT9JjXtKq6d5x094QepFTycAAAC0V9lgn34azsrKUlJSktv/9ttvXS+W9TAdcMABboggqoeD29dzwWr9lhz9Oj9NIcdKHp41UrrkB+lvY6RDi4agzvzI75YBAAAgBO1TuGrXrp0+/fRTV7L8m2++0dChQ93x1NRUenqqEeupOrant+bVx6G05tWOFQSb9bEVrKWuJ3nHFv4gzf/O75YBAACgOoQrG/pnhSNatWrlSq8HqvtZL1bJOVEIfycWVQ38dtYabd6Wq5BWv4PU7RRvoeH3zvbmXwEAAACVGa5OOeUUV2xi0qRJrucqYPDgwXr88cf35ZYIUT2apahNvZrKzivQN7NKr1kWkk54Xmo7WMrP9ioIAgAAAHtonysQ2EK/1ktlFfysQp+xXqxOnTrt6y0RgiIiIop7rz75w/t9ENKiY6Vh93v7c0ZJa//0u0UAAAAI53Bli/3ec889rmpGy5Yt3aNWrVq699573TlUL8f38sLVbwvXa016GCzI26CT1Pk4b/+tk6UJL0sbFvvdKgAAAIRjuPrXv/6lZ555Rg8++KArwW6P+++/35Vkv/322yu+lQhqLerWUN+WtV1l8w8nL1dYsN6reh2kzaukL2+Wnhsgpc2X0ldI75whTRvpdwsBAAAQDutcNWnSRC+88IKOO67oX/eLfPbZZ7ryyiu1cmWIVo4rwjpXe+/jKSt04/vTVC8xTr/+4zDFx0Qp5G3LkMY+6Q0PXDdHara/lLlG2rTMO3/HRq/KIAAAAMJWpa9ztWHDhjLnVtkxO4fqx0qyN0mJV1pmtj6aEgZzr0x8sjT4dunsD6TYRGnFhO3Byiwf72frAAAAEGT2KVz17NnTDQvckR3r0aNHRbQLIbjm1SUD27j9139bon3oEA1etVpIZ74ntR8qNe4pJXlre2n2//xuGQAAAEJ9WOBPP/2ko48+Wi1atChe42rcuHFuUeEvv/xSAwcOVChjWOC+Sd+aq/3/PdqVZf/sqoPUs3kthaXZo6SRZ0vJzaTrpklR0X63CAAAAKE6LHDQoEGaN2+eTjzxRG3atMk9TjrpJM2aNUtvvvnmvrYbIS4lIUbDuzVy++9PCpPCFmVpN1hKqCNlrJAmvyZlMRQWAAAA+9hztSvTpk1T7969lZ+fr1BGz9W++21Bms56ZbwS46I17rbDlRQfo7A0/iXpq79vf37kQ9IBl/vZIgAAAIRizxWwKwe0qau29WsqMztP708Kk8IWZel7odSg6/bn3/xTWvyLny0CAACAzwhXqFCRkRG6+GCvsMVrYxcrLz9MF5WOipEuGCVd8oPU4wypMF/68u/SrE+lH/5tK2373UIAAABUMcIVKtxJvZuqdo0Yrdi4Vd/+uVZhq0YdqVkfafhDUkxNad1s6YPzpZ8fluZ/63frAAAAUMX2qsyZFa3YHStsAdgCwuce0FJP/bBAr/yySEd1b6ywllBL6nWmNPGV7ccsXHU80s9WAQAAIJh7rmwi1+4eLVu21HnnnVd5rUXIOGdAS8VGRWrKsk2avHSjwt7+l0kRUaXDVTit9QUAAICqrRYYLqgWWDFu+XCaK2oxsH09vXlxf4W95RNt0pn02lFS3jbpinFSwy5+twoAAADlQLVABIVrDm+vmKgI/TI/Tb8vWq+w17yf1LSP1KpoEe1Zn2w/l53pVRPk3zIAAADCFuEKlaZ5nRo6o18Lt//k6PmqNnqd5W0nvSrlbpPSV0ovHy69fow07T2/WwcAAIBKQrhCpbri0LaKjozQuEXrNXNluqqFzsdJKc2lrDTph3ulV4dJaXO9czM/8rt1AAAAqCSEK1SqJrUSdEwPr1rgy78sUrUQFS0dcKW3P+4ZKX25lNzUe774Jyl7s6/NAwAAQOUgXKHSXTLQW1T4f9NWadG6TFUL/S+TDr9dqlFPara/dNnPUp22Un6OtGC0360DAABAJSBcodJ1a5qiIZ0bqKBQevL7ajL3KjJKOuRm6e8LpIu/lWrWkzod5Z2b8aG3Xfun9PMjUs4WX5sKAACAikG4QpW44YgObvv5tFWau6YaDYuLiPAeptc53nbOKGnFZOmd06Uf7pO++aevTQQAAEDFIFyhSnRtkqKjujdylcifGD1P1VKDTl6xC/PG8VL6Mm9/8ghp8c++Ng0AAADlR7hClblhSAfXifPVzDXVp3LgjgbdIkVESTlFvXc2H8t8fZtUUOBr0wAAAFA+hCtUmfYNk3R8zyZu//HvqmnvVaPu0uW/SMc8IZ3xrnTWSCkuRVo7U5rxgd+tAwAAQDkQrlClrhvSQVGREfp+TqqmLNuoaqlhV6nvhV6Bixp1pIE3eMfHPEDvFQAAQAgjXKFKta5XUyf39tZ8euirOSq0SVjV3f6XSvEp0sbFlGkHAAAIYYQr+NJ7FRcdqfGLN+jbP9f63Rz/xdaU9jvX25/wkrfNribrgQEAAIQRwhWqXNNaCfpb0cLCD3w5Wzl5DIVT34usbru04Dtp1A3SA02lia/43SoAAADsBcIVfHHFoW1VPylOS9Zn6Y1xS/xujv/qtpV6nObtT3rV246+R8rdKuVl+9o0AAAA7BnCFXxRMy5aNw/1FhZ+6vv52rglx+8m+e+Ie6TYxO3Ps9OlfzeSnukr5W7zs2UAAADYA4Qr+OaUPs3VuXGyMrbl6cnv5/vdHP8lNZJOfFHqcbrUsPv245uWSYt/8rNlAAAA2AOEK/jGSrLffnRnt//m70u1IJUiDup8jHTSS1Kf80sfn/ulXy0CAADAHiJcwVcHtqunIZ0bKr+g0BW3QJGeZ0pdT5Q6H+c9nzxCeusUadVUv1sGAACAXSBcwXe3HdVJ0UULC/86P83v5gSHuETp1BHSya9I0QneMask+PGlUn6u360DAABAGQhX8F3b+ok654CWbv++L/50vVgoEh0nDfq7VLO+9zxtrvTdHayDBQAAEIQIVwgK1w9pr5SEGM1Zs1kfTFrud3OCy8CbpL8vkI572nv++3PScwOkDYv8bhkAAABKIFwhKNSqEatrB7d3+w99PUepmyk9vpP9zpWOf05KaS6lL5NGHONVElz4g7R1k9+tAwAAqPYIVwga5w1oqa5NkrUxK1f//HiGCgsZHlhKRIS039nSJd9L9TpKGSulZ/tLb54ovX2KxK8XAACArwhXCBoxUZH6z2k9FRsVqdGzU/XDnFS/mxSckhpKZ38gJdSRcrO8YysmSvO+8btlAAAA1RrhCkGlU6NkXXRwa7d//5ezlZtf4HeTglPtltI5H0o9zpBaDfSOfX+39POj0mNdpWXj/W4hAABAtUO4QtC58rC2qlszVgvXbdGrvy72uznBq2kf6aQXpVNflxJqS6l/Sj/cK2WskMY84HfrAAAAqh3CFYJOcnyM/jG8k9t/7Lt5WpK2xe8mBbeadaVzPpbiUrYfWzRGWjBa2rLez5YBAABUK4QrBKVT+zTTQe3qKjuvQP/8hOIWf6lpb+mKX6XzR0mtD5FUKL11svTSoVIO4RQAAKAqEK4QlCIiIvTAiT0UHxOp3xau1/usffXXarWQWg+UBlyz/ZiVbP+taH0sAAAAVCrCFYJWi7o1dNMRHd3+v7+YrdQM1r7aIx2GStfP8NbEMmOflDYRTgEAACob4QpB7cKDWqlHsxRlbMvTHZ/NYnjg3vRi9TpLajHAK9c+6gYpO1N69yzpR4pdAAAAVAbCFYJadFSkHjq5h6IjI/T1rDV6Y9xSv5sUWosOH/uUFBUrLfhOeuskae4X0s8PS5msIQYAAFDRCFcIep0bJ+vWouqB9476UzNXpvvdpNBRv4N06G3e/vKita8KC6Q/P5M2LpE+vEhaMdnXJgIAAIQLwhVCwsUHt9bwbo2UV1Co2z+bqYIChgfusYOul9oc5u1HFP2Rn/mx9MVN0syPpO9u97V5AAAA4YJwhZCpHnjXcV1VMzZKfyzbpHcnLvO7SaEjMlI65VVp4E3SWR94x5b95q2DZZb+JmWslmw+W9YGX5sKAAAQyghXCBkNk+N101CveuB9o2Zr0bpMv5sUOmrUkQbfIbUfIvW/YoeThdLUt6U3jpMebiOtZJggAADAviBcIaRccGArHdi2rrbm5uv6kVOVm1/gd5NCz/AHpbPelw6+UTr8/7xjP9wrLf7ZC1qzR/ndQgAAgJBEuEJIiYyM0H9O66mUhBhNX5GuJ0bP87tJoanDMGnInVKfi6TarUufCxS+AAAAwF4hXCHkNE5J0AMndXf7z41ZqPGL1vvdpNBVs6507R/STXOli7/zjtmwwLyc7dfYnKznDvS2AAAA2CXCFULSUd0b69Q+zVwNhhvfn6b0rFy/mxTa62ElNZKa9ZMS6kh526Tlv3sFLszou6XUWdJvz/jdUgAAgKBGuELIuvO4rmpZt4ZWbtqqmz6YSnn2ighZLQZ4+68fK713tpQ62wtaZvFPpXu0AAAAUArhCiErMS5az57VW7HRkRo9O1Uv/LzQ7yaFvk5Hb9+f+4X03AHbn+dkMh8LAABgNwhXCGndmqbonuO6uv1Hv5mr3xam+d2k0NbrLOmaKdIBV5U+Xq+Dt33/PGn5RF+aBgAAEOwIVwh5p/drrlP6NJONCrz23T+0NmOb300K7aGBddt6lQRbHizV7yyd+Z406B/e+a0bpNePkbZu9LulAAAAQYdwhZAXERGhe4/vpk6NkpSWmaOr35nC+lflFR0nXfiFdNXvUsfhUpcTpENv885ZwYv5o/1uIQAAQNAhXCEsJMRG6YVz+igpLloTl2zUQ1/N8btJ4SUqWjr0Vm/h4cB8LAAAAJRCuELYaFWvph45tafbf+XXxfpqxmq/mxS+BS9mfSL9p7M05Q2/WwQAABA0CFcIK0d2a6RLD2nj9v/+4XQtWpfpd5PCS5PeUmIjb3/zKunza6QvbpbyWWcMAACAcIWwc8uwjtq/dR1lZufpyrenaGtOvt9NCh+RkdKJz0sHXCkddJ13bOLL0shzti86nDpHyt3qazMBAAD8QLhC2ImOitQzZ+6neolxmrNms/716QwVBn7wR/m1PVw68gHpiHukM96VouKkeV9LKyZKPz8iPddf+uxqv1sJAABQ5QhXCEsNkuP1zFn7KSoyQh9PWal3Jyz3u0nhqdNRUvdTvP3/HiH9cJ+3P/NDKX2lr00DAACoaoQrhK0D2tTV34d1dPt3fT5LM1ak+92k8NT34rKPTx5R1S0BAADwFeEKYe2yQ9roiC4NlZNfoCvenqwNW3L8blL4adpbaj1IioiUjrhXOuU17/ikV73FhrcRagEAQPUQUchklJ1kZGQoJSVF6enpSk5O9rs5KKf0rbk67plftXR9lvq1qq23LumvuOgov5sVXqyAhS0unFDbqxz43ABp/XypRl0pa73XuzX0Pim2ht8tBQAAqLRsQM8Vwl5KQoz+e34/JcV7Cwzf+hEFLipcTIIXrExUjHTcU96+BSsz6b/SJ5f61z4AAIAqQLhCtdCuQaKeP7uPK3DxyR8r9cwPC/xuUnhreaA0/GGp55nSiS9KkdHS7P9Jc7/2u2UAAACVhnCFauPg9vV07/Hd3P5/vpun/01b5XeTwlv/y6QTX5B6niENuMo79tHF0o/3S2tn+d06AACACke4QrVyVv8WuuTg1m7/pg+macqyjX43qXoY9A+pxYFSTqb000PS8wdKc7/yu1UAAAAVinCFaue2ozprSOeGyskr0KVvTNLyDVl+Nyn8xdaULvhCOukVqWkf79i4Z6VPr5QmvOx36wAAACoE4QrVjs27evKMXurSOFlpmTm65PVJ2rwt1+9mhb/ISKnHqd4cLLPkF2nq29JX/5DWL/S7dQAAAOVGuEK1VDMuWv+9oK8aJMVp7trNuvqdP5SXX+B3s6qHeu2luu22Py/Ml378t0QFRwAAEOIIV6i2GqckuBLt8TGR+mneOl31zhRlZuf53azq4ZC/e9tWA73tzI+k14+lBwsAAIQ0whWqte7NUvTkGfspJipC38xaq/NfnUAPVlXocbp01QTpvM+kI+6VouK8YYIvDJRmfOh36wAAAEIzXD377LNq1aqV4uPj1b9/f02YMGGX186aNUsnn3yyuz4iIkJPPPFEue8JDOvaSCMvG6CkuGhNXrpRr/y62O8mhb+ICKl+RykySjroWumaSV4vVu4Wr1z7hxdLo26UPr5Mytrgd2sBAACCP1yNHDlSN954o+68805NmTJFPXv21LBhw5Samlrm9VlZWWrTpo0efPBBNWrUqELuCZjeLWrr9mO6uP3Hvp2nCYv5gb5K1Wrh9WIdeK33fOaH0qT/StPfk/53HfOxAABASIgoLPTvpxbrVerXr5+eeeYZ97ygoEDNmzfXNddco1tvvXW3r7Weqeuvv949KuqeARkZGUpJSVF6erqSk5P3+fMhtNgfhSvfnqKvZq5RSkKM3rq4vxs2iCq2dJw0/1tvTaxJr0oFeVLXE6WBN0uNvEWgAQAAqsreZAPfeq5ycnI0efJkDRkyZHtjIiPd83HjxlXpPbOzs90vWskHqh8bavr46b3Uu0UtpW/N1ekvjdO4hev9blb103KANORO6ahHpMNv947N+kT671Bp0U9+tw4AACD4wlVaWpry8/PVsGHDUsft+Zo1a6r0ng888IBLo4GH9XSheoqPidLrF+2vge3rKSsnX5e/NVlL12/xu1nV18HXS5f8sH0+1junS7P/J/38qNertSXN7xYCAAAET0GLYHDbbbe5br7AY/ny5X43CT5Kio/Ry+f1Va/mXg8Wiwz7rFkf6ewPpTaHSXlbpZHnSD/cK426QfrkMr9bBwAA4H+4qlevnqKiorR27dpSx+35ropVVNY94+Li3PjJkg9Ub9aD9eK5fdwiw/NTM3X9e1Mp0e6nmHjplFel2q2850lNvO2SX6W8bF+bBgAA4Hu4io2NVZ8+ffT9998XH7PiE/Z8wIABQXNPVF8Nk+P10nl9FRsdqe/npOqWj6aroICqdb6pUUf624/SRd9KN/4p1Wwg5W2TVk7xu2UAAAD+Dwu0kukvv/yyXn/9dc2ePVtXXHGFtmzZogsvvNCdP++889yQvZIFK6ZOneoetr9y5Uq3v2DBgj2+J7A3bGjg02fup6jICH08ZaXu+t8sV1UQPgasFv29dbJaHugdW/qr/SuKlLHa79YBAIBqztdS7MZKpj/yyCOu4ESvXr301FNPuXLq5tBDD3Ul10eMGOGeL1myRK1bt97pHoMGDdKYMWP26J57glLs2NGnf6zUDe9Pdcst9WyWojuO7aI+Lev43azqbcLL0pc3e/vND5CW/y51Pk7asEiKryUNf4jS7QAAoNz2Jhv4Hq6CEeEKZRk5cZlu/3SWcvILVC8xVj/cfKiS42P8blb1tfZP6fndDPeNiJIOvNor5x7F9wQAAMJ4nSsg1Jzer4V+u+1wtalfU2mZOXr8u3l+N6l6a9BZOuAqqeVBUq9zpJP/KzXuJfW/wuvBKsyXxj4p/fa03y0FAADVBD1XZaDnCrvz87x1Ou/VCW7aj83HOqZHUeU6BJffX5C+/odXWfD66VJhgZSbJSXU9rtlAAAghNBzBVSiQzrU1/kDWrr5VzeOnKb/TVvld5NQlr4XehUFN6+Svvmn9GRP6dGOXm+WzdfKTPW7hQAAIMwQroB9cMexXXVMj8Zu/tU17/6hEWMX+90k7Cg6Tup7kbc/4SVp82opP1v67g6vEMbn1/rdQgAAEGai/W4AEIqsNPuTZ+yn+klxem3sEt31vz8VHRWpcw5o6XfTUNKB10gFudKKSVKj7lKNutK4Z6WsNGne19LGpVKtFl5pdwAAgHJizlUZmHOFPWV/fB7+Zq6eH7PQBa43LtpfB7Wr53ez8FfeOF5aVLR8Q3S81KS3dPIrUkpTv1sGAACCDHOugCoSERGhW4Z11En7NVV+QaErdPHPT2YoJ6/A76Zhd/pdsn0/b5u07Dfpt6f8bBEAAAgDhCugAgLW/Sd119AuDV3Aemf8Mj3z4wK/m4Xd6Xi0NOwB6ejHvBLu5o+3pezNfrcMAACEMMIVUAHiY6L00nl99fjpPd3zZ39coPcnLVdBAaNug1JkpDTgSqnfxVLXk6S67aWczayJBQAAyoVwBVSgE/drpuN6NnE9WLd8ON1VErR9BHnQOqiocuBPD0kvHiKNuoFeLAAAsNcIV0AFe/TUnrp1eCfFRkXqixmrdc//ZrnCFwhi+50rHf5/3v7qadKkV6X/DpVmfSo90UP65l9+txAAAIQAqgWWgWqBqAi2uLD1XJlrD2+nvx3SRknxMX43C7uz9k9p7Szp239JmWtLn7t2qlSntZS7VYpJ8KuFAACgilEtEAgCx/ZsojuO6eL2n/phgfrf/71+W5jmd7OwOw27SD1Olf72o9S4V+lz456RJv5X+ndj6dcn/GohAAAIYvRclYGeK1Sk18Yu1qtjF2v5hq1qUaeGvr3hEFcAA0HOeqiWjpUKCqR3TpWi4rweq22bvPOnvi51PcHvVgIAgEpGzxUQRC48qLW+uu4QNUqO17INWfrXJzOVm886WEHPglS7IVL7I6R2R0j52V6wiiwa2vnVLV4AM+sXSs8NkCa87GuTAQCAvwhXQBVIjIvWfSd0U0SE9NGUFbpoxERty833u1nYE/alnfiClNTEe37cU1JKc29O1uQR3rGPL5VS/5S+vFliMAAAANUWwwLLwLBAVJbvZ691RS6ycvJ1QJs6OqNfCzc3Kyoywu+m4a9sWu4Vu+gwTJr8mleuPSJSSm4qpS/fft3Vk6V67fxsKQAAqEAMCwSC1ODODfXK+X0VGx2p3xdt0PUjp+r/Pp1BqfZQUKu51PFIryer1zlS28FSYUHpYGUW/ehXCwEAgM8IV0AVO7BtPX165UG65ODW7uf0dycs17M/LvC7Wdgb0bHSuR9LN86WDrxGathd6nSMd27hD9Ls/3mLEa+Z6XdLAQBAFWJYYBkYFoiq8sa4Jbrjs1lu/+FTeui0vs39bhL21ao/pJcOlaITpOg4r/hF52Ol09/yzi/4Xhr/otRkP2nAlVJ8it8tBgAAe4BhgUCIOG9AK102qI3bv+XD6XpuzAIVFPDvHSHJ1sVqNVDK27q9XLv1YP3ymDR7lPTJ5dL8b6SfHpQ+vMjv1gIAgEpAz1UZ6LlCVbIwde8Xf+q1sUvc894taumFc/qoQXK8303D3tq8RnphoLQlVUpqLG1evetrL/leata3KlsHAAD2AT1XQAiJjIzQncd21b9P7OZKtk9ZtklnvTJeaZnZfjcNeyupkXTpj9LFo6XhD3vH4mttP3/GO14xjED59hkf+tNOAABQKei5KgM9V/DLsvVZOv2lcVqdvk1NayXoxXP7qFtT5uaErLQFUnJjafV0KSvNm4O1YZE3N2tbunfNpWO8eVgAACAo0XMFhKgWdWvo7Uv6q1XdGlq5aatOeeE3fTZ1pd/Nwr6y9a5ia0otB3jBytRpI101wSvlbv54S1o2Xsrd5j1PX+n1aPHvXgAAhBzCFRBk2tRP1GdXHaxDO9bXttwCXffeVN036k/lU+givIYPWsVAM/EV6dWh0te32gQ86Z3TpI8ulmZ+5HcrAQDAXiJcAUEopUaM/nt+P111WFv3/JVfF7PYcLhpc1jp+ViTX5Mm/VdaW7Q21rT3pO/ukOZ941sTAQDA3mHOVRmYc4VgYsMCbxg5VdZxdVzPJrr/pO6u8AXCwNR3vUC1YuKur4mpKV0zSUpuUpUtAwAARZhzBYSR43s11UMn91BUZIQ+n7ZKJz03Vss3ZPndLFSEXmdKl4yWTn97+7HkZqWvyd0ijb7L289YzVwsAACCGOEKCAGn9m2u9y87QA2S4jRvbaZOfG6spizb6HezUFE6HS2d9LJ08n+lK3+TBlztHU9saAMMpOkjvUWIH+skTXh5z++bl0MYAwCgCjEssAwMC0SwWp2+VReNmKTZqzMUGx2p/zu6s849oKUiIiL8bhoqkpVpn/KG1OMMr9DFzBLrYdVqKV3+qxQZLeXnSAt/kDoM86oSlpSZKr04yKtOeOEXVf4RAACojtmAcFUGwhWC2ZbsPFdBcPTste754E4N9PApPVQ3Mc7vpqEyrJsrPdtf0g5/VSfUkWrWl9LmSi0GSOd8LMXW2H5+1I3efC5z0zwpyXrBAADA3mLOFRDGasZF66Vz++iOY7ooNipS389J1ZFP/qKf563zu2moDPU7SgdcISU13r42ltm6wQtWZtk46fVjpY1LvecZq6SpJeZxrZxcxY0GAKB6IlwBISgyMkIXHdxan151kNo3SNS6zdk679UJbj2s7Lx8v5uHinbkA9JNc6ThD0kJtaUux0u9zpEadpOOe0aKS5FWTpJeOlSa/5307plSXtGixMbO7agg3xt+CAAAKgzDAsvAsECEkm25+fr3F7P15u9er0WXxsl68dw+al6nxBAxhA9baDhyh38Xsx6r98+TVk8tPWyw11nSuGek6ASp5+nS4bdLNet5520Nrd+eli74Umo5oGo/AwAAIYRhgUA1Eh8TpXtP6KaXz+urOjVj9efqDJ3ywm+au2az301DZdgxWJnaLaULRkmdj/XWxWraVzrnI6nH6d75vK3S5BHSL495z/NzpcmvS4UFXiVCAABQIViJFAgTR3RpqO5NB+rc/47X/NRMHfvMr7plWEddfHBrqglWB3FJ0ulveaXXA993fl7paya9KtWoLWVtkLZt8o4t+rHq2woAQJii5woII41S4jXysgE6tGN95eQV6L4vZuvWj2YoK2eHH7IRvkoG6aho6ZgnpP3O8eZlWQ/WD/dJvz+3/ZqNS6R187w5WAAAoFyYc1UG5lwh1Nkf6xG/LdG9o/5UQaHUtFaCzh3QUmf1b6Hk+Bi/mwc//PBv6eeHSx+zIYS5W7z9dkOks973Kgumr5DaD5XiEn1pKgAAwYR1rsqJcIVwYeXZb/t4hlZu2uqed2iYqLcu6a8GSfF+Nw1VLTtT+v15qeNwafJr0uY1UqMe0pj7t19Tt520foG33/ci6ZjHfWsuAADBgnBVToQrhBMbEvjpH6v05PfztDYjW41T4nX/id11WKcGfjcNfsvL9tbDmj1KWvh96XOxidLN86TYmju/7tvbpXlfe0UzarWosuYCAOAHqgUCKFYjNtoNB3z/sgFqWbeGVqdv04UjJurhr+e4eVmoxqLjvB6qU0dIiY2kqFjpvM+k2q2lnEzpx/u94DXrU6/CoEmd45VwT5vnzd8CAADF6LkqAz1XCOderIe/nuvmY5kmKfF68OQeOqRDfb+bBr9lpnoLD1tP1E+PSD/uEJwa95JOfU36/l5p1sdFByOky3+VGnXzo8UAAFQJhgWWE+EK4e7zaat036g/lbo5W7HRkXr2rN6ulDtQHLTeOF7ask5KaiRtWu6Vbo9NknKK1k9r2scrfmGFL3qeIcUlS+2P8M5lrJJq1peiKJ4CAAh9hKtyIlyhOtiWm6/r3vtD38xa654f3aOxzh/QSvu3ruN30xBsLCy9dYqUOst7PvgOqfPx0rP7S4UlSrgPe0CKiJS+/ofU62zphBIl3wEACFGEq3IiXKG6sDlX9385W6+PW+LWnjW26PA/j+qsqEgWHkYJtvDwz49KLQ6QuhznHfvf9V7lwV25dqpUp3WVNREAgMpAuConwhWqm5kr0/X6b0v0weQV7rkNEXzyjF6uGAawS1vWS9/dIbU5VNq0VBr7pJSdsf38/pdKQ++TFv/ilXi3a1oNlDod5WerAQDYK4SrciJcoTrPxbr5g2muR6t1vZq645gulGzHnsvdKq1f6M3VevMEb4hg7VbShkXbr4mKk26YKSXy+woAEBooxQ5gnxzXs4neuaS/6iXGaXHaFley/ZxXxuugB3/Qt7PW+N08BLuYBK9yoPVk9blAKizwglVCHanzsVKdNlJ+tjThJa/Xa+xT0rq5frcaAIAKQ89VGei5QnW3eVuuHvtunl4b65VsN3VrxuqHmw9VSgIV4LAH7H8tvz4urZsjDblbSm4s/fm59P653gLF8SlSxkqvJ2vovd4Qwgjm+QEAgg/DAsuJcAV4vp65WmPmrtN7E5e754M61Ne9x3dTi7o1/G4aQlFBvvTfodLKSd5zC1m2WLHpcrzU7WQppZnUeD+vMmFOltSiv69NBgAgg3BVPoQroLRxC9frnP+OV35BoWKiInR2/5a6dXgnxcdE+d00hOK8rPnfesMFbejg1Helb//lDSEMaNxTWjtLKsiT2g6WmvaWFnwvbUnzwtZJL9PLBQCoMoSrciJcAWVXFHzo6zn6ZX6ae967RS29cG4fNUiK97tpCHXLJ0rT3pVWTZFS50h5W3d//XmfS20GSaumSrVaSDVYmw0AUHkIV+VEuAJ2bczcVF377h/K2Jan5Pho3Tq8s87o11yRrIuFirD2T+nLv0vN+ngLEc/8WNqwUGp5kDT1HWnFBKnLCVLDrtKP//aGFvY4TWraR8rPlf54S+p3sdTrLL8/CQAgTBCuyolwBezegtTNumHkNM1Yme6e16oRo06NkvT46b3UOCXB7+YhXK2ZIb1w8F9fF5ciXTtFWjpWSp0t9btEqlmvKloIAAhDhKtyIlwBf83mX9nCw//5dq625OS7Yx0bJuk/p/VU1ybJimBODCrD68dKi3/21tA67F9So+7e89XTpM1rpPXzd35NnbZeT5YtYEyBDADAXiJclRPhCti7su3z1mbq8rcma93mbHdsaJeGeuKMXqoRG+138xBusjd7a2PVaikl1t/5/JQ3pM+v8fYTG0oRUdLmVd5zG0J43XQpO0NaNs5bd6tpXymK36cAgF0jXJUT4QrYt6GCD38915Vuz8kvULsGifr7sI4uaNGLhSqTly19cpkUHS8Nu1/K2+YtVjz++bKvb9hdOvNdqVbzqm4pACBEEK7KiXAF7LvJSzfob29M1oYtOe55j2YpumloRx3Svh4hC/6Z+7X07ulFTyK8Ahhp87xeLOvR6nCkdNC1Xhl4AABKIFyVE+EKKJ/0rFy9/MsivTp2sbKK5mMN69pQD5/cUyk1YvxuHqoj+1/dyHOktTOl456RWg+U0ldI750trZ66/bq+F0vD/i3FUJgFAOAhXJUT4QqoGGmZ2Xp+zEK9MW6JcvMLVT8pTtcNbq/T+zVXTFSk381DdWP/u9ux97SgwFtfa/wL0owPvGP1O0lHPSo16CLVrOtLUwEAwYNwVU6EK6BiTV+xya2NtWR9lnvetFaCm5P1t4FtdHB7SmQjSCz8QfrkcilzbdGBCKnNodLWDVKT3l51wqw06benvQWMe54u9TzLW4er2f5SJP9gAADhiHBVToQroOLl5BXonfFL9dQPC4rnY0VHRuj+E7vr1L7NmI+F4JC5Tvr6H9KSsVLmmtLnImO8nq987/evk1DHC1+1W0unviY12a/KmwwAqFyEq3IiXAGVJzM7TxOXbNBHk1do1PTV7tiQzg116/COatcgye/mAdvZAsTzvpFq1pcmviyt+sM73n6oVwRj1selr6/VQkpp4c3X6nuR97oGnaT1C7z9lGa+fAwAQPkQrsqJcAVUvoKCQj374wI99cN8Nx8rMkI6oVdTXT+kg1rUreF384Cd2fpaOZneEMHcrdJHF0uR0dLQ+6TXj5E2Ldv1a23R43odvdLwfS6Q9r9U+vUxL8C1GyLtdy7rbQFAkCJclRPhCqg689Zu1qPfzNW3f64tHip4at/munZwOzVOoWIbQsTyCdL750nN+kk16njPszZ4Qwtjk6SczaWvt54vC2oB/S6Rhj0gTXtXys2Sep8vxfKPDAAQDAhX5US4AqretOWb9J/v5unneevc89joSJ3dv4WuOqyd6iXG+d08YO/Z/163pHlhy4YGblgsbVknjXlAylgpRURJvc+VJo/wri8ZwmwuV+1WXgDrepJ02G2+fhQAqM4yCFflQ7gC/DNh8QY9+u1ctzU1YqN08cGtdcnANkpJYI0shAEbUjh9pFSvg9TyQOmX/0jf3+OdS2wkRcVI6ctLv8aGEbqesLVSzXpS1xOlzsftXFoeAFDhCFflRLgC/GV/Lf0yP82FrOkr0t2xpLhonTOgpS46qLVbLwsIKzZfKztTqtvWe756mtfLtegnacKLZb+mUXepfmepfkevgIYtirx+vtfj1bRPlTYfAMJZBuGqfAhXQHCwv56+mbVG//l2nuanevNT4qIjdVrf5u6xKC1TDZPjdUAbFnpFmCrIl7682Qtf1suV3ExaO1P6/XmpIHf7dVaN0MJYQOtBUmaqVxo+ubF3bNCtUn629O3/SXnZUsejpC7HVf1nAoAQQ7gqJ8IVEHyVBUfPXqvnxizU1OWbSp2zKoOvXtBPh3Zs4Fv7gCpnvVQrJ0sbFkmTXt1eqdB6rKxkfGHBzq+xMGWBa+Wk7cdOeU3qdlLVtRsAQhDhqpwIV0Bwsr+uxi1arxFjl+jHuamqERut9K25qhkbpZfP76sD29bzu4mAPwsfj31CajVQ6niktHyiNPMjqUkvaeGPUt5Wac6X23u64mtJbQZJf34mxdT05nC1PVzqfqrXs7VlvdR6oJTUyO9PBgBBgXBVToQrIDTk5BXowhETNHbBesVERejs/i11xv7N1bFhkiKY6A9sN3uUN3erbjup/xVSndbSK0Ok1VN3/Zq4ZG8ooj02r/VKxPc53xtqCADVSAbhqnwIV0Do2Jabr5s/mKZR01cXH+vSOFmXDWqjo7s3VnRUpK/tA4KW9VAtHiNFJ0hT3/bmctlixxaqdhW6ImOkDsO8Xq3oeCk/1ysnX6eNNOsTafNqqdUhUtZ6qVlfKZHhugBCH+GqnAhXQGixv8as9+qNcUs0Zu465eR7802a10nQpYe01al9mik+JsrvZgKhY1uGtHGxNOMDKWO1VxQjbYE076udr7X1uqynuCCv9PH4FOnQ26T6naQNC6W67aUWA6To2Cr7GABQEQhX5US4AkLXpqwcvTFuqV4bu1gbs7w5Jk1rJejyQ9tqeLdGLEgM7Cv7cWHRGCltvrfeVt42aeMSac4o73zt1t6wwxUTpdia3kLJO6rVQmrYzSusEZcoNeji3cOub9JbatrbKzFvVRLjkljHC0BQIFyVE+EKCH1ZOXl6f+JyvfDTIq3J2OaOxUZF6uQ+zXRAmzoa3LmhEuOi/W4mEPrWL/SGCCY32R6G8vOkya9JU9/xSsTbWlyrpkpZaXt+Xxtq2LCrlLVR6nailNjQKyG/Ld2rhmghzYYe2qLLAFCJCFflRLgCwsfWnHy9PX6pPp26UjNXZhQfb1Gnhh46uYcLWhS/AKpAzhZp+kgvIFkPloWutbO8Hq+cTGnlFGnVFK9XbE8l1JZ6nOFVRsza4N233RApJr4yPwmAaiaDcFU+hCsgPI1buF7/m75KP85J1ep0rzerfYNEnd6vuU7q3Ux1ajIXBPCV/UiydaO3/+en3r7N6Zr7pXcuOs4bLmjDBm344dYNO98jsZFXXj6psbdIcq2W0ro50poZ3rDF/S/17mPPLYzVqFPlHxNAaCFclRPhCghv6Vm5evDr2frkj5XalusVv7BS7kO7NtJFB7VSn5b8sAUEPRt6aHPAJr7iDRW0yoTLJ0ibV/11b5cFNFt42YJb7Zb245AUGSV1OUFKX+4V9LCCHLVbSXXbehUS7TVWHdHey8IbgGojg3BVPoQroHrI2Jarz6eu0siJyzVjZXrx8UEd6uucA1rqsI71KeUOhBIbcrhgtBQZLa36Q1r8i5SxwqtYaI9ZH3uhytg8MSvKsSdq1POGGy7+ySs3X6+D1H6ot1WhFFPDC2AWyKy3LS9HqlnfWxvMCncACGmEq3IiXAHVz8yV6Xpz3FJ9OGWF8gu8vxbrJ8VpSOeGOqZHYx3Yti5zs4BQZz1PFrqskqGFJZunZftWIGPDYq/yoRXfsB4rO2eVDJeOlTYs+ut7W1hreZC0+GepINdbE8y2rQdJzfp5ww9tOKIFwKhY7/5WBKTt4VJK06r49AD2EeGqnAhXQPW1OG2L3p2wTB9NXqH1W3KKj9euEaN2DRJ189CO6t+mrq9tBFDFww/nfuFVRUxu6oWhpb9KC3/wSsrbkEILX2lzS7zI/iFmL368qtdRatDZm0NmvV8WvrakeSHPhjFawY7GvbweuYRa0tLfvGGKVjExnp9TgMpGuConwhWAnLwCjV2QptGz1+rTP1ZqS05+8blDO9bXifs11SHt66s2RTAA2I9SVunQApet12U9VTY8sDBfmvWpN5TQeqpsGKL1cOVneyFq3VxpxaS9C2I7suGOXY6XWh/ihS/3voXeXDELZ24+WbQUyRBnoNqEq2effVaPPPKI1qxZo549e+rpp5/W/vvvv8vrP/jgA91+++1asmSJ2rdvr4ceekhHHXVU8fkLLrhAr7/+eqnXDBs2TF9//fUetYdwBWDHNbOWpGXprfFLXa9W4G/NhJgonTegpY7t2cQt79OhYZJimKMFYG9Y6Fo+3usZq1FXylovFeR5c7ZsfpitDWahLWOVVyXRhhpalUPbL2uh5rJYuLIqismNvSqKgaIeFvCMvbeFspYDpAOukqL5RyMgZMPVyJEjdd555+mFF15Q//799cQTT7jwNHfuXDVo0GCn63/77TcdcsgheuCBB3TMMcfonXfeceFqypQp6tatW3G4Wrt2rV577bXi18XFxal27dp71CbCFYBdWWLDBicuc+Xc563NLHXOyrqfO6CloiMjXXn3qEjmaAGo4CGKFrZsQWX7F50tRcHL5orZOmFW8TChjtdjtn6RlLN579/DAl5KMyki0gtc9h5xyV5P2KalUvP+Ut323vu7R5QX0prv7xX2sCIeNlyROaoIIyEVrixQ9evXT88884x7XlBQoObNm+uaa67RrbfeutP1p59+urZs2aJRo0YVHzvggAPUq1cvF9AC4WrTpk369NNP96gN2dnZ7lHyF9DaQLgCsCv2V+f3s1PdAsUTFm9QfmFhcVl3M6xrQx3VvbEGtq/P+lkAqp79eGe9YBaSLPBkrPbK1G9e45WTt0f2Zq8HrG4bb57X2Ce9xZ3LK6WFF+ysdyw6Qcrd6g19tGIhdWy4Ykuv3L3NU7N5bDZ8MT/HC2qNe3jVGQOVF42dZ2FohEi4ipaPcnJyNHnyZN12223FxyIjIzVkyBCNGzeuzNfY8RtvvHGnIX87BqkxY8a4ni/rrTr88MN13333qW7dsiehWy/Y3XffXSGfCUD1YJUDh3Rp6B4mNWOb7v9yttZkbNPkpRv1zay17lG3Zqz+MbyT+rXy1s5qWaeGIunRAlDZrOeoeD2uOl5v1F/pe7G0bra0Zb1XQdGGD1pPVvoyKX2FlNREWvKLtG2TF97sGgtnW1K9svcWoCzM2fUmEI4CbA0xjd6XDyOlNPdCoAW2wDGrvti0rxRb0wtj9nltiGNMgtfbZj151ptmxUKsEAhQBXwNV2lpacrPz1fDht4PJwH2fM6cOWW+xuZllXW9HQ848sgjddJJJ6l169ZauHCh/vnPf2r48OEumEVFRe10Twt3JQNboOcKAPZUg+R4PXHGfm7/90XrNWLsEs1du9lVH7zlw+nF19VLjNPlg9roooNaE7IABJfYGlLTPjsfr9du+377IWW/NrdozbC8rdKKyVJSIykn0yt/bwHH5oql/iltXOoNLwz0UllPmpW8t0If1sOWOlvauknKTvcCksnOKAp4RaGtpBkf7Nlnsx4yu7+1xdpm4cwNXSzaWiCz+Wi2GLX14rmKjyo99NHK6duwS7umbjvv1yvAwqYFzShff7RGEAjL3wFnnHFG8X737t3Vo0cPtW3b1vVmDR48eKfrbT6WPQCgIhzQpq57bMvN18s/L9Ln01ZpxcatKigsVFpmtu77YrZbuLhHs1pKio/WpYe0UZNaCX43GwD2XWDYnm13FcBaHLDn9yso8CocWmixeV8bFnol7y0gGTtuPWjLxnvP3XDHTVK99l6gs0Bm97Bj1mMWKP5hvWl7WgjkL0VIUTFeMLRgZcGy1UDvPe1ck/2kBl286pA2DDMQ0qx8vgVH26/Txgt+9hprmwU718Nnr+/lfZacLV7PmwVABD1fw1W9evVcT5IVnyjJnjdqZP+qsDM7vjfXmzZt2rj3WrBgQZnhCgAqQ3xMlK4Z3N49AuXdP5i8XPeNmq35qZnuYT6askIt6tRQ7xa1dcnA1q68e3J8jM+tBwAfBUrHW89RYn3vsTfhbMeKjGnzpbhEKSfLKwriSg7Y0Mai0gM2/yxzjbd2mVVrNIFr7LmFI5vDlmXhbEXRkMdCb66YPQIW/7R9f/XUPWzgnqyLFuGFsDqtvYImFuoCvWv2uezcxsXeUE3rnbOHBTXrPbThkTas0oKeBTq73ubC2dIAVknSFrPeVQESC4021JMCJaERrmJjY9WnTx99//33OuGEE4oLWtjzq6++uszXDBgwwJ2//vrri49999137viurFixQuvXr1fjxo0r4VMAwJ6JjY7U2f1b6ujujfXTvHVauWmrvpm5RtNWpGvWqgz3ePP3pe7/Yf1a1tGZ/Zvr6O5N3OsAAPvIhvO16F9x93O9ZulSXnbpcGUhbP633vBCCzRWwdF62yzoWG+VBR8r5OGKiWR4Ac3miFl1RxNT07uXhSfr7SouLlIUvizU2aOiufaleOExMH/NDedM8IZxWrhKaujNubNlACy02eexgGfFSuxat6aazbmL8s5bgHXz4Op7BUpsPpz18NkQUPee8d5zmzdn72/X2a+f3c8dj/W2UXEht0ZbUJRiP//88/Xiiy+6ta2sFPv777/v5lzZXCor0960aVNXdCJQin3QoEF68MEHdfTRR+u9997T/fffX1yKPTMz0xWnOPnkk11vls25uuWWW7R582bNmDFjj4b/UYodQFWx3qxxi9YrY2uuXvl1sWatTFdewfa/lhsmx2m/5rUVFRWhM/u10NTlG3Vgu3qulwsAEOKsJ82Cls3lKrm+mP14buHLgocFkMDQSFv7zEKXBZFAz5r1zNlQQqvEaKHFAord1wKahSXrfbIFq61nKzvTez+3oHWCNxcu0FMXjOq0ka79w+9WhE61wEBp9XXr1umOO+5wRSmspLot9hsoWrFs2TJXQTDgwAMPdGtb/d///Z8rVGGLCFulwMAaVzbMcPr06W4RYSvH3qRJEw0dOlT33nsv86oABB3rlRrUob7bt8WIzapNW/XxlBV6Y9xSrc3I1tezvII9X0xf7bZxPyzQs2f11mGdGrCWFgCEMiuKUbIwRoANYbAet4DyDo3ckQUze4+8HC+YBQKeDY+0OV5WmMOCmOuRkpS51rvOqkYGhknaumuBtm9Y7PVYFeR6PXrWE2X3tCBo1SetJ8vua2X4I2O8cGch0K7N3eLdL1AB0o4Fhklaz1WI8b3nKhjRcwUgWHq1vpm1Rus2Z2v84vWutHv9pDj33KQkxGhAm7o6uH09DWxfTy3r1iz1+tx8b92tmKjQGlIBAKhm8rK9+V82nNKiiQ01tJ4363ULgjL6IbWIcDAiXAEIRulbcxUXHak7P5ulL2asVmZ26aEctWvEqH2DJB3Ts7GycvL1zA8L1LJuDb136QFKokAGAAD7hHBVToQrAMEuL79A01ema+z8NP2yIE1Tlm4sNVerJBt2eM4BLdW5cZKa1kpwCyDbX/0ZW/OUUoPQBQDA7hCuyolwBSDUZOXkaen6LP08b51+XZCm/IJC7deill76eZFy87f/Nd+pUZKGdW2kMXNTNXNVhu49vpvO6t/C17YDABDMCFflRLgCEC5+W5CmDyev0Ow1m7UgdXOpoGWsHsbw7o3VsWGS2jdIVPuGiW7uFvO0AADwEK7KiXAFIFznbFnFwQmL1ysxPlpbcwrcAsY7sgqENn9r/9Z1dGrf5urTsjaLGgMAqq0MwlX5EK4AVAf21//YBes1a1W65qdmav7azW5rxTBKsmq91qtVu0asujRJVnpWrlanb9OtwzupZ3P/qzgBAFCZCFflRLgCUF3Z/xJsba1V6d5aWz/PS9OyDVllXmtDCi1sdW9aSz2apah70xR1bJSkqIgIRbL+FgAgTBCuyolwBQDbpW7eplmrMrRxS44mLtmg2KhIrd+So1FFixqXpUvjZDes0ErBH9CmrtrUr6m46KgqbTcAABWBcFVOhCsA+GsrN23V9OWbXEn46Ss2afqKdG3eVnrtrZJDC5ukJKhFnRpqVa+GWtWt6Xq9ujVJUe2asVXedgAA9hThqpwIVwCw9+x/J2mZOcrNL9DYBWlalLbF9XjZGlw7LnhcUp2asWqUHK9mtRPUuXGyGiTHqWWdmm5drrqJcVX6GQAA2BHhqpwIVwBQcex/MzaMcOn6LW4triXrs7QwNdMV0rD93amXGKt6iXGqW7RtkBTnFkLu26qOG2pYIza6yj4HAKB6ytiLbMD/lQAAlSoiIsIFI3v0aVmn1LnN23K1YuNWrU7fqsVpWZq7JkMbtuRo4botWpy2xfWE2WN3vV7W42WPxikJsn8u7NQ4Sfs1r+XKzVtZ+fqJca4NAABUNsIVAMA3SfEx6tzYHjv/S+CW7DwXsCxsrd+SrbTNOa64xry1mfpj2UZlbMtz5+xh8712xXq6Wter6Xq/LIB1bJSoxLgYNUz2esEsoBG+AAAVgXAFAAhKNeOi1a1pym4XRV65catWbMzS8o1blZqxTfkFha6ioYUyW6+roLDQFd6wx64kxESpae0ENamVoKT4aNWIiVKN2CglxEa7571b1Fa7BoluPz6GiocAgF0jXAEAQlJKQox7WNXBXbHer6nLN2ltxjatz8zR0g1btGjdFnfcFkJO3Zytrbn5WpCa6R5/JTEuWp0aJbkhjvbe9ZPiXCizuWAW5FrWrenmgsVERVbwpwUAhALCFQAgrHu/DmpXb5fnt+Xmu5BlPWC2cLKFLuvx2pqT77brMrM1ftF6F8KMVT2ctHTjX75vUly0UmrEqHaNWNWq4YUwC2TW82WVEa0XrGZclBueWLtGjGoVXUcoA4DQRrgCAFRbFnZsPpY9dqegoFCZOXlavcnmfG3WpqwcbcrKdaFr1aatLoTZrK35qZkulG3OznMPK9axNyyU2bpfgcC1fRurOjW9fesxSy7qtUt2IS1a2bkFLrBFRjJ3DAD8RLgCAOAvWGhJjo9RcqMYdWyUtMvrbM6XzQWz8LUxa/vWhiXavgUv27fFlm04om03ZuW411ilw0AoW7Zh79toc8cap8S7sGVzxmwIY82iR2KczSOLLnHMO7/92Pbr7T6ENADYN4QrAAAqiJV+t+qD9tgbFsoytuZqg+sRy9HGLbkudG0sGdKKjlkQs2utWmLJxZktrNnCzeVlhROtqIcXyqJ3G9YslEUowg2BtPAZFx2p2KKHhTQX4IqCW3w0oQ1A+CNcAQAQBKHMDQfcy1CWl1+grNx8F2psCGLa5mzXO2ahy+aPeVubPxbYt21+0dyy7eft+ZacPBUU2qLP0pacfPcIzDWrKLFRkYqLiXTDMeNtGx21fd9tix7R3nP7XHZ9bFSUYqIjvNdHR7q5aYEQV7xfYhs4VvLamKiI4vOU3gdQWQhXAACEqOioSCUXFcFoWz/RPfZVYWGhtuUWlApmFtS2hzQLYEVBrOiYFf6wKokbsnKVuS1XOfkFys0rVHaeVxDEvT4nzwU2Y+ftYcMh/eSCloWwHUJZqcBWdD4mMsKFXztm2+ioCEW7beCcF9y8c5FF54quKT4XWep13rkd71nimhL7O15jnX/WAxgZYQ8Vbb33t+cER8BfhCsAAOB+KE9w63tFueqGFcWKgWzLy3fBzaoz2vBF29rzbNuWOFe8zdt+PjvPC2S5RducvALl5hd4x4v2A6EucL54W3Q+z7rkSsjNL1Rufr6lPYWjQNAKBK+SgaxkCIva4Vyp6yIi3BDRkq/Z1Tnbj9pF4NvpukCbInd/nQ039bbe8cC+7dg2cH1gPxAqdzpedB/vXNFnL9ov657uqXe0xPPA67f/Ggeu2WFTKtzu+Lpd3b+k4s+xu/fcC4Uq3Lvr9+7yvby73D/i7A0bUjy4c0OFEsIVAACoNPYDuc3PqrF3Ix4rlAW8QK9ZcSDbKaQVFgWzfOUUBTULdDYfLregQHn5hS6k2VBMb1uo/IIC5RbY1sJa6Wu81xVdk1/6msA9vWPeNSVfW/KetvXOFbhhm3vCXpO/Tz/6AsGlTf2ahCsAAIBgC3jxkd58rlBmIdGGYVrI8rbevoUp6xGwrTdvrlD5gesKyrjOnheUfY+drgu8V0EZ1xWWft9S1xWds+f5gTbt7jp3fvt1Fgutk8P23LawjGNFPSHe+dLHC4p2iq8pfm3Jc6VfawIdK4FYur2jZXtQ3fmaoteW+K52dc2O571rdnjv4teW/Z7lGfhZnlGj+9JzVt73bZySoFBDuAIAAAgBblheuX60BlDZWAoeAAAAACoA4QoAAAAAKgDhCgAAAAAqAOEKAAAAACoA4QoAAAAAKgDhCgAAAAAqAOEKAAAAACoA4QoAAAAAKgDhCgAAAAAqAOEKAAAAACoA4QoAAAAAKgDhCgAAAAAqAOEKAAAAACoA4QoAAAAAKgDhCgAAAAAqAOEKAAAAACoA4QoAAAAAKgDhCgAAAAAqQHRF3CTcFBYWum1GRobfTQEAAADgo0AmCGSE3SFclWHz5s1u27x5c7+bAgAAACBIMkJKSspur4ko3JMIVs0UFBRo1apVSkpKUkREhO9J2ULe8uXLlZyc7GtbUPn4vqsXvu/qhe+7euH7rl74vsNbYWGhC1ZNmjRRZOTuZ1XRc1UG+0Vr1qyZgon9QeUPa/XB91298H1XL3zf1Qvfd/XC9x2+/qrHKoCCFgAAAABQAQhXAAAAAFABCFdBLi4uTnfeeafbIvzxfVcvfN/VC9939cL3Xb3wfSOAghYAAAAAUAHouQIAAACACkC4AgAAAIAKQLgCAAAAgApAuAIAAACACkC4CnLPPvusWrVqpfj4ePXv318TJkzwu0nYBz///LOOPfZYt7J3RESEPv3001Lnra7MHXfcocaNGyshIUFDhgzR/PnzS12zYcMGnX322W5xwlq1auniiy9WZmZmFX8S/JUHHnhA/fr1U1JSkho0aKATTjhBc+fOLXXNtm3bdNVVV6lu3bpKTEzUySefrLVr15a6ZtmyZTr66KNVo0YNd5+///3vysvLq+JPg7/y/PPPq0ePHsULhw4YMEBfffVV8Xm+6/D24IMPur/Tr7/++uJjfOfh46677nLfb8lHp06dis/zXaMshKsgNnLkSN14442utOeUKVPUs2dPDRs2TKmpqX43DXtpy5Yt7vuzsFyWhx9+WE899ZReeOEFjR8/XjVr1nTftf3FHWDBatasWfruu+80atQoF9guvfTSKvwU2BM//fST+5/t77//7r6r3NxcDR061P0eCLjhhhv0v//9Tx988IG7ftWqVTrppJOKz+fn57v/Gefk5Oi3337T66+/rhEjRrgAjuDSrFkz9wP25MmTNWnSJB1++OE6/vjj3Z9Vw3cdviZOnKgXX3zRheuS+M7DS9euXbV69erix6+//lp8ju8aZbJS7AhO+++/f+FVV11V/Dw/P7+wSZMmhQ888ICv7UL52B+7Tz75pPh5QUFBYaNGjQofeeSR4mObNm0qjIuLK3z33Xfd8z///NO9buLEicXXfPXVV4URERGFK1eurOJPgL2Rmprqvruffvqp+LuNiYkp/OCDD4qvmT17trtm3Lhx7vmXX35ZGBkZWbhmzZria55//vnC5OTkwuzsbB8+BfZG7dq1C1955RW+6zC2efPmwvbt2xd+9913hYMGDSq87rrr3HG+8/By5513Fvbs2bPMc3zX2BV6roKU/SuH/UuoDQ8LiIyMdM/HjRvna9tQsRYvXqw1a9aU+q5TUlLcMNDAd21bGwrYt2/f4mvsevs9YT1dCF7p6eluW6dOHbe1P9fWm1Xy+7ZhJi1atCj1fXfv3l0NGzYsvsZ6MjMyMop7RBB87F+p33vvPddLacMD+a7Dl/VOW49Eye/W8J2HHxuib0P627Rp40aQ2DA/w3eNXYne5Rn4Ki0tzf2PuuQfSGPP58yZ41u7UPEsWJmyvuvAOdvaWO2SoqOj3Q/sgWsQfAoKCtxcjIMOOkjdunVzx+z7io2NdWF5d993Wb8fAucQXGbMmOHClA3jtXkXn3zyibp06aKpU6fyXYchC9A2VN+GBe6IP9/hxf6R04bxdezY0Q0JvPvuuzVw4EDNnDmT7xq7RLgCgEr81237n3DJMfoIP/aDlwUp66X88MMPdf7557v5Fwg/y5cv13XXXefmU1qhKYS34cOHF+/b3DoLWy1bttT777/vik8BZWFYYJCqV6+eoqKidqo6Y88bNWrkW7tQ8QLf5+6+a9vuWMjEqg1ZBUF+PwSnq6++2hUe+fHHH13RgwD7vmzY76ZNm3b7fZf1+yFwDsHF/vW6Xbt26tOnj6sWacVrnnzySb7rMGRDwezv4t69e7vRA/awIG0FiWzfeiX4zsOX9VJ16NBBCxYs4M83dolwFcT/s7b/UX///felhhjZcxt+gvDRunVr95dsye/axmPbXKrAd21b+wvc/sce8MMPP7jfE/YvaQgeVrPEgpUNDbPvyL7fkuzPdUxMTKnv20q12zj+kt+3DTUrGajtX8qt1LcNN0Nwsz+X2dnZfNdhaPDgwe77sp7KwMPmwtpcnMA+33n4suVPFi5c6JZN4c83dmmXpS7gu/fee89VjBsxYoSrFnfppZcW1qpVq1TVGYROZak//vjDPeyP3WOPPeb2ly5d6s4/+OCD7rv97LPPCqdPn154/PHHF7Zu3bpw69atxfc48sgjC/fbb7/C8ePHF/7666+uUtWZZ57p46dCWa644orClJSUwjFjxhSuXr26+JGVlVV8zeWXX17YokWLwh9++KFw0qRJhQMGDHCPgLy8vMJu3boVDh06tHDq1KmFX3/9dWH9+vULb7vtNp8+FXbl1ltvdZUgFy9e7P7s2nOr4vntt9+683zX4a9ktUDDdx4+brrpJvd3uf35Hjt2bOGQIUMK69Wr56rAGr5rlIVwFeSefvpp9wc3NjbWlWb//fff/W4S9sGPP/7oQtWOj/PPP7+4HPvtt99e2LBhQxeoBw8eXDh37txS91i/fr0LU4mJia6M64UXXuhCG4JLWd+zPV577bXiayw0X3nlla5kd40aNQpPPPFEF8BKWrJkSeHw4cMLExIS3P/M7X/yubm5Pnwi7M5FF11U2LJlS/d3tP3QZH92A8HK8F1Xv3DFdx4+Tj/99MLGjRu7P99NmzZ1zxcsWFB8nu8aZYmw/+y6XwsAAAAAsCeYcwUAAAAAFYBwBQAAAAAVgHAFAAAAABWAcAUAAAAAFYBwBQAAAAAVgHAFAAAAABWAcAUAAAAAFYBwBQAAAAAVgHAFAEA5RURE6NNPP/W7GQAAnxGuAAAh7YILLnDhZsfHkUce6XfTAADVTLTfDQAAoLwsSL322muljsXFxfnWHgBA9UTPFQAg5FmQatSoUalH7dq13TnrxXr++ec1fPhwJSQkqE2bNvrwww9LvX7GjBk6/PDD3fm6devq0ksvVWZmZqlrXn31VXXt2tW9V+PGjXX11VeXOp+WlqYTTzxRNWrUUPv27fX5558Xn9u4caPOPvts1a9f372Hnd8xDAIAQh/hCgAQ9m6//XadfPLJmjZtmgs5Z5xxhmbPnu3ObdmyRcOGDXNhbOLEifrggw80evToUuHJwtlVV13lQpcFMQtO7dq1K/Ued999t0477TRNnz5dRx11lHufDRs2FL//n3/+qa+++sq9r92vXr16VfyrAACobBGFhYWFlf4uAABU4pyrt956S/Hx8aWO//Of/3QP67m6/PLLXaAJOOCAA9S7d28999xzevnll/WPf/xDy5cvV82aNd35L7/8Uscee6xWrVqlhg0bqmnTprrwwgt13333ldkGe4//+7//07333lsc2BITE12YsiGLxx13nAtT1vsFAAhfzLkCAIS8ww47rFR4MnXq1CneHzBgQKlz9nzq1Klu33qSevbsWRyszEEHHaSCggLNnTvXBScLWYMHD95tG3r06FG8b/dKTk5Wamqqe37FFVe4nrMpU6Zo6NChOuGEE3TggQeW81MDAIIN4QoAEPIszOw4TK+i2BypPRETE1PquYUyC2jG5nstXbrU9Yh99913LqjZMMNHH320UtoMAPAHc64AAGHv999/3+l5586d3b5tbS6WDeULGDt2rCIjI9WxY0clJSWpVatW+v7778vVBitmcf7557shjE888YReeumlct0PABB86LkCAIS87OxsrVmzptSx6Ojo4qIRVqSib9++Ovjgg/X2229rwoQJ+u9//+vOWeGJO++80wWfu+66S+vWrdM111yjc8891823Mnbc5m01aNDA9UJt3rzZBTC7bk/ccccd6tOnj6s2aG0dNWpUcbgDAIQPwhUAIOR9/fXXrjx6SdbrNGfOnOJKfu+9956uvPJKd927776rLl26uHNWOv2bb77Rddddp379+rnnNj/qscceK76XBa9t27bp8ccf18033+xC2ymnnLLH7YuNjdVtt92mJUuWuGGGAwcOdO0BAIQXqgUCAMKazX365JNPXBEJAAAqE3OuAAAAAKACEK4AAAAAoAIw5woAENYY/Q4AqCr0XAEAAABABSBcAQAAAEAFIFwBAAAAQAUgXAEAAABABSBcAQAAAEAFIFwBAAAAQAUgXAEAAABABSBcAQAAAIDK7/8BHO11QbdBNrcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 Evaluation Metric: 0.9583\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8477\n",
      "\n",
      "K-fold Cross-Validation Results:\n",
      "{'fold_metrics': [np.float64(0.88), np.float64(0.76), np.float64(0.84), np.float64(0.8), np.float64(0.9583333333333334)], 'average_metric': np.float64(0.8476666666666667)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold  # Added dependency for K-fold CV\n",
    "\n",
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "# ============================\n",
    "# Activation functions and their derivatives\n",
    "# ============================\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(z, a):\n",
    "    # Note: z is not used here; kept for uniform signature.\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(z, a):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(z, a):\n",
    "    return np.ones_like(a)\n",
    "\n",
    "activation_functions = {\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"relu\": relu,\n",
    "    \"linear\": linear\n",
    "}\n",
    "\n",
    "activation_derivatives = {\n",
    "    \"sigmoid\": lambda z, a: sigmoid_derivative(z, a),\n",
    "    \"relu\": lambda z, a: relu_derivative(z, a),\n",
    "    \"linear\": lambda z, a: linear_derivative(z, a)\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Loss functions and their derivatives\n",
    "# ============================\n",
    "\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Binary crossentropy loss for binary classification.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_crossentropy_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the binary crossentropy loss.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return - (y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Squared Error (MSE) loss, typically used for regression.\n",
    "    \"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the MSE loss.\n",
    "    \"\"\"\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "def mee_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Euclidean Error (MEE) loss, defined as:\n",
    "        E_MEE = (1/N) * sum over i [ ||y_true[i] - y_pred[i]||_2 ].\n",
    "    \"\"\"\n",
    "    diff = y_true - y_pred  # shape: (N, d) or (N, 1)\n",
    "    # Euclidean distance for each sample\n",
    "    dist = np.sqrt(np.sum(diff ** 2, axis=1))\n",
    "    return np.mean(dist)\n",
    "\n",
    "def mee_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the Mean Euclidean Error (MEE) loss.\n",
    "    For each sample i, derivative wrt y_pred[i] is:\n",
    "        (1/N) * ( (y_pred[i] - y_true[i]) / ||y_pred[i] - y_true[i]||_2 ).\n",
    "    We safely handle the case where the norm is zero.\n",
    "    \"\"\"\n",
    "    diff = y_pred - y_true\n",
    "    dist = np.sqrt(np.sum(diff ** 2, axis=1, keepdims=True))\n",
    "    epsilon = 1e-8  # Avoid division by zero\n",
    "    dist_safe = np.where(dist == 0, epsilon, dist)\n",
    "    N = y_true.shape[0]\n",
    "    derivative = diff / dist_safe / N\n",
    "    return derivative\n",
    "\n",
    "loss_functions = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_loss,\n",
    "    \"mse\": mse_loss,\n",
    "    \"mee\": mee_loss,  \n",
    "}\n",
    "\n",
    "loss_derivatives = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_derivative,\n",
    "    \"mse\": mse_derivative,\n",
    "    \"mee\": mee_derivative, \n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Regularization functions (modular)\n",
    "# ============================\n",
    "\n",
    "def compute_reg_gradient(W, lambda_reg, reg_type, m):\n",
    "    if reg_type == \"l2\":\n",
    "        return lambda_reg * W / m\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * np.sign(W) / m\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_reg_loss(W_list, lambda_reg, reg_type):\n",
    "    if reg_type == \"l2\":\n",
    "        return (lambda_reg / 2) * sum(np.sum(W ** 2) for W in W_list)\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * sum(np.sum(np.abs(W)) for W in W_list)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# ============================\n",
    "# Neural Network Class with Learning Rate Decay, Momentum, Custom Weight Initialization, and Early Stopping\n",
    "# ============================\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.01, lambda_reg=0.001, reg_type=\"l2\",\n",
    "                 loss_function_name=None,\n",
    "                 activation_function_name=\"relu\",\n",
    "                 output_activation_function_name=None,\n",
    "                 activation_function_names=None,\n",
    "                 task=\"classification\",\n",
    "                 lr_decay_type=\"none\",  # Options: \"none\", \"exponential\", \"linear\"\n",
    "                 decay_rate=0.0,\n",
    "                 weight_init=\"base\",  # \"base\" (fan-in scaling) or \"glorot\"\n",
    "                 momentum_type=\"none\",  # Options: \"none\", \"momentum\", \"nesterov momentum\"\n",
    "                 momentum_alpha=0.9):\n",
    "        \"\"\"\n",
    "        :param layers: List containing the size of each layer (input, hidden, output)\n",
    "        :param learning_rate: Initial learning rate\n",
    "        :param lambda_reg: Regularization coefficient\n",
    "        :param reg_type: Type of regularization (\"l2\", \"l1\", or other for none)\n",
    "        :param loss_function_name: Name of the loss function (if None, set based on task)\n",
    "        :param activation_function_name: Activation to use for hidden layers (if activation_function_names not provided)\n",
    "        :param output_activation_function_name: Activation for the output layer (if None, set based on task)\n",
    "        :param activation_function_names: List of activation function names for each layer (length = len(layers)-1)\n",
    "        :param task: \"classification\" or \"regression\"\n",
    "        :param lr_decay_type: Learning rate decay strategy (\"none\", \"exponential\", \"linear\")\n",
    "        :param decay_rate: Decay rate used in the learning rate schedule\n",
    "        :param weight_init: Weight initialization strategy (\"base\" uses fan-in scaling or \"glorot\")\n",
    "        :param momentum_type: Momentum strategy (\"none\", \"momentum\", \"nesterov momentum\")\n",
    "        :param momentum_alpha: Momentum coefficient (e.g., 0.9)\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.reg_type = reg_type\n",
    "        self.task = task\n",
    "        self.lr_decay_type = lr_decay_type\n",
    "        self.decay_rate = decay_rate\n",
    "        self.weight_init = weight_init\n",
    "        \n",
    "        # Set momentum parameters\n",
    "        if momentum_type not in {\"none\", \"momentum\", \"nesterov momentum\"}:\n",
    "            raise ValueError(\"momentum_type must be 'none', 'momentum', or 'nesterov momentum'.\")\n",
    "        self.momentum_type = momentum_type\n",
    "        self.momentum_alpha = momentum_alpha if momentum_type != \"none\" else 0.0\n",
    "        \n",
    "        # Set defaults based on task\n",
    "        if self.task == \"regression\":\n",
    "            self.loss_function_name = loss_function_name or \"mse\"\n",
    "            output_activation_function_name = output_activation_function_name or \"linear\"\n",
    "        else:\n",
    "            # Classification\n",
    "            self.loss_function_name = loss_function_name or \"binary_crossentropy\"\n",
    "            output_activation_function_name = output_activation_function_name or \"sigmoid\"\n",
    "        \n",
    "        # Set activation functions for layers\n",
    "        if activation_function_names is None:\n",
    "            self.activation_function_names = [activation_function_name] * (len(layers) - 1)\n",
    "            self.activation_function_names[-1] = output_activation_function_name\n",
    "        else:\n",
    "            if len(activation_function_names) != len(layers) - 1:\n",
    "                raise ValueError(\"activation_function_names must have length equal to len(layers)-1.\")\n",
    "            self.activation_function_names = activation_function_names\n",
    "        \n",
    "        self._initialize_parameters()\n",
    "        # Initialize momentum accumulators (even if not used, for consistency)\n",
    "        self.vW = [np.zeros_like(W) for W in self.W]\n",
    "        self.vb = [np.zeros_like(b) for b in self.b]\n",
    "        \n",
    "        # Initialize loss history lists (will be (re)initialized in train())\n",
    "        self.train_loss_history = []\n",
    "        self.val_loss_history = None\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            fan_in = self.layers[i]\n",
    "            fan_out = self.layers[i + 1]\n",
    "            if self.weight_init == \"base\":\n",
    "                std = np.sqrt(1.0 / fan_in)\n",
    "            elif self.weight_init == \"glorot\":\n",
    "                std = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported weight initialization strategy. Use 'base' or 'glorot'.\")\n",
    "            weight = np.random.randn(fan_in, fan_out) * std\n",
    "            self.W.append(weight)\n",
    "            self.b.append(np.zeros((1, fan_out)))\n",
    "    \n",
    "    def _apply_activation(self, x, func_name):\n",
    "        if func_name not in activation_functions:\n",
    "            raise ValueError(f\"Unsupported activation: {func_name}\")\n",
    "        return activation_functions[func_name](x)\n",
    "    \n",
    "    def _apply_activation_derivative(self, z, a, func_name):\n",
    "        if func_name not in activation_derivatives:\n",
    "            raise ValueError(f\"Unsupported activation derivative: {func_name}\")\n",
    "        return activation_derivatives[func_name](z, a)\n",
    "    \n",
    "    def _forward(self, X, weights=None, biases=None):\n",
    "        \"\"\"\n",
    "        Forward propagation. If weights and biases are provided, they are used;\n",
    "        otherwise the network's parameters are used.\n",
    "        Returns lists Z (pre-activations) and A (activations).\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.W\n",
    "        if biases is None:\n",
    "            biases = self.b\n",
    "            \n",
    "        A = [X]\n",
    "        Z = []\n",
    "        # Forward through hidden layers\n",
    "        for i in range(len(weights) - 1):\n",
    "            z_curr = np.dot(A[-1], weights[i]) + biases[i]\n",
    "            Z.append(z_curr)\n",
    "            a_curr = self._apply_activation(z_curr, self.activation_function_names[i])\n",
    "            A.append(a_curr)\n",
    "        # Forward through output layer\n",
    "        z_out = np.dot(A[-1], weights[-1]) + biases[-1]\n",
    "        Z.append(z_out)\n",
    "        a_out = self._apply_activation(z_out, self.activation_function_names[-1])\n",
    "        A.append(a_out)\n",
    "        return Z, A\n",
    "    \n",
    "    def _compute_gradients(self, X, y, Z, A, weights=None):\n",
    "        \"\"\"\n",
    "        Compute gradients dW and db given inputs X, target y, pre-activations Z and activations A.\n",
    "        Optionally, a custom set of weights (used in lookahead for Nesterov momentum) can be provided.\n",
    "        Returns lists dW and db.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.W\n",
    "        m = X.shape[0]\n",
    "        dA = loss_derivatives[self.loss_function_name](y, A[-1])\n",
    "        # Output layer\n",
    "        dZ = dA * self._apply_activation_derivative(Z[-1], A[-1], self.activation_function_names[-1])\n",
    "        reg_term = compute_reg_gradient(weights[-1], self.lambda_reg, self.reg_type, m)\n",
    "        dW = [np.dot(A[-2].T, dZ) / m + reg_term]\n",
    "        db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "        \n",
    "        # Backpropagate through hidden layers\n",
    "        for i in range(len(weights) - 2, -1, -1):\n",
    "            dA = np.dot(dZ, weights[i + 1].T)\n",
    "            dZ = dA * self._apply_activation_derivative(Z[i], A[i + 1], self.activation_function_names[i])\n",
    "            reg_term = compute_reg_gradient(weights[i], self.lambda_reg, self.reg_type, m)\n",
    "            dW.insert(0, np.dot(A[i].T, dZ) / m + reg_term)\n",
    "            db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "            \n",
    "        return dW, db\n",
    "    \n",
    "    def train(self, X, y, epochs=300, batch_size=32, verbose=True,\n",
    "              early_stopping=False, validation_data=None, patience=10, min_delta=0.0):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "\n",
    "        The loss histories for training and validation (if provided) are stored in:\n",
    "            self.train_loss_history and self.val_loss_history\n",
    "\n",
    "        :param X: Training data inputs.\n",
    "        :param y: Training data targets.\n",
    "        :param epochs: Maximum number of epochs to train.\n",
    "        :param batch_size: Mini-batch size.\n",
    "        :param verbose: Whether to print progress.\n",
    "        :param early_stopping: Enable early stopping if True.\n",
    "        :param validation_data: Tuple (X_val, y_val) for early stopping and validation loss logging.\n",
    "        :param patience: Number of epochs with no improvement to wait before stopping.\n",
    "        :param min_delta: Minimum change in the monitored loss to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        # Reinitialize loss histories\n",
    "        self.train_loss_history = []\n",
    "        if validation_data is not None:\n",
    "            self.val_loss_history = []\n",
    "        else:\n",
    "            self.val_loss_history = None\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        best_loss = np.inf\n",
    "        patience_counter = 0\n",
    "        best_weights = None\n",
    "        best_biases = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Update learning rate based on decay schedule\n",
    "            if self.lr_decay_type == \"exponential\":\n",
    "                self.learning_rate = self.initial_learning_rate * np.exp(-self.decay_rate * epoch)\n",
    "            elif self.lr_decay_type == \"linear\":\n",
    "                self.learning_rate = self.initial_learning_rate * max(0, 1 - self.decay_rate * epoch)\n",
    "            # Otherwise (\"none\"), keep the initial learning rate.\n",
    "            \n",
    "            # Shuffle training data\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            \n",
    "            # Process mini-batches\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                \n",
    "                if self.momentum_type == \"nesterov momentum\":\n",
    "                    weights_lookahead = [self.W[j] - self.momentum_alpha * self.vW[j] for j in range(len(self.W))]\n",
    "                    biases_lookahead = [self.b[j] - self.momentum_alpha * self.vb[j] for j in range(len(self.b))]\n",
    "                    Z, A = self._forward(X_batch, weights=weights_lookahead, biases=biases_lookahead)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A, weights=weights_lookahead)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.vW[j] = self.momentum_alpha * self.vW[j] + self.learning_rate * dW[j]\n",
    "                        self.vb[j] = self.momentum_alpha * self.vb[j] + self.learning_rate * db[j]\n",
    "                        self.W[j] -= self.vW[j]\n",
    "                        self.b[j] -= self.vb[j]\n",
    "                        \n",
    "                elif self.momentum_type == \"momentum\":\n",
    "                    Z, A = self._forward(X_batch)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.vW[j] = self.momentum_alpha * self.vW[j] + self.learning_rate * dW[j]\n",
    "                        self.vb[j] = self.momentum_alpha * self.vb[j] + self.learning_rate * db[j]\n",
    "                        self.W[j] -= self.vW[j]\n",
    "                        self.b[j] -= self.vb[j]\n",
    "                        \n",
    "                else:  # No momentum\n",
    "                    Z, A = self._forward(X_batch)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.W[j] -= self.learning_rate * dW[j]\n",
    "                        self.b[j] -= self.learning_rate * db[j]\n",
    "            \n",
    "            # Compute training loss\n",
    "            _, A_full = self._forward(X)\n",
    "            train_loss = loss_functions[self.loss_function_name](y, A_full[-1])\n",
    "            reg_loss = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "            total_train_loss = train_loss + reg_loss\n",
    "            self.train_loss_history.append(total_train_loss)\n",
    "            \n",
    "            # Compute validation loss if validation data is provided\n",
    "            if validation_data is not None:\n",
    "                X_val, y_val = validation_data\n",
    "                _, A_val = self._forward(X_val)\n",
    "                val_loss = loss_functions[self.loss_function_name](y_val, A_val[-1])\n",
    "                reg_loss_val = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "                total_val_loss = val_loss + reg_loss_val\n",
    "                self.val_loss_history.append(total_val_loss)\n",
    "            else:\n",
    "                total_val_loss = None\n",
    "\n",
    "            # Verbose logging\n",
    "            if verbose:\n",
    "                if total_val_loss is not None:\n",
    "                    print(f\"Epoch {epoch:4d}, Training Loss: {total_train_loss:.4f}, \"\n",
    "                          f\"Validation Loss: {total_val_loss:.4f}, Learning Rate: {self.learning_rate:.6f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch:4d}, Training Loss: {total_train_loss:.4f}, \"\n",
    "                          f\"Learning Rate: {self.learning_rate:.6f}\")\n",
    "            \n",
    "            # Early stopping check (only if validation data is provided)\n",
    "            if early_stopping and (validation_data is not None):\n",
    "                if total_val_loss < best_loss - min_delta:\n",
    "                    best_loss = total_val_loss\n",
    "                    patience_counter = 0\n",
    "                    best_weights = [w.copy() for w in self.W]\n",
    "                    best_biases = [b.copy() for b in self.b]\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        if verbose:\n",
    "                            print(f\"Early stopping triggered at epoch {epoch}. Restoring best model parameters.\")\n",
    "                        if best_weights is not None:\n",
    "                            self.W = best_weights\n",
    "                            self.b = best_biases\n",
    "                        break\n",
    "\n",
    "    def plot_loss_history(self):\n",
    "        \"\"\"\n",
    "        Plot the training loss history and, if available, the validation loss history.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.train_loss_history, label=\"Training Loss\")\n",
    "        if self.val_loss_history is not None and len(self.val_loss_history) > 0:\n",
    "            plt.plot(self.val_loss_history, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss History\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, A = self._forward(X)\n",
    "        output = A[-1]\n",
    "        if self.task == \"classification\":\n",
    "            if output.shape[1] == 1:\n",
    "                return (output > 0.5).astype(int)\n",
    "            else:\n",
    "                return np.argmax(output, axis=1)\n",
    "        else:\n",
    "            return output\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        if self.task == \"regression\":\n",
    "            return mse_loss(y, predictions)\n",
    "        else:\n",
    "            if y.ndim > 1 and y.shape[1] > 1:\n",
    "                y_true = np.argmax(y, axis=1)\n",
    "            else:\n",
    "                y_true = y\n",
    "            return np.mean(predictions == y_true)\n",
    "\n",
    "# ============================\n",
    "# K-fold Cross-Validation Function\n",
    "# ============================\n",
    "\n",
    "def k_fold_cross_validation(model_builder, X, y, k=5, epochs=1000, batch_size=32,\n",
    "                            verbose=True, early_stopping=False, patience=10, min_delta=1e-4):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross validation for a neural network.\n",
    "    \n",
    "    :param model_builder: A callable that returns a new instance of NeuralNetwork.\n",
    "    :param X: Input features.\n",
    "    :param y: Targets.\n",
    "    :param k: Number of folds.\n",
    "    :param epochs: Number of training epochs per fold.\n",
    "    :param batch_size: Batch size.\n",
    "    :param verbose: Verbosity flag.\n",
    "    :param early_stopping: Whether to use early stopping.\n",
    "    :param patience: Patience for early stopping.\n",
    "    :param min_delta: Minimum change in loss for early stopping.\n",
    "    :return: Dictionary containing per-fold metrics and the overall average metric.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold_metrics = []\n",
    "    fold = 1\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        print(f\"\\nFold {fold}/{k}\")\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "        \n",
    "        # Build a new model instance for this fold\n",
    "        nn_model = model_builder()\n",
    "        \n",
    "        # Train the model on the training fold with validation on the fold's validation set.\n",
    "        nn_model.train(\n",
    "            X_train_fold, y_train_fold,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=verbose,\n",
    "            early_stopping=early_stopping,\n",
    "            validation_data=(X_val_fold, y_val_fold),\n",
    "            patience=patience,\n",
    "            min_delta=min_delta\n",
    "        )\n",
    "        \n",
    "        # Evaluate the model on the validation fold.\n",
    "        metric = nn_model.evaluate(X_val_fold, y_val_fold)\n",
    "        print(f\"Fold {fold} Evaluation Metric: {metric:.4f}\")\n",
    "        fold_metrics.append(metric)\n",
    "        fold += 1\n",
    "    \n",
    "    avg_metric = np.mean(fold_metrics)\n",
    "    print(f\"\\nAverage Evaluation Metric over {k} folds: {avg_metric:.4f}\")\n",
    "    return {\"fold_metrics\": fold_metrics, \"average_metric\": avg_metric}\n",
    "\n",
    "# ============================\n",
    "# Testing on a monk's dataset\n",
    "# ============================\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(1, one_hot_encode=True)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_units = 10\n",
    "output_size = 1  # binary classification\n",
    "layers = [input_size, hidden_units, output_size]\n",
    "\n",
    "# Define activation functions for hidden and output layers\n",
    "activation_funcs = [\"relu\", \"sigmoid\"]\n",
    "\n",
    "# Example: Build a neural network classifier instance\n",
    "nn_clf = NeuralNetwork(\n",
    "    layers=layers,\n",
    "    learning_rate=0.2,\n",
    "    lambda_reg=0.001,\n",
    "    reg_type=\"l2\",\n",
    "    loss_function_name=\"mse\",       \n",
    "    activation_function_names=activation_funcs,\n",
    "    task=\"classification\",\n",
    "    lr_decay_type=\"linear\",    # Options: \"exponential\", \"linear\", or \"none\"\n",
    "    decay_rate=0.001,\n",
    "    weight_init=\"base\",        # \"base\" or \"glorot\"\n",
    ")\n",
    "\n",
    "# For early stopping, we provide validation data.\n",
    "nn_clf.train(\n",
    "    X_train, y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    verbose=True,\n",
    "    early_stopping=True,\n",
    "    validation_data=(X_test, y_test),\n",
    "    patience=10,\n",
    "    min_delta=1e-4\n",
    ")\n",
    "\n",
    "accuracy = nn_clf.evaluate(X_test, y_test)\n",
    "print(f\"\\nNeural Network Classification Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot the loss histories\n",
    "nn_clf.plot_loss_history()\n",
    "\n",
    "# ============================\n",
    "# K-fold Cross-Validation Example\n",
    "# ============================\n",
    "\n",
    "def build_nn_model():\n",
    "    \"\"\"\n",
    "    Returns a new instance of NeuralNetwork with the same configuration.\n",
    "    This is used for each fold in cross validation.\n",
    "    \"\"\"\n",
    "    return NeuralNetwork(\n",
    "        layers=layers,\n",
    "        learning_rate=0.2,\n",
    "        lambda_reg=0.001,\n",
    "        reg_type=\"l2\",\n",
    "        loss_function_name=\"mse\",       \n",
    "        activation_function_names=activation_funcs,\n",
    "        task=\"classification\",\n",
    "        lr_decay_type=\"linear\",    # Options: \"exponential\", \"linear\", or \"none\"\n",
    "        decay_rate=0.001,\n",
    "        weight_init=\"base\",        # \"base\" or \"glorot\"\n",
    "    )\n",
    "\n",
    "# Perform 5-fold cross validation on the training set.\n",
    "cv_results = k_fold_cross_validation(\n",
    "    model_builder=build_nn_model,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    k=5,\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    verbose=False,  # Set True to see per-epoch logging within each fold.\n",
    "    early_stopping=True,\n",
    "    patience=10,\n",
    "    min_delta=1e-4\n",
    ")\n",
    "\n",
    "print(\"\\nK-fold Cross-Validation Results:\")\n",
    "print(cv_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-1-train.csv\n",
      "Using cached ../datasets/monks/monk-1-test.csv\n",
      "One-hot encoding MONK-1 dataset...\n",
      "Epoch    0, Training Loss: 0.2536, Validation Loss: 0.2569, Learning Rate: 0.200000\n",
      "Epoch    1, Training Loss: 0.2504, Validation Loss: 0.2549, Learning Rate: 0.199800\n",
      "Epoch    2, Training Loss: 0.2474, Validation Loss: 0.2530, Learning Rate: 0.199600\n",
      "Epoch    3, Training Loss: 0.2446, Validation Loss: 0.2512, Learning Rate: 0.199400\n",
      "Epoch    4, Training Loss: 0.2416, Validation Loss: 0.2494, Learning Rate: 0.199200\n",
      "Epoch    5, Training Loss: 0.2387, Validation Loss: 0.2475, Learning Rate: 0.199000\n",
      "Epoch    6, Training Loss: 0.2356, Validation Loss: 0.2454, Learning Rate: 0.198800\n",
      "Epoch    7, Training Loss: 0.2325, Validation Loss: 0.2432, Learning Rate: 0.198600\n",
      "Epoch    8, Training Loss: 0.2293, Validation Loss: 0.2410, Learning Rate: 0.198400\n",
      "Epoch    9, Training Loss: 0.2253, Validation Loss: 0.2382, Learning Rate: 0.198200\n",
      "Epoch   10, Training Loss: 0.2205, Validation Loss: 0.2348, Learning Rate: 0.198000\n",
      "Epoch   11, Training Loss: 0.2166, Validation Loss: 0.2319, Learning Rate: 0.197800\n",
      "Epoch   12, Training Loss: 0.2127, Validation Loss: 0.2291, Learning Rate: 0.197600\n",
      "Epoch   13, Training Loss: 0.2083, Validation Loss: 0.2260, Learning Rate: 0.197400\n",
      "Epoch   14, Training Loss: 0.2028, Validation Loss: 0.2223, Learning Rate: 0.197200\n",
      "Epoch   15, Training Loss: 0.1976, Validation Loss: 0.2188, Learning Rate: 0.197000\n",
      "Epoch   16, Training Loss: 0.1929, Validation Loss: 0.2157, Learning Rate: 0.196800\n",
      "Epoch   17, Training Loss: 0.1887, Validation Loss: 0.2127, Learning Rate: 0.196600\n",
      "Epoch   18, Training Loss: 0.1846, Validation Loss: 0.2098, Learning Rate: 0.196400\n",
      "Epoch   19, Training Loss: 0.1807, Validation Loss: 0.2070, Learning Rate: 0.196200\n",
      "Epoch   20, Training Loss: 0.1771, Validation Loss: 0.2043, Learning Rate: 0.196000\n",
      "Epoch   21, Training Loss: 0.1737, Validation Loss: 0.2019, Learning Rate: 0.195800\n",
      "Epoch   22, Training Loss: 0.1704, Validation Loss: 0.1996, Learning Rate: 0.195600\n",
      "Epoch   23, Training Loss: 0.1672, Validation Loss: 0.1972, Learning Rate: 0.195400\n",
      "Epoch   24, Training Loss: 0.1641, Validation Loss: 0.1951, Learning Rate: 0.195200\n",
      "Epoch   25, Training Loss: 0.1614, Validation Loss: 0.1928, Learning Rate: 0.195000\n",
      "Epoch   26, Training Loss: 0.1593, Validation Loss: 0.1910, Learning Rate: 0.194800\n",
      "Epoch   27, Training Loss: 0.1564, Validation Loss: 0.1891, Learning Rate: 0.194600\n",
      "Epoch   28, Training Loss: 0.1540, Validation Loss: 0.1875, Learning Rate: 0.194400\n",
      "Epoch   29, Training Loss: 0.1519, Validation Loss: 0.1860, Learning Rate: 0.194200\n",
      "Epoch   30, Training Loss: 0.1499, Validation Loss: 0.1844, Learning Rate: 0.194000\n",
      "Epoch   31, Training Loss: 0.1480, Validation Loss: 0.1830, Learning Rate: 0.193800\n",
      "Epoch   32, Training Loss: 0.1462, Validation Loss: 0.1818, Learning Rate: 0.193600\n",
      "Epoch   33, Training Loss: 0.1445, Validation Loss: 0.1801, Learning Rate: 0.193400\n",
      "Epoch   34, Training Loss: 0.1428, Validation Loss: 0.1791, Learning Rate: 0.193200\n",
      "Epoch   35, Training Loss: 0.1414, Validation Loss: 0.1783, Learning Rate: 0.193000\n",
      "Epoch   36, Training Loss: 0.1398, Validation Loss: 0.1772, Learning Rate: 0.192800\n",
      "Epoch   37, Training Loss: 0.1382, Validation Loss: 0.1755, Learning Rate: 0.192600\n",
      "Epoch   38, Training Loss: 0.1368, Validation Loss: 0.1742, Learning Rate: 0.192400\n",
      "Epoch   39, Training Loss: 0.1354, Validation Loss: 0.1733, Learning Rate: 0.192200\n",
      "Epoch   40, Training Loss: 0.1340, Validation Loss: 0.1721, Learning Rate: 0.192000\n",
      "Epoch   41, Training Loss: 0.1327, Validation Loss: 0.1711, Learning Rate: 0.191800\n",
      "Epoch   42, Training Loss: 0.1314, Validation Loss: 0.1706, Learning Rate: 0.191600\n",
      "Epoch   43, Training Loss: 0.1302, Validation Loss: 0.1696, Learning Rate: 0.191400\n",
      "Epoch   44, Training Loss: 0.1290, Validation Loss: 0.1684, Learning Rate: 0.191200\n",
      "Epoch   45, Training Loss: 0.1277, Validation Loss: 0.1676, Learning Rate: 0.191000\n",
      "Epoch   46, Training Loss: 0.1267, Validation Loss: 0.1661, Learning Rate: 0.190800\n",
      "Epoch   47, Training Loss: 0.1255, Validation Loss: 0.1662, Learning Rate: 0.190600\n",
      "Epoch   48, Training Loss: 0.1243, Validation Loss: 0.1648, Learning Rate: 0.190400\n",
      "Epoch   49, Training Loss: 0.1232, Validation Loss: 0.1637, Learning Rate: 0.190200\n",
      "Epoch   50, Training Loss: 0.1222, Validation Loss: 0.1629, Learning Rate: 0.190000\n",
      "Epoch   51, Training Loss: 0.1212, Validation Loss: 0.1632, Learning Rate: 0.189800\n",
      "Epoch   52, Training Loss: 0.1202, Validation Loss: 0.1622, Learning Rate: 0.189600\n",
      "Epoch   53, Training Loss: 0.1194, Validation Loss: 0.1620, Learning Rate: 0.189400\n",
      "Epoch   54, Training Loss: 0.1181, Validation Loss: 0.1598, Learning Rate: 0.189200\n",
      "Epoch   55, Training Loss: 0.1172, Validation Loss: 0.1592, Learning Rate: 0.189000\n",
      "Epoch   56, Training Loss: 0.1163, Validation Loss: 0.1588, Learning Rate: 0.188800\n",
      "Epoch   57, Training Loss: 0.1153, Validation Loss: 0.1580, Learning Rate: 0.188600\n",
      "Epoch   58, Training Loss: 0.1144, Validation Loss: 0.1568, Learning Rate: 0.188400\n",
      "Epoch   59, Training Loss: 0.1137, Validation Loss: 0.1552, Learning Rate: 0.188200\n",
      "Epoch   60, Training Loss: 0.1127, Validation Loss: 0.1547, Learning Rate: 0.188000\n",
      "Epoch   61, Training Loss: 0.1119, Validation Loss: 0.1537, Learning Rate: 0.187800\n",
      "Epoch   62, Training Loss: 0.1111, Validation Loss: 0.1529, Learning Rate: 0.187600\n",
      "Epoch   63, Training Loss: 0.1101, Validation Loss: 0.1527, Learning Rate: 0.187400\n",
      "Epoch   64, Training Loss: 0.1093, Validation Loss: 0.1528, Learning Rate: 0.187200\n",
      "Epoch   65, Training Loss: 0.1084, Validation Loss: 0.1514, Learning Rate: 0.187000\n",
      "Epoch   66, Training Loss: 0.1076, Validation Loss: 0.1514, Learning Rate: 0.186800\n",
      "Epoch   67, Training Loss: 0.1068, Validation Loss: 0.1498, Learning Rate: 0.186600\n",
      "Epoch   68, Training Loss: 0.1060, Validation Loss: 0.1490, Learning Rate: 0.186400\n",
      "Epoch   69, Training Loss: 0.1053, Validation Loss: 0.1496, Learning Rate: 0.186200\n",
      "Epoch   70, Training Loss: 0.1044, Validation Loss: 0.1478, Learning Rate: 0.186000\n",
      "Epoch   71, Training Loss: 0.1036, Validation Loss: 0.1472, Learning Rate: 0.185800\n",
      "Epoch   72, Training Loss: 0.1029, Validation Loss: 0.1466, Learning Rate: 0.185600\n",
      "Epoch   73, Training Loss: 0.1021, Validation Loss: 0.1457, Learning Rate: 0.185400\n",
      "Epoch   74, Training Loss: 0.1014, Validation Loss: 0.1455, Learning Rate: 0.185200\n",
      "Epoch   75, Training Loss: 0.1008, Validation Loss: 0.1437, Learning Rate: 0.185000\n",
      "Epoch   76, Training Loss: 0.0999, Validation Loss: 0.1440, Learning Rate: 0.184800\n",
      "Epoch   77, Training Loss: 0.0996, Validation Loss: 0.1451, Learning Rate: 0.184600\n",
      "Epoch   78, Training Loss: 0.0986, Validation Loss: 0.1436, Learning Rate: 0.184400\n",
      "Epoch   79, Training Loss: 0.0978, Validation Loss: 0.1411, Learning Rate: 0.184200\n",
      "Epoch   80, Training Loss: 0.0971, Validation Loss: 0.1404, Learning Rate: 0.184000\n",
      "Epoch   81, Training Loss: 0.0963, Validation Loss: 0.1403, Learning Rate: 0.183800\n",
      "Epoch   82, Training Loss: 0.0955, Validation Loss: 0.1398, Learning Rate: 0.183600\n",
      "Epoch   83, Training Loss: 0.0951, Validation Loss: 0.1382, Learning Rate: 0.183400\n",
      "Epoch   84, Training Loss: 0.0943, Validation Loss: 0.1378, Learning Rate: 0.183200\n",
      "Epoch   85, Training Loss: 0.0935, Validation Loss: 0.1378, Learning Rate: 0.183000\n",
      "Epoch   86, Training Loss: 0.0928, Validation Loss: 0.1372, Learning Rate: 0.182800\n",
      "Epoch   87, Training Loss: 0.0923, Validation Loss: 0.1374, Learning Rate: 0.182600\n",
      "Epoch   88, Training Loss: 0.0915, Validation Loss: 0.1361, Learning Rate: 0.182400\n",
      "Epoch   89, Training Loss: 0.0909, Validation Loss: 0.1352, Learning Rate: 0.182200\n",
      "Epoch   90, Training Loss: 0.0903, Validation Loss: 0.1345, Learning Rate: 0.182000\n",
      "Epoch   91, Training Loss: 0.0896, Validation Loss: 0.1340, Learning Rate: 0.181800\n",
      "Epoch   92, Training Loss: 0.0890, Validation Loss: 0.1335, Learning Rate: 0.181600\n",
      "Epoch   93, Training Loss: 0.0884, Validation Loss: 0.1325, Learning Rate: 0.181400\n",
      "Epoch   94, Training Loss: 0.0878, Validation Loss: 0.1318, Learning Rate: 0.181200\n",
      "Epoch   95, Training Loss: 0.0872, Validation Loss: 0.1319, Learning Rate: 0.181000\n",
      "Epoch   96, Training Loss: 0.0866, Validation Loss: 0.1312, Learning Rate: 0.180800\n",
      "Epoch   97, Training Loss: 0.0862, Validation Loss: 0.1296, Learning Rate: 0.180600\n",
      "Epoch   98, Training Loss: 0.0856, Validation Loss: 0.1307, Learning Rate: 0.180400\n",
      "Epoch   99, Training Loss: 0.0849, Validation Loss: 0.1287, Learning Rate: 0.180200\n",
      "Epoch  100, Training Loss: 0.0843, Validation Loss: 0.1284, Learning Rate: 0.180000\n",
      "Epoch  101, Training Loss: 0.0841, Validation Loss: 0.1272, Learning Rate: 0.179800\n",
      "Epoch  102, Training Loss: 0.0833, Validation Loss: 0.1272, Learning Rate: 0.179600\n",
      "Epoch  103, Training Loss: 0.0828, Validation Loss: 0.1279, Learning Rate: 0.179400\n",
      "Epoch  104, Training Loss: 0.0821, Validation Loss: 0.1265, Learning Rate: 0.179200\n",
      "Epoch  105, Training Loss: 0.0816, Validation Loss: 0.1259, Learning Rate: 0.179000\n",
      "Epoch  106, Training Loss: 0.0811, Validation Loss: 0.1259, Learning Rate: 0.178800\n",
      "Epoch  107, Training Loss: 0.0805, Validation Loss: 0.1246, Learning Rate: 0.178600\n",
      "Epoch  108, Training Loss: 0.0803, Validation Loss: 0.1232, Learning Rate: 0.178400\n",
      "Epoch  109, Training Loss: 0.0796, Validation Loss: 0.1242, Learning Rate: 0.178200\n",
      "Epoch  110, Training Loss: 0.0793, Validation Loss: 0.1246, Learning Rate: 0.178000\n",
      "Epoch  111, Training Loss: 0.0785, Validation Loss: 0.1232, Learning Rate: 0.177800\n",
      "Epoch  112, Training Loss: 0.0781, Validation Loss: 0.1229, Learning Rate: 0.177600\n",
      "Epoch  113, Training Loss: 0.0776, Validation Loss: 0.1206, Learning Rate: 0.177400\n",
      "Epoch  114, Training Loss: 0.0772, Validation Loss: 0.1198, Learning Rate: 0.177200\n",
      "Epoch  115, Training Loss: 0.0766, Validation Loss: 0.1211, Learning Rate: 0.177000\n",
      "Epoch  116, Training Loss: 0.0759, Validation Loss: 0.1198, Learning Rate: 0.176800\n",
      "Epoch  117, Training Loss: 0.0759, Validation Loss: 0.1210, Learning Rate: 0.176600\n",
      "Epoch  118, Training Loss: 0.0751, Validation Loss: 0.1182, Learning Rate: 0.176400\n",
      "Epoch  119, Training Loss: 0.0746, Validation Loss: 0.1177, Learning Rate: 0.176200\n",
      "Epoch  120, Training Loss: 0.0741, Validation Loss: 0.1181, Learning Rate: 0.176000\n",
      "Epoch  121, Training Loss: 0.0736, Validation Loss: 0.1168, Learning Rate: 0.175800\n",
      "Epoch  122, Training Loss: 0.0733, Validation Loss: 0.1173, Learning Rate: 0.175600\n",
      "Epoch  123, Training Loss: 0.0729, Validation Loss: 0.1156, Learning Rate: 0.175400\n",
      "Epoch  124, Training Loss: 0.0722, Validation Loss: 0.1156, Learning Rate: 0.175200\n",
      "Epoch  125, Training Loss: 0.0718, Validation Loss: 0.1146, Learning Rate: 0.175000\n",
      "Epoch  126, Training Loss: 0.0714, Validation Loss: 0.1148, Learning Rate: 0.174800\n",
      "Epoch  127, Training Loss: 0.0709, Validation Loss: 0.1135, Learning Rate: 0.174600\n",
      "Epoch  128, Training Loss: 0.0709, Validation Loss: 0.1153, Learning Rate: 0.174400\n",
      "Epoch  129, Training Loss: 0.0700, Validation Loss: 0.1129, Learning Rate: 0.174200\n",
      "Epoch  130, Training Loss: 0.0702, Validation Loss: 0.1115, Learning Rate: 0.174000\n",
      "Epoch  131, Training Loss: 0.0692, Validation Loss: 0.1120, Learning Rate: 0.173800\n",
      "Epoch  132, Training Loss: 0.0688, Validation Loss: 0.1118, Learning Rate: 0.173600\n",
      "Epoch  133, Training Loss: 0.0684, Validation Loss: 0.1104, Learning Rate: 0.173400\n",
      "Epoch  134, Training Loss: 0.0680, Validation Loss: 0.1102, Learning Rate: 0.173200\n",
      "Epoch  135, Training Loss: 0.0676, Validation Loss: 0.1105, Learning Rate: 0.173000\n",
      "Epoch  136, Training Loss: 0.0675, Validation Loss: 0.1110, Learning Rate: 0.172800\n",
      "Epoch  137, Training Loss: 0.0671, Validation Loss: 0.1106, Learning Rate: 0.172600\n",
      "Epoch  138, Training Loss: 0.0671, Validation Loss: 0.1070, Learning Rate: 0.172400\n",
      "Epoch  139, Training Loss: 0.0660, Validation Loss: 0.1076, Learning Rate: 0.172200\n",
      "Epoch  140, Training Loss: 0.0656, Validation Loss: 0.1076, Learning Rate: 0.172000\n",
      "Epoch  141, Training Loss: 0.0652, Validation Loss: 0.1075, Learning Rate: 0.171800\n",
      "Epoch  142, Training Loss: 0.0648, Validation Loss: 0.1060, Learning Rate: 0.171600\n",
      "Epoch  143, Training Loss: 0.0645, Validation Loss: 0.1055, Learning Rate: 0.171400\n",
      "Epoch  144, Training Loss: 0.0640, Validation Loss: 0.1057, Learning Rate: 0.171200\n",
      "Epoch  145, Training Loss: 0.0637, Validation Loss: 0.1057, Learning Rate: 0.171000\n",
      "Epoch  146, Training Loss: 0.0633, Validation Loss: 0.1050, Learning Rate: 0.170800\n",
      "Epoch  147, Training Loss: 0.0630, Validation Loss: 0.1050, Learning Rate: 0.170600\n",
      "Epoch  148, Training Loss: 0.0627, Validation Loss: 0.1033, Learning Rate: 0.170400\n",
      "Epoch  149, Training Loss: 0.0623, Validation Loss: 0.1029, Learning Rate: 0.170200\n",
      "Epoch  150, Training Loss: 0.0619, Validation Loss: 0.1029, Learning Rate: 0.170000\n",
      "Epoch  151, Training Loss: 0.0619, Validation Loss: 0.1015, Learning Rate: 0.169800\n",
      "Epoch  152, Training Loss: 0.0613, Validation Loss: 0.1014, Learning Rate: 0.169600\n",
      "Epoch  153, Training Loss: 0.0610, Validation Loss: 0.1024, Learning Rate: 0.169400\n",
      "Epoch  154, Training Loss: 0.0606, Validation Loss: 0.1021, Learning Rate: 0.169200\n",
      "Epoch  155, Training Loss: 0.0605, Validation Loss: 0.1024, Learning Rate: 0.169000\n",
      "Epoch  156, Training Loss: 0.0601, Validation Loss: 0.0997, Learning Rate: 0.168800\n",
      "Epoch  157, Training Loss: 0.0596, Validation Loss: 0.0998, Learning Rate: 0.168600\n",
      "Epoch  158, Training Loss: 0.0593, Validation Loss: 0.0993, Learning Rate: 0.168400\n",
      "Epoch  159, Training Loss: 0.0594, Validation Loss: 0.1012, Learning Rate: 0.168200\n",
      "Epoch  160, Training Loss: 0.0586, Validation Loss: 0.0988, Learning Rate: 0.168000\n",
      "Epoch  161, Training Loss: 0.0583, Validation Loss: 0.0980, Learning Rate: 0.167800\n",
      "Epoch  162, Training Loss: 0.0581, Validation Loss: 0.0990, Learning Rate: 0.167600\n",
      "Epoch  163, Training Loss: 0.0578, Validation Loss: 0.0964, Learning Rate: 0.167400\n",
      "Epoch  164, Training Loss: 0.0575, Validation Loss: 0.0981, Learning Rate: 0.167200\n",
      "Epoch  165, Training Loss: 0.0570, Validation Loss: 0.0966, Learning Rate: 0.167000\n",
      "Epoch  166, Training Loss: 0.0567, Validation Loss: 0.0963, Learning Rate: 0.166800\n",
      "Epoch  167, Training Loss: 0.0568, Validation Loss: 0.0974, Learning Rate: 0.166600\n",
      "Epoch  168, Training Loss: 0.0561, Validation Loss: 0.0950, Learning Rate: 0.166400\n",
      "Epoch  169, Training Loss: 0.0558, Validation Loss: 0.0954, Learning Rate: 0.166200\n",
      "Epoch  170, Training Loss: 0.0555, Validation Loss: 0.0949, Learning Rate: 0.166000\n",
      "Epoch  171, Training Loss: 0.0552, Validation Loss: 0.0937, Learning Rate: 0.165800\n",
      "Epoch  172, Training Loss: 0.0549, Validation Loss: 0.0933, Learning Rate: 0.165600\n",
      "Epoch  173, Training Loss: 0.0546, Validation Loss: 0.0933, Learning Rate: 0.165400\n",
      "Epoch  174, Training Loss: 0.0544, Validation Loss: 0.0922, Learning Rate: 0.165200\n",
      "Epoch  175, Training Loss: 0.0541, Validation Loss: 0.0921, Learning Rate: 0.165000\n",
      "Epoch  176, Training Loss: 0.0539, Validation Loss: 0.0914, Learning Rate: 0.164800\n",
      "Epoch  177, Training Loss: 0.0537, Validation Loss: 0.0927, Learning Rate: 0.164600\n",
      "Epoch  178, Training Loss: 0.0542, Validation Loss: 0.0943, Learning Rate: 0.164400\n",
      "Epoch  179, Training Loss: 0.0530, Validation Loss: 0.0911, Learning Rate: 0.164200\n",
      "Epoch  180, Training Loss: 0.0530, Validation Loss: 0.0894, Learning Rate: 0.164000\n",
      "Epoch  181, Training Loss: 0.0528, Validation Loss: 0.0914, Learning Rate: 0.163800\n",
      "Epoch  182, Training Loss: 0.0522, Validation Loss: 0.0895, Learning Rate: 0.163600\n",
      "Epoch  183, Training Loss: 0.0520, Validation Loss: 0.0892, Learning Rate: 0.163400\n",
      "Epoch  184, Training Loss: 0.0517, Validation Loss: 0.0887, Learning Rate: 0.163200\n",
      "Epoch  185, Training Loss: 0.0515, Validation Loss: 0.0882, Learning Rate: 0.163000\n",
      "Epoch  186, Training Loss: 0.0513, Validation Loss: 0.0884, Learning Rate: 0.162800\n",
      "Epoch  187, Training Loss: 0.0510, Validation Loss: 0.0880, Learning Rate: 0.162600\n",
      "Epoch  188, Training Loss: 0.0508, Validation Loss: 0.0876, Learning Rate: 0.162400\n",
      "Epoch  189, Training Loss: 0.0508, Validation Loss: 0.0861, Learning Rate: 0.162200\n",
      "Epoch  190, Training Loss: 0.0505, Validation Loss: 0.0858, Learning Rate: 0.162000\n",
      "Epoch  191, Training Loss: 0.0501, Validation Loss: 0.0861, Learning Rate: 0.161800\n",
      "Epoch  192, Training Loss: 0.0499, Validation Loss: 0.0856, Learning Rate: 0.161600\n",
      "Epoch  193, Training Loss: 0.0496, Validation Loss: 0.0860, Learning Rate: 0.161400\n",
      "Epoch  194, Training Loss: 0.0494, Validation Loss: 0.0849, Learning Rate: 0.161200\n",
      "Epoch  195, Training Loss: 0.0494, Validation Loss: 0.0841, Learning Rate: 0.161000\n",
      "Epoch  196, Training Loss: 0.0490, Validation Loss: 0.0843, Learning Rate: 0.160800\n",
      "Epoch  197, Training Loss: 0.0488, Validation Loss: 0.0836, Learning Rate: 0.160600\n",
      "Epoch  198, Training Loss: 0.0486, Validation Loss: 0.0833, Learning Rate: 0.160400\n",
      "Epoch  199, Training Loss: 0.0484, Validation Loss: 0.0825, Learning Rate: 0.160200\n",
      "Epoch  200, Training Loss: 0.0482, Validation Loss: 0.0826, Learning Rate: 0.160000\n",
      "Epoch  201, Training Loss: 0.0479, Validation Loss: 0.0828, Learning Rate: 0.159800\n",
      "Epoch  202, Training Loss: 0.0477, Validation Loss: 0.0825, Learning Rate: 0.159600\n",
      "Epoch  203, Training Loss: 0.0476, Validation Loss: 0.0827, Learning Rate: 0.159400\n",
      "Epoch  204, Training Loss: 0.0474, Validation Loss: 0.0816, Learning Rate: 0.159200\n",
      "Epoch  205, Training Loss: 0.0471, Validation Loss: 0.0813, Learning Rate: 0.159000\n",
      "Epoch  206, Training Loss: 0.0470, Validation Loss: 0.0808, Learning Rate: 0.158800\n",
      "Epoch  207, Training Loss: 0.0469, Validation Loss: 0.0816, Learning Rate: 0.158600\n",
      "Epoch  208, Training Loss: 0.0466, Validation Loss: 0.0804, Learning Rate: 0.158400\n",
      "Epoch  209, Training Loss: 0.0467, Validation Loss: 0.0818, Learning Rate: 0.158200\n",
      "Epoch  210, Training Loss: 0.0462, Validation Loss: 0.0798, Learning Rate: 0.158000\n",
      "Epoch  211, Training Loss: 0.0461, Validation Loss: 0.0793, Learning Rate: 0.157800\n",
      "Epoch  212, Training Loss: 0.0459, Validation Loss: 0.0799, Learning Rate: 0.157600\n",
      "Epoch  213, Training Loss: 0.0459, Validation Loss: 0.0804, Learning Rate: 0.157400\n",
      "Epoch  214, Training Loss: 0.0457, Validation Loss: 0.0783, Learning Rate: 0.157200\n",
      "Epoch  215, Training Loss: 0.0455, Validation Loss: 0.0780, Learning Rate: 0.157000\n",
      "Epoch  216, Training Loss: 0.0452, Validation Loss: 0.0787, Learning Rate: 0.156800\n",
      "Epoch  217, Training Loss: 0.0451, Validation Loss: 0.0786, Learning Rate: 0.156600\n",
      "Epoch  218, Training Loss: 0.0449, Validation Loss: 0.0775, Learning Rate: 0.156400\n",
      "Epoch  219, Training Loss: 0.0449, Validation Loss: 0.0784, Learning Rate: 0.156200\n",
      "Epoch  220, Training Loss: 0.0448, Validation Loss: 0.0763, Learning Rate: 0.156000\n",
      "Epoch  221, Training Loss: 0.0445, Validation Loss: 0.0764, Learning Rate: 0.155800\n",
      "Epoch  222, Training Loss: 0.0443, Validation Loss: 0.0770, Learning Rate: 0.155600\n",
      "Epoch  223, Training Loss: 0.0442, Validation Loss: 0.0771, Learning Rate: 0.155400\n",
      "Epoch  224, Training Loss: 0.0440, Validation Loss: 0.0762, Learning Rate: 0.155200\n",
      "Epoch  225, Training Loss: 0.0438, Validation Loss: 0.0757, Learning Rate: 0.155000\n",
      "Epoch  226, Training Loss: 0.0437, Validation Loss: 0.0753, Learning Rate: 0.154800\n",
      "Epoch  227, Training Loss: 0.0436, Validation Loss: 0.0750, Learning Rate: 0.154600\n",
      "Epoch  228, Training Loss: 0.0434, Validation Loss: 0.0754, Learning Rate: 0.154400\n",
      "Epoch  229, Training Loss: 0.0433, Validation Loss: 0.0752, Learning Rate: 0.154200\n",
      "Epoch  230, Training Loss: 0.0432, Validation Loss: 0.0750, Learning Rate: 0.154000\n",
      "Epoch  231, Training Loss: 0.0430, Validation Loss: 0.0740, Learning Rate: 0.153800\n",
      "Epoch  232, Training Loss: 0.0429, Validation Loss: 0.0745, Learning Rate: 0.153600\n",
      "Epoch  233, Training Loss: 0.0428, Validation Loss: 0.0744, Learning Rate: 0.153400\n",
      "Epoch  234, Training Loss: 0.0427, Validation Loss: 0.0744, Learning Rate: 0.153200\n",
      "Epoch  235, Training Loss: 0.0425, Validation Loss: 0.0730, Learning Rate: 0.153000\n",
      "Epoch  236, Training Loss: 0.0424, Validation Loss: 0.0733, Learning Rate: 0.152800\n",
      "Epoch  237, Training Loss: 0.0424, Validation Loss: 0.0723, Learning Rate: 0.152600\n",
      "Epoch  238, Training Loss: 0.0421, Validation Loss: 0.0727, Learning Rate: 0.152400\n",
      "Epoch  239, Training Loss: 0.0421, Validation Loss: 0.0720, Learning Rate: 0.152200\n",
      "Epoch  240, Training Loss: 0.0419, Validation Loss: 0.0723, Learning Rate: 0.152000\n",
      "Epoch  241, Training Loss: 0.0418, Validation Loss: 0.0719, Learning Rate: 0.151800\n",
      "Epoch  242, Training Loss: 0.0417, Validation Loss: 0.0717, Learning Rate: 0.151600\n",
      "Epoch  243, Training Loss: 0.0416, Validation Loss: 0.0716, Learning Rate: 0.151400\n",
      "Epoch  244, Training Loss: 0.0414, Validation Loss: 0.0716, Learning Rate: 0.151200\n",
      "Epoch  245, Training Loss: 0.0416, Validation Loss: 0.0726, Learning Rate: 0.151000\n",
      "Epoch  246, Training Loss: 0.0412, Validation Loss: 0.0707, Learning Rate: 0.150800\n",
      "Epoch  247, Training Loss: 0.0411, Validation Loss: 0.0708, Learning Rate: 0.150600\n",
      "Epoch  248, Training Loss: 0.0410, Validation Loss: 0.0701, Learning Rate: 0.150400\n",
      "Epoch  249, Training Loss: 0.0409, Validation Loss: 0.0699, Learning Rate: 0.150200\n",
      "Epoch  250, Training Loss: 0.0408, Validation Loss: 0.0698, Learning Rate: 0.150000\n",
      "Epoch  251, Training Loss: 0.0408, Validation Loss: 0.0704, Learning Rate: 0.149800\n",
      "Epoch  252, Training Loss: 0.0406, Validation Loss: 0.0697, Learning Rate: 0.149600\n",
      "Epoch  253, Training Loss: 0.0405, Validation Loss: 0.0695, Learning Rate: 0.149400\n",
      "Epoch  254, Training Loss: 0.0404, Validation Loss: 0.0693, Learning Rate: 0.149200\n",
      "Epoch  255, Training Loss: 0.0404, Validation Loss: 0.0697, Learning Rate: 0.149000\n",
      "Epoch  256, Training Loss: 0.0402, Validation Loss: 0.0691, Learning Rate: 0.148800\n",
      "Epoch  257, Training Loss: 0.0401, Validation Loss: 0.0690, Learning Rate: 0.148600\n",
      "Epoch  258, Training Loss: 0.0401, Validation Loss: 0.0689, Learning Rate: 0.148400\n",
      "Epoch  259, Training Loss: 0.0399, Validation Loss: 0.0683, Learning Rate: 0.148200\n",
      "Epoch  260, Training Loss: 0.0398, Validation Loss: 0.0680, Learning Rate: 0.148000\n",
      "Epoch  261, Training Loss: 0.0398, Validation Loss: 0.0673, Learning Rate: 0.147800\n",
      "Epoch  262, Training Loss: 0.0397, Validation Loss: 0.0675, Learning Rate: 0.147600\n",
      "Epoch  263, Training Loss: 0.0397, Validation Loss: 0.0670, Learning Rate: 0.147400\n",
      "Epoch  264, Training Loss: 0.0395, Validation Loss: 0.0675, Learning Rate: 0.147200\n",
      "Epoch  265, Training Loss: 0.0394, Validation Loss: 0.0672, Learning Rate: 0.147000\n",
      "Epoch  266, Training Loss: 0.0393, Validation Loss: 0.0668, Learning Rate: 0.146800\n",
      "Epoch  267, Training Loss: 0.0393, Validation Loss: 0.0671, Learning Rate: 0.146600\n",
      "Epoch  268, Training Loss: 0.0392, Validation Loss: 0.0667, Learning Rate: 0.146400\n",
      "Epoch  269, Training Loss: 0.0392, Validation Loss: 0.0658, Learning Rate: 0.146200\n",
      "Epoch  270, Training Loss: 0.0391, Validation Loss: 0.0659, Learning Rate: 0.146000\n",
      "Epoch  271, Training Loss: 0.0389, Validation Loss: 0.0660, Learning Rate: 0.145800\n",
      "Epoch  272, Training Loss: 0.0390, Validation Loss: 0.0666, Learning Rate: 0.145600\n",
      "Epoch  273, Training Loss: 0.0388, Validation Loss: 0.0661, Learning Rate: 0.145400\n",
      "Epoch  274, Training Loss: 0.0388, Validation Loss: 0.0660, Learning Rate: 0.145200\n",
      "Epoch  275, Training Loss: 0.0389, Validation Loss: 0.0666, Learning Rate: 0.145000\n",
      "Epoch  276, Training Loss: 0.0386, Validation Loss: 0.0652, Learning Rate: 0.144800\n",
      "Epoch  277, Training Loss: 0.0385, Validation Loss: 0.0654, Learning Rate: 0.144600\n",
      "Epoch  278, Training Loss: 0.0385, Validation Loss: 0.0647, Learning Rate: 0.144400\n",
      "Epoch  279, Training Loss: 0.0384, Validation Loss: 0.0653, Learning Rate: 0.144200\n",
      "Epoch  280, Training Loss: 0.0383, Validation Loss: 0.0648, Learning Rate: 0.144000\n",
      "Epoch  281, Training Loss: 0.0382, Validation Loss: 0.0645, Learning Rate: 0.143800\n",
      "Epoch  282, Training Loss: 0.0382, Validation Loss: 0.0643, Learning Rate: 0.143600\n",
      "Epoch  283, Training Loss: 0.0381, Validation Loss: 0.0637, Learning Rate: 0.143400\n",
      "Epoch  284, Training Loss: 0.0381, Validation Loss: 0.0635, Learning Rate: 0.143200\n",
      "Epoch  285, Training Loss: 0.0380, Validation Loss: 0.0635, Learning Rate: 0.143000\n",
      "Epoch  286, Training Loss: 0.0380, Validation Loss: 0.0630, Learning Rate: 0.142800\n",
      "Epoch  287, Training Loss: 0.0379, Validation Loss: 0.0635, Learning Rate: 0.142600\n",
      "Epoch  288, Training Loss: 0.0380, Validation Loss: 0.0646, Learning Rate: 0.142400\n",
      "Epoch  289, Training Loss: 0.0378, Validation Loss: 0.0636, Learning Rate: 0.142200\n",
      "Epoch  290, Training Loss: 0.0377, Validation Loss: 0.0631, Learning Rate: 0.142000\n",
      "Epoch  291, Training Loss: 0.0377, Validation Loss: 0.0634, Learning Rate: 0.141800\n",
      "Epoch  292, Training Loss: 0.0376, Validation Loss: 0.0632, Learning Rate: 0.141600\n",
      "Epoch  293, Training Loss: 0.0376, Validation Loss: 0.0620, Learning Rate: 0.141400\n",
      "Epoch  294, Training Loss: 0.0375, Validation Loss: 0.0623, Learning Rate: 0.141200\n",
      "Epoch  295, Training Loss: 0.0374, Validation Loss: 0.0621, Learning Rate: 0.141000\n",
      "Epoch  296, Training Loss: 0.0374, Validation Loss: 0.0625, Learning Rate: 0.140800\n",
      "Epoch  297, Training Loss: 0.0373, Validation Loss: 0.0622, Learning Rate: 0.140600\n",
      "Epoch  298, Training Loss: 0.0372, Validation Loss: 0.0617, Learning Rate: 0.140400\n",
      "Epoch  299, Training Loss: 0.0372, Validation Loss: 0.0620, Learning Rate: 0.140200\n",
      "Epoch  300, Training Loss: 0.0371, Validation Loss: 0.0617, Learning Rate: 0.140000\n",
      "Epoch  301, Training Loss: 0.0371, Validation Loss: 0.0617, Learning Rate: 0.139800\n",
      "Epoch  302, Training Loss: 0.0370, Validation Loss: 0.0612, Learning Rate: 0.139600\n",
      "Epoch  303, Training Loss: 0.0371, Validation Loss: 0.0607, Learning Rate: 0.139400\n",
      "Epoch  304, Training Loss: 0.0369, Validation Loss: 0.0608, Learning Rate: 0.139200\n",
      "Epoch  305, Training Loss: 0.0369, Validation Loss: 0.0613, Learning Rate: 0.139000\n",
      "Epoch  306, Training Loss: 0.0369, Validation Loss: 0.0607, Learning Rate: 0.138800\n",
      "Epoch  307, Training Loss: 0.0369, Validation Loss: 0.0603, Learning Rate: 0.138600\n",
      "Epoch  308, Training Loss: 0.0368, Validation Loss: 0.0604, Learning Rate: 0.138400\n",
      "Epoch  309, Training Loss: 0.0367, Validation Loss: 0.0603, Learning Rate: 0.138200\n",
      "Epoch  310, Training Loss: 0.0367, Validation Loss: 0.0605, Learning Rate: 0.138000\n",
      "Epoch  311, Training Loss: 0.0367, Validation Loss: 0.0608, Learning Rate: 0.137800\n",
      "Epoch  312, Training Loss: 0.0366, Validation Loss: 0.0605, Learning Rate: 0.137600\n",
      "Epoch  313, Training Loss: 0.0367, Validation Loss: 0.0593, Learning Rate: 0.137400\n",
      "Epoch  314, Training Loss: 0.0365, Validation Loss: 0.0599, Learning Rate: 0.137200\n",
      "Epoch  315, Training Loss: 0.0365, Validation Loss: 0.0597, Learning Rate: 0.137000\n",
      "Epoch  316, Training Loss: 0.0364, Validation Loss: 0.0593, Learning Rate: 0.136800\n",
      "Epoch  317, Training Loss: 0.0364, Validation Loss: 0.0600, Learning Rate: 0.136600\n",
      "Epoch  318, Training Loss: 0.0363, Validation Loss: 0.0594, Learning Rate: 0.136400\n",
      "Epoch  319, Training Loss: 0.0363, Validation Loss: 0.0594, Learning Rate: 0.136200\n",
      "Epoch  320, Training Loss: 0.0362, Validation Loss: 0.0589, Learning Rate: 0.136000\n",
      "Epoch  321, Training Loss: 0.0362, Validation Loss: 0.0588, Learning Rate: 0.135800\n",
      "Epoch  322, Training Loss: 0.0362, Validation Loss: 0.0589, Learning Rate: 0.135600\n",
      "Epoch  323, Training Loss: 0.0361, Validation Loss: 0.0586, Learning Rate: 0.135400\n",
      "Epoch  324, Training Loss: 0.0361, Validation Loss: 0.0591, Learning Rate: 0.135200\n",
      "Epoch  325, Training Loss: 0.0361, Validation Loss: 0.0589, Learning Rate: 0.135000\n",
      "Epoch  326, Training Loss: 0.0360, Validation Loss: 0.0584, Learning Rate: 0.134800\n",
      "Epoch  327, Training Loss: 0.0360, Validation Loss: 0.0587, Learning Rate: 0.134600\n",
      "Epoch  328, Training Loss: 0.0360, Validation Loss: 0.0584, Learning Rate: 0.134400\n",
      "Epoch  329, Training Loss: 0.0359, Validation Loss: 0.0579, Learning Rate: 0.134200\n",
      "Epoch  330, Training Loss: 0.0359, Validation Loss: 0.0578, Learning Rate: 0.134000\n",
      "Epoch  331, Training Loss: 0.0359, Validation Loss: 0.0578, Learning Rate: 0.133800\n",
      "Epoch  332, Training Loss: 0.0358, Validation Loss: 0.0579, Learning Rate: 0.133600\n",
      "Epoch  333, Training Loss: 0.0358, Validation Loss: 0.0576, Learning Rate: 0.133400\n",
      "Epoch  334, Training Loss: 0.0358, Validation Loss: 0.0576, Learning Rate: 0.133200\n",
      "Epoch  335, Training Loss: 0.0358, Validation Loss: 0.0572, Learning Rate: 0.133000\n",
      "Epoch  336, Training Loss: 0.0357, Validation Loss: 0.0577, Learning Rate: 0.132800\n",
      "Epoch  337, Training Loss: 0.0357, Validation Loss: 0.0575, Learning Rate: 0.132600\n",
      "Epoch  338, Training Loss: 0.0357, Validation Loss: 0.0570, Learning Rate: 0.132400\n",
      "Epoch  339, Training Loss: 0.0356, Validation Loss: 0.0570, Learning Rate: 0.132200\n",
      "Epoch  340, Training Loss: 0.0356, Validation Loss: 0.0569, Learning Rate: 0.132000\n",
      "Epoch  341, Training Loss: 0.0356, Validation Loss: 0.0565, Learning Rate: 0.131800\n",
      "Epoch  342, Training Loss: 0.0355, Validation Loss: 0.0569, Learning Rate: 0.131600\n",
      "Epoch  343, Training Loss: 0.0355, Validation Loss: 0.0565, Learning Rate: 0.131400\n",
      "Epoch  344, Training Loss: 0.0355, Validation Loss: 0.0564, Learning Rate: 0.131200\n",
      "Epoch  345, Training Loss: 0.0355, Validation Loss: 0.0563, Learning Rate: 0.131000\n",
      "Epoch  346, Training Loss: 0.0354, Validation Loss: 0.0564, Learning Rate: 0.130800\n",
      "Epoch  347, Training Loss: 0.0354, Validation Loss: 0.0561, Learning Rate: 0.130600\n",
      "Epoch  348, Training Loss: 0.0354, Validation Loss: 0.0560, Learning Rate: 0.130400\n",
      "Epoch  349, Training Loss: 0.0354, Validation Loss: 0.0564, Learning Rate: 0.130200\n",
      "Epoch  350, Training Loss: 0.0353, Validation Loss: 0.0559, Learning Rate: 0.130000\n",
      "Epoch  351, Training Loss: 0.0353, Validation Loss: 0.0560, Learning Rate: 0.129800\n",
      "Epoch  352, Training Loss: 0.0353, Validation Loss: 0.0560, Learning Rate: 0.129600\n",
      "Epoch  353, Training Loss: 0.0352, Validation Loss: 0.0559, Learning Rate: 0.129400\n",
      "Epoch  354, Training Loss: 0.0352, Validation Loss: 0.0556, Learning Rate: 0.129200\n",
      "Epoch  355, Training Loss: 0.0352, Validation Loss: 0.0558, Learning Rate: 0.129000\n",
      "Epoch  356, Training Loss: 0.0352, Validation Loss: 0.0555, Learning Rate: 0.128800\n",
      "Epoch  357, Training Loss: 0.0352, Validation Loss: 0.0555, Learning Rate: 0.128600\n",
      "Epoch  358, Training Loss: 0.0352, Validation Loss: 0.0559, Learning Rate: 0.128400\n",
      "Epoch  359, Training Loss: 0.0351, Validation Loss: 0.0558, Learning Rate: 0.128200\n",
      "Epoch  360, Training Loss: 0.0351, Validation Loss: 0.0553, Learning Rate: 0.128000\n",
      "Epoch  361, Training Loss: 0.0351, Validation Loss: 0.0551, Learning Rate: 0.127800\n",
      "Epoch  362, Training Loss: 0.0351, Validation Loss: 0.0547, Learning Rate: 0.127600\n",
      "Epoch  363, Training Loss: 0.0350, Validation Loss: 0.0553, Learning Rate: 0.127400\n",
      "Epoch  364, Training Loss: 0.0350, Validation Loss: 0.0548, Learning Rate: 0.127200\n",
      "Epoch  365, Training Loss: 0.0350, Validation Loss: 0.0548, Learning Rate: 0.127000\n",
      "Epoch  366, Training Loss: 0.0350, Validation Loss: 0.0546, Learning Rate: 0.126800\n",
      "Epoch  367, Training Loss: 0.0349, Validation Loss: 0.0547, Learning Rate: 0.126600\n",
      "Epoch  368, Training Loss: 0.0349, Validation Loss: 0.0547, Learning Rate: 0.126400\n",
      "Epoch  369, Training Loss: 0.0349, Validation Loss: 0.0544, Learning Rate: 0.126200\n",
      "Epoch  370, Training Loss: 0.0349, Validation Loss: 0.0546, Learning Rate: 0.126000\n",
      "Epoch  371, Training Loss: 0.0349, Validation Loss: 0.0546, Learning Rate: 0.125800\n",
      "Epoch  372, Training Loss: 0.0349, Validation Loss: 0.0545, Learning Rate: 0.125600\n",
      "Epoch  373, Training Loss: 0.0349, Validation Loss: 0.0541, Learning Rate: 0.125400\n",
      "Epoch  374, Training Loss: 0.0348, Validation Loss: 0.0542, Learning Rate: 0.125200\n",
      "Epoch  375, Training Loss: 0.0348, Validation Loss: 0.0540, Learning Rate: 0.125000\n",
      "Epoch  376, Training Loss: 0.0348, Validation Loss: 0.0540, Learning Rate: 0.124800\n",
      "Epoch  377, Training Loss: 0.0348, Validation Loss: 0.0541, Learning Rate: 0.124600\n",
      "Epoch  378, Training Loss: 0.0348, Validation Loss: 0.0541, Learning Rate: 0.124400\n",
      "Epoch  379, Training Loss: 0.0347, Validation Loss: 0.0542, Learning Rate: 0.124200\n",
      "Epoch  380, Training Loss: 0.0347, Validation Loss: 0.0538, Learning Rate: 0.124000\n",
      "Epoch  381, Training Loss: 0.0347, Validation Loss: 0.0537, Learning Rate: 0.123800\n",
      "Epoch  382, Training Loss: 0.0347, Validation Loss: 0.0538, Learning Rate: 0.123600\n",
      "Epoch  383, Training Loss: 0.0347, Validation Loss: 0.0535, Learning Rate: 0.123400\n",
      "Epoch  384, Training Loss: 0.0347, Validation Loss: 0.0534, Learning Rate: 0.123200\n",
      "Epoch  385, Training Loss: 0.0346, Validation Loss: 0.0534, Learning Rate: 0.123000\n",
      "Epoch  386, Training Loss: 0.0346, Validation Loss: 0.0533, Learning Rate: 0.122800\n",
      "Epoch  387, Training Loss: 0.0346, Validation Loss: 0.0532, Learning Rate: 0.122600\n",
      "Epoch  388, Training Loss: 0.0346, Validation Loss: 0.0534, Learning Rate: 0.122400\n",
      "Epoch  389, Training Loss: 0.0346, Validation Loss: 0.0534, Learning Rate: 0.122200\n",
      "Epoch  390, Training Loss: 0.0346, Validation Loss: 0.0537, Learning Rate: 0.122000\n",
      "Epoch  391, Training Loss: 0.0346, Validation Loss: 0.0531, Learning Rate: 0.121800\n",
      "Epoch  392, Training Loss: 0.0345, Validation Loss: 0.0529, Learning Rate: 0.121600\n",
      "Epoch  393, Training Loss: 0.0345, Validation Loss: 0.0530, Learning Rate: 0.121400\n",
      "Epoch  394, Training Loss: 0.0345, Validation Loss: 0.0531, Learning Rate: 0.121200\n",
      "Epoch  395, Training Loss: 0.0345, Validation Loss: 0.0529, Learning Rate: 0.121000\n",
      "Epoch  396, Training Loss: 0.0345, Validation Loss: 0.0527, Learning Rate: 0.120800\n",
      "Epoch  397, Training Loss: 0.0345, Validation Loss: 0.0529, Learning Rate: 0.120600\n",
      "Epoch  398, Training Loss: 0.0345, Validation Loss: 0.0527, Learning Rate: 0.120400\n",
      "Epoch  399, Training Loss: 0.0345, Validation Loss: 0.0529, Learning Rate: 0.120200\n",
      "Epoch  400, Training Loss: 0.0344, Validation Loss: 0.0525, Learning Rate: 0.120000\n",
      "Epoch  401, Training Loss: 0.0344, Validation Loss: 0.0528, Learning Rate: 0.119800\n",
      "Epoch  402, Training Loss: 0.0344, Validation Loss: 0.0527, Learning Rate: 0.119600\n",
      "Epoch  403, Training Loss: 0.0344, Validation Loss: 0.0526, Learning Rate: 0.119400\n",
      "Epoch  404, Training Loss: 0.0344, Validation Loss: 0.0524, Learning Rate: 0.119200\n",
      "Epoch  405, Training Loss: 0.0344, Validation Loss: 0.0524, Learning Rate: 0.119000\n",
      "Epoch  406, Training Loss: 0.0344, Validation Loss: 0.0524, Learning Rate: 0.118800\n",
      "Epoch  407, Training Loss: 0.0344, Validation Loss: 0.0520, Learning Rate: 0.118600\n",
      "Epoch  408, Training Loss: 0.0344, Validation Loss: 0.0521, Learning Rate: 0.118400\n",
      "Epoch  409, Training Loss: 0.0343, Validation Loss: 0.0522, Learning Rate: 0.118200\n",
      "Epoch  410, Training Loss: 0.0343, Validation Loss: 0.0520, Learning Rate: 0.118000\n",
      "Epoch  411, Training Loss: 0.0343, Validation Loss: 0.0521, Learning Rate: 0.117800\n",
      "Epoch  412, Training Loss: 0.0343, Validation Loss: 0.0520, Learning Rate: 0.117600\n",
      "Epoch  413, Training Loss: 0.0343, Validation Loss: 0.0519, Learning Rate: 0.117400\n",
      "Epoch  414, Training Loss: 0.0343, Validation Loss: 0.0518, Learning Rate: 0.117200\n",
      "Epoch  415, Training Loss: 0.0343, Validation Loss: 0.0519, Learning Rate: 0.117000\n",
      "Epoch  416, Training Loss: 0.0343, Validation Loss: 0.0516, Learning Rate: 0.116800\n",
      "Epoch  417, Training Loss: 0.0343, Validation Loss: 0.0518, Learning Rate: 0.116600\n",
      "Epoch  418, Training Loss: 0.0343, Validation Loss: 0.0517, Learning Rate: 0.116400\n",
      "Epoch  419, Training Loss: 0.0343, Validation Loss: 0.0518, Learning Rate: 0.116200\n",
      "Epoch  420, Training Loss: 0.0342, Validation Loss: 0.0516, Learning Rate: 0.116000\n",
      "Epoch  421, Training Loss: 0.0342, Validation Loss: 0.0515, Learning Rate: 0.115800\n",
      "Epoch  422, Training Loss: 0.0342, Validation Loss: 0.0514, Learning Rate: 0.115600\n",
      "Epoch  423, Training Loss: 0.0342, Validation Loss: 0.0512, Learning Rate: 0.115400\n",
      "Epoch  424, Training Loss: 0.0342, Validation Loss: 0.0511, Learning Rate: 0.115200\n",
      "Epoch  425, Training Loss: 0.0342, Validation Loss: 0.0513, Learning Rate: 0.115000\n",
      "Epoch  426, Training Loss: 0.0342, Validation Loss: 0.0515, Learning Rate: 0.114800\n",
      "Epoch  427, Training Loss: 0.0342, Validation Loss: 0.0514, Learning Rate: 0.114600\n",
      "Epoch  428, Training Loss: 0.0342, Validation Loss: 0.0513, Learning Rate: 0.114400\n",
      "Epoch  429, Training Loss: 0.0342, Validation Loss: 0.0512, Learning Rate: 0.114200\n",
      "Epoch  430, Training Loss: 0.0342, Validation Loss: 0.0512, Learning Rate: 0.114000\n",
      "Epoch  431, Training Loss: 0.0342, Validation Loss: 0.0510, Learning Rate: 0.113800\n",
      "Epoch  432, Training Loss: 0.0342, Validation Loss: 0.0508, Learning Rate: 0.113600\n",
      "Epoch  433, Training Loss: 0.0342, Validation Loss: 0.0508, Learning Rate: 0.113400\n",
      "Epoch  434, Training Loss: 0.0341, Validation Loss: 0.0509, Learning Rate: 0.113200\n",
      "Epoch  435, Training Loss: 0.0341, Validation Loss: 0.0508, Learning Rate: 0.113000\n",
      "Epoch  436, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.112800\n",
      "Epoch  437, Training Loss: 0.0341, Validation Loss: 0.0507, Learning Rate: 0.112600\n",
      "Epoch  438, Training Loss: 0.0341, Validation Loss: 0.0509, Learning Rate: 0.112400\n",
      "Epoch  439, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.112200\n",
      "Epoch  440, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.112000\n",
      "Epoch  441, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.111800\n",
      "Epoch  442, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.111600\n",
      "Epoch  443, Training Loss: 0.0341, Validation Loss: 0.0503, Learning Rate: 0.111400\n",
      "Epoch  444, Training Loss: 0.0341, Validation Loss: 0.0504, Learning Rate: 0.111200\n",
      "Epoch  445, Training Loss: 0.0341, Validation Loss: 0.0505, Learning Rate: 0.111000\n",
      "Epoch  446, Training Loss: 0.0341, Validation Loss: 0.0502, Learning Rate: 0.110800\n",
      "Epoch  447, Training Loss: 0.0341, Validation Loss: 0.0502, Learning Rate: 0.110600\n",
      "Epoch  448, Training Loss: 0.0341, Validation Loss: 0.0502, Learning Rate: 0.110400\n",
      "Epoch  449, Training Loss: 0.0340, Validation Loss: 0.0502, Learning Rate: 0.110200\n",
      "Epoch  450, Training Loss: 0.0340, Validation Loss: 0.0502, Learning Rate: 0.110000\n",
      "Epoch  451, Training Loss: 0.0340, Validation Loss: 0.0501, Learning Rate: 0.109800\n",
      "Epoch  452, Training Loss: 0.0340, Validation Loss: 0.0502, Learning Rate: 0.109600\n",
      "Epoch  453, Training Loss: 0.0340, Validation Loss: 0.0501, Learning Rate: 0.109400\n",
      "Epoch  454, Training Loss: 0.0340, Validation Loss: 0.0500, Learning Rate: 0.109200\n",
      "Epoch  455, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.109000\n",
      "Epoch  456, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.108800\n",
      "Epoch  457, Training Loss: 0.0340, Validation Loss: 0.0497, Learning Rate: 0.108600\n",
      "Epoch  458, Training Loss: 0.0340, Validation Loss: 0.0498, Learning Rate: 0.108400\n",
      "Epoch  459, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.108200\n",
      "Epoch  460, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.108000\n",
      "Epoch  461, Training Loss: 0.0340, Validation Loss: 0.0496, Learning Rate: 0.107800\n",
      "Epoch  462, Training Loss: 0.0340, Validation Loss: 0.0497, Learning Rate: 0.107600\n",
      "Epoch  463, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.107400\n",
      "Epoch  464, Training Loss: 0.0340, Validation Loss: 0.0496, Learning Rate: 0.107200\n",
      "Epoch  465, Training Loss: 0.0340, Validation Loss: 0.0496, Learning Rate: 0.107000\n",
      "Epoch  466, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.106800\n",
      "Epoch  467, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.106600\n",
      "Epoch  468, Training Loss: 0.0340, Validation Loss: 0.0496, Learning Rate: 0.106400\n",
      "Epoch  469, Training Loss: 0.0340, Validation Loss: 0.0496, Learning Rate: 0.106200\n",
      "Epoch  470, Training Loss: 0.0340, Validation Loss: 0.0496, Learning Rate: 0.106000\n",
      "Epoch  471, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.105800\n",
      "Epoch  472, Training Loss: 0.0340, Validation Loss: 0.0494, Learning Rate: 0.105600\n",
      "Epoch  473, Training Loss: 0.0340, Validation Loss: 0.0492, Learning Rate: 0.105400\n",
      "Epoch  474, Training Loss: 0.0340, Validation Loss: 0.0493, Learning Rate: 0.105200\n",
      "Epoch  475, Training Loss: 0.0339, Validation Loss: 0.0492, Learning Rate: 0.105000\n",
      "Epoch  476, Training Loss: 0.0339, Validation Loss: 0.0492, Learning Rate: 0.104800\n",
      "Epoch  477, Training Loss: 0.0339, Validation Loss: 0.0492, Learning Rate: 0.104600\n",
      "Epoch  478, Training Loss: 0.0339, Validation Loss: 0.0491, Learning Rate: 0.104400\n",
      "Epoch  479, Training Loss: 0.0339, Validation Loss: 0.0490, Learning Rate: 0.104200\n",
      "Epoch  480, Training Loss: 0.0339, Validation Loss: 0.0491, Learning Rate: 0.104000\n",
      "Epoch  481, Training Loss: 0.0339, Validation Loss: 0.0490, Learning Rate: 0.103800\n",
      "Epoch  482, Training Loss: 0.0339, Validation Loss: 0.0489, Learning Rate: 0.103600\n",
      "Epoch  483, Training Loss: 0.0339, Validation Loss: 0.0489, Learning Rate: 0.103400\n",
      "Epoch  484, Training Loss: 0.0339, Validation Loss: 0.0490, Learning Rate: 0.103200\n",
      "Epoch  485, Training Loss: 0.0339, Validation Loss: 0.0488, Learning Rate: 0.103000\n",
      "Epoch  486, Training Loss: 0.0339, Validation Loss: 0.0488, Learning Rate: 0.102800\n",
      "Epoch  487, Training Loss: 0.0339, Validation Loss: 0.0489, Learning Rate: 0.102600\n",
      "Epoch  488, Training Loss: 0.0339, Validation Loss: 0.0488, Learning Rate: 0.102400\n",
      "Epoch  489, Training Loss: 0.0339, Validation Loss: 0.0488, Learning Rate: 0.102200\n",
      "Epoch  490, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.102000\n",
      "Epoch  491, Training Loss: 0.0339, Validation Loss: 0.0487, Learning Rate: 0.101800\n",
      "Epoch  492, Training Loss: 0.0339, Validation Loss: 0.0488, Learning Rate: 0.101600\n",
      "Epoch  493, Training Loss: 0.0339, Validation Loss: 0.0487, Learning Rate: 0.101400\n",
      "Epoch  494, Training Loss: 0.0339, Validation Loss: 0.0487, Learning Rate: 0.101200\n",
      "Epoch  495, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.101000\n",
      "Epoch  496, Training Loss: 0.0339, Validation Loss: 0.0485, Learning Rate: 0.100800\n",
      "Epoch  497, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.100600\n",
      "Epoch  498, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.100400\n",
      "Epoch  499, Training Loss: 0.0339, Validation Loss: 0.0485, Learning Rate: 0.100200\n",
      "Epoch  500, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.100000\n",
      "Epoch  501, Training Loss: 0.0339, Validation Loss: 0.0485, Learning Rate: 0.099800\n",
      "Epoch  502, Training Loss: 0.0339, Validation Loss: 0.0484, Learning Rate: 0.099600\n",
      "Epoch  503, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.099400\n",
      "Epoch  504, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.099200\n",
      "Epoch  505, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.099000\n",
      "Epoch  506, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.098800\n",
      "Epoch  507, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.098600\n",
      "Epoch  508, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.098400\n",
      "Epoch  509, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.098200\n",
      "Epoch  510, Training Loss: 0.0339, Validation Loss: 0.0482, Learning Rate: 0.098000\n",
      "Epoch  511, Training Loss: 0.0339, Validation Loss: 0.0482, Learning Rate: 0.097800\n",
      "Epoch  512, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.097600\n",
      "Epoch  513, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.097400\n",
      "Epoch  514, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.097200\n",
      "Epoch  515, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.097000\n",
      "Epoch  516, Training Loss: 0.0339, Validation Loss: 0.0482, Learning Rate: 0.096800\n",
      "Epoch  517, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.096600\n",
      "Epoch  518, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.096400\n",
      "Epoch  519, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.096200\n",
      "Epoch  520, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.096000\n",
      "Epoch  521, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.095800\n",
      "Epoch  522, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.095600\n",
      "Epoch  523, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.095400\n",
      "Epoch  524, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.095200\n",
      "Epoch  525, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.095000\n",
      "Epoch  526, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.094800\n",
      "Epoch  527, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.094600\n",
      "Epoch  528, Training Loss: 0.0339, Validation Loss: 0.0477, Learning Rate: 0.094400\n",
      "Epoch  529, Training Loss: 0.0339, Validation Loss: 0.0477, Learning Rate: 0.094200\n",
      "Epoch  530, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.094000\n",
      "Epoch  531, Training Loss: 0.0339, Validation Loss: 0.0477, Learning Rate: 0.093800\n",
      "Epoch  532, Training Loss: 0.0339, Validation Loss: 0.0477, Learning Rate: 0.093600\n",
      "Epoch  533, Training Loss: 0.0339, Validation Loss: 0.0477, Learning Rate: 0.093400\n",
      "Epoch  534, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.093200\n",
      "Epoch  535, Training Loss: 0.0339, Validation Loss: 0.0475, Learning Rate: 0.093000\n",
      "Epoch  536, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.092800\n",
      "Epoch  537, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.092600\n",
      "Epoch  538, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.092400\n",
      "Epoch  539, Training Loss: 0.0339, Validation Loss: 0.0474, Learning Rate: 0.092200\n",
      "Epoch  540, Training Loss: 0.0339, Validation Loss: 0.0475, Learning Rate: 0.092000\n",
      "Epoch  541, Training Loss: 0.0339, Validation Loss: 0.0475, Learning Rate: 0.091800\n",
      "Epoch  542, Training Loss: 0.0339, Validation Loss: 0.0475, Learning Rate: 0.091600\n",
      "Epoch  543, Training Loss: 0.0339, Validation Loss: 0.0474, Learning Rate: 0.091400\n",
      "Epoch  544, Training Loss: 0.0339, Validation Loss: 0.0474, Learning Rate: 0.091200\n",
      "Epoch  545, Training Loss: 0.0339, Validation Loss: 0.0475, Learning Rate: 0.091000\n",
      "Epoch  546, Training Loss: 0.0339, Validation Loss: 0.0474, Learning Rate: 0.090800\n",
      "Epoch  547, Training Loss: 0.0339, Validation Loss: 0.0474, Learning Rate: 0.090600\n",
      "Epoch  548, Training Loss: 0.0339, Validation Loss: 0.0474, Learning Rate: 0.090400\n",
      "Epoch  549, Training Loss: 0.0339, Validation Loss: 0.0474, Learning Rate: 0.090200\n",
      "Epoch  550, Training Loss: 0.0339, Validation Loss: 0.0472, Learning Rate: 0.090000\n",
      "Epoch  551, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.089800\n",
      "Epoch  552, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.089600\n",
      "Epoch  553, Training Loss: 0.0339, Validation Loss: 0.0474, Learning Rate: 0.089400\n",
      "Epoch  554, Training Loss: 0.0339, Validation Loss: 0.0472, Learning Rate: 0.089200\n",
      "Epoch  555, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.089000\n",
      "Epoch  556, Training Loss: 0.0339, Validation Loss: 0.0472, Learning Rate: 0.088800\n",
      "Epoch  557, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.088600\n",
      "Epoch  558, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.088400\n",
      "Epoch  559, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.088200\n",
      "Epoch  560, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.088000\n",
      "Epoch  561, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.087800\n",
      "Epoch  562, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.087600\n",
      "Epoch  563, Training Loss: 0.0339, Validation Loss: 0.0470, Learning Rate: 0.087400\n",
      "Epoch  564, Training Loss: 0.0339, Validation Loss: 0.0470, Learning Rate: 0.087200\n",
      "Epoch  565, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.087000\n",
      "Epoch  566, Training Loss: 0.0339, Validation Loss: 0.0469, Learning Rate: 0.086800\n",
      "Epoch  567, Training Loss: 0.0339, Validation Loss: 0.0469, Learning Rate: 0.086600\n",
      "Epoch  568, Training Loss: 0.0339, Validation Loss: 0.0470, Learning Rate: 0.086400\n",
      "Epoch  569, Training Loss: 0.0339, Validation Loss: 0.0469, Learning Rate: 0.086200\n",
      "Epoch  570, Training Loss: 0.0339, Validation Loss: 0.0469, Learning Rate: 0.086000\n",
      "Epoch  571, Training Loss: 0.0339, Validation Loss: 0.0470, Learning Rate: 0.085800\n",
      "Epoch  572, Training Loss: 0.0339, Validation Loss: 0.0469, Learning Rate: 0.085600\n",
      "Epoch  573, Training Loss: 0.0339, Validation Loss: 0.0469, Learning Rate: 0.085400\n",
      "Epoch  574, Training Loss: 0.0339, Validation Loss: 0.0468, Learning Rate: 0.085200\n",
      "Epoch  575, Training Loss: 0.0339, Validation Loss: 0.0468, Learning Rate: 0.085000\n",
      "Epoch  576, Training Loss: 0.0339, Validation Loss: 0.0467, Learning Rate: 0.084800\n",
      "Epoch  577, Training Loss: 0.0339, Validation Loss: 0.0467, Learning Rate: 0.084600\n",
      "Epoch  578, Training Loss: 0.0339, Validation Loss: 0.0467, Learning Rate: 0.084400\n",
      "Epoch  579, Training Loss: 0.0339, Validation Loss: 0.0466, Learning Rate: 0.084200\n",
      "Epoch  580, Training Loss: 0.0339, Validation Loss: 0.0467, Learning Rate: 0.084000\n",
      "Epoch  581, Training Loss: 0.0339, Validation Loss: 0.0467, Learning Rate: 0.083800\n",
      "Epoch  582, Training Loss: 0.0339, Validation Loss: 0.0467, Learning Rate: 0.083600\n",
      "Epoch  583, Training Loss: 0.0339, Validation Loss: 0.0467, Learning Rate: 0.083400\n",
      "Epoch  584, Training Loss: 0.0339, Validation Loss: 0.0467, Learning Rate: 0.083200\n",
      "Epoch  585, Training Loss: 0.0339, Validation Loss: 0.0466, Learning Rate: 0.083000\n",
      "Epoch  586, Training Loss: 0.0339, Validation Loss: 0.0466, Learning Rate: 0.082800\n",
      "Epoch  587, Training Loss: 0.0339, Validation Loss: 0.0466, Learning Rate: 0.082600\n",
      "Epoch  588, Training Loss: 0.0339, Validation Loss: 0.0466, Learning Rate: 0.082400\n",
      "Epoch  589, Training Loss: 0.0339, Validation Loss: 0.0466, Learning Rate: 0.082200\n",
      "Epoch  590, Training Loss: 0.0339, Validation Loss: 0.0465, Learning Rate: 0.082000\n",
      "Epoch  591, Training Loss: 0.0339, Validation Loss: 0.0465, Learning Rate: 0.081800\n",
      "Epoch  592, Training Loss: 0.0339, Validation Loss: 0.0466, Learning Rate: 0.081600\n",
      "Epoch  593, Training Loss: 0.0339, Validation Loss: 0.0466, Learning Rate: 0.081400\n",
      "Epoch  594, Training Loss: 0.0339, Validation Loss: 0.0465, Learning Rate: 0.081200\n",
      "Epoch  595, Training Loss: 0.0339, Validation Loss: 0.0465, Learning Rate: 0.081000\n",
      "Epoch  596, Training Loss: 0.0339, Validation Loss: 0.0466, Learning Rate: 0.080800\n",
      "Epoch  597, Training Loss: 0.0339, Validation Loss: 0.0465, Learning Rate: 0.080600\n",
      "Epoch  598, Training Loss: 0.0339, Validation Loss: 0.0466, Learning Rate: 0.080400\n",
      "Epoch  599, Training Loss: 0.0339, Validation Loss: 0.0465, Learning Rate: 0.080200\n",
      "Epoch  600, Training Loss: 0.0339, Validation Loss: 0.0465, Learning Rate: 0.080000\n",
      "Epoch  601, Training Loss: 0.0339, Validation Loss: 0.0464, Learning Rate: 0.079800\n",
      "Epoch  602, Training Loss: 0.0339, Validation Loss: 0.0464, Learning Rate: 0.079600\n",
      "Epoch  603, Training Loss: 0.0339, Validation Loss: 0.0464, Learning Rate: 0.079400\n",
      "Epoch  604, Training Loss: 0.0339, Validation Loss: 0.0464, Learning Rate: 0.079200\n",
      "Epoch  605, Training Loss: 0.0339, Validation Loss: 0.0464, Learning Rate: 0.079000\n",
      "Epoch  606, Training Loss: 0.0339, Validation Loss: 0.0464, Learning Rate: 0.078800\n",
      "Epoch  607, Training Loss: 0.0339, Validation Loss: 0.0464, Learning Rate: 0.078600\n",
      "Epoch  608, Training Loss: 0.0339, Validation Loss: 0.0463, Learning Rate: 0.078400\n",
      "Epoch  609, Training Loss: 0.0339, Validation Loss: 0.0463, Learning Rate: 0.078200\n",
      "Epoch  610, Training Loss: 0.0339, Validation Loss: 0.0463, Learning Rate: 0.078000\n",
      "Epoch  611, Training Loss: 0.0339, Validation Loss: 0.0464, Learning Rate: 0.077800\n",
      "Epoch  612, Training Loss: 0.0339, Validation Loss: 0.0463, Learning Rate: 0.077600\n",
      "Epoch  613, Training Loss: 0.0339, Validation Loss: 0.0463, Learning Rate: 0.077400\n",
      "Early stopping triggered at epoch 613. Restoring best model parameters.\n",
      "\n",
      "Neural Network Classification Accuracy: 0.9931\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAIjCAYAAADvBuGTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAg45JREFUeJzt3Qd8VFX6xvEnvUASCL333gVE7AqKWLHr2l3X7uqq6+rftayua13XtSuu4qpr771XREGa9N5bCJBOev6f99xMSCAgkHIzM7/vZ2fnzp2bmRMGIQ/vOe+JKCsrKxMAAAAAoEYia/blAAAAAABDuAIAAACAWkC4AgAAAIBaQLgCAAAAgFpAuAIAAACAWkC4AgAAAIBaQLgCAAAAgFpAuAIAAACAWkC4AgAAAIBaQLgCAGAPRURE6Pbbb/d7GACABoZwBQCoExMmTHAh5JdfflFDZiHJxpmenl7t8507d9axxx5b4/f53//+p4ceeqjGrwMAaLii/R4AAADBZuvWrYqOjt7jcDV79mxdc801dTYuAIC/CFcAAOyh+Ph4NQTFxcUqLS1VbGys30MBADAtEADgt+nTp2vs2LFKTk5W48aNNWrUKP30009VrikqKtLf/vY39ejRwwWbZs2a6cADD9Tnn39ecc369et1wQUXqH379oqLi1ObNm10wgknaPny5XW+5io7O9tVpGwKob13y5YtdcQRR2jatGnu+UMPPVQffvihVqxY4b7WbnZtQFpamn7/+9+rVatW7vsbNGiQnn/++Srvad+Hfd0DDzzgphd269bNvdfkyZPVqFEjXX311TuMc/Xq1YqKitLdd99d678GAIAdUbkCAPhmzpw5Ouigg1ywuuGGGxQTE6OnnnrKhZFvv/1WI0aMcNdZkLGAcNFFF2nfffdVVlaWW8tl4cVCjDn55JPd61111VUuuFhgsfC1cuXKKkFmZzZv3lzteasM/ZZLL71Ub7zxhq688kr17dtXmzZt0g8//KB58+Zpn3320c0336zMzEwXdv71r3+5r7EgGZhiaN/v4sWL3dd36dJFr7/+us4//3xlZGTsEJqee+455efn6+KLL3bhqmPHjjrxxBP16quv6sEHH3RhKuDll19WWVmZzjrrrN/8HgAAtaAMAIA68Nxzz5XZXzNTpkzZ6TXjxo0ri42NLVuyZEnFubVr15YlJSWVHXzwwRXnBg0aVHbMMcfs9HW2bNni3uv+++/f43Hedttt7mt3ddv+ve2cfV1ASkpK2RVXXLHL97HX6NSp0w7nH3roIfd6L774YsW5wsLCspEjR5Y1bty4LCsry51btmyZuy45ObksLS2tymt8+umn7rmPP/64yvmBAweWHXLIIXv4KwIA2FtMCwQA+KKkpESfffaZxo0bp65du1act+l8v/vd71zlxypUpkmTJq4qtWjRompfKyEhwa07+uabb7Rly5a9Gs+bb77pKl3b32yq3m+x8f38889au3btHr/vRx99pNatW+vMM8+sOGcVvD/+8Y/KyclxFbzKrELXokWLKudGjx6ttm3b6qWXXqo4Z80zfv31V5199tl7PCYAwN4hXAEAfLFx40bl5eWpV69eOzzXp08fNx1v1apV7vEdd9zhpsj17NlTAwYM0J///GcXHAJsety9996rjz/+2IWhgw8+WPfdd59bh7W77GsspGx/253mFfZeFmY6dOjgpi3aNMalS5fu1vvaOixbSxYZGbnDr0Hg+cps2uD27Gtt6t8777zjfk2NBS0b+6mnnrpb4wAA1BzhCgDQ4FnwWbJkiZ599ln1799fzzzzjFvLZPcB1lBi4cKFbm2WhYpbbrnFBRRrmFHXTjvtNBemHnnkEVdBuv/++9WvXz8X9mqbVemqc+6557pKlwUsm7lord9tf66UlJRaHwMAoHqEKwCAL2xqW2JiohYsWLDDc/Pnz3fVGKsEBaSmprpugNakwSpaAwcOrNKxz1gHveuuu85NN7RKUmFhof75z3/Wy/dj0xkvv/xyF26WLVvmOhreddddFc9bp7/qdOrUyU133L5xhv0aBJ7fHRY6hwwZ4ipW33//vWvkcc4559ToewIA7BnCFQDAF9bV7sgjj9S7775bpV36hg0bXNXFWq1bF0Fj3fcqs0573bt3V0FBgXtsU+Gsg972QSspKanimrpcO2adACuzVuxWwar83tYuffvrzNFHH+2mL1q3v8r7V1kVzL7PQw45ZLfHYmHKgqW1ardwZy3uAQD1h1bsAIA6ZVP5Pvnkkx3OW4vxv//9765phAUpq/pER0e7VuwWSmwdU4C1N7d25UOHDnUVLGvDHmh9bmw6oO2PZdPz7Fp7nbffftsFtTPOOKNOvz/b48r21jrllFPc/lQWiL744gtNmTKlStXMxm4B6tprr9Xw4cPddccdd5xrqW7fs7Venzp1qmsbb9/bxIkTXUiygLi7rBGItbS37/2yyy5zjTEAAPWHcAUAqFNPPPFEtectTNi6JJvCdtNNN7m1UjY1zva2evHFFyv2uDLWOe+9995zVRkLXjZVzoKZNbYwNn3Quu19+eWXeuGFF1y46t27t1577TXXXa8u2dRGC4Y2trfeest9D1ZVe/zxx13ACbBrZsyY4fapsr2u7HuwcGVrqKzL4Y033ug2DrYOidbkw66zX6M9Yc08rBpoHQiZEggA9S/C+rH78L4AAKAO2IbCs2bNcpsSAwDqF2uuAAAIEevWrdOHH35I1QoAfMK0QAAAgpx1J7Q1Wtaa3tZZXXLJJX4PCQDCEpUrAACC3LfffuuqVRaybN1W69at/R4SAIQl1lwBAAAAQC2gcgUAAAAAtYBwBQAAAAC1gIYW1bA9StauXes2boyIiPB7OAAAAAB8YquobMP4tm3bKjJy17UpwlU1LFjZhpQAAAAAYFatWqX27dtrVwhX1bCKVeAXMDk52e/hAAAAAPBJVlaWK7wEMsKuEK6qEZgKaMGKcAUAAAAgYjeWC9HQAgAAAABqAeEKAAAAAGoB4QoAAAAAagFrrgAAABAUSkpKVFRU5PcwEGKioqIUHR1dK1swEa4AAADQ4OXk5Gj16tVuzyGgtiUmJqpNmzaKjY2t0esQrgAAANDgK1YWrOwH4BYtWtRKhQEwFtYLCwu1ceNGLVu2TD169PjNjYJ3hXAFAACABs2mAtoPwRasEhIS/B4OQkxCQoJiYmK0YsUKF7Ti4+P3+rVoaAEAAICgQMUKdaUm1aoqr1MrrwIAAAAAYY5wBQAAAAC1gHAFAAAABInOnTvroYce2u3rv/nmGzedMiMjo07HBQ/hCgAAAKhlFmh2dbv99tv36nWnTJmiiy++eLev33///bVu3TqlpKSoLhHiPHQLBAAAAGqZBZqAV199VbfeeqsWLFhQca5x48YVx9YJ0drN20a2v8U6Ju4J27epdevWe/Q12HtUrgAAABBULIzkFRb7ctvdTYwt0ARuVjWyqk7g8fz585WUlKSPP/5YQ4cOVVxcnH744QctWbJEJ5xwglq1auXC1/Dhw/XFF1/sclqgve4zzzyjE0880e0DZvs0vffeezutKE2YMEFNmjTRp59+qj59+rj3Oeqoo6qEweLiYv3xj3901zVr1kx/+ctfdN5552ncuHF7/Zlt2bJF5557rpo2berGOXbsWC1atKjieWuDftxxx7nnGzVqpH79+umjjz6q+NqzzjqrohW/fY/PPfecGiIqVwAAAAgqW4tK1PfWT31577l3jFFibO38CH3jjTfqgQceUNeuXV2oWLVqlY4++mjdddddLnD997//dYHDKl4dO3bc6ev87W9/03333af7779fjzzyiAsiFlZSU1OrvT4vL8+97wsvvOBakJ999tm6/vrr9dJLL7nn7733XndsAcYC2L///W+98847Ouyww/b6ez3//PNdmLLgl5yc7AKbfa9z5851e0xdccUVbo+p7777zoUrOx+o7t1yyy3usYXR5s2ba/Hixdq6dasaIsIVAAAA4IM77rhDRxxxRMVjC0ODBg2qeHznnXfq7bffdoHkyiuv3GVwOfPMM93xP/7xDz388MOaPHmyq0jtbFPmJ598Ut26dXOP7bVtLAEW0G666SZXDTOPPvpoRRVpbywqD1UTJ050a8CMhbcOHTq40Hbqqadq5cqVOvnkkzVgwAD3vAXOAHtuyJAhGjZsWEX1rqEiXDV0m5dKCz6Rhv9eio7zezQAAAC+S4iJchUkv967tgTCQkBOTo5rdPHhhx+6aXo2Pc8qNBYudmXgwIEVx1b1scpQWlraTq+3aXmBYGXatGlTcX1mZqY2bNigfffdt+L5qKgoN32xtLR0r77PefPmufVkI0aMqDhn0w179erlnjM2DfGyyy7TZ599ptGjR7ugFfi+7Lw9njZtmo488kg3PTEQ0hoa1lw1ZPYb+NmjpE9vkpb/4PdoAAAAGgRbQ2RT8/y42XvXFgtCldnUPKtUWfXp+++/14wZM1wlx6bL7YpNq9v+12dXQai663d3LVldueiii7R06VKdc845mjVrlgueVkEztj7Lpjn+6U9/0tq1azVq1Cj3a9UQEa4asshIqWd5OXfB3pdiAQAA0PDZtDmb4mfT8SxUWfOL5cuX1+sYrPmGNdSwlu8B1snQqkZ7q0+fPq4K9/PPP1ec27Rpk1tL1rdv34pzNk3w0ksv1VtvvaXrrrtO48ePr3jOmllYU40XX3zRNfR4+umn1RAxLbCh632MNO15acHH0tEP2D8t+D0iAAAA1AHrgmfBwppYWDXJGjns7VS8mrjqqqt09913q3v37urdu7erIFnHvt2p2s2aNct1Qgywr7F1ZNYF8Q9/+IOeeuop97w182jXrp07b6655hpXoerZs6d7r6+//tqFMmNt7G1aonUQLCgo0AcffFDxXENDuGrouhwsxSRKWWukdTOltoP9HhEAAADqwIMPPqgLL7zQrSeyrnjWUS8rK6vex2Hvu379etc63dZb2abFY8aMcce/5eCDD67y2L7GqlbWefDqq6/Wscce66Y52nXWJCMwRdGqY9YxcPXq1W7NmDXj+Ne//lWxV5c12LAqnrViP+igg/TKK6+oIYoo83uCZQNkv4mtJGoL+uzD9d0rZ0nzP5AOuVE67Ca/RwMAAFCv8vPztWzZMnXp0kXx8fF+DyfsWPXMKkWnnXaa62AYbr/HsvYgG7DmqoErLS1TWtvDvQcLPvR7OAAAAAhx1jzC1jstXLjQTfOzbn0WPH73u9/5PbQGj3DVwI1+8Fsd9VGiyiIipfWzpIxVfg8JAAAAIcw2Fp4wYYKGDx+uAw44wAWsL774osGuc2pIWHPVwHVu3khfpScrLWWQWmVM9xpbjLjY72EBAAAgRFnXPutciD1H5aqB69/Wm9c5JX4/78S89/wdEAAAAIBqEa4auH7tUtz9G3nDvRO2mXDWOn8HBQAAAKBhhqvHHntMnTt3dp05RowYocmTJ+/0WltcZ+0XmzZt6m6jR4/e4XrbfM166le+WTvHYNS/PFz9kJ6g0nbDJJVJc9/1e1gAAAAAGlq4evXVV3Xttdfqtttuczs/2yZj1kc/LS2t2uu/+eYbnXnmmW5jsUmTJrk5oUceeaTWrFlT5ToLU+vWrau4vfzyywpGbVPi1TQxRsWlZVrf4Wjv5Jy3/B4WAAAAgIYWrmyzNNut+YILLlDfvn315JNPKjExUc8++2y117/00ku6/PLLNXjwYLdj9DPPPON673/55ZdVrouLi1Pr1q0rblblCkZWdevX1qteTU60TdkipFU/S5mr/R4aAAAAgIYSrmx35qlTp7qpfRUDiox0j60qtTvy8vJUVFSk1NTUHSpcLVu2VK9evVxv/k2bNu30NQoKCtzmYJVvDUm/duVNLTbFSx1HeifnvO3voAAAAAA0nHCVnp6ukpIStWrVqsp5e7x+/frdeo2//OUvatu2bZWAZlMC//vf/7pq1r333qtvv/1WY8eOde9Vnbvvvtvtuhy42VTDhqR/eeVq9tosqf9J3snZb/o7KAAAANS5Qw89VNdcc03FY+tT8NBDD/3mzKd33nmnxu9dW68TTnyfFlgT99xzj1555RW9/fbbrhlGwBlnnKHjjz9eAwYM0Lhx4/TBBx9oypQprppVnZtuukmZmZkVt1WrVjXIphbz12WpuNdx3tTAtdOlzKrrzAAAANAwHHfccTttqPb999+74PLrr7/u8evaz7QXX1y7e57efvvtbsnN9qxvgRUo6tKECRPUpEkThQpfw1Xz5s0VFRWlDRs2VDlvj22d1K488MADLlx99tlnGjhw4C6v7dq1q3uvxYsXV/u8rc9KTk6ucmtIOqUmqnFctAqKS7VkayOpfXlb9oUf+z00AAAAVOP3v/+9Pv/8c61eveM6+eeee07Dhg37zZ9hq9OiRQvXn6A+2M/j9nMygiRcxcbGaujQoVWaUQSaU4wcWb62qBr33Xef7rzzTn3yySfuN+Zvsd/UtuaqTZs2CkaRkRHqW76Z8Ow1mVKv8n9BWEC4AgAAYaisTCrM9edm770bjj32WBeErDJTWU5Ojl5//XUXvuznU+uC3a5dOxeYbNbVb3W43n5a4KJFi3TwwQe7WVzWHM4CXXXLaHr27Onew4oOt9xyi+tZYGx8f/vb3zRz5syKLYwCY95+WuCsWbN0+OGHKyEhQc2aNXMVNPt+Km+HNG7cOFcEsZ+77Zorrrii4r32xsqVK3XCCSeocePGrgBy2mmnVSnM2LgPO+wwJSUluectW/zyyy/uuRUrVrgKojW2a9Sokfr166ePPvpIdSlaPrM27Oedd54LSfvuu6/7zZKbm+u6B5pzzz3X/YazdVHG1lDdeuut+t///ud+cwXWZtkvuN3sA7bfICeffLJL20uWLNENN9yg7t27uxbvwapf22RNXrZZs9Zk6uSRx0hf/k1a9p1UkC3FJfk9PAAAgPpTlCf9o60/7/1/a6XYRr95WXR0tPs51oLKzTff7IKKsWBlfQAsVNnPrRYGLPxYMPjwww91zjnnqFu3bu7n4t9iRYmTTjrJ9Sv4+eef3fKWyuuzAix42DisT4EFJOvUbefsZ+TTTz9ds2fPdkWLL774wl1vPQi2Zz+f28/SVgCxqYm2bdJFF12kK6+8skqA/Prrr12wsnubNWavb1MO7T33lH1/gWBlPRSKi4tdWLPXDCz3OeusszRkyBA98cQTbkbcjBkzFBMT456za62B3nfffefC1dy5c91rhXS4sl+cjRs3usBkQcl+8e3DDTS5sLRqHQQD7BfOfpFOOeWUKq9j+2TZfFH7RbX5q88//7wyMjLcbyLbB8sqXcFc1hzU3puLOnN1htR8fym1m7R5ibT4S6nfOL+HBwAAgO1ceOGFuv/++10wsMYUgSmBVgQINFK7/vrrK66/6qqr9Omnn+q1117brXBlYWj+/Pnua+xnXvOPf/xjh3VSf/3rXyuOrThh72l9CyxcWRXKAoeFwV0ty7HCRn5+vmsaZ0HFPProo64yZMWPwM/uTZs2deftZ3LbNumYY45xs9L2JlzZ11kYXLZsWUXDOXt/q0BZwBs+fLjLCn/+85/de5kePXpUfL09Z7/WVhE0VrWra76HK2OJ127V2b4JxfLly3f5WvYbxH6DhZpBHbxwNWdtlgpLyhRrUwMnPSot+IhwBQAAwktMoldB8uu9d5P9wL///vu7/VstXFklx5pZ3HHHHe55q2BZGLIwtWbNGldAsC2CdndN1bx581zoCAQrU93SmldffVUPP/ywm9Fl1TKrAO1pjwF7r0GDBlUEK3PAAQe46tKCBQsqwlW/fv1csAqwKpYFpL0R+P4qd/K2qY/WAMOes3Bls+CsgvbCCy+47uGnnnqqq/yZP/7xj25LJuvRYM9Z0NqbdW5h0y0wnHRulqgmiTEqLC7V/PVZUu9jvCcWfiqVFPs9PAAAgPpjU+xsap4ft/LpfbvL1la9+eabys7OdlUr+8H/kEMOcc9ZVevf//63mxZo0+hsSptNvbOQVVts71ibOnf00Ue7DtrTp0930xRr8z0qiymfkhdg0yEtgNUVm7k2Z84cVyH76quvXPiyTuLGQtfSpUvdVEsLeLYM6ZFHHlFdIlwFCfuNGZgaOGNVhtRhhJSQKuVnSKt+9nt4AAAAqIY1YLAlLjatzqa02VTBwPqriRMnujVFZ599tqsK2bS1hQsX7vZr9+nTx20hZC3TA3766acq1/z444/q1KmTC1QWLmzanDV62L7J3M72g638XtY8wtZeBdj47Xvr1auX6kKf8u+v8jZJtm7Klv5YiAqwZh1/+tOfXIXK1qBZiA2wqtell16qt956S9ddd53Gjx+vukS4CsKpgS5cRUZJ3Ud5TyzZ1m0RAAAADYetZ7IeA7avqoUg66gXYEHHuvtZALJpbpdccskOWxTtik11s2BhzeEs+NiUQwtRldl72NojW2Nl0wJtemCgslN5HZata7LKWXp6upuauD2rfllHQnsva4BhlTZbI2ZVocCUwL1lwc7eu/LNfj3s+7P1Uvbe06ZN0+TJk12TEKv8WVDcunWrW1pky4gsMFrYs7VYFsqMNfew5UL2vdnX25gDz9UVwlUQGVI5XJlu5eHKmloAAACgQbKpgVu2bHFT/iqvj7JGE/vss487b2uyrKGEtTLfXVY1sqBkIcMaYNg0uLvuuqvKNccff7yr6lgIscZxFuSsFXtlthbJNjy2lubWPr66dvC2DsyCyubNm91aJ2suN2rUKNe8oqZycnJcx7/KN2uUYRW+d9991zXJsHbzFrasumdryIyt7bJ29ha4LGRaldCaeVjn8EBos46BFqjs+7NrHn/8cdWliLKy3WzWH0aysrJc9xZrZ9mQNhTenFuofe709i6YeduRSineLP2zp/fk9Yulxi38HSAAAEAdsC51Vn3o0qWLq54A9fl7bE+yAZWrIJLaKFYdU73uMb9aS/akVlIrr7Wkln7t7+AAAACAMEe4CjKDA1MDV5ZPDQysu2JqIAAAAOArwlWQhiu3mbCpaGrxlW1j7ePIAAAAgPBGuArijoFuuVyH/aSYRlJumrRhtt/DAwAAAMIW4SrI9GubrKjICKXnFGp9Vr4UHSt1OWhb9QoAACBE0YcNDf33FuEqyMTHRKlHy8bueNbqTO9kpwO8+9VTfBwZAABA3bCW26awsNDvoSBE5eXlufuYmJgavU50LY0H9WhAuxTNX5+tWWsydWS/1lL7Yd4Ta6b6PTQAAIBaFx0d7fZZ2rhxo/vh1/Z3AmqrYmXBKi0tTU2aNKkI8nuLcBWEBrZP0etTV7tw5bQZLEVESdnrpMw1Uko7v4cIAABQa2wz2TZt2rh9iFasWOH3cBCCmjRp4jZxrinCVRDq3y7F3du0QEvbEbGJUqu+0vpZ0ppfCFcAACDkxMbGqkePHkwNRK2zamhNK1YBhKsg1KdNsqIjI7Qpt1DrMvPVtkmC1G6YF65W/yL1PcHvIQIAANQ6mw4YHx/v9zCAnWLCarA2tWiV5I5/DTS1aLePd7/+Vx9HBgAAAIQvwlWQGlg+NXB2YN1Vq37e/YY5Po4KAAAACF+EqyDVv70Xrn4NhKsWfaSISCl3o5ST5u/gAAAAgDBEuAqBypXb9MyaWqR2857cMNvfwQEAAABhiHAVpHq1TnJNLTbnFmpNxtaqUwPXE64AAACA+ka4CuKmFj3Lm1psW3fV37tn3RUAAABQ7whXQb6ZcJWOga3LwxUdAwEAAIB6R7gKhc2EA5WrNoO9+43zpcJcH0cGAAAAhB/CVQhUrmYFmlokt5GS2kplpdK6mX4PDwAAAAgrhKsgb2oRExWhjLwird6ytepmwmum+To2AAAAINwQroJYXHSUC1hVpgZWhKupPo4MAAAACD+EqyDXr403NXDu2izvRNvycLWWyhUAAABQnwhXQa5fu2R3P2dteeWq7RDvfstyKXeTjyMDAAAAwgvhKsj1beOFq7nryitXCU2kZt2947XTfRwZAAAAEF4IV0GuT5tkRURIG7IKlJ5TUHVqIOuuAAAAgHpDuApyjeKi1aVZo6rrrtoN9e5ZdwUAAADUG8JVCOjTdrupgZXbsdv+VwAAAADqHOEqBPRs6bVjX5KW451oPUCKjJZy06TM1f4ODgAAAAgThKsQ0KWFNy1wWXqudyImQWrZ1ztmaiAAAABQLwhXIaBrcy9cLQ2EK8NmwgAAAEC9IlyFgC7l4WpzbqEy8gqrNrWwdVcAAAAA6hzhKkQ6BrZOjq9avQq0Y187Qyot9XF0AAAAQHggXIVY9WrZxvJw1aK3FJMoFWZLmxb5OzgAAAAgDBCuQkTX7ZtaREVLrfp7xxtm+zgyAAAAIDwQrkKtclW5qUXL3t592jyfRgUAAACED8JViOiQmujuV23J23Yy0I6dcAUAAADUOcJViOgYCFebK4erPt494QoAAACoc4SrEKtcbckrUnZ+kXeyRXm42rJMKtrq4+gAAACA0Ee4ChGN46LVNDHGHa/aXB6kGreUElKlslIpfaG/AwQAAABCHOEqlNddRURUmho438eRAQAAAKGPcBVCOjTd1bqruT6NCgAAAAgPhKtQrFxVDle2mbDZSOUKAAAAqEuEqxDSITXB3a/asrWaduxUrgAAAIC6RLgKwXbsyzfl7jgtMGOlVJDj08gAAACA0Ee4CiFdmjeqmBZYXFLqnUxMlRq38o43LvBxdAAAAEBoI1yFkLYpCYqLjlRRSZlWV5kaSFMLAAAAoK4RrkJIZGRERfVqWXrlqYGsuwIAAADqGuEqxATC1dLK4ar1QO9+7QyfRgUAAACEPsJVqIarjZWaV7Qd7N2v/1UqLV+LBQAAAKBWEa5CTNcWjXecFti8pxSTKBXmSJsW+zc4AAAAIIQRrkJMtWuuIqOk1gO843VMDQQAAADqAuEqxHRr4YWrdZn5yiss3vZEm/Kpgay7AgAAAOoE4SrENEmMVdPEmB2rV4F1V1SuAAAAgDpBuAqXqYGBytU6mloAAAAAdYFwFcJNLZZu3K6pRXSCVJgtbV7i3+AAAACAEEW4CpfKVVT0tqYWa6f7NDIAAAAgdBGuQlDX6jYSrrzuiqYWAAAAQK0jXIX0tMAclZWVVbPuinAFAAAA1DbCVQjq1CxRERFSdn6xNuUWbnui3T7bKlelJb6NDwAAAAhFhKsQFB8TpbYpCTuuu7KmFrFJUlGulDbPvwECAAAAIYhwFaK6lm8mbFMDK0RGbaterZ7i08gAAACA0ES4CremFu2HefdrfvFhVAAAAEDoIlyFejv2yntdmfbDvfvVhCsAAACgNhGuQr1j4PaVq3bllauNC6T8TB9GBgAAAIQmwlWIV65WbMpVSWmlduyNW0hNOkkqk9ZM82+AAAAAQIghXIWodk0SFBsdqaKSMq3ZsrX6dVdMDQQAAABqDeEqREVGRqhTaqI7Xr5pJ+uuaGoBAAAA1BrCVQhr28Tb62pd5tbq111ZO/aySlMGAQAAAOw1wlUIa9sk3t2vzciv+kSbgVJUrJS3Sdqy3J/BAQAAACGGcBXC2qTspHIVHSe1HuAds+4KAAAAqBWEqxDWJsWrXK3L3K5yZVh3BQAAANQqwlUYrLlam7Fd5Wr7dVcAAAAAaoxwFSaVq7LtG1cE2rGvnyUVF/gwOgAAACC0EK7CYM1VXmGJsrYWV32yaWcpsZlUUiitm+nPAAEAAIAQQrgKYQmxUWqaGOOO127f1CIiQuo40jte/oMPowMAAABCC+EqXDsGmi4He/fLvqvnUQEAAAChh3AVNk0t8ncerlb+xLorAAAAoIYIV2GykXC1lasWvaVGLaTirex3BQAAAIRCuHrsscfUuXNnxcfHa8SIEZo8efJOrx0/frwOOuggNW3a1N1Gjx69w/XWGe/WW29VmzZtlJCQ4K5ZtGiRwnpaYHWVK1t3xdRAAAAAIDTC1auvvqprr71Wt912m6ZNm6ZBgwZpzJgxSktLq/b6b775Rmeeeaa+/vprTZo0SR06dNCRRx6pNWvWVFxz33336eGHH9aTTz6pn3/+WY0aNXKvmZ9fTcAIk8rVDg0tAghXAAAAQK2IKNthA6T6ZZWq4cOH69FHH3WPS0tLXWC66qqrdOONN/7m15eUlLgKln39ueee66pWbdu21XXXXafrr7/eXZOZmalWrVppwoQJOuOMM37zNbOyspSSkuK+Ljk5WcFs8rLNOu2pSerULFHf/vmwHS/YvFR6eIgUGSPduEKKbeTHMAEAAIAGaU+yga+Vq8LCQk2dOtVN26sYUGSke2xVqd2Rl5enoqIipaamusfLli3T+vXrq7ym/WJYiNvZaxYUFLhftMq3kNtIOCNfpaXV5OimXaSUDlJpkdfYAgAAAMBe8TVcpaenu8qTVZUqs8cWkHbHX/7yF1epCoSpwNftyWvefffdLoAFblY5CxWtU+Ld0qrCklJtyi3c8QLWXQEAAAChseaqJu655x698sorevvtt10zjL110003uTJf4LZq1SqFipioSLVoHLfzjoGGcAUAAAAEd7hq3ry5oqKitGHDhirn7XHr1q13+bUPPPCAC1efffaZBg4cWHE+8HV78ppxcXFu/mTlWyhps6u9riqHq3UzpK0Z9TgyAAAAIHT4Gq5iY2M1dOhQffnllxXnrKGFPR45cuROv866Ad5555365JNPNGzYsCrPdenSxYWoyq9pa6isa+CuXjOUtQ2su9pZ5Sq5rdSsh1RWKq34sX4HBwAAAIQI36cFWht227vq+eef17x583TZZZcpNzdXF1xwgXveOgDatL2Ae++9V7fccoueffZZtzeWraOyW05Ojns+IiJC11xzjf7+97/rvffe06xZs9xr2LqscePGKRy1rahc7SRcGaYGAgAAADUSLZ+dfvrp2rhxo9v010LS4MGDXUUq0JBi5cqVroNgwBNPPOG6DJ5yyilVXsf2ybr99tvd8Q033OAC2sUXX6yMjAwdeOCB7jVrsi4rmHVMTXT3Kzbl7Tpc/fIfafEX9TcwAAAAIIT4vs9VQxRK+1yZrxek6YLnpqh36yR9ck15hWp7+ZnSfV2l0mLpqmlSs271PUwAAACgwQmafa5QPzqVV65Wbs5zmyxXKz5F6rS/d7zos3ocHQAAABAaCFdhoH3TREVGSHmFJdqYU7DzC3se5d0v+LjexgYAAACECsJVGIiNjlSbFK+pxcpdrbsKhKsVE6X8rHoaHQAAABAaCFdholOz3WhqYeusmnX31l0t+ar+BgcAAACEAMJVuIWrzbsIV5WrVws/rYdRAQAAAKGDcBUmOqY2cvcrN+XuXriyphalJfUwMgAAACA0EK7CrHK1fFfTAk3H/aS4FCkvXVozrX4GBwAAAIQAwlWYbSRs7dh3KSpG6j7KO174ST2MDAAAAAgNhKswq1xtzi1Udn7Rri9m3RUAAACwxwhXYSIpPkbNGsX+dsdA0320FBEpbZglZayqnwECAAAAQY5wFUY6NtvNqYGNmkkdR3rH896vh5EBAAAAwY9wFUY6la+7+s3Klel7gnc/7706HhUAAAAQGghXYaRjs/J27Jt/ox276XOcd7/yJylrXR2PDAAAAAh+hKswskeVq+S2UocRksqkOW/V/eAAAACAIEe4CiOdm5fvdZW+G5UrM/A0737my3U4KgAAACA0EK7CSNfmjd392sx85RUW//YX9DtJioyR1s+S1s+u+wECAAAAQYxwFUaaNopV08QYd7x0425UrxJTpV7le15Nf6GORwcAAAAEN8JVmOnawqteLd3dqYFDz/fuZ/xPKsipw5EBAAAAwY1wFWa6tfA6Bi7duJtBqevhUmo3qSBLmvVa3Q4OAAAACGKEqzCtXC3ZnWmBJjJSGv5773gaUwMBAACAnSFchZmuzfewcmUGnCZFRktrp0kbF9Td4AAAAIAgRrgKM91alq+52pir0tKy3fuixi2k7kd4x7RlBwAAAKpFuAozHVMTFR0Zoa1FJVqflb/7Xzj4TO9+2n+lwt2cUggAAACEEcJVmImJilTHZom73449oNcxUtPOUt4maerzdTdAAAAAIEgRrsJ4M+Ele7LuKipaOvBP3vGPD0vFBXU0OgAAACA4Ea7C0B63Yw8YdKaU1FbKXifNeKluBgcAAAAEKcJVGOq2pxsJB0THSQf80Tv+4SGptKQORgcAAAAEJ8JVGOpaXrlakraHlSuzz3lSQlMpY4W09JvaHxwAAAAQpAhXYbyR8NrMfOUVFu/ZF8cmSv1O8o5/fa0ORgcAAAAEJ8JVGEptFOtuZknaXrRVH3i6dz/vfalgL6pfAAAAQAgiXIWpnq286tWCDdl7/sUd9pVSu0pFudJPj9f+4AAAAIAgRLgKU71aJbn7hXsTriIipMNu9o6/f1DKWFXLowMAAACCD+EqTPVs7YWrBev3IlyZ/idLHfeXirdKb1woFRfW7gABAACAIEO4ClM1qlwFqlfjHpfiUqTVk6Xv/1m7AwQAAACCDOEqzCtX6zLzlbm1aO9eJLWLdNy/vOOJ/5ay1tbiCAEAAIDgQrgKU8nxMWqbEu+OF+1t9cpYW/YOI7zpgd89UHsDBAAAAIIM4SqMBapX8/d23VVFc4v/845nvSEV5dfS6AAAAIDgQrgKY71a13DdVUDng6XkdlJBprTo09oZHAAAABBkCFdhLNDUYq87BgZERkoDTvWOZ75aCyMDAAAAgg/hKoz1rNQxsKysrGYvNuhM737hx9LmZbUwOgAAACC4EK7CWPeWjRUZIW3JK9LGnIKavVjL3lL30VJZqfTT47U1RAAAACBoEK7CWHxMlDo3b+SOF67PqfkL7n+Vdz/1eWnjgpq/HgAAABBECFdhLrDuav76rJq/WJdDvOpVSYH09iVSSXHNXxMAAAAIEoSrMNe3TbK7n7O2FsKVtWU//hEpPkVaO12a8WLNXxMAAAAIEoSrMNevXSBcZdbOCya3lQ69yTv+6i6poIadCAEAAIAgQbgKc/3aprj7xWk52lpYUjsvOuz3UmpXKTdNmvhw7bwmAAAA0MARrsJcy6Q4NW8cp9KyWlp3ZaJjpdF/845/fETavLR2XhcAAABowAhXYS4iIkL92tbiuquAPsdJHfeXirdK40dJq6fW3msDAAAADRDhCpXCVS2tuwo0tzj5GanNYGnrZumtP0hF+bX3+gAAAEADQ7iC+rdLqf3KlUlpJ533ntS4tbR5ifTDg7X7+gAAAEADQrhCReVq/vpsFZWU1u6LW1v2sfd4x5Mek/I21+7rAwAAAA0E4Qrq0DRRSXHRKiwudV0Da13fcVKrAVJhjvTzk7X/+gAAAEADQLiCIiMj1KcumlpUXn910LXe8cR/S+tm1v57AAAAAD4jXMHpX77fVa02tdi+etXjSKk4X3rtXKmkqG7eBwAAAPAJ4QpOnbRjrywyUjppvNSohbRlubTg47p5HwAAAMAnhCs4/dp54Wru2iyV2o7CdSGhiTTkbO/4tXOkqc9LpbXcQAMAAADwCeEKTvcWjRUXHamcgmKt3JxXd2+0z3m2CMs7fv+P0owX6+69AAAAgHpEuIITHRWp3q2T6nZqoEntIh1647bHPz8lldVRpQwAAACoR4QrVOhb3tRidl01tQiwcPWX5VJMorRhtrRiYt2+HwAAAFAPCFeov6YWlSU0lQad4R1/eSfVKwAAAAQ9whUq9G/nVa5mrc5QWX2EnYP/LEUnSKt+kua+W/fvBwAAANQhwhUq9G2TrNjoSG3JK9LyTXXY1CIgua20/1Xe8evnSS+eIk0eX/fvCwAAANQBwhUqWLAaWF69mrpiS/286UHXSa0HeMeLP5c+ul5aMal+3hsAAACoRYQrVDG0U9P6DVcx8dKpz0vth2879+bvpV9fq5/3BwAAAGoJ4QpVDOnohavpK+spXJlm3aSLvvA6CKZ0lLLWSG/9QVo9tf7GAAAAANQQ4QrVVq4WbMjWltzC+n1z6yB46fdSt8O9xws/rt/3BwAAAGqAcIUqWiTFqVerJNcZ/fvF6fU/gIQm0oBTveNFn9f/+wMAAAB7iXCFHRzaq4W7/2ZBmj8D6DbKu183Q8rZ6M8YAAAAgD1EuMIODunphavvFqartNSHzX2TWkmtB3rHH/9ZKq7n6YkAAADAXiBcYQfDOqcqMTZK6TkFbu2VL0bdKkXGSHPelp4+VEqb5884AAAAgN1EuEK1+10N7tDEHc9YleHPIHocIf3uFSkhVUqbI/3vdCk/y5+xAAAAALuBcIVqVYSrlT6FK9N9tHTFZKlJRyljhfTpTf6NBQAAAPgNhCtUy/fKVUDjFtJJ473j6S9JafP9HQ8AAACwE4Qr7DJcLUzLVk5Bsb+D6bif1PtYSWXSN3f7OxYAAABgJwhXqFbL5Hi1TYl3+13NWp3p93Ckw/7Pu5/3npS5xu/RAAAAADsgXGGn9unU1N1PWb7Z76FIrfpJnQ6QykqlmS/7PRoAAABgB4Qr7NSIrs3c/c/LNqlBGHyWdz/9Raloq9+jAQAAAKogXGGn9uuS6u6nrtiiwuJSv4cj9T1BikuRtiyTJhxDa3YAAAA0KIQr7FT3lo3VrFGs8otK9etqn7sGmrjG0hkvSQlNpTVTpS//5veIAAAAgAqEK+xURESERnT1qlcTFzeQqYFdDpJOfd47nvKMtOw7v0cEAAAAOIQr7NLBPVq4+6/mb1CD0fUQaZ/zvOPXL5BWTJI2L5VKivweGQAAAMIY4Qq7dHjvlu5+5upMpWXnq8E46h6p9UApL1167ijp4SHSiyfJ9Y4HAAAAfEC4wm/udzWwfYo7/np+mhqM2ETpd69JA8+QohO8czZFcNVkv0cGAACAMOV7uHrsscfUuXNnxcfHa8SIEZo8eec/HM+ZM0cnn3yyu97WAz300EM7XHP77be75yrfevfuXcffRWgb1buVu/9iXgMKVya5jXTSU9Jf10tDzvHOWRfBma/6PTIAAACEob0KV6tWrdLq1asrHlsguuaaa/T000/v0eu8+uqruvbaa3Xbbbdp2rRpGjRokMaMGaO0tOp/iM/Ly1PXrl11zz33qHXr1jt93X79+mndunUVtx9++GGPxoWqRvXxpgb+sChd+UUlapBGXuHdlxZJb18srZri94gAAAAQZvYqXP3ud7/T119/7Y7Xr1+vI444wgWsm2++WXfcccduv86DDz6oP/zhD7rgggvUt29fPfnkk0pMTNSzzz5b7fXDhw/X/fffrzPOOENxcXE7fd3o6GgXvgK35s2b78V3iYB+bZPVOjleW4tKNGlpA+kauL2WfaTTXpAiorzH01/we0QAAAAIM3sVrmbPnq19993XHb/22mvq37+/fvzxR7300kuaMGHCbr1GYWGhpk6dqtGjR28bTGSkezxp0iTVxKJFi9S2bVtX5TrrrLO0cuXKXV5fUFCgrKysKjdsY1MrDy+vXn05rwF1Ddxe3+Ol897zjme/JRXkePthFRf4PTIAAACEgb0KV0VFRRWVoy+++ELHH3+8O7a1TTYNb3ekp6erpKRErVp563kC7LFVw/aWrduygPfJJ5/oiSee0LJly3TQQQcpOzt7p19z9913KyUlpeLWoUOHvX7/UDW6PFx9PneDSksbcEe+TgdITbtIhdnS4yOl8YdLX9/l96gAAAAQBvYqXNmaJpvC9/333+vzzz/XUUcd5c6vXbtWzZo1k5/Gjh2rU089VQMHDnTrtz766CNlZGS4CtvO3HTTTcrMzKy42ZoyVHVA9+ZKiovWhqwCTV25RQ1WRIQ05h/ecWZ5xfKnJ6XSUl+HBQAAgNC3V+Hq3nvv1VNPPaVDDz1UZ555pmtEYd57772K6YK/xdZBRUVFacOGqtPM7PGumlXsqSZNmqhnz55avHjxTq+xKlxycnKVG6qKi47S6L5elfHDX3evOumb3kdLo26VYht7j0sKpNU0uAAAAEADDFcWqmxan90qN5+4+OKLXUVrd8TGxmro0KH68ssvK86Vlpa6xyNHjlRtycnJ0ZIlS9SmTZtae81wdcwA79fw49nrGvbUQHPQddKNq6QBp3mPf33F7xEBAAAgxO1VuNq6datrAtG0aVP3eMWKFW7PqQULFqhlS29tzu6wNuzjx4/X888/r3nz5umyyy5Tbm6u6x5ozj33XDdlr3ITjBkzZribHa9Zs8YdV65KXX/99fr222+1fPly12TjxBNPdBUyq7ChZg7quW1q4LSGPDUwIDJSGnCqd/zLs9K39/k9IgAAAISwvQpXJ5xwgv773/+6Y1vPZE0k/vnPf2rcuHGuicTuOv300/XAAw/o1ltv1eDBg11QskYUgSYX1uWvcoMMW9M1ZMgQd7Pz9rV2fNFFF1VcY/tvWZDq1auXTjvtNLcG7KefflKLFi325lvFzqYGzmrgUwMDehwhHVoe0K2xxaTH/R4RAAAAQlREWVnZHs/vsvVSVh2yxhbPPPOMHnnkEU2fPl1vvvmmC0pWhQpm1ordugZacwvWX1Vl3QL/8N9f3L5XP954uCIjIxQUvntA+upO+y0vnf+h1PkAv0cEAACAEMsGe1W5ysvLU1JSkjv+7LPPdNJJJ7k9qvbbbz83RRCh66Ae3tTA9Vn5mrJ8s4KGrcEafLakMun186Sv7pKK8v0eFQAAAELIXoWr7t2765133nEtyz/99FMdeeSR7nxaWhqVnhAXHxOlsQO8bo5vT1+joGEt2sfeKzXvKeVulL67T5r5spSfKb19mTTnbb9HCAAAgHAMVzb1zxpHdO7c2bVeD3T3syqWrYFCaDtxSPuKdVf5RSUKGnGNpT98JQ37vfd47rvSZ7dIM/8nvX4+lSwAAADUf7g65ZRTXLOJX375xVWuAkaNGqV//etfNRsRGrwRXVLVNiVe2fnF+mp+moJKXJI08grveOnX0rTntz03733fhgUAAIAwDVfGNvq1KpV18LMOfcaqWL17967N8aEBsiYWJwxp547fmhZEUwMDmnWTWvXf8XzloAUAAADUR7iyzX7vuOMO1zWjU6dO7takSRPdeeed7jmEvpPKw9U3C9K0ObdQQWf/P0rR8d4+WDZV0KyYKG3N8HtkAAAACFLRe/NFN998s/7zn//onnvu0QEHeC2tf/jhB91+++3Kz8/XXXfdVdvjRAPTo1WS+rVN1py1WXpvxhqdf0AXBZVBp3u3gNRu0uYl0sqfpF5H+TkyAAAAhFPl6vnnn3f7W1122WUaOHCgu11++eUaP368JkyYUPujRIN02rAO7v7lyau0F9ulNSyBfa9W/OD3SAAAABBO4Wrz5s3Vrq2yc/YcwsO4Ie0UHxOpBRuyNW1lkE+n63yQd798ot8jAQAAQDiFq0GDBunRRx/d4bydsyoWwkNKQoyOHdjWHb88eaWCWqfyytW6GdL6WX6PBgAAAOGy5uq+++7TMcccoy+++KJij6tJkya5TYU/+uij2h4jGrAz9+2oN6au1ge/rtUtx/Z1gSsopbSTuo+WFn8hTThG6nKINPp2r7MgAAAAUFeVq0MOOUQLFy7UiSeeqIyMDHc76aSTNGfOHL3wwgt785IIUvt0bKJerZKUX1Sqt6d5LfmD1sn/8Vq052dK896TnhktrZ/t96gAAAAQJCLKarETwcyZM7XPPvuopKREwSwrK8u1mc/MzFRycrLfw2nwJkxcptvfn6s+bZL18dXla5eCVXGhtHKS9MVt0trpUq+jpTNf9ntUAAAACIJssNebCAOVG1vERkVq3roszV6TqaAWHSt1PUQ68Wnv8cJPpKy1fo8KAAAAQYBwhRprkhirI/q1cse2/ioktOjpNbkoK5V+eMh2zvZ7RAAAAGjgCFeo1T2v3py6Wtn5RQoJ+/7Bu5/8lHRnM+m5o6XNS6Uty/0eGQAAAIK9W6A1rdgVa2yB8HRQ9+bq2qKRlm7M1atTVumig7oq6PUdJ429T/r0Zqm0SFoxUXp4iBQRJZ3/odTJ65QJAAAA7HHlyhZy7erWqVMnnXvuufzKhqHIyAj9oTxQPTdxuYpKQmAaXUSENOIS6YYl0rnvSgmp3vmyEum7+/0eHQAAAEK5W2CooFvg3skvKtGB936l9JxC/fuMwTphcDuFlOwN3ibDL5/hrcW65DupzSC/RwUAAIA6RLdA+CI+Jkrnjuzsjsd/v1Qhl9uTWkk9x0j9yqfHfnCtVBrc2w4AAACg9hCuUKvO3q+T4mMiNXtNliYt3aSQdMQdUlyytOYX6eu7/B4NAAAAGgjCFWpVaqNYnTrU6xw4/rulCkkp7bxGF+b7f0pf3SWFWpUOAAAAe4xwhVr3+wO7uF4QXy/YqEUbshWSBp8pjb7dO/7uPuntS6TiAr9HBQAAAB8RrlDrOjdvpCP7epsKP/P9MoWsA/8kHfdvrzX7r69KL53CGiwAAIAwRrhCnbj4YK8t+9vT1ygtO18ha+j50tlvSLGNpWXfSb886/eIAAAA4BPCFerE0E6p2qdjExWWlOq/P65QSOt2+LYpgl/dKW0O4WodAAAAdopwhTqvXr3w0wrlFRYrpA27UGq7j5SfKT08WHpkqLT4C79HBQAAgHpEuEKdOaJva3VqlqjMrUV6bcoqhbTIKOnMl6WmXbzHmxZLr54jrZnq98gAAABQTwhXqDNRkRG66CCvevXUd0tVUBzizR6SWksXfSGd+rzUZrBUlCd9cbv06+vS8ol+jw4AAAB1jHCFOnXasPZqnRyvdZn5eu2X1Qp5jZpL/cZJp/3Xe2xNLt66SHrhRKkohBt7AAAAgHCFuhUXHaXLDu3mjh//enHoV68CmnaS2g/f9rikQFr2rZ8jAgAAQB0jXKHOnT68g1olx7nq1evhUL0KGHJO1cfzP/RrJAAAAKgHhCvUufiYKF12yLbqVWFxqcLC4LOkI+6QDv+r93j+B9LaGX6PCgAAAHWEcIV6cca+HdUyKU5rrXo1NcQ7BwZERUsHXC3tf7WU0kHK2yQ9M1paN1MqK5NW/CgVZPs9SgAAANQSwhXqrXp1aUX1aonyi8Jk7ZWJjpUu+FjqfJBUWiR9eYf02V+l58ZK717p9+gAAABQSwhXqDe/G9FRbVLitSZjq574ZonCSpMO0vEPS5HR3ubCkx71zs99R9qy3O/RAQAAoBYQrlCv1atbju3rjp/4dolWb8lTWEntKu13uXccEbXt/KTHfRsSAAAAag/hCvVqbP/WGtm1mWtq8djXixV2jrxTumGZ9OfF0llveucmPyW9fr703QPeWiwAAAAEJcIV6lVERISuO7KnO7a27Ks2h1n1yiSmerfuo6T9r/LOzXlb+upOafGXfo8OAAAAe4lwhXo3rHOqDurRXMWlZXrkq0UKWxER0hF3Sqc8J0XHe+cmPbLt+ay1Ukmxb8MDAADAniFcwRd/OsKrXr05bY2Wp+cqrANW/5OkK6d467CWfiMtnyi9foH0YB/p/T/6PUIAAADsJsIVfLFPx6Y6tFcLlZSW6f7PFvg9HP816Sj1G+cdTzhamvOWdzzjJSlzta9DAwAAwO4hXME3N4zp7Qo3H/66Tr8s3+z3cPw3crs9ryLK//P88RGpNIz2BQMAAAhShCv4pm/bZJ0+rIM7vvODuSotDfNOee32kTod6B0PvcBbi2V+flJ6ZpRUWurr8AAAALBrhCv46toje6pRbJRmrs7UuzPX+D0c/530tHT0A9JRd0t9jpcOvUmKipXWTpfS5vg9OgAAAOwC4Qq+apkUr8sP6+6O7/pwvjLyChXWUtpJ+/5BikmQIiOlQ2+Uuh7qPWfNLgAAANBgEa7gu98f2EXdWzZWek6B7vhgrt/DaXgqh6uMVdI390r5mdLKn6TcdL9HBwAAgHKEK/guPiZK950y0DW3eGvaGk2huUX14WrFj9IbF0rf/EN66hDp2THSO5f5PToAAACUI1yhwbRmP2N4R3d867tzVFRC84YKLftKSW2lojxp9WTv3JZl3v2iz9hoGAAAoIEgXKHBuP7InkpJiNG8dVn652cL/R5Ow2ElvbH37vx5Gl0AAAA0CIQrNBjNGsfp3pMHuOMnv12iX1dn+D2khqPv8dKoW6U2g6TjHpZiErc9t/oXP0cGAACAcoQrNChH9W+jEwa3dcePfrXY7+E0LAddJ13ynTT0POnmddLBN3jnCVcAAAANAuEKDc5Vh3d3M+E+m7tBc9dm+T2chqv9MO9+yVfS+1dL/zvD6yIIAAAAXxCu0OB0b5mkYwa0ccc3vT1LJaVlfg+pYep0gJTcTspZL02dIC38WPrhX9K896WifL9HBwAAEHYIV2iQ/npMXyXFRWvmqgw9+0N5ZzxUFddYuvATqc3gbecsXL16tvT9A36ODAAAICwRrtAgtU6J11+P7eOOH/hsgZal5/o9pIapSUfp4m+kW9Kl5Pbbzk97QSqlnT0AAEB9IlyhwTptWAcd2L25CopL9Zc3f1Up0wOrZwvUomKkEx6R+p/snbOpglPGMz0QAACgHhGu0GBFRETo7pMGKDE2SpOXbdZLk1f6PaSGrdvh0inPSkPO9h5/fIP0wonbKljpi6Sv7pIK83wdJgAAQKgiXKFB65CaqL8c1dsd3/PRPK3eQjD4TSOvlJr18I5X/ig91F+a9Jj09GHSd/dJ3+5iQ2IAAADsNcIVGrxz9uuk4Z2bKrewxE0PLC5hLdEutewjXfWLNPY+73HWGunT/5MKs73Hiz73dXgAAAChinCFBi8yMkL3njxQ8TGRmrh4k255d47Kylh/9ZuGXyQdcPWO5zNWSCVFfowIAAAgpBGuEBS6tmisf58xRJER0suTV+qT2ev9HlLDFxklHXGHdN3CqucLc6R1M/0aFQAAQMgiXCFojOnXWlcc1t0d3/reHG3JLfR7SMEhqZV00niv0UW3Ud65pV/7PSoAAICQQ7hCULny8O7q1qKRNmYX6JpXZ6iE9uy7Z+Bp0gmPSf3GbdsHa/5H0sqfJKZYAgAA1ArCFYJKXHSUHjlzH7f+6tuFG/XUd0v8HlJw6X+KlNDUW3f1ypnSs2OkF8ZJpSV+jwwAACDoEa4QdPq2TdYdx/d3x//6fKHmrM30e0jBIzZRGnrBtsdRcdLSb6QFH3mPc9KkiQ9L6Yt9GyIAAECwIlwhKJ06rL2O6NtKRSVluvbVmcovovKy2w68Rtr3Eumcd6T9r/LOff+gNP9D6ckDpc9vkZ46SFr4qd8jBQAACCqEKwSliIgI3X3SADVvHKsFG7L14OfbdcTDzsWnSEffJ3U7TBpxiVe9WjtNeuV3Us4GKaaRVJQnfX2X3yMFAAAIKoQrBK3mjeN0z0kD3fH475fqp6Wb/B5S8GncUjp1gpTaVYqMlkZeKf2+vGK1cQFrsQAAAPYA4QpBbXTfVjp9WAfX8O6612YqO5/NcfdY76OlK6dKN66UxtwltewrRSdIxfnS5mV+jw4AACBoEK4Q9G45rq86pCZoTcZW3f7eXL+HE5wiI6XYRuXHUVKLXt5xGr+eAAAAu4twhaDXOC5a/zptsCIjpDenrdZb01b7PaTgZ9Urs3a6tGGOlL5IKi31e1QAAAANGuEKIWFY51T9cVQPd3zz27O1aEO230MKbi37ePc/PCg9sb/06DDpnz29dVgAAACoFuEKIeOqw3vowO7NtbWoRJe/NE15hcV+Dyn4K1cmNsm7z90oTXrUtyEBAAA0dIQrhIyoyAj96/TBapkUp0VpOfrrO7NVZp0usOc6jpBSu0ndDpf+NFs6733v/Kw3pXw2bQYAAKhORBk/fe4gKytLKSkpyszMVHJyst/DwR6yluy/G/+TSsukW4/tqwsP7OL3kIKT/dEQEbHt+LERUvoCaZ/zpOY9pa2bpe6jpU77+z1SAACABpENqFwh5OzXtZluOKq3O77jg7n68Nd1fg8pOAWCVeD4kBu842nPS5/dLH3/T2nCMdLXd0vrfvVtmAAAAA0F4Qoh6ZKDu+r8/Tu745vfmaWN2QV+Dyn4DThFOmm81KiF1OUQqethUlmp9O090lMHS2um+T1CAAAAXxGuEJIiIiJ08zF91LdNsjLyinTTW7NUavMEUTMDT5P+vFg67z3prNelEZeWP1Emzfifz4MDAADwF+EKISsmKlL3nTJQsVGR+mLeBj353RK/hxRaomKksfdKZ7/pPZ7zllRS5PeoAAAAfEO4Qkjr3y5Ftx/fzx0/8OkC/bAo3e8hhZ4uh0qJzaW8TdKXd0hz35O2bvF7VAAAAPWOcIWQd+a+HXTasPaue+BVL0/Tmoytfg8ptERFS/td5h3/+LD02jnSO5d7j9PmSYu/lApyvGMAAIAQRrhCWKy/uuOE/urfLllb8op02YtTlV9U4vewQstB10knPi2ldvUeL/hYWjFJeuYI6cWTpPu6So/vJ81+y++RAgAAhG64euyxx9S5c2fFx8drxIgRmjx58k6vnTNnjk4++WR3vf3A/NBDD9X4NREe4mOi9MRZQ9UkMUa/rs7Un16dQYOL2mSt2gedLv1xutTjSK/BxXNHSYXZ3vMl5d0aF37i6zABAABCNly9+uqruvbaa3Xbbbdp2rRpGjRokMaMGaO0tLRqr8/Ly1PXrl11zz33qHXr1rXymggfHVIT9eTZQxUTFaGPZ6/XNa/OUGFxqd/DCj0jLqn6+NQJ27oKrvzJuy8plp49SnruaKmUKiIAAAgNEWVlZb79871VlYYPH65HH33UPS4tLVWHDh101VVX6cYbb9zl11pl6pprrnG32nrNvdmFGcHHNhW++pXpKi4t0+8P7KJbju3r95BCi/2RMudtafUUqfVAafCZUn6WdE9Hr6J13QIpfaH0/HHe9Rd/I7Ud4veoAQAAapwNfKtcFRYWaurUqRo9evS2wURGuseTJk2q19csKChwv2iVbwhdxwxso0d/5/0w/+zEZXp3xhoVl1DBqtUpgv1Pko662wtWJj5ZauV1bdSr50jf3rft+mXf+TNOAACAWuZbuEpPT1dJSYlatWpV5bw9Xr9+fb2+5t133+3SaOBmlS6EtqP6t9FJQ9q5IsvVr8zQZS9Nk49F3PDQYV/vfvVkafn3284TrgAAQIjwvaFFQ3DTTTe5Ml/gtmrVKr+HhHpw57j+uvjgrm6T4c/nbtBnczf4PaTQNvwPUof9tnUUjIjy7q2r4LwPpKcPk9b96usQAQAAaiJaPmnevLmioqK0YUPVH2jt8c6aVdTVa8bFxbkbwkujuGj939F9XLh69OvFuuP9uRrRJVVNEmP9HlpoatVX+v2nUnGBNPFhqXV/6b2rpNyN0qtnedd8d5809j4pIVWKifd7xAAAAMFRuYqNjdXQoUP15ZdfVpyz5hP2eOTIkQ3mNRH6Lj+smzqmJrrNha9/fSYt2utadJx0yJ+lXmOl0X+r+ty896WHBkhvXODX6AAAAIJzWqC1TB8/fryef/55zZs3T5dddplyc3N1wQXeD1bnnnuum7JXuWHFjBkz3M2O16xZ444XL168268JbC8xNlqPn7WPq2B9MS9N93463+8hhY8hZ0kjr5SiKlWOS4ulBR9JafOlL/4m/edIaWuGn6MEAABo+K3YjbVMv//++13DicGDB+vhhx927dTNoYce6lquT5gwwT1evny5unTpssNrHHLIIfrmm2926zV3B63Yw9Pb01frT6/OdMcPnzlExw9q6/eQwsu/B0lblm97bGuzNi/1jk943AtiAAAA9WxPsoHv4aohIlyFr/s/na/Hvl6ixnHR+uCqA9W5eSO/hxQ+nh0rrfyx+ueGnC2d8Fh9jwgAAEBBsc8V0BD9aXRPDevUVDkFxbrw+SnKyCv0e0jhY8xdUqOWXogafbsUl7LtuWXfS5lrvA2KAQAAGigqV9WgchXe0rLyNe6xiVqbme8aXTx97lD1bs3vg3pXXCjlZ0oPdN92rv8p0rjHvaYYAAAA9YDKFVADLZPjNeHCfdW+aYJWbs7Tec9OdoEL9Sw6VmrcQmq7z7Zzs9+QPrp+11/HvxcBAACfEK6AavRsleTWXPVo2Vgbsgp0yYtTlV9U4vewwtPhf5X6HC8dda/3ePqLUvqi6q/NXi/9q7/01iX1OkQAAABDuAJ2wjYTHn/uMKUkxGj6ygz99Z3ZYhatD7qPkk5/QdrvUqnX0VJZqfTuFdLiL711WJV9/08pa7X06yvetEIAAIB6RLgCdsG6BT76uyGKjJDemLpaz06s1Coc/lSxouOlVT9LL54kPTpM2rTEe660VFrwybZrN87zbZgAACA8Ea6A33BQjxa6+Zi+7vjvH87VM98vpYLll1b9pEu+k/qd6D0uypM++6tUWiIt/UrKXLnt2nXenmUAAAD1hXAF7IYLD+is8/fv7Hol/P3DeXqOCpZ/WvSSTp0gXf6zFBElLfhIeqCH9OIpVa9b96tfIwQAAGGKcAXshoiICN12XF9dd0TPigrWV/M3+D2s8Nayt3TUPVJskpS3ydoEeudH3bqtcrVmqvTNvay/AgAA9YJwBexBwLry8O46Y3gHlZZJV/1vuuaszfR7WOFtxMXSDUukzgd5jweeLvU+zjveMFt6+zLpm39IM170dZgAACA8EK6APQxYd5zQXyO7NlNuYYnOeuZn/bo6w+9hhTfbUPjMV6RxT0pH3y816y41aumtx0pf4F0z732/RwkAAMIA4QrYQ7HRkXrynKEa3KGJMvKK3CbDSzfm+D2s8BbXWBp8phSfIkVGbmt4EbDsO2nrFr9GBwAAwgThCtgLtvfVixeN0MD2KdqSV6Rz/jNZKzbl+j0sBPQ/uerj0mLpw+ulnI1+jQgAAIQBwhWwlxrHRevZ84era/NGWpOxVWc8/ZOWpROwGoQO+0ot+3p7Yh38Z+/c7Dekx0d4e2FZ20dr3w4AAFCLIsrYsGcHWVlZSklJUWZmppKTk/0eDhq4tKx8/e6Zn7U4LUetkuP0vz/sp24tGvs9LORukgqzpaadpRWTpA+vk9Lm2B97UmKqFNNIOuVZqcNwv0cKAABCJBtQuQJqqGVyvF7+w37q1SpJG7IKXAVrcVq238NCo2ZesDKdRkoXfyMNPd9r2W6t223D4WfHSB/dIJUU+T1aAAAQAghXQC1okWQVqxHq0yZZG7MtYHmVLDQg0bHSsQ9Jp78onfGy1P8UqaxEmvyUNHm836MDAAAhgHAF1JJmjeP00kUj1Lt1ktJzCnTm+J80f32W38NCZRERUp/jpN5HS6f8RzryLu/81Oe8dVgAAAA1QLgCalFqo1i35soCllWwjnvkBz393RKxtLGB2udcb+1V+kLp56ekd6+Upr8kLflaWvip36MDAABBhoYW1aChBWpqU06BbnjjV305P809Pm9kJ91+fD+3CTEamPev8SpX1Tn/I6nzAfU9IgAA0IDQ0AJoAFMEnzlvmG4/rq+bifb8pBV65vtlfg8L1Tny79K+l0hxKVLHkVWf++I2pgsCAIDdRuWqGlSuUJue/WGZ7vhgrju+aWxv/f7ALoqO4t81Gqw1U6WCHOnlM6SiPGnM3VKvo6SmXbw1WwAAIKxkUbkCGo4LDuisPxzUxR3f/fF8HfrAN1q4gVbtDVa7oVLXQ6TDbvYef3qT9PAQ6YM/UcUCAAC7RLgC6pits/q/o/votuP6uoYXq7ds1eUvTVNeYbHfQ8OujLxCGnDatse2LuuNC6QNXhUSAABge0wLrAbTAlGXjS7G/vt7pWUXaEjHJnry7KFqlRzv97CwM6WlUvY6ad770id/8c7Fp0gXfysV5kjZ66XOB0n5GVLjVkwbBAAgzLMB4aoahCvUpV+Wb9aFE6YoK79YXZs30uuXjnQNMNDArf5F+uh6ae30qucjorzNiA+6Xhp1i1+jAwAAdYQ1V0ADNqxzqt6/6kC1a5Kgpem5+t34n7U2Y6vfw8JvaT9MOv0lKamN9zgqTmrU0gtWZuJDUvoi77ggW8pcI23dIv38tNcgAwAAhDwqV9WgcoX6sGRjjs54+ie32XCr5Dg9e/5w9Wub4vew8FuKtkp5m6XEVCkyWtq8zJsyuOQrKaWjdPhfpR8e9M4nt5W2LJP2u1w66m6/Rw4AAPYC0wJriHCF+rJ6S54ueG6KFqXlqFFslB49ax8d1qul38PCntq8VJpwrJS1ZufX3JbBmiwAAIIQ0wKBING+aaLeuGx/7d+tmXILS1zQOuqh7zR/fZbfQ8OeSO0qXfmLNPLKnV+TvlDK3SQ9f5w0fpRUXFCfIwQAAPWAylU1qFyhvhUWl+r29+fo1SmrVFJapg6pCXrz0v3Vkk6CwWfpt5LKpIWfSlP+I8XES/mZ0tALpOU/SJvK12Wd/ZbUfZTfowUAAL+BaYE1RLiCX2z91UlPTNSqzVsVGxWpG8f21oUHehsQI8jYH62lxdKvr0nvXr7j8yMuk8be48fIAADAHmBaIBCkWiTF6dnzhmtQhyYqLCnVnR/O1Q+L0v0eFvaGra+KipEGnSkd/GfvXLMe0lHlgWrRZ9Kqyd40wXculzYtkXLTvQ6DAAAgKFG5qgaVKzQEN775q16Zskpx0ZG6aWxvnbd/Z0XQECF4ZaySGrf01lrd18WralXWpJO3GbFtUnzVNC+YAQAA31G5AkLA7cf302G9WqjArceaq/Oem6INWfl+Dwt7q0kHKTpOik+W+p+y7byFqZhGUsYKb21WxkqvogUAAIIO4QpooOJjotzeV3ec0M9Vr75buNF1Epy5KsPvoaGmTnhMOv4Racg50mWTvL2xKlv8ubdP1qc3S6+dK62Z6tdIAQDAHmBaYDWYFoiGZnFatq5+ZYbmrM1SSkKMXrl4P/Vpw+/NkFFcKH13n7RpsTTnbe9cYjMpb5N3nNxOuuxHKaGJr8MEACAcZTEtEAgt3Vsm6dVLRmpIxybK3Fqkc/7zsx77erHWZGz1e2ioDdGxXvXqmAft37y8cxasmvf0jm1z4jculApyfB0mAADYNcIVECQax0VrwgX7qm+bZKXnFOr+Txfo5Md/1ObcQr+HhtqSmCodeqPUYT/pgKulP3wlXfSVFB0vLflS+s8R0trpUvpiacb/vKmDAACgwWBaYDWYFoiGLCOvUM//uEJvTlutlZvztF/XVD11zjA3XRAhatUU6ZUzpdyNXmUrMloqLfKeazNIOvqfUofhfo8SAICQxCbCNUS4QjCYvz5LJz72o7YWlahri0Yaf+4wdWvR2O9hoa7YHlif3CjNet173Ky7V7kqK5EiIqVeR0ulJV67d+tIePit3nRDAABQI4SrGiJcIVjMXpOpi//7i9Zm5ispLloPnDZIY/q19ntYqEvrZkrZG6QeR3jrsmyq4OalO1533MPS0PP8GCEAACGFhhZAmOjfLkXvXXWg9u2cquyCYl3ywlT96dUZ2phd4PfQUFdsGmDPIyXbULpRc+n4R6XoBKllP+mIO6XGrbzr3v+jNHm8t3cWAACoF1SuqkHlCsGmqKTUNbgY//1S2X/RyfHRuu+UQTqqP1WssGABKi7ZC1xbM6QH+0pFud5zPcdKQ86WWvWVUrtW//VZ66S4xlJcUr0OGwCAYMC0wBoiXCFYzViVob++M0uz12QpOjJCD585REcPaOP3sFDfvrtf+urvVc9ZE4w+x3sdCQ+/ZdueWWumSc8eJbUZKF30hS/DBQCgISNc1RDhCsGsuKRU178+U+/MWOsenzeyk246uo/iY6L8Hhrq2+e3SRMf2vF8Ulup+yip9UDps79KJeXTSK+YIrUo31sLAAA4hKsaIlwhFALWPz6ar2cnevsg9WqVpMfOGqKuzRsrMrJ8k1qEvpJiae47UkoH6b2rvK6CNl0wY2X114+6VTrouvoeJQAADRrhqoYIVwgV3yxIc1Us23TYJMZG6elzhunAHs39Hhrqm/1Rb2uyCnKkpV970wHT5kqlxd6UwYWfeNfZBsanvyg1buE9tutjG3lfCwBAGMoiXNUM4QqhJC0rX9e+NlM/LE6vOHfMgDa64IDOGtY51dexoYHISZP+1U8q8UK4Oh0oHXW3tGmx9M5lUucDpTNeZt8sAEBYyiJc1QzhCqEoPadApzzxo5ZvynOPWyTF6cvrDlFyfIzfQ0NDsGKStPgL6fsHqn+++2hp6AVSfoa08ifpkL9ITTrU9ygBAKh3hKsaIlwhVC3ZmKMHP1uoD2etc49PGtJOd588QHHRNLtAueUTvYC1arJUmONNE1w9WSorrXpdcntpn3Ol1C5e8LIuhAAAhCDCVQ0RrhDqvl+0Uef8Z7I77tGyse47ZaCGdGzq97DQ0JphZK6UmnSW1s+UZr4qzf9QKt4qxSRUbYphQeuCj6SmnfwcMQAAdYJwVUOEK4SDT2avd3tiWbML61Vw7n6ddN2YXkwTxG/L2yxNf0FKXygt/VbKXCXFp0gjLvOmC0ZG+j1CAABqDeGqhghXCBdbcgt15wdz9db0Ne5x88axOqJvK52zX2f1bcvvfeyGrHXSiydLaXO8xykdpR5HSMMv8tZwtejt7akVydRTAEBwIlzVEOEK4Wbi4nTd8s5sLU3PdY9tK6zURrE6sl9r/ePEAX4PDw1daYk05Rnp4xuqf77VAKnNIK8ZxhF3SM261fcIAQDYa4SrGiJcIRwVFJfo+4Xpenv6moqGF+aVi/fTfl2b+To2BInv7pcWfipt3eK1cW/UUiopkPIzt10TnSCd957UYV9py3LpnSuklHbS2PukSY9Ks16Xfvea1KKXn98JAAAVCFc1RLhCuFu1OU//9/Ysfb8oXQPapWjCBcPVrHGc38NCsNiaIc17X+o5xnv82S1SWYmUtVZaMVGKjJEGnynN/0jK27b/WoX+J0unPFvvwwYAoDqEqxoiXAHShqx8HfbAN8orLFFKQoxuGttbpw3roEibMwjsjfws6amDpS3Ltp1r2ddr+V65+6Dpfax08PVS7iYpLknqOKLehwsAgCFc1RDhCvDMXJWhG9+apXnrstzjoZ2a6pZj+2pwhyZ+Dw3BauNCacp4KSrWmxrYY4zX7GLNNEll0pd3Sit+8K6NivOmFZrhf5AO/6uUsJPfeyVFXnhrxBRWAEDtIlzVEOEK2Ka4pFQTflyuf32+ULmFJe7cmft20G3H9VN8DB3gUMs2zJW+u09a9LlX0aosLsXrRGhNMWydVoD9NfbK77yvOedtqctB9T5sAEDoIlzVEOEK2NG6zK3652cL9ea01e5n2S7NG+nPY3ppbP/WirCNsoDaZE0wvr1Paj9Mim/idSK0fbVMk07SuCek5LZSk47Sos+kl8/wnmveS7psohTFfm0AgNpBuKohwhWwc98v2qg/vTrDbT5sbIqgrccaQUdB1KWSYmn1FOmdy6qu2WrWXSrMlbK3dbhU10O99vAWvkZeKbUZ6MuQAQChgXBVQ4QrYNey84s0/rulGv/9Mm0t8qYKjurdUrcf308dUhP9Hh5CWfYG6as7pTlvS8X5Ummxd755T+nAP0nvXy2VeMG/Yirh1TOkxFSptFTKWC417SJRbQUA7CbCVQ0RroDdk5adr39/sUivTFmlktIyNYqN0s3H9NWoPi3VMimO6YKoO/ZXV94m6cPrpJgE6ah7vGYXa6dLPz4qtdtHmvq8lL7Au77nUV51a91Mb0Nj21crtasUHSfFp/j93QAAGjDCVQ0RroA9s2Rjjm56c5YmL99ccW6fjk307PnD1SQx1texIYwt+HjbWqztRURKZaVeR8Juh0kteksHXiMlNK3vUQIAGjjCVQ0RroA9Z5Wr5yYu07+/XKTsfG+qVtcWjVxXwUN6tvB7eAhH9tfb57d6Fau2+3jTCHuNlX74l/Trqzten9xeOv2/riO8vrhNajdUGnWr1yq+suJCKZp/NACAcJFFuKoZwhWw9+yPlEVpOTr7mZ+Vlu3tUXRA92Ya06+1ThnaXomx0X4PEeHO/tpb/YuU3EbKWudNJfzpca9RRqCiFdDnOOmwm6XCPCkmXpo6wbu1Higd84DUdoif3wkAoB4QrmqIcAXUXGZekR75apGen7RcRSXeHzMdUhN070kDtX/35n4PD6jKNiB+8yJp0afe4+5HSEu/kUqLdv41CanSue9KWzdLnQ+SivKk2W9J3UdX3YcLABDUCFc1RLgCas+KTbn64Nd1+t/PK7UmY2vFJsRj+7dR2yYJ6t6ysd9DBDz21+HmpVJiM685hlW33rvK21/LgpSFqK6HScMu8KYbblq87WsbtZByN3rHSW2l0bdLHUdITTv79u0AAGoH4aqGCFdA7cspKNa9H8/XCz+tqDgXGx2pVy7eT/t0pIkAGjDbM8vWXdlfl4EOmFaheuOCXX+dNcuwJhnWJr4gy9vw+PsHpWbdpH0vlloPqJfhAwBqhnBVQ4QroO78tHST7vl4vmasynCPUxvF6i9H9dLxg9opIXa7xgFAQ2V7Zr1/ldfcYuy90oyXpMVfSPteIi340Gv5vn7Wzr8+MkZq0sELbAf8UdrnfCkysj6/AwDAbiJc1RDhCqh7uQXFOv3pSZq9Jss9ToyN0nn7d9ZFB3ZRs8Zxfg8PqBn7q9U6Etpmx7npXuXKphf2GOM1zVj4cdXruxwiDTpDKsz1rm/ZW+p2uBTTyKua2ZRDm3rI3nEAUO8IVzVEuALqR35Rif47abmbKrhqs7ceKyYqQhce2EU3jOmtqEh+kESIKCmWNi/xpgia2W96AcxC05d3SMXe7/8d2Pov60y49GupaRfp8L9KA06p16EDQLjLIlzVDOEKqF/2x9CX89Jcd8GZqzPduQO7N9cRfVu5gHXmvh0JWghdGxdKU56R0uZK8SleM40Fn0h56dVfH5cipXaW+p8spXTwGm9Yk43mPep75AAQFrIIVzVDuAL88+6MNfrz67+qsGTbXkOXHNxVNx3dx9dxAfXeRKMwR/ruAWn599Ko26SVk6Rv77N/jtjxeptq2LyX1KqfVFIoNe0kDb3AW/e15Ctp/z9Kzbt7FTSbZsj0QgDYbYSrGiJcAf5aujFHT327VD8uTa+YLmiVrMsO7ab9uzVTBD8YIlzlbPRawlvQ+vFRKWut1HawtGLirr/O1m7FNZZy0qT4ZG89V+cDvU2U+x4vFW2VJo+XouOlvidIPY+sr+8IABo8wlUNEa6AhuPJb5fo/k8XqKTU+6NqQLsUXX5oN43p11qRTBVEOLO/vkuKpOhYLyRZlWrDbCkq1tsMecWPUmS0lNrVm3K4J/qOkw66znst64S4z7lMOwQQtrIIVzVDuAIallWb8/SfH5bplSkrlV/kTRfs3TpJV4/q4dZlRUfRwhrYgQUvtzdXpLTmF68qldRaylgl/fCglLVGSmrjTRs0/U+RYhtJv/xHKiuflhuTKBXlSbFJUvuhUoveXliz7odxyVLXQ701X9bJ0KYdNulMS3kAIYdwVUOEK6Bh2pxbqOcmLtNzE5e7TYlNuyYJOnu/TjpjeAc1jo/WO9PXqFfrJA1s38Tv4QLBae0MaeK/pTlvbZtSWJS7e1/bqr/U5WAv1PU5ToqK8aYuWpDLXie1GSz1O4kABiCoEK5qiHAFNGwZeYWukvXiTyu0Ja/InYuNjlSLxnFak7FVSXHR+u6Gw9S0UazfQwWC16op0uop0uDfeWu88jZLq372qlaxjb3nNs6X2u4jFRdImxZLJQW//bpW+Upu5zXWsKYbrQdIcUle23k7Zz+WFGR71bbFX0rDLpSadauP7xgAqkW4qiHCFRA8+2S9P3Otnp+0vGIz4oAeLRvr+jG9dFivli54AahlpaXelEFrlGFyN0k//lvKz/RuqyZ7YcmmHlqYskYav76+8yqYre9q0tELVjkbtp23rz/2X1JkjJS5SoqO8xpy2FTELcu9r7HuinaeZjcA6gDhqoYIV0BwsT/GFqfluFtuYYmuf31mxXOtkuP0+Fn7aGinVF/HCEBe9cvWaFn4siYbs96Qtm6RCrN3vNbCVKlXma5WQlPva23tlwWyFr28NWEbF0jJbaWmnb01YY1bete37OPtIwYAe4hwVUOEKyB42R9pT3y7RDNWZmjGqgylZRcoPiZSvz+wi84Y3lEdUhP9HiKA7VnlKXO1tGWZFBUntRnoNdPITZe+ulNa+ZO3fssqYLlp0trp5V9olard/DHGwtjgs7yOitaSvuNIqThfSkyVmnbxpifa1ETbJ8xCmgtuSV7HRSpiQFjLCrZw9dhjj+n+++/X+vXrNWjQID3yyCPad999d3r966+/rltuuUXLly9Xjx49dO+99+roo4+ueP7888/X888/X+VrxowZo08++WS3xkO4AkLD1sISXfbSVH2zYKN7bD8fHdqzhc7ct6MO691SMXQZBIJT9gZp8xKpzSCvUmVdDic95oU0a6SRu1HaMEda9p0XoKyyZaFsdyW1lbLXloc3SSntpY77Se2He0052g/zpjxaSJv/odeZ0SplLXp6965LI4EMCBVBFa5effVVnXvuuXryySc1YsQIPfTQQy48LViwQC1blpfyK/nxxx918MEH6+6779axxx6r//3vfy5cTZs2Tf37968IVxs2bNBzzz1X8XVxcXFq2rTpbo2JcAWEDtsf69M56/W/n1fqh8XpFeebN47VSfu0d/tlxUVHqk+bZEWxbxYQmooLpUmPepWxtkOk+CZeQw6rZtlUxc1LpdWTpcI8rxJm1atdadRSatLBq6AF2tYH2NRDe53uo6Wh53nTEbes8BqBpHSQWg+U8tK9NWkWDu11ADRoQRWuLFANHz5cjz76qHtcWlqqDh066KqrrtKNN964w/Wnn366cnNz9cEHH1Sc22+//TR48GAX0ALhKiMjQ++8885ejYlwBYSmZem5ennySr01bbXSc6r+8NStRSPdelw/HdKzhW/jA+Aj+3HIglL2emndTKnDCKmsxKuGpS+QVv4srZshrZjorRkLaDfM2z/MmmtY98RSb5uInQrsHWZsCqSFL9uLzG42BdEqbzYOC2AWviyk2R5lhbneWGwao1XNLBTatEWbLgmgTu1JNoiWjwoLCzV16lTddNNNFeciIyM1evRoTZo0qdqvsfPXXnvtDlP+tg9S33zzjat8WbXq8MMP19///nc1a9as2tcsKChwt8q/gABCT5fmjfR/R/fRn8f00tfz0/TaL6vcuiybPrhkY67Oe3ayjh7QWkcPaOM2J46LjvJ7yADqi03ji4iSUtp5t8qS23jNMYy1nV8z1VsjZtMDrbV8QH6WF7LstWa+4jXssCCW1MrrbmhTGK2C5V6zvZS12gtsla366bcG6oWqwEbOFs4s3Nm6MauaWWXO9hXLWle+fqyX1H2UFxjTF0k9x3gdFgHUCV/DVXp6ukpKStSqVasq5+3x/Pnzq/0aW5dV3fV2PuCoo47SSSedpC5dumjJkiX6v//7P40dO9YFs6ioHX9YsimGf/vb32rt+wLQsNlaqyP7tXY3k51fpPs/XaD/Tlqhj2atd7emiTFuyqAFMmuEkZLIvw4DsJ+c4qRO+1f/nLWbt2YcxhpkjLmr6vMWzCx8WQdDm5po+4ZtzfCmIlpVyqYtLv3Wm65oLe4XfuYFsCrKtgU0u7fX2BMf/dkbpzXtiEnwKnXWvMPWjVkoTO0mterrVeQat/IqcdGxXpUtNlFq1sP7egANL1zVlTPOOKPieMCAARo4cKC6devmqlmjRo3a4XqrnFWuhlnlyqYmAggPSfExuuOE/jplaHt9PHu93pm+Rusy8/XKlFXu+Wd+WKZxg9u6sDW0U1NFsFAdwN4GM6skBViTjO3tf1XVx7Z+yypQtnGz/dmTt8mbEmjVNdu4OWOVlLHSq1ZlllfCmnTyGmvYNMMFH3n7hrl1Yh29zZndPmQ7CWX2+rb+bOqEnX8fgcqXTWW097HAZVMds9dJzXt568gC69esuhYTL7XfVyrM8a6x78WmYVpIs02oo6K9x/Z92q9RYMUKf9YiCPkarpo3b+4qSdZ8ojJ73Lq19y/K27Pze3K96dq1q3uvxYsXVxuurNmF3QCEt4Htm7jb9Uf20ncLN+qXFZtd2Fq6MVfjv1/mbrZv1kE9Wuic/TppUIcmfg8ZQKizapEqbSFhVa/A3l02BdBuu3LMP8s3e07yHlt7e+umaNMEjU0pTEj1NnG2cJO+UFo/y2u4YRs+W0CzhiAlBd60x5z1XpgLsErc0m+2PV7y1Z59f7afma1Ds+Bla9wsjBVt9daSWYhzt05ex0YLc7b3mVX5rNJmVTUbd7PuXnWtVX8v7NnUSRu7BTx7XbvOpk3aNUAdaxANLazturVfDzS06Nixo6688sqdNrTIy8vT+++/X3Fu//33d9WpQEOL7a1evdq9pq3LOv74439zTDS0ABCQX1Sij2ev0/eL0vXRrHXKL9rWGczClVW7RnZt5hpiUNECEPKsapY2zws/FlosjFn1zKpTic29dWXW9t4Ck7FGHFY5WzvD20fMwpy1xremHFZpy7dpkfXAQpw1A7GqmQVGq97Zn9luo+lWXhMRC202LpsuaU1EdnZv11vAdWOP8H4trH1/YNNrq74hpARdK/bzzjtPTz31lAtZ1or9tddec2uubC2VtWlv166dWxcVaMV+yCGH6J577tExxxyjV155Rf/4xz8qWrHn5OS49VMnn3yyq2bZmqsbbrhB2dnZmjVr1m5VqAhXAKpjjS+mr9qiN35ZrQ9+XafCkm1Bq21KvLeOq28rDe+Syh5aAPBbLMxkrfUqVbbGzIKLVdYsqFllKmOF18be7u06CzIWjGwaod1bYCrM9q6xEGcbRFdpox/hhSmbnmgVrLoUm+RVCK36ZsHS1s1ZmLQ92FylL8ILZ+6W6AW9Zt286ppV5Kw7pE3dtOYpJcVeE5TmPb3n7Huy79WqmPa1gdewmwU7/mGvzgVVuDLWhj2wibC1VH/44YddRcsceuih6ty5syZM2Db31/bB+utf/1qxifB9991XsYnw1q1bNW7cOE2fPt21Y2/btq2OPPJI3XnnnTs0wtgZwhWA37Ixu8Ctzfps7nrNWpNZpaIVGx2pAe1SdNqw9m7pgK3VatqI6SgAUKcslFhzDmOhxkKIBQ/7gzhzldc8xE0TLPGet8qTtdi3KpZVrCywWTXONp4uypeKt1Zzv9Wr1Nm0w8DaMZsy+Vst+OuKdbi078W6UVrAtJs9tnFaKLNKoU2dtMBq0z8t1Fnws+NA4xT79bBg6F4rcduat0C1LnALTMWsXOmzoBud4HWqtBCbm74t/Lkw2Mi7JsgFXbhqaAhXAPZ06uAPi9Jd0PpiXpo251bdQ6tdkwSdvV8nHdi9uQa0t7+AAABBq9SCRdG26X8l5dMjE5p4wcYCnjUGyd3kdXS0NWMWMCzwWIixe6vEbV7mrYWzoGJBxwKgrWuLjJQy10iblnjHbj1c0bavtwBlFbJgEWXhLsGr7lnIszV89utn5+z7t3V2du8qePZrlu39eli4s8Ytx/3b7++AcFVThCsAe6u0tEyrtuTppZ9XavKyza7CtSZja8Xz8TGRapEUpzF9W+vogW00pEMT1moBAPaMhS0LWRa47N66MFpYsRb/Vp2zkGeVOAtxFt4slNl6OQt+WzdvO7ZrXVORBC+w2etZyLF44Cp45YHOjt1G1zHemrqIQKWvPPQFtgdISPW2HLDKnr1vTdnUyCunyG+EqxoiXAGoLVn5RXrxpxX6dVWmvlqQpsLiqn/Z2FqtsQPauI2LOzdLVEJslBJjg38KBQAgjFj1zsJUdPkUeBfOCsordVZts1u2d84qWa4LZb5XpbJgZvc25dDWqVkVy8KfBUg77vvbzejqGuGqhghXAOpCTkGxtuQWas7aTLdR8ZfzNii3cMepHRayrh/TS0f2be3WbwEAAP8QrmqIcAWgvtZqfbtwoz6etc6t1bLwVVlibJR6tGys0X1aaXTfVurZKklRkUwhBACgPhGuaohwBcCPtVomO79Y/5m4TP/7eaXScwqqXNMoNsptcjy4YxO3VsvuWybF+zRiAADCQxbhqmYIVwAaQthasjFH01dl6P2ZazV9ZcYOlS3TvmmCBndooqGdmrr2711bNFYqbd8BAKg1hKsaIlwBaGhKSsu0OC1H01ducUFrxqoMLUzLrtiOpDLrRrh/t2bar2szNY6L1r5dUtUqmQoXAAB7g3BVQ4QrAMEgO79Iv67OdIHrlxVbtHB9ttZm5u9wXXRkhHq3SVKPlkk6qEdz9WtrFa5GiomiWQYAAL+FcFVDhCsAwWprYYlmrs5wmxpbdWtLnnUnLN9/pJLYqEj1aNVY3Vs21tqMre7+4oO7qVNqoiJpmgEAQAXCVQ0RrgCEksVp2Vqenqcpyze7Ctf8dVnVtoA3CTFRLnT1a5usvm2SFR8T5SpdfdoksdkxACAsZRGuaoZwBSDUm2Ws2pKneeuy3DquZo3j9N6MtfplxWYVlVT/V0LzxnHq2aqxa6CRHB+j4V1S1ad1sto1TaA9PAAgpGURrmqGcAUgHBWVlGrl5jwtWJ+t2WsytXBDtvIKS1y1q7C4tNqvsU2OuzRr5NZwtU6JV9PEWNe50O67tWykuOioev8+AACoTYSrGiJcAcA2eYXFWrghR4s2ZGtDVr7WZ+VryrItWpaeq8KS6kOXibPg1byR2jdNVMfURHVITXBdC63BRq/WSe4cUw0BAA0d4aqGCFcAsHvt4dds2aol6TlaujHXbXq8anOea6CxObdQmVuLdvn1yfHRSoqPcftyWeUrOjLSdUA8ZWh7t3dX00axdDQEAPiOcFVDhCsAqBn7q2XFpjwt25Sr1ZvztGrLVhe8LIBtLSrRwvU5u6x6BTRJjFHr5Hh1apaoTs0aqUNqopo1inXn2zXx1n8lxUerpKzMdUpsksgGygAA/7JBdC2/NwAAbrpf5+aN3K06toZraXqOC0Rp2QWu8pVfVKLcgmK9N3OtC2GlZVJGXpG7zV+fvdP3snVfURERLrSN7NpMgzs2UWJMlJo0ilXLpDh3s+mItrkylTAAQF2iclUNKlcA4P+Uw4y8QqXnFLp9uFZsytWKzXluGqLt3bXJzmduVX7Rb1e/KksNBK7k+PLQFec6IVoDDquG2b1dY8eN46JZEwYAEJUrAEBQs/bu1iLebtb8YlchzDocFpeUuj25Pp+7wQWxguJSt+5rQ3aBNmbla2NOgWszb+fstqtKWEBMVIRSEmLVtDx0NW3k3dvUQwtfWVuL1CguWkPK14elJMQoOSFGjWKjCGUAEKYIVwCAoA5h1pEw4MIDu+x0by+reNkURHfLyq+4T88tdFWyLbk2BbFQm/MKXUXMwphNT7TbnrBuiBayXNiKj644DoSvwLFVxhrFRSkxNlqNYqOVGBfl1o/ZOjILigCA4EO4AgCEvMhKlbA+bX77elv/tWW7wLXF1n/llt/nFbqqlbWmt42Ys/KLXHdEC2TFpdsqZHvL1pFZyIqNinDNOmzqot0sfCXGRikhJsoFsDh3H1nx2I7jowPPRXrn3OPy40rX2K8JAKB2Ea4AANiOhZA2KQnutrtsCbM11cjaWuyClt1s6mDFcXkAC5zPKSh2mzTnlt/bY7vZSmhr+FG5YrYha8+qZ7sjNirSBbAqwaw8jFUEM/c4cLztnO1hZvc7fG1MpNs4evvr7TWiaSYCIAwQrgAAqAW2zsqm+NmtdUr8Xr2GTV/MKSx24Ss7v1hFJaWKUIQ25uQrPbtQuYVeELPKmncr9e6Ly+93OL/tuKCotEr7ezu2m71PfbDpklUDWKRio6Ncdc66OEaX38dWOvZuVY8tpMVEevfuushINz008Jy9T+D1bO+0yufdLfA65V9nmS8ywu4jqtxHRsp1oXSP7bodrvE+cwCojHAFAEADYT/E23RAu1WVUiuvbw1ACioFLrtZtc0eF2wXxrYPaAXbXV851HnPlZZfWynQFW8LczZd0qvOKWR4Yc0LePbZ2ePAfVSlmzsf4V0bZaEuwlsvaOHMQlogtNlji2t2znvsBbgdH1e9zh64QOgOvWAYUflx+ddFVL6u0mPvtXb/ukCmDIzXO3bDqLjenS9/3ntc6Xylc+X/q3it3c2ru3PdttHV/LV21+4E7j15u8otvatr8F35VFmlqwPnqz6/42vt8Irbnaj8mlVet8px2Q7POZXeY9t4qr6vPfSer2bsKlOThFidNryDggnhCgCAMGE/xHvVtfp5P6vEWcDavooWCGBWmbObTYMsKi1TUXGpikutolbmOkB6z5e5++Lye29dW/m9nS/11rkVV3oucO0O593j8nOlZW58tqatpNJxaam8+7KyKj+YVifw3vnasy0BAOyebi0aEa4AAACMVXESrAFHbHB2PywrD16VQ1flIFY5xJWUlspmXVqIKw3cl18T+Lri8q+1e3tt2yjbvV6lMGfnyiqOvWvsX/CrPb/dY7s39h7bf90Or1/puoqvq+a6ah/vUMEor12Uv6erRlSqPmyrTpSfqeZrA9f81udR88/0N57/zVHsxmvU0ntUVAgr1bq2VQ13/lx1lbTKFcOK57a7ZvvzO3/d8tfcrhJZeTwR1Y2z4njnY9v+uZbJcQo2hCsAAIBq2A94bu2W3wMBEDRo3QMAAAAAtYBwBQAAAAC1gHAFAAAAALWAcAUAAAAAtYBwBQAAAAC1gHAFAAAAALWAcAUAAAAAtYBwBQAAAAC1gHAFAAAAALWAcAUAAAAAtYBwBQAAAAC1gHAFAAAAALWAcAUAAAAAtYBwBQAAAAC1gHAFAAAAALWAcAUAAAAAtYBwBQAAAAC1gHAFAAAAALUgujZeJNSUlZW5+6ysLL+HAgAAAMBHgUwQyAi7QriqRnZ2trvv0KGD30MBAAAA0EAyQkpKyi6viSjbnQgWZkpLS7V27VolJSUpIiLC96RsIW/VqlVKTk72dSyoGT7L0MFnGTr4LEMHn2Xo4LMMHVkh8llaXLJg1bZtW0VG7npVFZWratgvWvv27dWQ2G/IYP5NiW34LEMHn2Xo4LMMHXyWoYPPMnQkh8Bn+VsVqwAaWgAAAABALSBcAQAAAEAtIFw1cHFxcbrtttvcPYIbn2Xo4LMMHXyWoYPPMnTwWYaOuDD8LGloAQAAAAC1gMoVAAAAANQCwhUAAAAA1ALCFQAAAADUAsIVAAAAANQCwlUD99hjj6lz586Kj4/XiBEjNHnyZL+HhO189913Ou6449yu3REREXrnnXeqPG89Y2699Va1adNGCQkJGj16tBYtWlTlms2bN+uss85yG+w1adJEv//975WTk1PP30l4u/vuuzV8+HAlJSWpZcuWGjdunBYsWFDlmvz8fF1xxRVq1qyZGjdurJNPPlkbNmyocs3KlSt1zDHHKDEx0b3On//8ZxUXF9fzdxPennjiCQ0cOLBi08qRI0fq448/rniezzF43XPPPe7P2WuuuabiHJ9ncLj99tvdZ1f51rt374rn+RyDy5o1a3T22We7z8t+thkwYIB++eWXiufD+WcfwlUD9uqrr+raa691LSynTZumQYMGacyYMUpLS/N7aKgkNzfXfTYWhKtz33336eGHH9aTTz6pn3/+WY0aNXKfo/1FEmB/uMyZM0eff/65PvjgAxfYLr744nr8LvDtt9+6v9h/+ukn9zkUFRXpyCOPdJ9vwJ/+9Ce9//77ev311931a9eu1UknnVTxfElJifuLv7CwUD/++KOef/55TZgwwf0Fg/rTvn1790P41KlT3V/2hx9+uE444QT335jhcwxOU6ZM0VNPPeWCc2V8nsGjX79+WrduXcXthx9+qHiOzzF4bNmyRQcccIBiYmLcP1zNnTtX//znP9W0adOKa8L6Zx9rxY6Gad999y274oorKh6XlJSUtW3btuzuu+/2dVzYOftP6u233654XFpaWta6deuy+++/v+JcRkZGWVxcXNnLL7/sHs+dO9d93ZQpUyqu+fjjj8siIiLK1qxZU8/fAQLS0tLc5/Ltt99WfG4xMTFlr7/+esU18+bNc9dMmjTJPf7oo4/KIiMjy9avX19xzRNPPFGWnJxcVlBQ4MN3gYCmTZuWPfPMM3yOQSo7O7usR48eZZ9//nnZIYccUnb11Ve783yeweO2224rGzRoULXP8TkGl7/85S9lBx544E6fLw3zn32oXDVQ9i8z9q+uVkYNiIyMdI8nTZrk69iw+5YtW6b169dX+RxTUlLcFM/A52j3Vg4fNmxYxTV2vX3e9q898EdmZqa7T01Ndff236NVsyp/ljalpWPHjlU+S5sa0apVq4pr7F/qsrKyKqomqF/2r92vvPKKq0Da9EA+x+BkVWWrWlT+3AyfZ3CxaWE2hb5r166uamHT/AyfY3B577333M8sp556qpueOWTIEI0fP77i+WVh/rMP4aqBSk9Pdz8UVP5DxNhj+w2L4BD4rHb1Odq9/eFUWXR0tPuhns/aH6WlpW5Nh0176N+/vztnn0VsbKz7y2BXn2V1n3XgOdSfWbNmuXUbcXFxuvTSS/X222+rb9++fI5ByMKxTY23dZHb4/MMHvaDtU3j++STT9y6SPsB/KCDDlJ2djafY5BZunSp+wx79OihTz/9VJdddpn++Mc/uqmaJtx/9on2ewAA0BD/lXz27NlV1gMguPTq1UszZsxwFcg33nhD5513nlvHgeCyatUqXX311W5NhjV2QvAaO3ZsxbGtm7Ow1alTJ7322muu4QGC6x8greL0j3/8wz22ypX9nWnrq8477zyFOypXDVTz5s0VFRW1Q6cce9y6dWvfxoU9E/isdvU52v32TUqs+5F10eGzrn9XXnmlW1j79ddfu8YIAfZZ2HTdjIyMXX6W1X3WgedQf+xfwbt3766hQ4e6ioc1nfn3v//N5xhkbLqY/fm4zz77uH/VtpuFZFsob8f2L+F8nsHJqlQ9e/bU4sWL+e8yyFgHQJsJUFmfPn0qpnm2DvOffQhXDfgHA/uh4Msvv6zyLwX22NYNIDh06dLF/SFR+XO0+eE2nzjwOdq9/YViP0QEfPXVV+7ztn/ZQ/2wfiQWrGz6mP3622dXmf33aJ2RKn+W1qrd/jKp/FnadLTKf2HYv7hbm9nt/yJC/bL/ngoKCvgcg8yoUaPcZ2FVyMDN/sXc1usEjvk8g5O13F6yZIn7QZ3/LoOLTZnffquShQsXukqkCfufffzuqIGde+WVV1xnlQkTJriuKhdffHFZkyZNqnTKQcPoYjV9+nR3s/+kHnzwQXe8YsUK9/w999zjPrd333237Ndffy074YQTyrp06VK2devWitc46qijyoYMGVL2888/l/3www+uK9aZZ57p43cVfi677LKylJSUsm+++aZs3bp1Fbe8vLyKay699NKyjh07ln311Vdlv/zyS9nIkSPdLaC4uLisf//+ZUceeWTZjBkzyj755JOyFi1alN10000+fVfh6cYbb3RdHpctW+b+m7PH1oHqs88+c8/zOQa3yt0CDZ9ncLjuuuvcn6/23+XEiRPLRo8eXda8eXPXmdXwOQaPyZMnl0VHR5fdddddZYsWLSp76aWXyhITE8tefPHFimvC+WcfwlUD98gjj7g/bGJjY11r9p9++snvIWE7X3/9tQtV29/OO++8ipakt9xyS1mrVq1cWB41alTZggULqrzGpk2b3B8ojRs3dm1lL7jgAhfaUH+q+wzt9txzz1VcY38pXH755a6tt/1FcuKJJ7oAVtny5cvLxo4dW5aQkOB+cLAfKIqKinz4jsLXhRdeWNapUyf356b98GX/zQWCleFzDK1wxecZHE4//fSyNm3auP8u27Vr5x4vXry44nk+x+Dy/vvvu7BrP9f07t277Omnn67yfGkY/+wTYf/nd/UMAAAAAIIda64AAAAAoBYQrgAAAACgFhCuAAAAAKAWEK4AAAAAoBYQrgAAAACgFhCuAAAAAKAWEK4AAAAAoBYQrgAAAACgFhCuAACooYiICL3zzjt+DwMA4DPCFQAgqJ1//vku3Gx/O+qoo/weGgAgzET7PQAAAGrKgtRzzz1X5VxcXJxv4wEAhCcqVwCAoGdBqnXr1lVuTZs2dc9ZFeuJJ57Q2LFjlZCQoK5du+qNN96o8vWzZs3S4Ycf7p5v1qyZLr74YuXk5FS55tlnn1W/fv3ce7Vp00ZXXnlllefT09N14oknKjExUT169NB7771X8dyWLVt01llnqUWLFu497PntwyAAIPgRrgAAIe+WW27RySefrJkzZ7qQc8YZZ2jevHnuudzcXI0ZM8aFsSlTpuj111/XF198USU8WTi74oorXOiyIGbBqXv37lXe429/+5tOO+00/frrrzr66KPd+2zevLni/efOnauPP/7Yva+9XvPmzev5VwEAUNciysrKyur8XQAAqMM1Vy+++KLi4+OrnP+///s/d7PK1aWXXuoCTcB+++2nffbZR48//rjGjx+vv/zlL1q1apUaNWrknv/oo4903HHHae3atWrVqpXatWunCy64QH//+9+rHYO9x1//+lfdeeedFYGtcePGLkzZlMXjjz/ehSmrfgEAQhdrrgAAQe+www6rEp5MampqxfHIkSOrPGePZ8yY4Y6tkjRo0KCKYGUOOOAAlZaWasGCBS44WcgaNWrULscwcODAimN7reTkZKWlpbnHl112maucTZs2TUceeaTGjRun/fffv4bfNQCgoSFcAQCCnoWZ7afp1RZbI7U7YmJiqjy2UGYBzdh6rxUrVriK2Oeff+6Cmk0zfOCBB+pkzAAAf7DmCgAQ8n766acdHvfp08cd272txbKpfAETJ05UZGSkevXqpaSkJHXu3FlffvlljcZgzSzOO+88N4XxoYce0tNPP12j1wMANDxUrgAAQa+goEDr16+vci46OrqiaYQ1qRg2bJgOPPBAvfTSS5o8ebL+85//uOes8cRtt93mgs/tt9+ujRs36qqrrtI555zj1lsZO2/rtlq2bOmqUNnZ2S6A2XW749Zbb9XQoUNdt0Eb6wcffFAR7gAAoYNwBQAIep988olrj16ZVZ3mz59f0cnvlVde0eWXX+6ue/nll9W3b1/3nLVO//TTT3X11Vdr+PDh7rGtj3rwwQcrXsuCV35+vv71r3/p+uuvd6HtlFNO2e3xxcbG6qabbtLy5cvdNMODDjrIjQcAEFroFggACGm29untt992TSQAAKhLrLkCAAAAgFpAuAIAAACAWsCaKwBASGP2OwCgvlC5AgAAAIBaQLgCAAAAgFpAuAIAAACAWkC4AgAAAIBaQLgCAAAAgFpAuAIAAACAWkC4AgAAAIBaQLgCAAAAANXc/wNz9UqnpBQT8wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 Evaluation Metric: 0.9600\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8460\n",
      "\n",
      "K-fold Cross-Validation Results:\n",
      "{'fold_metrics': [np.float64(0.84), np.float64(0.96), np.float64(0.8), np.float64(0.88), np.float64(0.75)], 'average_metric': np.float64(0.8459999999999999)}\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 3/5\n",
      "Fold 2 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 3/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 3/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Fold 1 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 2/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 1 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 2/5\n",
      "Fold 1 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 2/5\n",
      "Fold 3 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 4/5\n",
      "Fold 3 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 4/5\n",
      "Fold 2 Evaluation Metric: 0.9600\n",
      "\n",
      "Fold 3/5\n",
      "Fold 2 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 3/5\n",
      "Fold 4 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 5/5\n",
      "Fold 2 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 3/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 2 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 3/5\n",
      "Fold 5 Evaluation Metric: 0.7083\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8217\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.8217\n",
      "Fold 2 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 3/5\n",
      "Fold 5 Evaluation Metric: 0.7083\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7977\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.7977\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 5 Evaluation Metric: 0.7083\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7977\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.7977\n",
      "\n",
      "Fold 1/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 5/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 Evaluation Metric: 0.7083\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7817\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.7817\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Fold 5 Evaluation Metric: 0.7083\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7817\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.7817\n",
      "Fold 2 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 3/5\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8460\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.8460\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8060\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.8060\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8300\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.8300\n",
      "Fold 2 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 3/5\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8383\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.8383\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 2 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 3/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7820\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.7820\n",
      "Fold 3 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 4/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 2 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 3/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 2 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 2 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 3/5\n",
      "Fold 5 Evaluation Metric: 0.7083\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7737\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.7737\n",
      "\n",
      "Fold 1/5\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7823\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.7823\n",
      "Fold 2 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8223\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.8223\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8223\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.8223\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 2 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 3/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8063\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.8063\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 3/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 2 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 3/5\n",
      "Fold 5 Evaluation Metric: 0.7083\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7897\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.7897\n",
      "Fold 1 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 2/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 3/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 3 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 4/5\n",
      "Fold 1 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 3/5\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 3 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 4/5\n",
      "Fold 1 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 2/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 2 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 3/5\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 5 Evaluation Metric: 0.7083\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7977\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.7977\n",
      "\n",
      "Fold 1/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 2 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 3/5\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8060\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.8060\n",
      "\n",
      "Fold 1/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Fold 4 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 5/5\n",
      "Fold 2 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 5 Evaluation Metric: 0.7083\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7977\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.7977\n",
      "\n",
      "Fold 1/5\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Fold 4 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 5/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8060\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.8060\n",
      "\n",
      "Fold 1/5\n",
      "Fold 5 Evaluation Metric: 0.7083\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8217\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.8217\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8383\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.8383\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 5 Evaluation Metric: 0.7083\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8217\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.8217\n",
      "Fold 1 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 3/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 2 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 3/5\n",
      "Fold 2 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 3/5\n",
      "Fold 5 Evaluation Metric: 0.7083\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7817\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.7817\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Fold 2 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 3/5\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7820\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.7820\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 2 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 3/5\n",
      "Fold 2 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8383\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.8383\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8223\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.8223\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8063\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.8063\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7820\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.7820\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8223\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.8223\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8063\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.8063\n",
      "Fold 5 Evaluation Metric: 0.7083\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7817\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.7817\n",
      "\n",
      "Grid Search Results:\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.7977\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.7897\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.7977\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.8060\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.8460\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.8300\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.8383\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.8217\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.7817\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.7737\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.7817\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.7820\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.7823\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.8223\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.8223\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.8063\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.7977\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.8060\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.7977\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.8060\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.8383\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.8217\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.8383\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.8217\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.7817\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.7820\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.7817\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.7820\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.8223\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.8063\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.8223\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.8063\n",
      "\n",
      "Best Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Best Average Metric: 0.8460\n",
      "\n",
      "Final Grid Search Best Result:\n",
      "{'params': {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'}, 'average_metric': np.float64(0.8459999999999999)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, ParameterGrid  # Added ParameterGrid dependency\n",
    "from joblib import Parallel, delayed  # For parallel grid search\n",
    "\n",
    "from lib.data_loader import get_monks_dataset\n",
    "\n",
    "# ============================\n",
    "# Activation functions and their derivatives\n",
    "# ============================\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(z, a):\n",
    "    # Note: z is not used here; kept for uniform signature.\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(z, a):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(z, a):\n",
    "    return np.ones_like(a)\n",
    "\n",
    "activation_functions = {\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"relu\": relu,\n",
    "    \"linear\": linear\n",
    "}\n",
    "\n",
    "activation_derivatives = {\n",
    "    \"sigmoid\": lambda z, a: sigmoid_derivative(z, a),\n",
    "    \"relu\": lambda z, a: relu_derivative(z, a),\n",
    "    \"linear\": lambda z, a: linear_derivative(z, a)\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Loss functions and their derivatives\n",
    "# ============================\n",
    "\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Binary crossentropy loss for binary classification.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_crossentropy_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the binary crossentropy loss.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return - (y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Squared Error (MSE) loss, typically used for regression.\n",
    "    \"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the MSE loss.\n",
    "    \"\"\"\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "def mee_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Euclidean Error (MEE) loss, defined as:\n",
    "        E_MEE = (1/N) * sum over i [ ||y_true[i] - y_pred[i]||_2 ].\n",
    "    \"\"\"\n",
    "    diff = y_true - y_pred  # shape: (N, d) or (N, 1)\n",
    "    # Euclidean distance for each sample\n",
    "    dist = np.sqrt(np.sum(diff ** 2, axis=1))\n",
    "    return np.mean(dist)\n",
    "\n",
    "def mee_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the Mean Euclidean Error (MEE) loss.\n",
    "    For each sample i, derivative wrt y_pred[i] is:\n",
    "        (1/N) * ( (y_pred[i] - y_true[i]) / ||y_pred[i] - y_true[i]||_2 ).\n",
    "    We safely handle the case where the norm is zero.\n",
    "    \"\"\"\n",
    "    diff = y_pred - y_true\n",
    "    dist = np.sqrt(np.sum(diff ** 2, axis=1, keepdims=True))\n",
    "    epsilon = 1e-8  # Avoid division by zero\n",
    "    dist_safe = np.where(dist == 0, epsilon, dist)\n",
    "    N = y_true.shape[0]\n",
    "    derivative = diff / dist_safe / N\n",
    "    return derivative\n",
    "\n",
    "loss_functions = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_loss,\n",
    "    \"mse\": mse_loss,\n",
    "    \"mee\": mee_loss,  \n",
    "}\n",
    "\n",
    "loss_derivatives = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_derivative,\n",
    "    \"mse\": mse_derivative,\n",
    "    \"mee\": mee_derivative, \n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Regularization functions (modular)\n",
    "# ============================\n",
    "\n",
    "def compute_reg_gradient(W, lambda_reg, reg_type, m):\n",
    "    if reg_type == \"l2\":\n",
    "        return lambda_reg * W / m\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * np.sign(W) / m\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_reg_loss(W_list, lambda_reg, reg_type):\n",
    "    if reg_type == \"l2\":\n",
    "        return (lambda_reg / 2) * sum(np.sum(W ** 2) for W in W_list)\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * sum(np.sum(np.abs(W)) for W in W_list)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# ============================\n",
    "# Neural Network Class with Learning Rate Decay, Momentum, Custom Weight Initialization, and Early Stopping\n",
    "# ============================\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.01, lambda_reg=0.001, reg_type=\"l2\",\n",
    "                 loss_function_name=None,\n",
    "                 activation_function_name=\"relu\",\n",
    "                 output_activation_function_name=None,\n",
    "                 activation_function_names=None,\n",
    "                 task=\"classification\",\n",
    "                 lr_decay_type=\"none\",  # Options: \"none\", \"exponential\", \"linear\"\n",
    "                 decay_rate=0.0,\n",
    "                 weight_init=\"base\",  # \"base\" (fan-in scaling) or \"glorot\"\n",
    "                 momentum_type=\"none\",  # Options: \"none\", \"momentum\", \"nesterov momentum\"\n",
    "                 momentum_alpha=0.9):\n",
    "        \"\"\"\n",
    "        :param layers: List containing the size of each layer (input, hidden, output)\n",
    "        :param learning_rate: Initial learning rate\n",
    "        :param lambda_reg: Regularization coefficient\n",
    "        :param reg_type: Type of regularization (\"l2\", \"l1\", or other for none)\n",
    "        :param loss_function_name: Name of the loss function (if None, set based on task)\n",
    "        :param activation_function_name: Activation to use for hidden layers (if activation_function_names not provided)\n",
    "        :param output_activation_function_name: Activation for the output layer (if None, set based on task)\n",
    "        :param activation_function_names: List of activation function names for each layer (length = len(layers)-1)\n",
    "        :param task: \"classification\" or \"regression\"\n",
    "        :param lr_decay_type: Learning rate decay strategy (\"none\", \"exponential\", \"linear\")\n",
    "        :param decay_rate: Decay rate used in the learning rate schedule\n",
    "        :param weight_init: Weight initialization strategy (\"base\" uses fan-in scaling or \"glorot\")\n",
    "        :param momentum_type: Momentum strategy (\"none\", \"momentum\", \"nesterov momentum\")\n",
    "        :param momentum_alpha: Momentum coefficient (e.g., 0.9)\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.reg_type = reg_type\n",
    "        self.task = task\n",
    "        self.lr_decay_type = lr_decay_type\n",
    "        self.decay_rate = decay_rate\n",
    "        self.weight_init = weight_init\n",
    "        \n",
    "        # Set momentum parameters\n",
    "        if momentum_type not in {\"none\", \"momentum\", \"nesterov momentum\"}:\n",
    "            raise ValueError(\"momentum_type must be 'none', 'momentum', or 'nesterov momentum'.\")\n",
    "        self.momentum_type = momentum_type\n",
    "        self.momentum_alpha = momentum_alpha if momentum_type != \"none\" else 0.0\n",
    "        \n",
    "        # Set defaults based on task\n",
    "        if self.task == \"regression\":\n",
    "            self.loss_function_name = loss_function_name or \"mse\"\n",
    "            output_activation_function_name = output_activation_function_name or \"linear\"\n",
    "        else:\n",
    "            # Classification\n",
    "            self.loss_function_name = loss_function_name or \"binary_crossentropy\"\n",
    "            output_activation_function_name = output_activation_function_name or \"sigmoid\"\n",
    "        \n",
    "        # Set activation functions for layers\n",
    "        if activation_function_names is None:\n",
    "            self.activation_function_names = [activation_function_name] * (len(layers) - 1)\n",
    "            self.activation_function_names[-1] = output_activation_function_name\n",
    "        else:\n",
    "            if len(activation_function_names) != len(layers) - 1:\n",
    "                raise ValueError(\"activation_function_names must have length equal to len(layers)-1.\")\n",
    "            self.activation_function_names = activation_function_names\n",
    "        \n",
    "        self._initialize_parameters()\n",
    "        # Initialize momentum accumulators (even if not used, for consistency)\n",
    "        self.vW = [np.zeros_like(W) for W in self.W]\n",
    "        self.vb = [np.zeros_like(b) for b in self.b]\n",
    "        \n",
    "        # Initialize loss history lists (will be (re)initialized in train())\n",
    "        self.train_loss_history = []\n",
    "        self.val_loss_history = None\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            fan_in = self.layers[i]\n",
    "            fan_out = self.layers[i + 1]\n",
    "            if self.weight_init == \"base\":\n",
    "                std = np.sqrt(1.0 / fan_in)\n",
    "            elif self.weight_init == \"glorot\":\n",
    "                std = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported weight initialization strategy. Use 'base' or 'glorot'.\")\n",
    "            weight = np.random.randn(fan_in, fan_out) * std\n",
    "            self.W.append(weight)\n",
    "            self.b.append(np.zeros((1, fan_out)))\n",
    "    \n",
    "    def _apply_activation(self, x, func_name):\n",
    "        if func_name not in activation_functions:\n",
    "            raise ValueError(f\"Unsupported activation: {func_name}\")\n",
    "        return activation_functions[func_name](x)\n",
    "    \n",
    "    def _apply_activation_derivative(self, z, a, func_name):\n",
    "        if func_name not in activation_derivatives:\n",
    "            raise ValueError(f\"Unsupported activation derivative: {func_name}\")\n",
    "        return activation_derivatives[func_name](z, a)\n",
    "    \n",
    "    def _forward(self, X, weights=None, biases=None):\n",
    "        \"\"\"\n",
    "        Forward propagation. If weights and biases are provided, they are used;\n",
    "        otherwise the network's parameters are used.\n",
    "        Returns lists Z (pre-activations) and A (activations).\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.W\n",
    "        if biases is None:\n",
    "            biases = self.b\n",
    "            \n",
    "        A = [X]\n",
    "        Z = []\n",
    "        # Forward through hidden layers\n",
    "        for i in range(len(weights) - 1):\n",
    "            z_curr = np.dot(A[-1], weights[i]) + biases[i]\n",
    "            Z.append(z_curr)\n",
    "            a_curr = self._apply_activation(z_curr, self.activation_function_names[i])\n",
    "            A.append(a_curr)\n",
    "        # Forward through output layer\n",
    "        z_out = np.dot(A[-1], weights[-1]) + biases[-1]\n",
    "        Z.append(z_out)\n",
    "        a_out = self._apply_activation(z_out, self.activation_function_names[-1])\n",
    "        A.append(a_out)\n",
    "        return Z, A\n",
    "    \n",
    "    def _compute_gradients(self, X, y, Z, A, weights=None):\n",
    "        \"\"\"\n",
    "        Compute gradients dW and db given inputs X, target y, pre-activations Z and activations A.\n",
    "        Optionally, a custom set of weights (used in lookahead for Nesterov momentum) can be provided.\n",
    "        Returns lists dW and db.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.W\n",
    "        m = X.shape[0]\n",
    "        dA = loss_derivatives[self.loss_function_name](y, A[-1])\n",
    "        # Output layer\n",
    "        dZ = dA * self._apply_activation_derivative(Z[-1], A[-1], self.activation_function_names[-1])\n",
    "        reg_term = compute_reg_gradient(weights[-1], self.lambda_reg, self.reg_type, m)\n",
    "        dW = [np.dot(A[-2].T, dZ) / m + reg_term]\n",
    "        db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "        \n",
    "        # Backpropagate through hidden layers\n",
    "        for i in range(len(weights) - 2, -1, -1):\n",
    "            dA = np.dot(dZ, weights[i + 1].T)\n",
    "            dZ = dA * self._apply_activation_derivative(Z[i], A[i + 1], self.activation_function_names[i])\n",
    "            reg_term = compute_reg_gradient(weights[i], self.lambda_reg, self.reg_type, m)\n",
    "            dW.insert(0, np.dot(A[i].T, dZ) / m + reg_term)\n",
    "            db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "            \n",
    "        return dW, db\n",
    "    \n",
    "    def train(self, X, y, epochs=300, batch_size=32, verbose=True,\n",
    "              early_stopping=False, validation_data=None, patience=10, min_delta=0.0):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "\n",
    "        The loss histories for training and validation (if provided) are stored in:\n",
    "            self.train_loss_history and self.val_loss_history\n",
    "\n",
    "        :param X: Training data inputs.\n",
    "        :param y: Training data targets.\n",
    "        :param epochs: Maximum number of epochs to train.\n",
    "        :param batch_size: Mini-batch size.\n",
    "        :param verbose: Whether to print progress.\n",
    "        :param early_stopping: Enable early stopping if True.\n",
    "        :param validation_data: Tuple (X_val, y_val) for early stopping and validation loss logging.\n",
    "        :param patience: Number of epochs with no improvement to wait before stopping.\n",
    "        :param min_delta: Minimum change in the monitored loss to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        # Reinitialize loss histories\n",
    "        self.train_loss_history = []\n",
    "        if validation_data is not None:\n",
    "            self.val_loss_history = []\n",
    "        else:\n",
    "            self.val_loss_history = None\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        best_loss = np.inf\n",
    "        patience_counter = 0\n",
    "        best_weights = None\n",
    "        best_biases = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Update learning rate based on decay schedule\n",
    "            if self.lr_decay_type == \"exponential\":\n",
    "                self.learning_rate = self.initial_learning_rate * np.exp(-self.decay_rate * epoch)\n",
    "            elif self.lr_decay_type == \"linear\":\n",
    "                self.learning_rate = self.initial_learning_rate * max(0, 1 - self.decay_rate * epoch)\n",
    "            # Otherwise (\"none\"), keep the initial learning rate.\n",
    "            \n",
    "            # Shuffle training data\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            \n",
    "            # Process mini-batches\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                \n",
    "                if self.momentum_type == \"nesterov momentum\":\n",
    "                    weights_lookahead = [self.W[j] - self.momentum_alpha * self.vW[j] for j in range(len(self.W))]\n",
    "                    biases_lookahead = [self.b[j] - self.momentum_alpha * self.vb[j] for j in range(len(self.b))]\n",
    "                    Z, A = self._forward(X_batch, weights=weights_lookahead, biases=biases_lookahead)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A, weights=weights_lookahead)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.vW[j] = self.momentum_alpha * self.vW[j] + self.learning_rate * dW[j]\n",
    "                        self.vb[j] = self.momentum_alpha * self.vb[j] + self.learning_rate * db[j]\n",
    "                        self.W[j] -= self.vW[j]\n",
    "                        self.b[j] -= self.vb[j]\n",
    "                        \n",
    "                elif self.momentum_type == \"momentum\":\n",
    "                    Z, A = self._forward(X_batch)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.vW[j] = self.momentum_alpha * self.vW[j] + self.learning_rate * dW[j]\n",
    "                        self.vb[j] = self.momentum_alpha * self.vb[j] + self.learning_rate * db[j]\n",
    "                        self.W[j] -= self.vW[j]\n",
    "                        self.b[j] -= self.vb[j]\n",
    "                        \n",
    "                else:  # No momentum\n",
    "                    Z, A = self._forward(X_batch)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.W[j] -= self.learning_rate * dW[j]\n",
    "                        self.b[j] -= self.learning_rate * db[j]\n",
    "            \n",
    "            # Compute training loss\n",
    "            _, A_full = self._forward(X)\n",
    "            train_loss = loss_functions[self.loss_function_name](y, A_full[-1])\n",
    "            reg_loss = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "            total_train_loss = train_loss + reg_loss\n",
    "            self.train_loss_history.append(total_train_loss)\n",
    "            \n",
    "            # Compute validation loss if validation data is provided\n",
    "            if validation_data is not None:\n",
    "                X_val, y_val = validation_data\n",
    "                _, A_val = self._forward(X_val)\n",
    "                val_loss = loss_functions[self.loss_function_name](y_val, A_val[-1])\n",
    "                reg_loss_val = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "                total_val_loss = val_loss + reg_loss_val\n",
    "                self.val_loss_history.append(total_val_loss)\n",
    "            else:\n",
    "                total_val_loss = None\n",
    "\n",
    "            # Verbose logging\n",
    "            if verbose:\n",
    "                if total_val_loss is not None:\n",
    "                    print(f\"Epoch {epoch:4d}, Training Loss: {total_train_loss:.4f}, \"\n",
    "                          f\"Validation Loss: {total_val_loss:.4f}, Learning Rate: {self.learning_rate:.6f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch:4d}, Training Loss: {total_train_loss:.4f}, \"\n",
    "                          f\"Learning Rate: {self.learning_rate:.6f}\")\n",
    "            \n",
    "            # Early stopping check (only if validation data is provided)\n",
    "            if early_stopping and (validation_data is not None):\n",
    "                if total_val_loss < best_loss - min_delta:\n",
    "                    best_loss = total_val_loss\n",
    "                    patience_counter = 0\n",
    "                    best_weights = [w.copy() for w in self.W]\n",
    "                    best_biases = [b.copy() for b in self.b]\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        if verbose:\n",
    "                            print(f\"Early stopping triggered at epoch {epoch}. Restoring best model parameters.\")\n",
    "                        if best_weights is not None:\n",
    "                            self.W = best_weights\n",
    "                            self.b = best_biases\n",
    "                        break\n",
    "\n",
    "    def plot_loss_history(self):\n",
    "        \"\"\"\n",
    "        Plot the training loss history and, if available, the validation loss history.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.train_loss_history, label=\"Training Loss\")\n",
    "        if self.val_loss_history is not None and len(self.val_loss_history) > 0:\n",
    "            plt.plot(self.val_loss_history, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss History\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, A = self._forward(X)\n",
    "        output = A[-1]\n",
    "        if self.task == \"classification\":\n",
    "            if output.shape[1] == 1:\n",
    "                return (output > 0.5).astype(int)\n",
    "            else:\n",
    "                return np.argmax(output, axis=1)\n",
    "        else:\n",
    "            return output\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        if self.task == \"regression\":\n",
    "            return mse_loss(y, predictions)\n",
    "        else:\n",
    "            if y.ndim > 1 and y.shape[1] > 1:\n",
    "                y_true = np.argmax(y, axis=1)\n",
    "            else:\n",
    "                y_true = y\n",
    "            return np.mean(predictions == y_true)\n",
    "\n",
    "# ============================\n",
    "# K-fold Cross-Validation Function\n",
    "# ============================\n",
    "\n",
    "def k_fold_cross_validation(model_builder, X, y, k=5, epochs=1000, batch_size=32,\n",
    "                            verbose=True, early_stopping=False, patience=10, min_delta=1e-4):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross validation for a neural network.\n",
    "    \n",
    "    :param model_builder: A callable that returns a new instance of NeuralNetwork.\n",
    "    :param X: Input features.\n",
    "    :param y: Targets.\n",
    "    :param k: Number of folds.\n",
    "    :param epochs: Number of training epochs per fold.\n",
    "    :param batch_size: Batch size.\n",
    "    :param verbose: Verbosity flag.\n",
    "    :param early_stopping: Whether to use early stopping.\n",
    "    :param patience: Patience for early stopping.\n",
    "    :param min_delta: Minimum change in loss for early stopping.\n",
    "    :return: Dictionary containing per-fold metrics and the overall average metric.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold_metrics = []\n",
    "    fold = 1\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        print(f\"\\nFold {fold}/{k}\")\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "        \n",
    "        # Build a new model instance for this fold\n",
    "        nn_model = model_builder()\n",
    "        \n",
    "        # Train the model on the training fold with validation on the fold's validation set.\n",
    "        nn_model.train(\n",
    "            X_train_fold, y_train_fold,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=verbose,\n",
    "            early_stopping=early_stopping,\n",
    "            validation_data=(X_val_fold, y_val_fold),\n",
    "            patience=patience,\n",
    "            min_delta=min_delta\n",
    "        )\n",
    "        \n",
    "        # Evaluate the model on the validation fold.\n",
    "        metric = nn_model.evaluate(X_val_fold, y_val_fold)\n",
    "        print(f\"Fold {fold} Evaluation Metric: {metric:.4f}\")\n",
    "        fold_metrics.append(metric)\n",
    "        fold += 1\n",
    "    \n",
    "    avg_metric = np.mean(fold_metrics)\n",
    "    print(f\"\\nAverage Evaluation Metric over {k} folds: {avg_metric:.4f}\")\n",
    "    return {\"fold_metrics\": fold_metrics, \"average_metric\": avg_metric}\n",
    "\n",
    "# ============================\n",
    "# Grid Search Function with Parallelism\n",
    "# ============================\n",
    "\n",
    "def grid_search(model_builder, param_grid, X, y, k=5, epochs=1000, batch_size=32,\n",
    "                early_stopping=False, patience=10, min_delta=1e-4, n_jobs=-1,\n",
    "                maximize=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Perform grid search over the given parameter grid using k-fold cross validation.\n",
    "    \n",
    "    :param model_builder: A callable that accepts hyperparameters as keyword arguments and returns a NeuralNetwork.\n",
    "    :param param_grid: Dictionary of hyperparameters to try.\n",
    "    :param X: Input features.\n",
    "    :param y: Targets.\n",
    "    :param k: Number of folds for cross validation.\n",
    "    :param epochs: Number of epochs per fold.\n",
    "    :param batch_size: Batch size.\n",
    "    :param early_stopping: Whether to use early stopping.\n",
    "    :param patience: Patience for early stopping.\n",
    "    :param min_delta: Minimum change in loss for early stopping.\n",
    "    :param n_jobs: Number of parallel jobs (-1 means using all processors).\n",
    "    :param maximize: If True, higher evaluation metric is better.\n",
    "    :param verbose: Verbosity flag.\n",
    "    :return: A tuple (best_result, all_results) where best_result is a dict with best params and metric.\n",
    "    \"\"\"\n",
    "    grid = list(ParameterGrid(param_grid))\n",
    "    results = []\n",
    "\n",
    "    def evaluate_params(params):\n",
    "        # Create a new model builder that injects the current hyperparameters.\n",
    "        def builder():\n",
    "            return model_builder(**params)\n",
    "        cv_result = k_fold_cross_validation(builder, X, y, k=k, epochs=epochs,\n",
    "                                              batch_size=batch_size,\n",
    "                                              early_stopping=early_stopping,\n",
    "                                              patience=patience,\n",
    "                                              min_delta=min_delta,\n",
    "                                              verbose=False)\n",
    "        metric = cv_result[\"average_metric\"]\n",
    "        if verbose:\n",
    "            print(f\"Params: {params} => Average Metric: {metric:.4f}\")\n",
    "        return (params, metric)\n",
    "\n",
    "    evaluated_results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(evaluate_params)(params) for params in grid\n",
    "    )\n",
    "\n",
    "    for params, metric in evaluated_results:\n",
    "        results.append({\"params\": params, \"average_metric\": metric})\n",
    "\n",
    "    if maximize:\n",
    "        best_result = max(results, key=lambda x: x[\"average_metric\"])\n",
    "    else:\n",
    "        best_result = min(results, key=lambda x: x[\"average_metric\"])\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nGrid Search Results:\")\n",
    "        for res in results:\n",
    "            print(f\"Params: {res['params']}, Average Metric: {res['average_metric']:.4f}\")\n",
    "        print(f\"\\nBest Params: {best_result['params']}, Best Average Metric: {best_result['average_metric']:.4f}\")\n",
    "\n",
    "    return best_result, results\n",
    "\n",
    "# ============================\n",
    "# Testing on a Monk's Dataset\n",
    "# ============================\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(1, one_hot_encode=True)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_units = 10\n",
    "output_size = 1  # binary classification\n",
    "layers = [input_size, hidden_units, output_size]\n",
    "\n",
    "# Define activation functions for hidden and output layers\n",
    "activation_funcs = [\"relu\", \"sigmoid\"]\n",
    "\n",
    "# Example: Build a neural network classifier instance\n",
    "nn_clf = NeuralNetwork(\n",
    "    layers=layers,\n",
    "    learning_rate=0.2,\n",
    "    lambda_reg=0.001,\n",
    "    reg_type=\"l2\",\n",
    "    loss_function_name=\"mse\",       \n",
    "    activation_function_names=activation_funcs,\n",
    "    task=\"classification\",\n",
    "    lr_decay_type=\"linear\",    # Options: \"exponential\", \"linear\", or \"none\"\n",
    "    decay_rate=0.001,\n",
    "    weight_init=\"base\",        # \"base\" or \"glorot\"\n",
    ")\n",
    "\n",
    "# For early stopping, we provide validation data.\n",
    "nn_clf.train(\n",
    "    X_train, y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    verbose=True,\n",
    "    early_stopping=True,\n",
    "    validation_data=(X_test, y_test),\n",
    "    patience=10,\n",
    "    min_delta=1e-4\n",
    ")\n",
    "\n",
    "accuracy = nn_clf.evaluate(X_test, y_test)\n",
    "print(f\"\\nNeural Network Classification Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot the loss histories\n",
    "nn_clf.plot_loss_history()\n",
    "\n",
    "# ============================\n",
    "# K-fold Cross-Validation Example\n",
    "# ============================\n",
    "\n",
    "def build_nn_model():\n",
    "    \"\"\"\n",
    "    Returns a new instance of NeuralNetwork with the same configuration.\n",
    "    This is used for each fold in cross validation.\n",
    "    \"\"\"\n",
    "    return NeuralNetwork(\n",
    "        layers=layers,\n",
    "        learning_rate=0.2,\n",
    "        lambda_reg=0.001,\n",
    "        reg_type=\"l2\",\n",
    "        loss_function_name=\"mse\",       \n",
    "        activation_function_names=activation_funcs,\n",
    "        task=\"classification\",\n",
    "        lr_decay_type=\"linear\",    # Options: \"exponential\", \"linear\", or \"none\"\n",
    "        decay_rate=0.001,\n",
    "        weight_init=\"base\",        # \"base\" or \"glorot\"\n",
    "    )\n",
    "\n",
    "# Perform 5-fold cross validation on the training set.\n",
    "cv_results = k_fold_cross_validation(\n",
    "    model_builder=build_nn_model,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    k=5,\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    verbose=False,  # Set True to see per-epoch logging within each fold.\n",
    "    early_stopping=True,\n",
    "    patience=10,\n",
    "    min_delta=1e-4\n",
    ")\n",
    "\n",
    "print(\"\\nK-fold Cross-Validation Results:\")\n",
    "print(cv_results)\n",
    "\n",
    "# ============================\n",
    "# Grid Search Example\n",
    "# ============================\n",
    "# Here we define a model builder that accepts hyperparameters as keyword arguments.\n",
    "def build_nn_model_with_params(learning_rate=0.2, lambda_reg=0.001, reg_type=\"l2\",\n",
    "                               lr_decay_type=\"linear\", decay_rate=0.001, weight_init=\"base\"):\n",
    "    return NeuralNetwork(\n",
    "        layers=layers,\n",
    "        learning_rate=learning_rate,\n",
    "        lambda_reg=lambda_reg,\n",
    "        reg_type=reg_type,\n",
    "        loss_function_name=\"mse\",       \n",
    "        activation_function_names=activation_funcs,\n",
    "        task=\"classification\",\n",
    "        lr_decay_type=lr_decay_type,\n",
    "        decay_rate=decay_rate,\n",
    "        weight_init=weight_init\n",
    "    )\n",
    "\n",
    "# Define the grid of hyperparameters to search.\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.1, 0.2],\n",
    "    \"lambda_reg\": [0.001, 0.01],\n",
    "    \"lr_decay_type\": [\"linear\", \"none\"],\n",
    "    \"decay_rate\": [0.001, 0.0],\n",
    "    \"weight_init\": [\"base\", \"glorot\"],\n",
    "}\n",
    "\n",
    "# Perform grid search with 5-fold cross validation.\n",
    "best_params, all_results = grid_search(\n",
    "    model_builder=build_nn_model_with_params,\n",
    "    param_grid=param_grid,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    k=5,\n",
    "    epochs=500,       # You can reduce epochs for grid search to speed up computation.\n",
    "    batch_size=32,\n",
    "    early_stopping=True,\n",
    "    patience=10,\n",
    "    min_delta=1e-4,\n",
    "    n_jobs=-1,        # Use all available processors.\n",
    "    maximize=True,    # For classification accuracy, higher is better.\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Grid Search Best Result:\")\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-1-train.csv\n",
      "Using cached ../datasets/monks/monk-1-test.csv\n",
      "One-hot encoding MONK-1 dataset...\n",
      "Epoch    0, Training Loss: 0.2535, Validation Loss: 0.2568, Learning Rate: 0.200000\n",
      "Epoch    1, Training Loss: 0.2502, Validation Loss: 0.2547, Learning Rate: 0.199800\n",
      "Epoch    2, Training Loss: 0.2471, Validation Loss: 0.2527, Learning Rate: 0.199600\n",
      "Epoch    3, Training Loss: 0.2443, Validation Loss: 0.2509, Learning Rate: 0.199400\n",
      "Epoch    4, Training Loss: 0.2414, Validation Loss: 0.2491, Learning Rate: 0.199200\n",
      "Epoch    5, Training Loss: 0.2384, Validation Loss: 0.2471, Learning Rate: 0.199000\n",
      "Epoch    6, Training Loss: 0.2354, Validation Loss: 0.2449, Learning Rate: 0.198800\n",
      "Epoch    7, Training Loss: 0.2322, Validation Loss: 0.2427, Learning Rate: 0.198600\n",
      "Epoch    8, Training Loss: 0.2287, Validation Loss: 0.2403, Learning Rate: 0.198400\n",
      "Epoch    9, Training Loss: 0.2245, Validation Loss: 0.2375, Learning Rate: 0.198200\n",
      "Epoch   10, Training Loss: 0.2201, Validation Loss: 0.2344, Learning Rate: 0.198000\n",
      "Epoch   11, Training Loss: 0.2162, Validation Loss: 0.2316, Learning Rate: 0.197800\n",
      "Epoch   12, Training Loss: 0.2122, Validation Loss: 0.2289, Learning Rate: 0.197600\n",
      "Epoch   13, Training Loss: 0.2074, Validation Loss: 0.2255, Learning Rate: 0.197400\n",
      "Epoch   14, Training Loss: 0.2017, Validation Loss: 0.2217, Learning Rate: 0.197200\n",
      "Epoch   15, Training Loss: 0.1966, Validation Loss: 0.2183, Learning Rate: 0.197000\n",
      "Epoch   16, Training Loss: 0.1922, Validation Loss: 0.2153, Learning Rate: 0.196800\n",
      "Epoch   17, Training Loss: 0.1880, Validation Loss: 0.2124, Learning Rate: 0.196600\n",
      "Epoch   18, Training Loss: 0.1840, Validation Loss: 0.2094, Learning Rate: 0.196400\n",
      "Epoch   19, Training Loss: 0.1802, Validation Loss: 0.2067, Learning Rate: 0.196200\n",
      "Epoch   20, Training Loss: 0.1766, Validation Loss: 0.2042, Learning Rate: 0.196000\n",
      "Epoch   21, Training Loss: 0.1733, Validation Loss: 0.2017, Learning Rate: 0.195800\n",
      "Epoch   22, Training Loss: 0.1699, Validation Loss: 0.1993, Learning Rate: 0.195600\n",
      "Epoch   23, Training Loss: 0.1668, Validation Loss: 0.1970, Learning Rate: 0.195400\n",
      "Epoch   24, Training Loss: 0.1638, Validation Loss: 0.1949, Learning Rate: 0.195200\n",
      "Epoch   25, Training Loss: 0.1612, Validation Loss: 0.1928, Learning Rate: 0.195000\n",
      "Epoch   26, Training Loss: 0.1586, Validation Loss: 0.1912, Learning Rate: 0.194800\n",
      "Epoch   27, Training Loss: 0.1562, Validation Loss: 0.1891, Learning Rate: 0.194600\n",
      "Epoch   28, Training Loss: 0.1540, Validation Loss: 0.1875, Learning Rate: 0.194400\n",
      "Epoch   29, Training Loss: 0.1519, Validation Loss: 0.1860, Learning Rate: 0.194200\n",
      "Epoch   30, Training Loss: 0.1499, Validation Loss: 0.1844, Learning Rate: 0.194000\n",
      "Epoch   31, Training Loss: 0.1481, Validation Loss: 0.1831, Learning Rate: 0.193800\n",
      "Epoch   32, Training Loss: 0.1463, Validation Loss: 0.1820, Learning Rate: 0.193600\n",
      "Epoch   33, Training Loss: 0.1445, Validation Loss: 0.1805, Learning Rate: 0.193400\n",
      "Epoch   34, Training Loss: 0.1429, Validation Loss: 0.1789, Learning Rate: 0.193200\n",
      "Epoch   35, Training Loss: 0.1413, Validation Loss: 0.1776, Learning Rate: 0.193000\n",
      "Epoch   36, Training Loss: 0.1397, Validation Loss: 0.1767, Learning Rate: 0.192800\n",
      "Epoch   37, Training Loss: 0.1384, Validation Loss: 0.1753, Learning Rate: 0.192600\n",
      "Epoch   38, Training Loss: 0.1369, Validation Loss: 0.1745, Learning Rate: 0.192400\n",
      "Epoch   39, Training Loss: 0.1355, Validation Loss: 0.1735, Learning Rate: 0.192200\n",
      "Epoch   40, Training Loss: 0.1342, Validation Loss: 0.1723, Learning Rate: 0.192000\n",
      "Epoch   41, Training Loss: 0.1328, Validation Loss: 0.1715, Learning Rate: 0.191800\n",
      "Epoch   42, Training Loss: 0.1319, Validation Loss: 0.1717, Learning Rate: 0.191600\n",
      "Epoch   43, Training Loss: 0.1303, Validation Loss: 0.1696, Learning Rate: 0.191400\n",
      "Epoch   44, Training Loss: 0.1290, Validation Loss: 0.1688, Learning Rate: 0.191200\n",
      "Epoch   45, Training Loss: 0.1279, Validation Loss: 0.1674, Learning Rate: 0.191000\n",
      "Epoch   46, Training Loss: 0.1268, Validation Loss: 0.1666, Learning Rate: 0.190800\n",
      "Epoch   47, Training Loss: 0.1256, Validation Loss: 0.1656, Learning Rate: 0.190600\n",
      "Epoch   48, Training Loss: 0.1245, Validation Loss: 0.1651, Learning Rate: 0.190400\n",
      "Epoch   49, Training Loss: 0.1234, Validation Loss: 0.1638, Learning Rate: 0.190200\n",
      "Epoch   50, Training Loss: 0.1223, Validation Loss: 0.1632, Learning Rate: 0.190000\n",
      "Epoch   51, Training Loss: 0.1214, Validation Loss: 0.1618, Learning Rate: 0.189800\n",
      "Epoch   52, Training Loss: 0.1204, Validation Loss: 0.1608, Learning Rate: 0.189600\n",
      "Epoch   53, Training Loss: 0.1194, Validation Loss: 0.1598, Learning Rate: 0.189400\n",
      "Epoch   54, Training Loss: 0.1182, Validation Loss: 0.1598, Learning Rate: 0.189200\n",
      "Epoch   55, Training Loss: 0.1173, Validation Loss: 0.1593, Learning Rate: 0.189000\n",
      "Epoch   56, Training Loss: 0.1163, Validation Loss: 0.1581, Learning Rate: 0.188800\n",
      "Epoch   57, Training Loss: 0.1154, Validation Loss: 0.1579, Learning Rate: 0.188600\n",
      "Epoch   58, Training Loss: 0.1144, Validation Loss: 0.1568, Learning Rate: 0.188400\n",
      "Epoch   59, Training Loss: 0.1136, Validation Loss: 0.1564, Learning Rate: 0.188200\n",
      "Epoch   60, Training Loss: 0.1128, Validation Loss: 0.1546, Learning Rate: 0.188000\n",
      "Epoch   61, Training Loss: 0.1119, Validation Loss: 0.1540, Learning Rate: 0.187800\n",
      "Epoch   62, Training Loss: 0.1109, Validation Loss: 0.1539, Learning Rate: 0.187600\n",
      "Epoch   63, Training Loss: 0.1102, Validation Loss: 0.1540, Learning Rate: 0.187400\n",
      "Epoch   64, Training Loss: 0.1093, Validation Loss: 0.1529, Learning Rate: 0.187200\n",
      "Epoch   65, Training Loss: 0.1087, Validation Loss: 0.1510, Learning Rate: 0.187000\n",
      "Epoch   66, Training Loss: 0.1079, Validation Loss: 0.1501, Learning Rate: 0.186800\n",
      "Epoch   67, Training Loss: 0.1068, Validation Loss: 0.1499, Learning Rate: 0.186600\n",
      "Epoch   68, Training Loss: 0.1060, Validation Loss: 0.1498, Learning Rate: 0.186400\n",
      "Epoch   69, Training Loss: 0.1053, Validation Loss: 0.1485, Learning Rate: 0.186200\n",
      "Epoch   70, Training Loss: 0.1045, Validation Loss: 0.1487, Learning Rate: 0.186000\n",
      "Epoch   71, Training Loss: 0.1037, Validation Loss: 0.1473, Learning Rate: 0.185800\n",
      "Epoch   72, Training Loss: 0.1030, Validation Loss: 0.1461, Learning Rate: 0.185600\n",
      "Epoch   73, Training Loss: 0.1025, Validation Loss: 0.1448, Learning Rate: 0.185400\n",
      "Epoch   74, Training Loss: 0.1016, Validation Loss: 0.1444, Learning Rate: 0.185200\n",
      "Epoch   75, Training Loss: 0.1008, Validation Loss: 0.1438, Learning Rate: 0.185000\n",
      "Epoch   76, Training Loss: 0.0999, Validation Loss: 0.1437, Learning Rate: 0.184800\n",
      "Epoch   77, Training Loss: 0.0992, Validation Loss: 0.1432, Learning Rate: 0.184600\n",
      "Epoch   78, Training Loss: 0.0985, Validation Loss: 0.1426, Learning Rate: 0.184400\n",
      "Epoch   79, Training Loss: 0.0979, Validation Loss: 0.1411, Learning Rate: 0.184200\n",
      "Epoch   80, Training Loss: 0.0977, Validation Loss: 0.1401, Learning Rate: 0.184000\n",
      "Epoch   81, Training Loss: 0.0964, Validation Loss: 0.1402, Learning Rate: 0.183800\n",
      "Epoch   82, Training Loss: 0.0957, Validation Loss: 0.1403, Learning Rate: 0.183600\n",
      "Epoch   83, Training Loss: 0.0952, Validation Loss: 0.1404, Learning Rate: 0.183400\n",
      "Epoch   84, Training Loss: 0.0945, Validation Loss: 0.1397, Learning Rate: 0.183200\n",
      "Epoch   85, Training Loss: 0.0936, Validation Loss: 0.1377, Learning Rate: 0.183000\n",
      "Epoch   86, Training Loss: 0.0929, Validation Loss: 0.1375, Learning Rate: 0.182800\n",
      "Epoch   87, Training Loss: 0.0923, Validation Loss: 0.1364, Learning Rate: 0.182600\n",
      "Epoch   88, Training Loss: 0.0919, Validation Loss: 0.1372, Learning Rate: 0.182400\n",
      "Epoch   89, Training Loss: 0.0910, Validation Loss: 0.1350, Learning Rate: 0.182200\n",
      "Epoch   90, Training Loss: 0.0904, Validation Loss: 0.1356, Learning Rate: 0.182000\n",
      "Epoch   91, Training Loss: 0.0900, Validation Loss: 0.1358, Learning Rate: 0.181800\n",
      "Epoch   92, Training Loss: 0.0891, Validation Loss: 0.1339, Learning Rate: 0.181600\n",
      "Epoch   93, Training Loss: 0.0886, Validation Loss: 0.1338, Learning Rate: 0.181400\n",
      "Epoch   94, Training Loss: 0.0883, Validation Loss: 0.1315, Learning Rate: 0.181200\n",
      "Epoch   95, Training Loss: 0.0873, Validation Loss: 0.1317, Learning Rate: 0.181000\n",
      "Epoch   96, Training Loss: 0.0867, Validation Loss: 0.1309, Learning Rate: 0.180800\n",
      "Epoch   97, Training Loss: 0.0861, Validation Loss: 0.1311, Learning Rate: 0.180600\n",
      "Epoch   98, Training Loss: 0.0857, Validation Loss: 0.1309, Learning Rate: 0.180400\n",
      "Epoch   99, Training Loss: 0.0852, Validation Loss: 0.1303, Learning Rate: 0.180200\n",
      "Epoch  100, Training Loss: 0.0844, Validation Loss: 0.1289, Learning Rate: 0.180000\n",
      "Epoch  101, Training Loss: 0.0839, Validation Loss: 0.1276, Learning Rate: 0.179800\n",
      "Epoch  102, Training Loss: 0.0836, Validation Loss: 0.1268, Learning Rate: 0.179600\n",
      "Epoch  103, Training Loss: 0.0827, Validation Loss: 0.1269, Learning Rate: 0.179400\n",
      "Epoch  104, Training Loss: 0.0822, Validation Loss: 0.1260, Learning Rate: 0.179200\n",
      "Epoch  105, Training Loss: 0.0816, Validation Loss: 0.1258, Learning Rate: 0.179000\n",
      "Epoch  106, Training Loss: 0.0813, Validation Loss: 0.1247, Learning Rate: 0.178800\n",
      "Epoch  107, Training Loss: 0.0807, Validation Loss: 0.1243, Learning Rate: 0.178600\n",
      "Epoch  108, Training Loss: 0.0802, Validation Loss: 0.1247, Learning Rate: 0.178400\n",
      "Epoch  109, Training Loss: 0.0798, Validation Loss: 0.1251, Learning Rate: 0.178200\n",
      "Epoch  110, Training Loss: 0.0791, Validation Loss: 0.1238, Learning Rate: 0.178000\n",
      "Epoch  111, Training Loss: 0.0786, Validation Loss: 0.1227, Learning Rate: 0.177800\n",
      "Epoch  112, Training Loss: 0.0781, Validation Loss: 0.1220, Learning Rate: 0.177600\n",
      "Epoch  113, Training Loss: 0.0777, Validation Loss: 0.1227, Learning Rate: 0.177400\n",
      "Epoch  114, Training Loss: 0.0771, Validation Loss: 0.1212, Learning Rate: 0.177200\n",
      "Epoch  115, Training Loss: 0.0769, Validation Loss: 0.1220, Learning Rate: 0.177000\n",
      "Epoch  116, Training Loss: 0.0761, Validation Loss: 0.1206, Learning Rate: 0.176800\n",
      "Epoch  117, Training Loss: 0.0759, Validation Loss: 0.1183, Learning Rate: 0.176600\n",
      "Epoch  118, Training Loss: 0.0751, Validation Loss: 0.1191, Learning Rate: 0.176400\n",
      "Epoch  119, Training Loss: 0.0747, Validation Loss: 0.1191, Learning Rate: 0.176200\n",
      "Epoch  120, Training Loss: 0.0742, Validation Loss: 0.1172, Learning Rate: 0.176000\n",
      "Epoch  121, Training Loss: 0.0739, Validation Loss: 0.1165, Learning Rate: 0.175800\n",
      "Epoch  122, Training Loss: 0.0733, Validation Loss: 0.1173, Learning Rate: 0.175600\n",
      "Epoch  123, Training Loss: 0.0730, Validation Loss: 0.1155, Learning Rate: 0.175400\n",
      "Epoch  124, Training Loss: 0.0729, Validation Loss: 0.1146, Learning Rate: 0.175200\n",
      "Epoch  125, Training Loss: 0.0720, Validation Loss: 0.1162, Learning Rate: 0.175000\n",
      "Epoch  126, Training Loss: 0.0715, Validation Loss: 0.1144, Learning Rate: 0.174800\n",
      "Epoch  127, Training Loss: 0.0716, Validation Loss: 0.1132, Learning Rate: 0.174600\n",
      "Epoch  128, Training Loss: 0.0707, Validation Loss: 0.1147, Learning Rate: 0.174400\n",
      "Epoch  129, Training Loss: 0.0703, Validation Loss: 0.1123, Learning Rate: 0.174200\n",
      "Epoch  130, Training Loss: 0.0697, Validation Loss: 0.1124, Learning Rate: 0.174000\n",
      "Epoch  131, Training Loss: 0.0693, Validation Loss: 0.1126, Learning Rate: 0.173800\n",
      "Epoch  132, Training Loss: 0.0690, Validation Loss: 0.1125, Learning Rate: 0.173600\n",
      "Epoch  133, Training Loss: 0.0689, Validation Loss: 0.1100, Learning Rate: 0.173400\n",
      "Epoch  134, Training Loss: 0.0681, Validation Loss: 0.1108, Learning Rate: 0.173200\n",
      "Epoch  135, Training Loss: 0.0676, Validation Loss: 0.1099, Learning Rate: 0.173000\n",
      "Epoch  136, Training Loss: 0.0672, Validation Loss: 0.1097, Learning Rate: 0.172800\n",
      "Epoch  137, Training Loss: 0.0670, Validation Loss: 0.1102, Learning Rate: 0.172600\n",
      "Epoch  138, Training Loss: 0.0666, Validation Loss: 0.1081, Learning Rate: 0.172400\n",
      "Epoch  139, Training Loss: 0.0662, Validation Loss: 0.1091, Learning Rate: 0.172200\n",
      "Epoch  140, Training Loss: 0.0657, Validation Loss: 0.1084, Learning Rate: 0.172000\n",
      "Epoch  141, Training Loss: 0.0660, Validation Loss: 0.1060, Learning Rate: 0.171800\n",
      "Epoch  142, Training Loss: 0.0649, Validation Loss: 0.1065, Learning Rate: 0.171600\n",
      "Epoch  143, Training Loss: 0.0646, Validation Loss: 0.1073, Learning Rate: 0.171400\n",
      "Epoch  144, Training Loss: 0.0649, Validation Loss: 0.1047, Learning Rate: 0.171200\n",
      "Epoch  145, Training Loss: 0.0638, Validation Loss: 0.1054, Learning Rate: 0.171000\n",
      "Epoch  146, Training Loss: 0.0634, Validation Loss: 0.1049, Learning Rate: 0.170800\n",
      "Epoch  147, Training Loss: 0.0630, Validation Loss: 0.1045, Learning Rate: 0.170600\n",
      "Epoch  148, Training Loss: 0.0627, Validation Loss: 0.1041, Learning Rate: 0.170400\n",
      "Epoch  149, Training Loss: 0.0625, Validation Loss: 0.1028, Learning Rate: 0.170200\n",
      "Epoch  150, Training Loss: 0.0620, Validation Loss: 0.1031, Learning Rate: 0.170000\n",
      "Epoch  151, Training Loss: 0.0626, Validation Loss: 0.1012, Learning Rate: 0.169800\n",
      "Epoch  152, Training Loss: 0.0613, Validation Loss: 0.1018, Learning Rate: 0.169600\n",
      "Epoch  153, Training Loss: 0.0610, Validation Loss: 0.1015, Learning Rate: 0.169400\n",
      "Epoch  154, Training Loss: 0.0606, Validation Loss: 0.1013, Learning Rate: 0.169200\n",
      "Epoch  155, Training Loss: 0.0603, Validation Loss: 0.1010, Learning Rate: 0.169000\n",
      "Epoch  156, Training Loss: 0.0600, Validation Loss: 0.1009, Learning Rate: 0.168800\n",
      "Epoch  157, Training Loss: 0.0600, Validation Loss: 0.0990, Learning Rate: 0.168600\n",
      "Epoch  158, Training Loss: 0.0593, Validation Loss: 0.0997, Learning Rate: 0.168400\n",
      "Epoch  159, Training Loss: 0.0591, Validation Loss: 0.0984, Learning Rate: 0.168200\n",
      "Epoch  160, Training Loss: 0.0593, Validation Loss: 0.0973, Learning Rate: 0.168000\n",
      "Epoch  161, Training Loss: 0.0584, Validation Loss: 0.0982, Learning Rate: 0.167800\n",
      "Epoch  162, Training Loss: 0.0582, Validation Loss: 0.0968, Learning Rate: 0.167600\n",
      "Epoch  163, Training Loss: 0.0580, Validation Loss: 0.0963, Learning Rate: 0.167400\n",
      "Epoch  164, Training Loss: 0.0574, Validation Loss: 0.0972, Learning Rate: 0.167200\n",
      "Epoch  165, Training Loss: 0.0572, Validation Loss: 0.0974, Learning Rate: 0.167000\n",
      "Epoch  166, Training Loss: 0.0569, Validation Loss: 0.0955, Learning Rate: 0.166800\n",
      "Epoch  167, Training Loss: 0.0572, Validation Loss: 0.0983, Learning Rate: 0.166600\n",
      "Epoch  168, Training Loss: 0.0563, Validation Loss: 0.0947, Learning Rate: 0.166400\n",
      "Epoch  169, Training Loss: 0.0563, Validation Loss: 0.0935, Learning Rate: 0.166200\n",
      "Epoch  170, Training Loss: 0.0559, Validation Loss: 0.0958, Learning Rate: 0.166000\n",
      "Epoch  171, Training Loss: 0.0555, Validation Loss: 0.0932, Learning Rate: 0.165800\n",
      "Epoch  172, Training Loss: 0.0551, Validation Loss: 0.0930, Learning Rate: 0.165600\n",
      "Epoch  173, Training Loss: 0.0557, Validation Loss: 0.0918, Learning Rate: 0.165400\n",
      "Epoch  174, Training Loss: 0.0549, Validation Loss: 0.0914, Learning Rate: 0.165200\n",
      "Epoch  175, Training Loss: 0.0542, Validation Loss: 0.0928, Learning Rate: 0.165000\n",
      "Epoch  176, Training Loss: 0.0539, Validation Loss: 0.0921, Learning Rate: 0.164800\n",
      "Epoch  177, Training Loss: 0.0536, Validation Loss: 0.0919, Learning Rate: 0.164600\n",
      "Epoch  178, Training Loss: 0.0539, Validation Loss: 0.0900, Learning Rate: 0.164400\n",
      "Epoch  179, Training Loss: 0.0531, Validation Loss: 0.0903, Learning Rate: 0.164200\n",
      "Epoch  180, Training Loss: 0.0528, Validation Loss: 0.0900, Learning Rate: 0.164000\n",
      "Epoch  181, Training Loss: 0.0526, Validation Loss: 0.0898, Learning Rate: 0.163800\n",
      "Epoch  182, Training Loss: 0.0523, Validation Loss: 0.0898, Learning Rate: 0.163600\n",
      "Epoch  183, Training Loss: 0.0523, Validation Loss: 0.0908, Learning Rate: 0.163400\n",
      "Epoch  184, Training Loss: 0.0518, Validation Loss: 0.0892, Learning Rate: 0.163200\n",
      "Epoch  185, Training Loss: 0.0516, Validation Loss: 0.0891, Learning Rate: 0.163000\n",
      "Epoch  186, Training Loss: 0.0515, Validation Loss: 0.0892, Learning Rate: 0.162800\n",
      "Epoch  187, Training Loss: 0.0511, Validation Loss: 0.0878, Learning Rate: 0.162600\n",
      "Epoch  188, Training Loss: 0.0509, Validation Loss: 0.0869, Learning Rate: 0.162400\n",
      "Epoch  189, Training Loss: 0.0506, Validation Loss: 0.0872, Learning Rate: 0.162200\n",
      "Epoch  190, Training Loss: 0.0504, Validation Loss: 0.0868, Learning Rate: 0.162000\n",
      "Epoch  191, Training Loss: 0.0502, Validation Loss: 0.0859, Learning Rate: 0.161800\n",
      "Epoch  192, Training Loss: 0.0500, Validation Loss: 0.0856, Learning Rate: 0.161600\n",
      "Epoch  193, Training Loss: 0.0498, Validation Loss: 0.0866, Learning Rate: 0.161400\n",
      "Epoch  194, Training Loss: 0.0495, Validation Loss: 0.0856, Learning Rate: 0.161200\n",
      "Epoch  195, Training Loss: 0.0494, Validation Loss: 0.0841, Learning Rate: 0.161000\n",
      "Epoch  196, Training Loss: 0.0491, Validation Loss: 0.0853, Learning Rate: 0.160800\n",
      "Epoch  197, Training Loss: 0.0489, Validation Loss: 0.0840, Learning Rate: 0.160600\n",
      "Epoch  198, Training Loss: 0.0486, Validation Loss: 0.0841, Learning Rate: 0.160400\n",
      "Epoch  199, Training Loss: 0.0487, Validation Loss: 0.0851, Learning Rate: 0.160200\n",
      "Epoch  200, Training Loss: 0.0485, Validation Loss: 0.0826, Learning Rate: 0.160000\n",
      "Epoch  201, Training Loss: 0.0487, Validation Loss: 0.0818, Learning Rate: 0.159800\n",
      "Epoch  202, Training Loss: 0.0478, Validation Loss: 0.0823, Learning Rate: 0.159600\n",
      "Epoch  203, Training Loss: 0.0476, Validation Loss: 0.0822, Learning Rate: 0.159400\n",
      "Epoch  204, Training Loss: 0.0474, Validation Loss: 0.0820, Learning Rate: 0.159200\n",
      "Epoch  205, Training Loss: 0.0473, Validation Loss: 0.0815, Learning Rate: 0.159000\n",
      "Epoch  206, Training Loss: 0.0471, Validation Loss: 0.0810, Learning Rate: 0.158800\n",
      "Epoch  207, Training Loss: 0.0469, Validation Loss: 0.0812, Learning Rate: 0.158600\n",
      "Epoch  208, Training Loss: 0.0467, Validation Loss: 0.0805, Learning Rate: 0.158400\n",
      "Epoch  209, Training Loss: 0.0466, Validation Loss: 0.0801, Learning Rate: 0.158200\n",
      "Epoch  210, Training Loss: 0.0464, Validation Loss: 0.0807, Learning Rate: 0.158000\n",
      "Epoch  211, Training Loss: 0.0463, Validation Loss: 0.0792, Learning Rate: 0.157800\n",
      "Epoch  212, Training Loss: 0.0460, Validation Loss: 0.0799, Learning Rate: 0.157600\n",
      "Epoch  213, Training Loss: 0.0458, Validation Loss: 0.0792, Learning Rate: 0.157400\n",
      "Epoch  214, Training Loss: 0.0456, Validation Loss: 0.0791, Learning Rate: 0.157200\n",
      "Epoch  215, Training Loss: 0.0455, Validation Loss: 0.0794, Learning Rate: 0.157000\n",
      "Epoch  216, Training Loss: 0.0455, Validation Loss: 0.0778, Learning Rate: 0.156800\n",
      "Epoch  217, Training Loss: 0.0454, Validation Loss: 0.0797, Learning Rate: 0.156600\n",
      "Epoch  218, Training Loss: 0.0450, Validation Loss: 0.0778, Learning Rate: 0.156400\n",
      "Epoch  219, Training Loss: 0.0448, Validation Loss: 0.0780, Learning Rate: 0.156200\n",
      "Epoch  220, Training Loss: 0.0447, Validation Loss: 0.0772, Learning Rate: 0.156000\n",
      "Epoch  221, Training Loss: 0.0445, Validation Loss: 0.0772, Learning Rate: 0.155800\n",
      "Epoch  222, Training Loss: 0.0444, Validation Loss: 0.0767, Learning Rate: 0.155600\n",
      "Epoch  223, Training Loss: 0.0442, Validation Loss: 0.0766, Learning Rate: 0.155400\n",
      "Epoch  224, Training Loss: 0.0441, Validation Loss: 0.0764, Learning Rate: 0.155200\n",
      "Epoch  225, Training Loss: 0.0439, Validation Loss: 0.0765, Learning Rate: 0.155000\n",
      "Epoch  226, Training Loss: 0.0438, Validation Loss: 0.0763, Learning Rate: 0.154800\n",
      "Epoch  227, Training Loss: 0.0437, Validation Loss: 0.0761, Learning Rate: 0.154600\n",
      "Epoch  228, Training Loss: 0.0436, Validation Loss: 0.0748, Learning Rate: 0.154400\n",
      "Epoch  229, Training Loss: 0.0435, Validation Loss: 0.0744, Learning Rate: 0.154200\n",
      "Epoch  230, Training Loss: 0.0433, Validation Loss: 0.0740, Learning Rate: 0.154000\n",
      "Epoch  231, Training Loss: 0.0431, Validation Loss: 0.0744, Learning Rate: 0.153800\n",
      "Epoch  232, Training Loss: 0.0430, Validation Loss: 0.0740, Learning Rate: 0.153600\n",
      "Epoch  233, Training Loss: 0.0429, Validation Loss: 0.0736, Learning Rate: 0.153400\n",
      "Epoch  234, Training Loss: 0.0429, Validation Loss: 0.0751, Learning Rate: 0.153200\n",
      "Epoch  235, Training Loss: 0.0426, Validation Loss: 0.0740, Learning Rate: 0.153000\n",
      "Epoch  236, Training Loss: 0.0425, Validation Loss: 0.0734, Learning Rate: 0.152800\n",
      "Epoch  237, Training Loss: 0.0423, Validation Loss: 0.0731, Learning Rate: 0.152600\n",
      "Epoch  238, Training Loss: 0.0423, Validation Loss: 0.0738, Learning Rate: 0.152400\n",
      "Epoch  239, Training Loss: 0.0422, Validation Loss: 0.0722, Learning Rate: 0.152200\n",
      "Epoch  240, Training Loss: 0.0420, Validation Loss: 0.0725, Learning Rate: 0.152000\n",
      "Epoch  241, Training Loss: 0.0419, Validation Loss: 0.0724, Learning Rate: 0.151800\n",
      "Epoch  242, Training Loss: 0.0418, Validation Loss: 0.0719, Learning Rate: 0.151600\n",
      "Epoch  243, Training Loss: 0.0417, Validation Loss: 0.0722, Learning Rate: 0.151400\n",
      "Epoch  244, Training Loss: 0.0415, Validation Loss: 0.0718, Learning Rate: 0.151200\n",
      "Epoch  245, Training Loss: 0.0414, Validation Loss: 0.0719, Learning Rate: 0.151000\n",
      "Epoch  246, Training Loss: 0.0413, Validation Loss: 0.0708, Learning Rate: 0.150800\n",
      "Epoch  247, Training Loss: 0.0412, Validation Loss: 0.0706, Learning Rate: 0.150600\n",
      "Epoch  248, Training Loss: 0.0411, Validation Loss: 0.0705, Learning Rate: 0.150400\n",
      "Epoch  249, Training Loss: 0.0410, Validation Loss: 0.0704, Learning Rate: 0.150200\n",
      "Epoch  250, Training Loss: 0.0410, Validation Loss: 0.0697, Learning Rate: 0.150000\n",
      "Epoch  251, Training Loss: 0.0408, Validation Loss: 0.0696, Learning Rate: 0.149800\n",
      "Epoch  252, Training Loss: 0.0408, Validation Loss: 0.0692, Learning Rate: 0.149600\n",
      "Epoch  253, Training Loss: 0.0406, Validation Loss: 0.0692, Learning Rate: 0.149400\n",
      "Epoch  254, Training Loss: 0.0407, Validation Loss: 0.0686, Learning Rate: 0.149200\n",
      "Epoch  255, Training Loss: 0.0405, Validation Loss: 0.0685, Learning Rate: 0.149000\n",
      "Epoch  256, Training Loss: 0.0403, Validation Loss: 0.0687, Learning Rate: 0.148800\n",
      "Epoch  257, Training Loss: 0.0402, Validation Loss: 0.0688, Learning Rate: 0.148600\n",
      "Epoch  258, Training Loss: 0.0402, Validation Loss: 0.0679, Learning Rate: 0.148400\n",
      "Epoch  259, Training Loss: 0.0400, Validation Loss: 0.0687, Learning Rate: 0.148200\n",
      "Epoch  260, Training Loss: 0.0399, Validation Loss: 0.0686, Learning Rate: 0.148000\n",
      "Epoch  261, Training Loss: 0.0398, Validation Loss: 0.0682, Learning Rate: 0.147800\n",
      "Epoch  262, Training Loss: 0.0397, Validation Loss: 0.0680, Learning Rate: 0.147600\n",
      "Epoch  263, Training Loss: 0.0396, Validation Loss: 0.0676, Learning Rate: 0.147400\n",
      "Epoch  264, Training Loss: 0.0397, Validation Loss: 0.0682, Learning Rate: 0.147200\n",
      "Epoch  265, Training Loss: 0.0395, Validation Loss: 0.0674, Learning Rate: 0.147000\n",
      "Epoch  266, Training Loss: 0.0395, Validation Loss: 0.0679, Learning Rate: 0.146800\n",
      "Epoch  267, Training Loss: 0.0394, Validation Loss: 0.0666, Learning Rate: 0.146600\n",
      "Epoch  268, Training Loss: 0.0393, Validation Loss: 0.0662, Learning Rate: 0.146400\n",
      "Epoch  269, Training Loss: 0.0392, Validation Loss: 0.0665, Learning Rate: 0.146200\n",
      "Epoch  270, Training Loss: 0.0392, Validation Loss: 0.0656, Learning Rate: 0.146000\n",
      "Epoch  271, Training Loss: 0.0390, Validation Loss: 0.0662, Learning Rate: 0.145800\n",
      "Epoch  272, Training Loss: 0.0389, Validation Loss: 0.0656, Learning Rate: 0.145600\n",
      "Epoch  273, Training Loss: 0.0389, Validation Loss: 0.0661, Learning Rate: 0.145400\n",
      "Epoch  274, Training Loss: 0.0388, Validation Loss: 0.0654, Learning Rate: 0.145200\n",
      "Epoch  275, Training Loss: 0.0387, Validation Loss: 0.0657, Learning Rate: 0.145000\n",
      "Epoch  276, Training Loss: 0.0387, Validation Loss: 0.0657, Learning Rate: 0.144800\n",
      "Epoch  277, Training Loss: 0.0386, Validation Loss: 0.0654, Learning Rate: 0.144600\n",
      "Epoch  278, Training Loss: 0.0385, Validation Loss: 0.0650, Learning Rate: 0.144400\n",
      "Epoch  279, Training Loss: 0.0385, Validation Loss: 0.0651, Learning Rate: 0.144200\n",
      "Epoch  280, Training Loss: 0.0384, Validation Loss: 0.0649, Learning Rate: 0.144000\n",
      "Epoch  281, Training Loss: 0.0385, Validation Loss: 0.0656, Learning Rate: 0.143800\n",
      "Epoch  282, Training Loss: 0.0383, Validation Loss: 0.0641, Learning Rate: 0.143600\n",
      "Epoch  283, Training Loss: 0.0383, Validation Loss: 0.0650, Learning Rate: 0.143400\n",
      "Epoch  284, Training Loss: 0.0381, Validation Loss: 0.0644, Learning Rate: 0.143200\n",
      "Epoch  285, Training Loss: 0.0381, Validation Loss: 0.0642, Learning Rate: 0.143000\n",
      "Epoch  286, Training Loss: 0.0380, Validation Loss: 0.0640, Learning Rate: 0.142800\n",
      "Epoch  287, Training Loss: 0.0380, Validation Loss: 0.0640, Learning Rate: 0.142600\n",
      "Epoch  288, Training Loss: 0.0379, Validation Loss: 0.0632, Learning Rate: 0.142400\n",
      "Epoch  289, Training Loss: 0.0378, Validation Loss: 0.0633, Learning Rate: 0.142200\n",
      "Epoch  290, Training Loss: 0.0378, Validation Loss: 0.0638, Learning Rate: 0.142000\n",
      "Epoch  291, Training Loss: 0.0377, Validation Loss: 0.0632, Learning Rate: 0.141800\n",
      "Epoch  292, Training Loss: 0.0376, Validation Loss: 0.0627, Learning Rate: 0.141600\n",
      "Epoch  293, Training Loss: 0.0376, Validation Loss: 0.0626, Learning Rate: 0.141400\n",
      "Epoch  294, Training Loss: 0.0375, Validation Loss: 0.0623, Learning Rate: 0.141200\n",
      "Epoch  295, Training Loss: 0.0375, Validation Loss: 0.0625, Learning Rate: 0.141000\n",
      "Epoch  296, Training Loss: 0.0374, Validation Loss: 0.0624, Learning Rate: 0.140800\n",
      "Epoch  297, Training Loss: 0.0376, Validation Loss: 0.0635, Learning Rate: 0.140600\n",
      "Epoch  298, Training Loss: 0.0373, Validation Loss: 0.0623, Learning Rate: 0.140400\n",
      "Epoch  299, Training Loss: 0.0373, Validation Loss: 0.0617, Learning Rate: 0.140200\n",
      "Epoch  300, Training Loss: 0.0372, Validation Loss: 0.0622, Learning Rate: 0.140000\n",
      "Epoch  301, Training Loss: 0.0372, Validation Loss: 0.0620, Learning Rate: 0.139800\n",
      "Epoch  302, Training Loss: 0.0371, Validation Loss: 0.0616, Learning Rate: 0.139600\n",
      "Epoch  303, Training Loss: 0.0370, Validation Loss: 0.0613, Learning Rate: 0.139400\n",
      "Epoch  304, Training Loss: 0.0370, Validation Loss: 0.0613, Learning Rate: 0.139200\n",
      "Epoch  305, Training Loss: 0.0369, Validation Loss: 0.0610, Learning Rate: 0.139000\n",
      "Epoch  306, Training Loss: 0.0369, Validation Loss: 0.0613, Learning Rate: 0.138800\n",
      "Epoch  307, Training Loss: 0.0369, Validation Loss: 0.0604, Learning Rate: 0.138600\n",
      "Epoch  308, Training Loss: 0.0368, Validation Loss: 0.0604, Learning Rate: 0.138400\n",
      "Epoch  309, Training Loss: 0.0368, Validation Loss: 0.0601, Learning Rate: 0.138200\n",
      "Epoch  310, Training Loss: 0.0368, Validation Loss: 0.0609, Learning Rate: 0.138000\n",
      "Epoch  311, Training Loss: 0.0367, Validation Loss: 0.0603, Learning Rate: 0.137800\n",
      "Epoch  312, Training Loss: 0.0366, Validation Loss: 0.0599, Learning Rate: 0.137600\n",
      "Epoch  313, Training Loss: 0.0366, Validation Loss: 0.0598, Learning Rate: 0.137400\n",
      "Epoch  314, Training Loss: 0.0366, Validation Loss: 0.0596, Learning Rate: 0.137200\n",
      "Epoch  315, Training Loss: 0.0365, Validation Loss: 0.0593, Learning Rate: 0.137000\n",
      "Epoch  316, Training Loss: 0.0365, Validation Loss: 0.0594, Learning Rate: 0.136800\n",
      "Epoch  317, Training Loss: 0.0365, Validation Loss: 0.0602, Learning Rate: 0.136600\n",
      "Epoch  318, Training Loss: 0.0364, Validation Loss: 0.0593, Learning Rate: 0.136400\n",
      "Epoch  319, Training Loss: 0.0363, Validation Loss: 0.0596, Learning Rate: 0.136200\n",
      "Epoch  320, Training Loss: 0.0363, Validation Loss: 0.0594, Learning Rate: 0.136000\n",
      "Epoch  321, Training Loss: 0.0363, Validation Loss: 0.0590, Learning Rate: 0.135800\n",
      "Epoch  322, Training Loss: 0.0362, Validation Loss: 0.0590, Learning Rate: 0.135600\n",
      "Epoch  323, Training Loss: 0.0362, Validation Loss: 0.0585, Learning Rate: 0.135400\n",
      "Epoch  324, Training Loss: 0.0362, Validation Loss: 0.0584, Learning Rate: 0.135200\n",
      "Epoch  325, Training Loss: 0.0361, Validation Loss: 0.0586, Learning Rate: 0.135000\n",
      "Epoch  326, Training Loss: 0.0361, Validation Loss: 0.0585, Learning Rate: 0.134800\n",
      "Epoch  327, Training Loss: 0.0360, Validation Loss: 0.0583, Learning Rate: 0.134600\n",
      "Epoch  328, Training Loss: 0.0360, Validation Loss: 0.0587, Learning Rate: 0.134400\n",
      "Epoch  329, Training Loss: 0.0360, Validation Loss: 0.0581, Learning Rate: 0.134200\n",
      "Epoch  330, Training Loss: 0.0359, Validation Loss: 0.0583, Learning Rate: 0.134000\n",
      "Epoch  331, Training Loss: 0.0359, Validation Loss: 0.0579, Learning Rate: 0.133800\n",
      "Epoch  332, Training Loss: 0.0359, Validation Loss: 0.0584, Learning Rate: 0.133600\n",
      "Epoch  333, Training Loss: 0.0358, Validation Loss: 0.0579, Learning Rate: 0.133400\n",
      "Epoch  334, Training Loss: 0.0358, Validation Loss: 0.0581, Learning Rate: 0.133200\n",
      "Epoch  335, Training Loss: 0.0358, Validation Loss: 0.0572, Learning Rate: 0.133000\n",
      "Epoch  336, Training Loss: 0.0357, Validation Loss: 0.0574, Learning Rate: 0.132800\n",
      "Epoch  337, Training Loss: 0.0358, Validation Loss: 0.0570, Learning Rate: 0.132600\n",
      "Epoch  338, Training Loss: 0.0357, Validation Loss: 0.0570, Learning Rate: 0.132400\n",
      "Epoch  339, Training Loss: 0.0357, Validation Loss: 0.0571, Learning Rate: 0.132200\n",
      "Epoch  340, Training Loss: 0.0356, Validation Loss: 0.0573, Learning Rate: 0.132000\n",
      "Epoch  341, Training Loss: 0.0356, Validation Loss: 0.0569, Learning Rate: 0.131800\n",
      "Epoch  342, Training Loss: 0.0356, Validation Loss: 0.0571, Learning Rate: 0.131600\n",
      "Epoch  343, Training Loss: 0.0355, Validation Loss: 0.0567, Learning Rate: 0.131400\n",
      "Epoch  344, Training Loss: 0.0355, Validation Loss: 0.0568, Learning Rate: 0.131200\n",
      "Epoch  345, Training Loss: 0.0355, Validation Loss: 0.0569, Learning Rate: 0.131000\n",
      "Epoch  346, Training Loss: 0.0355, Validation Loss: 0.0563, Learning Rate: 0.130800\n",
      "Epoch  347, Training Loss: 0.0354, Validation Loss: 0.0565, Learning Rate: 0.130600\n",
      "Epoch  348, Training Loss: 0.0354, Validation Loss: 0.0562, Learning Rate: 0.130400\n",
      "Epoch  349, Training Loss: 0.0354, Validation Loss: 0.0562, Learning Rate: 0.130200\n",
      "Epoch  350, Training Loss: 0.0354, Validation Loss: 0.0562, Learning Rate: 0.130000\n",
      "Epoch  351, Training Loss: 0.0353, Validation Loss: 0.0561, Learning Rate: 0.129800\n",
      "Epoch  352, Training Loss: 0.0353, Validation Loss: 0.0560, Learning Rate: 0.129600\n",
      "Epoch  353, Training Loss: 0.0353, Validation Loss: 0.0562, Learning Rate: 0.129400\n",
      "Epoch  354, Training Loss: 0.0353, Validation Loss: 0.0556, Learning Rate: 0.129200\n",
      "Epoch  355, Training Loss: 0.0352, Validation Loss: 0.0558, Learning Rate: 0.129000\n",
      "Epoch  356, Training Loss: 0.0352, Validation Loss: 0.0556, Learning Rate: 0.128800\n",
      "Epoch  357, Training Loss: 0.0352, Validation Loss: 0.0556, Learning Rate: 0.128600\n",
      "Epoch  358, Training Loss: 0.0352, Validation Loss: 0.0555, Learning Rate: 0.128400\n",
      "Epoch  359, Training Loss: 0.0351, Validation Loss: 0.0555, Learning Rate: 0.128200\n",
      "Epoch  360, Training Loss: 0.0351, Validation Loss: 0.0553, Learning Rate: 0.128000\n",
      "Epoch  361, Training Loss: 0.0351, Validation Loss: 0.0557, Learning Rate: 0.127800\n",
      "Epoch  362, Training Loss: 0.0351, Validation Loss: 0.0555, Learning Rate: 0.127600\n",
      "Epoch  363, Training Loss: 0.0351, Validation Loss: 0.0553, Learning Rate: 0.127400\n",
      "Epoch  364, Training Loss: 0.0350, Validation Loss: 0.0551, Learning Rate: 0.127200\n",
      "Epoch  365, Training Loss: 0.0350, Validation Loss: 0.0552, Learning Rate: 0.127000\n",
      "Epoch  366, Training Loss: 0.0350, Validation Loss: 0.0551, Learning Rate: 0.126800\n",
      "Epoch  367, Training Loss: 0.0350, Validation Loss: 0.0550, Learning Rate: 0.126600\n",
      "Epoch  368, Training Loss: 0.0350, Validation Loss: 0.0547, Learning Rate: 0.126400\n",
      "Epoch  369, Training Loss: 0.0350, Validation Loss: 0.0547, Learning Rate: 0.126200\n",
      "Epoch  370, Training Loss: 0.0349, Validation Loss: 0.0547, Learning Rate: 0.126000\n",
      "Epoch  371, Training Loss: 0.0349, Validation Loss: 0.0547, Learning Rate: 0.125800\n",
      "Epoch  372, Training Loss: 0.0349, Validation Loss: 0.0546, Learning Rate: 0.125600\n",
      "Epoch  373, Training Loss: 0.0349, Validation Loss: 0.0546, Learning Rate: 0.125400\n",
      "Epoch  374, Training Loss: 0.0349, Validation Loss: 0.0547, Learning Rate: 0.125200\n",
      "Epoch  375, Training Loss: 0.0349, Validation Loss: 0.0546, Learning Rate: 0.125000\n",
      "Epoch  376, Training Loss: 0.0348, Validation Loss: 0.0544, Learning Rate: 0.124800\n",
      "Epoch  377, Training Loss: 0.0348, Validation Loss: 0.0541, Learning Rate: 0.124600\n",
      "Epoch  378, Training Loss: 0.0348, Validation Loss: 0.0540, Learning Rate: 0.124400\n",
      "Epoch  379, Training Loss: 0.0348, Validation Loss: 0.0540, Learning Rate: 0.124200\n",
      "Epoch  380, Training Loss: 0.0348, Validation Loss: 0.0542, Learning Rate: 0.124000\n",
      "Epoch  381, Training Loss: 0.0347, Validation Loss: 0.0539, Learning Rate: 0.123800\n",
      "Epoch  382, Training Loss: 0.0347, Validation Loss: 0.0540, Learning Rate: 0.123600\n",
      "Epoch  383, Training Loss: 0.0347, Validation Loss: 0.0538, Learning Rate: 0.123400\n",
      "Epoch  384, Training Loss: 0.0347, Validation Loss: 0.0535, Learning Rate: 0.123200\n",
      "Epoch  385, Training Loss: 0.0347, Validation Loss: 0.0535, Learning Rate: 0.123000\n",
      "Epoch  386, Training Loss: 0.0347, Validation Loss: 0.0534, Learning Rate: 0.122800\n",
      "Epoch  387, Training Loss: 0.0347, Validation Loss: 0.0536, Learning Rate: 0.122600\n",
      "Epoch  388, Training Loss: 0.0346, Validation Loss: 0.0536, Learning Rate: 0.122400\n",
      "Epoch  389, Training Loss: 0.0346, Validation Loss: 0.0535, Learning Rate: 0.122200\n",
      "Epoch  390, Training Loss: 0.0346, Validation Loss: 0.0532, Learning Rate: 0.122000\n",
      "Epoch  391, Training Loss: 0.0346, Validation Loss: 0.0532, Learning Rate: 0.121800\n",
      "Epoch  392, Training Loss: 0.0346, Validation Loss: 0.0533, Learning Rate: 0.121600\n",
      "Epoch  393, Training Loss: 0.0346, Validation Loss: 0.0531, Learning Rate: 0.121400\n",
      "Epoch  394, Training Loss: 0.0346, Validation Loss: 0.0531, Learning Rate: 0.121200\n",
      "Epoch  395, Training Loss: 0.0346, Validation Loss: 0.0527, Learning Rate: 0.121000\n",
      "Epoch  396, Training Loss: 0.0345, Validation Loss: 0.0528, Learning Rate: 0.120800\n",
      "Epoch  397, Training Loss: 0.0345, Validation Loss: 0.0532, Learning Rate: 0.120600\n",
      "Epoch  398, Training Loss: 0.0345, Validation Loss: 0.0528, Learning Rate: 0.120400\n",
      "Epoch  399, Training Loss: 0.0345, Validation Loss: 0.0530, Learning Rate: 0.120200\n",
      "Epoch  400, Training Loss: 0.0345, Validation Loss: 0.0528, Learning Rate: 0.120000\n",
      "Epoch  401, Training Loss: 0.0345, Validation Loss: 0.0526, Learning Rate: 0.119800\n",
      "Epoch  402, Training Loss: 0.0345, Validation Loss: 0.0527, Learning Rate: 0.119600\n",
      "Epoch  403, Training Loss: 0.0344, Validation Loss: 0.0526, Learning Rate: 0.119400\n",
      "Epoch  404, Training Loss: 0.0344, Validation Loss: 0.0525, Learning Rate: 0.119200\n",
      "Epoch  405, Training Loss: 0.0344, Validation Loss: 0.0525, Learning Rate: 0.119000\n",
      "Epoch  406, Training Loss: 0.0344, Validation Loss: 0.0523, Learning Rate: 0.118800\n",
      "Epoch  407, Training Loss: 0.0344, Validation Loss: 0.0523, Learning Rate: 0.118600\n",
      "Epoch  408, Training Loss: 0.0344, Validation Loss: 0.0524, Learning Rate: 0.118400\n",
      "Epoch  409, Training Loss: 0.0344, Validation Loss: 0.0521, Learning Rate: 0.118200\n",
      "Epoch  410, Training Loss: 0.0344, Validation Loss: 0.0522, Learning Rate: 0.118000\n",
      "Epoch  411, Training Loss: 0.0344, Validation Loss: 0.0520, Learning Rate: 0.117800\n",
      "Epoch  412, Training Loss: 0.0343, Validation Loss: 0.0520, Learning Rate: 0.117600\n",
      "Epoch  413, Training Loss: 0.0344, Validation Loss: 0.0517, Learning Rate: 0.117400\n",
      "Epoch  414, Training Loss: 0.0343, Validation Loss: 0.0517, Learning Rate: 0.117200\n",
      "Epoch  415, Training Loss: 0.0343, Validation Loss: 0.0518, Learning Rate: 0.117000\n",
      "Epoch  416, Training Loss: 0.0343, Validation Loss: 0.0518, Learning Rate: 0.116800\n",
      "Epoch  417, Training Loss: 0.0343, Validation Loss: 0.0520, Learning Rate: 0.116600\n",
      "Epoch  418, Training Loss: 0.0343, Validation Loss: 0.0518, Learning Rate: 0.116400\n",
      "Epoch  419, Training Loss: 0.0343, Validation Loss: 0.0518, Learning Rate: 0.116200\n",
      "Epoch  420, Training Loss: 0.0343, Validation Loss: 0.0517, Learning Rate: 0.116000\n",
      "Epoch  421, Training Loss: 0.0343, Validation Loss: 0.0515, Learning Rate: 0.115800\n",
      "Epoch  422, Training Loss: 0.0343, Validation Loss: 0.0514, Learning Rate: 0.115600\n",
      "Epoch  423, Training Loss: 0.0343, Validation Loss: 0.0512, Learning Rate: 0.115400\n",
      "Epoch  424, Training Loss: 0.0343, Validation Loss: 0.0511, Learning Rate: 0.115200\n",
      "Epoch  425, Training Loss: 0.0342, Validation Loss: 0.0514, Learning Rate: 0.115000\n",
      "Epoch  426, Training Loss: 0.0342, Validation Loss: 0.0515, Learning Rate: 0.114800\n",
      "Epoch  427, Training Loss: 0.0342, Validation Loss: 0.0512, Learning Rate: 0.114600\n",
      "Epoch  428, Training Loss: 0.0342, Validation Loss: 0.0512, Learning Rate: 0.114400\n",
      "Epoch  429, Training Loss: 0.0342, Validation Loss: 0.0512, Learning Rate: 0.114200\n",
      "Epoch  430, Training Loss: 0.0342, Validation Loss: 0.0511, Learning Rate: 0.114000\n",
      "Epoch  431, Training Loss: 0.0342, Validation Loss: 0.0511, Learning Rate: 0.113800\n",
      "Epoch  432, Training Loss: 0.0342, Validation Loss: 0.0511, Learning Rate: 0.113600\n",
      "Epoch  433, Training Loss: 0.0342, Validation Loss: 0.0508, Learning Rate: 0.113400\n",
      "Epoch  434, Training Loss: 0.0342, Validation Loss: 0.0509, Learning Rate: 0.113200\n",
      "Epoch  435, Training Loss: 0.0342, Validation Loss: 0.0510, Learning Rate: 0.113000\n",
      "Epoch  436, Training Loss: 0.0342, Validation Loss: 0.0507, Learning Rate: 0.112800\n",
      "Epoch  437, Training Loss: 0.0341, Validation Loss: 0.0508, Learning Rate: 0.112600\n",
      "Epoch  438, Training Loss: 0.0341, Validation Loss: 0.0508, Learning Rate: 0.112400\n",
      "Epoch  439, Training Loss: 0.0341, Validation Loss: 0.0507, Learning Rate: 0.112200\n",
      "Epoch  440, Training Loss: 0.0341, Validation Loss: 0.0509, Learning Rate: 0.112000\n",
      "Epoch  441, Training Loss: 0.0341, Validation Loss: 0.0507, Learning Rate: 0.111800\n",
      "Epoch  442, Training Loss: 0.0341, Validation Loss: 0.0507, Learning Rate: 0.111600\n",
      "Epoch  443, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.111400\n",
      "Epoch  444, Training Loss: 0.0341, Validation Loss: 0.0506, Learning Rate: 0.111200\n",
      "Epoch  445, Training Loss: 0.0341, Validation Loss: 0.0507, Learning Rate: 0.111000\n",
      "Epoch  446, Training Loss: 0.0341, Validation Loss: 0.0503, Learning Rate: 0.110800\n",
      "Epoch  447, Training Loss: 0.0341, Validation Loss: 0.0503, Learning Rate: 0.110600\n",
      "Epoch  448, Training Loss: 0.0341, Validation Loss: 0.0505, Learning Rate: 0.110400\n",
      "Epoch  449, Training Loss: 0.0341, Validation Loss: 0.0503, Learning Rate: 0.110200\n",
      "Epoch  450, Training Loss: 0.0341, Validation Loss: 0.0502, Learning Rate: 0.110000\n",
      "Epoch  451, Training Loss: 0.0341, Validation Loss: 0.0501, Learning Rate: 0.109800\n",
      "Epoch  452, Training Loss: 0.0341, Validation Loss: 0.0503, Learning Rate: 0.109600\n",
      "Epoch  453, Training Loss: 0.0341, Validation Loss: 0.0502, Learning Rate: 0.109400\n",
      "Epoch  454, Training Loss: 0.0341, Validation Loss: 0.0500, Learning Rate: 0.109200\n",
      "Epoch  455, Training Loss: 0.0341, Validation Loss: 0.0499, Learning Rate: 0.109000\n",
      "Epoch  456, Training Loss: 0.0341, Validation Loss: 0.0499, Learning Rate: 0.108800\n",
      "Epoch  457, Training Loss: 0.0340, Validation Loss: 0.0500, Learning Rate: 0.108600\n",
      "Epoch  458, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.108400\n",
      "Epoch  459, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.108200\n",
      "Epoch  460, Training Loss: 0.0340, Validation Loss: 0.0498, Learning Rate: 0.108000\n",
      "Epoch  461, Training Loss: 0.0340, Validation Loss: 0.0499, Learning Rate: 0.107800\n",
      "Epoch  462, Training Loss: 0.0340, Validation Loss: 0.0497, Learning Rate: 0.107600\n",
      "Epoch  463, Training Loss: 0.0340, Validation Loss: 0.0498, Learning Rate: 0.107400\n",
      "Epoch  464, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.107200\n",
      "Epoch  465, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.107000\n",
      "Epoch  466, Training Loss: 0.0340, Validation Loss: 0.0496, Learning Rate: 0.106800\n",
      "Epoch  467, Training Loss: 0.0340, Validation Loss: 0.0496, Learning Rate: 0.106600\n",
      "Epoch  468, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.106400\n",
      "Epoch  469, Training Loss: 0.0340, Validation Loss: 0.0494, Learning Rate: 0.106200\n",
      "Epoch  470, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.106000\n",
      "Epoch  471, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.105800\n",
      "Epoch  472, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.105600\n",
      "Epoch  473, Training Loss: 0.0340, Validation Loss: 0.0495, Learning Rate: 0.105400\n",
      "Epoch  474, Training Loss: 0.0340, Validation Loss: 0.0494, Learning Rate: 0.105200\n",
      "Epoch  475, Training Loss: 0.0340, Validation Loss: 0.0493, Learning Rate: 0.105000\n",
      "Epoch  476, Training Loss: 0.0340, Validation Loss: 0.0493, Learning Rate: 0.104800\n",
      "Epoch  477, Training Loss: 0.0340, Validation Loss: 0.0492, Learning Rate: 0.104600\n",
      "Epoch  478, Training Loss: 0.0340, Validation Loss: 0.0492, Learning Rate: 0.104400\n",
      "Epoch  479, Training Loss: 0.0340, Validation Loss: 0.0493, Learning Rate: 0.104200\n",
      "Epoch  480, Training Loss: 0.0340, Validation Loss: 0.0492, Learning Rate: 0.104000\n",
      "Epoch  481, Training Loss: 0.0340, Validation Loss: 0.0492, Learning Rate: 0.103800\n",
      "Epoch  482, Training Loss: 0.0340, Validation Loss: 0.0491, Learning Rate: 0.103600\n",
      "Epoch  483, Training Loss: 0.0340, Validation Loss: 0.0490, Learning Rate: 0.103400\n",
      "Epoch  484, Training Loss: 0.0340, Validation Loss: 0.0490, Learning Rate: 0.103200\n",
      "Epoch  485, Training Loss: 0.0340, Validation Loss: 0.0491, Learning Rate: 0.103000\n",
      "Epoch  486, Training Loss: 0.0340, Validation Loss: 0.0489, Learning Rate: 0.102800\n",
      "Epoch  487, Training Loss: 0.0340, Validation Loss: 0.0490, Learning Rate: 0.102600\n",
      "Epoch  488, Training Loss: 0.0339, Validation Loss: 0.0490, Learning Rate: 0.102400\n",
      "Epoch  489, Training Loss: 0.0339, Validation Loss: 0.0490, Learning Rate: 0.102200\n",
      "Epoch  490, Training Loss: 0.0339, Validation Loss: 0.0489, Learning Rate: 0.102000\n",
      "Epoch  491, Training Loss: 0.0339, Validation Loss: 0.0488, Learning Rate: 0.101800\n",
      "Epoch  492, Training Loss: 0.0339, Validation Loss: 0.0488, Learning Rate: 0.101600\n",
      "Epoch  493, Training Loss: 0.0339, Validation Loss: 0.0488, Learning Rate: 0.101400\n",
      "Epoch  494, Training Loss: 0.0339, Validation Loss: 0.0487, Learning Rate: 0.101200\n",
      "Epoch  495, Training Loss: 0.0339, Validation Loss: 0.0487, Learning Rate: 0.101000\n",
      "Epoch  496, Training Loss: 0.0339, Validation Loss: 0.0485, Learning Rate: 0.100800\n",
      "Epoch  497, Training Loss: 0.0339, Validation Loss: 0.0486, Learning Rate: 0.100600\n",
      "Epoch  498, Training Loss: 0.0339, Validation Loss: 0.0485, Learning Rate: 0.100400\n",
      "Epoch  499, Training Loss: 0.0339, Validation Loss: 0.0485, Learning Rate: 0.100200\n",
      "Epoch  500, Training Loss: 0.0339, Validation Loss: 0.0485, Learning Rate: 0.100000\n",
      "Epoch  501, Training Loss: 0.0339, Validation Loss: 0.0484, Learning Rate: 0.099800\n",
      "Epoch  502, Training Loss: 0.0339, Validation Loss: 0.0484, Learning Rate: 0.099600\n",
      "Epoch  503, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.099400\n",
      "Epoch  504, Training Loss: 0.0339, Validation Loss: 0.0484, Learning Rate: 0.099200\n",
      "Epoch  505, Training Loss: 0.0339, Validation Loss: 0.0484, Learning Rate: 0.099000\n",
      "Epoch  506, Training Loss: 0.0339, Validation Loss: 0.0485, Learning Rate: 0.098800\n",
      "Epoch  507, Training Loss: 0.0339, Validation Loss: 0.0484, Learning Rate: 0.098600\n",
      "Epoch  508, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.098400\n",
      "Epoch  509, Training Loss: 0.0339, Validation Loss: 0.0482, Learning Rate: 0.098200\n",
      "Epoch  510, Training Loss: 0.0339, Validation Loss: 0.0482, Learning Rate: 0.098000\n",
      "Epoch  511, Training Loss: 0.0339, Validation Loss: 0.0482, Learning Rate: 0.097800\n",
      "Epoch  512, Training Loss: 0.0339, Validation Loss: 0.0482, Learning Rate: 0.097600\n",
      "Epoch  513, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.097400\n",
      "Epoch  514, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.097200\n",
      "Epoch  515, Training Loss: 0.0339, Validation Loss: 0.0483, Learning Rate: 0.097000\n",
      "Epoch  516, Training Loss: 0.0339, Validation Loss: 0.0482, Learning Rate: 0.096800\n",
      "Epoch  517, Training Loss: 0.0339, Validation Loss: 0.0482, Learning Rate: 0.096600\n",
      "Epoch  518, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.096400\n",
      "Epoch  519, Training Loss: 0.0339, Validation Loss: 0.0481, Learning Rate: 0.096200\n",
      "Epoch  520, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.096000\n",
      "Epoch  521, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.095800\n",
      "Epoch  522, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.095600\n",
      "Epoch  523, Training Loss: 0.0339, Validation Loss: 0.0479, Learning Rate: 0.095400\n",
      "Epoch  524, Training Loss: 0.0339, Validation Loss: 0.0479, Learning Rate: 0.095200\n",
      "Epoch  525, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.095000\n",
      "Epoch  526, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.094800\n",
      "Epoch  527, Training Loss: 0.0339, Validation Loss: 0.0480, Learning Rate: 0.094600\n",
      "Epoch  528, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.094400\n",
      "Epoch  529, Training Loss: 0.0339, Validation Loss: 0.0479, Learning Rate: 0.094200\n",
      "Epoch  530, Training Loss: 0.0339, Validation Loss: 0.0479, Learning Rate: 0.094000\n",
      "Epoch  531, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.093800\n",
      "Epoch  532, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.093600\n",
      "Epoch  533, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.093400\n",
      "Epoch  534, Training Loss: 0.0339, Validation Loss: 0.0478, Learning Rate: 0.093200\n",
      "Epoch  535, Training Loss: 0.0339, Validation Loss: 0.0477, Learning Rate: 0.093000\n",
      "Epoch  536, Training Loss: 0.0339, Validation Loss: 0.0477, Learning Rate: 0.092800\n",
      "Epoch  537, Training Loss: 0.0339, Validation Loss: 0.0477, Learning Rate: 0.092600\n",
      "Epoch  538, Training Loss: 0.0339, Validation Loss: 0.0477, Learning Rate: 0.092400\n",
      "Epoch  539, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.092200\n",
      "Epoch  540, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.092000\n",
      "Epoch  541, Training Loss: 0.0339, Validation Loss: 0.0476, Learning Rate: 0.091800\n",
      "Epoch  542, Training Loss: 0.0339, Validation Loss: 0.0475, Learning Rate: 0.091600\n",
      "Epoch  543, Training Loss: 0.0339, Validation Loss: 0.0475, Learning Rate: 0.091400\n",
      "Epoch  544, Training Loss: 0.0339, Validation Loss: 0.0475, Learning Rate: 0.091200\n",
      "Epoch  545, Training Loss: 0.0339, Validation Loss: 0.0475, Learning Rate: 0.091000\n",
      "Epoch  546, Training Loss: 0.0339, Validation Loss: 0.0474, Learning Rate: 0.090800\n",
      "Epoch  547, Training Loss: 0.0339, Validation Loss: 0.0474, Learning Rate: 0.090600\n",
      "Epoch  548, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.090400\n",
      "Epoch  549, Training Loss: 0.0339, Validation Loss: 0.0474, Learning Rate: 0.090200\n",
      "Epoch  550, Training Loss: 0.0339, Validation Loss: 0.0474, Learning Rate: 0.090000\n",
      "Epoch  551, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.089800\n",
      "Epoch  552, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.089600\n",
      "Epoch  553, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.089400\n",
      "Epoch  554, Training Loss: 0.0339, Validation Loss: 0.0474, Learning Rate: 0.089200\n",
      "Epoch  555, Training Loss: 0.0339, Validation Loss: 0.0473, Learning Rate: 0.089000\n",
      "Epoch  556, Training Loss: 0.0339, Validation Loss: 0.0472, Learning Rate: 0.088800\n",
      "Epoch  557, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.088600\n",
      "Epoch  558, Training Loss: 0.0339, Validation Loss: 0.0472, Learning Rate: 0.088400\n",
      "Epoch  559, Training Loss: 0.0339, Validation Loss: 0.0472, Learning Rate: 0.088200\n",
      "Epoch  560, Training Loss: 0.0339, Validation Loss: 0.0472, Learning Rate: 0.088000\n",
      "Epoch  561, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.087800\n",
      "Epoch  562, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.087600\n",
      "Epoch  563, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.087400\n",
      "Epoch  564, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.087200\n",
      "Epoch  565, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.087000\n",
      "Epoch  566, Training Loss: 0.0339, Validation Loss: 0.0471, Learning Rate: 0.086800\n",
      "Epoch  567, Training Loss: 0.0339, Validation Loss: 0.0470, Learning Rate: 0.086600\n",
      "Epoch  568, Training Loss: 0.0339, Validation Loss: 0.0470, Learning Rate: 0.086400\n",
      "Epoch  569, Training Loss: 0.0339, Validation Loss: 0.0470, Learning Rate: 0.086200\n",
      "Epoch  570, Training Loss: 0.0339, Validation Loss: 0.0470, Learning Rate: 0.086000\n",
      "Epoch  571, Training Loss: 0.0339, Validation Loss: 0.0470, Learning Rate: 0.085800\n",
      "Epoch  572, Training Loss: 0.0339, Validation Loss: 0.0470, Learning Rate: 0.085600\n",
      "Epoch  573, Training Loss: 0.0339, Validation Loss: 0.0470, Learning Rate: 0.085400\n",
      "Epoch  574, Training Loss: 0.0339, Validation Loss: 0.0470, Learning Rate: 0.085200\n",
      "Epoch  575, Training Loss: 0.0339, Validation Loss: 0.0469, Learning Rate: 0.085000\n",
      "Epoch  576, Training Loss: 0.0339, Validation Loss: 0.0469, Learning Rate: 0.084800\n",
      "Epoch  577, Training Loss: 0.0339, Validation Loss: 0.0468, Learning Rate: 0.084600\n",
      "Epoch  578, Training Loss: 0.0339, Validation Loss: 0.0468, Learning Rate: 0.084400\n",
      "Epoch  579, Training Loss: 0.0339, Validation Loss: 0.0468, Learning Rate: 0.084200\n",
      "Epoch  580, Training Loss: 0.0339, Validation Loss: 0.0468, Learning Rate: 0.084000\n",
      "Epoch  581, Training Loss: 0.0339, Validation Loss: 0.0468, Learning Rate: 0.083800\n",
      "Epoch  582, Training Loss: 0.0339, Validation Loss: 0.0468, Learning Rate: 0.083600\n",
      "Epoch  583, Training Loss: 0.0339, Validation Loss: 0.0468, Learning Rate: 0.083400\n",
      "Epoch  584, Training Loss: 0.0339, Validation Loss: 0.0467, Learning Rate: 0.083200\n",
      "Epoch  585, Training Loss: 0.0339, Validation Loss: 0.0468, Learning Rate: 0.083000\n",
      "Epoch  586, Training Loss: 0.0339, Validation Loss: 0.0468, Learning Rate: 0.082800\n",
      "Epoch  587, Training Loss: 0.0339, Validation Loss: 0.0468, Learning Rate: 0.082600\n",
      "Early stopping triggered at epoch 587. Restoring best model parameters.\n",
      "\n",
      "Neural Network Classification Accuracy: 0.9907\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 Evaluation Metric: 0.7083\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7817\n",
      "\n",
      "K-fold Cross-Validation Results:\n",
      "{'fold_metrics': [np.float64(0.72), np.float64(0.8), np.float64(0.88), np.float64(0.8), np.float64(0.7083333333333334)], 'average_metric': np.float64(0.7816666666666667)}\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.6800\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.6800\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "Fold 1 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 3/5\n",
      "Fold 2 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 3/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 3/5\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 3/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 3 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 3 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 1 Evaluation Metric: 0.6800\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 2/5\n",
      "Fold 3 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 4/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 4/5\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "Fold 2 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 3/5\n",
      "\n",
      "Fold 3/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Fold 1 Evaluation Metric: 0.6800\n",
      "\n",
      "Fold 2/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 2 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 3/5\n",
      "Fold 4 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 5/5\n",
      "Fold 3 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 2 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 3/5\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 Evaluation Metric: 0.7083\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7817\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.7817\n",
      "\n",
      "Fold 1/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 3 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 4/5\n",
      "Fold 3 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 5/5\n",
      "Fold 3 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 4/5\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7743\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.7743\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 2/5\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7743\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.7743\n",
      "\n",
      "Fold 1/5\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7903\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.7903\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 2/5\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7903\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.7903\n",
      "Fold 2 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 3/5\n",
      "Fold 2 Evaluation Metric: 0.6800\n",
      "\n",
      "Fold 3/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 2/5\n",
      "Fold 5 Evaluation Metric: 0.9167\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8393\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.8393\n",
      "Fold 4 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 5/5\n",
      "Fold 2 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 3/5\n",
      "Fold 4 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 5/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 2 Evaluation Metric: 0.6800\n",
      "\n",
      "Fold 3/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 2/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 5 Evaluation Metric: 0.9167\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8313\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.8313\n",
      "\n",
      "Fold 1/5\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7500\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.7500\n",
      "Fold 3 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 5 Evaluation Metric: 0.9167\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8393\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.8393\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7743\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.7743\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 1 Evaluation Metric: 0.6800\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 Evaluation Metric: 0.7083\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7497\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.7497\n",
      "Fold 2 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 3/5\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7667\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.7667\n",
      "\n",
      "Fold 1/5\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7827\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.7827\n",
      "Fold 1 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 2/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.6800\n",
      "\n",
      "Fold 2/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7580\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.7580\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 3/5\n",
      "Fold 1 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 3/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7823Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.7823\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 2/5\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7827\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.7827\n",
      "\n",
      "Fold 1/5\n",
      "Fold 2 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 3/5\n",
      "Fold 2 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 3/5\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 3 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 3 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 2/5\n",
      "Fold 1 Evaluation Metric: 0.6800\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 4/5\n",
      "Fold 3 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 2 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 3/5\n",
      "Fold 2 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 3/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 1 Evaluation Metric: 0.6800\n",
      "\n",
      "Fold 2/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 3 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 3 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 4/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 5/5\n",
      "Fold 2 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 3/5\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Fold 3 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 3 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Fold 4 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7903\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.7903\n",
      "\n",
      "Fold 1/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 3 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 4/5\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 2/5\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7743\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.7743\n",
      "\n",
      "Fold 1/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 2/5\n",
      "Fold 4 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7903\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.7903\n",
      "\n",
      "Fold 1/5\n",
      "Fold 5 Evaluation Metric: 0.9167\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8393\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.8393\n",
      "Fold 2 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 3/5\n",
      "Fold 5 Evaluation Metric: 0.9167\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8393\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.8393\n",
      "\n",
      "Fold 1/5\n",
      "Fold 4 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 Evaluation Metric: 0.9167\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8393\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.8393\n",
      "Fold 5 Evaluation Metric: 0.9167\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8393\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.8393\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 2/5\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7500\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.7500\n",
      "\n",
      "Fold 1/5\n",
      "Fold 2 Evaluation Metric: 0.6800\n",
      "\n",
      "Fold 3/5\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7743\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.7743\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7823\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.7823\n",
      "Fold 1 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 Evaluation Metric: 0.6800\n",
      "\n",
      "Fold 3/5\n",
      "Fold 2 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 3/5\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7500\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.7500\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 3 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 2 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 3 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7580\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.7580\n",
      "Fold 4 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7580\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.7580\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7827\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.7827\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7827\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.7827\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7823\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.7823\n",
      "\n",
      "Grid Search Results:\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.7743\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.7903\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.7743\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.7903\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.7817\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.8313\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.8393\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.8393\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.7667\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.7743\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.7500\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.7823\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.7497\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.7827\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.7580\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.7827\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.7743\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.7903\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.7743\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.7903\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.8393\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.8393\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.8393\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.8393\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.7500\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.7823\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.7500\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.7823\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.7580\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.7827\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.7580\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.7827\n",
      "\n",
      "Best Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'}, Best Average Metric: 0.8393\n",
      "\n",
      "Final Grid Search Best Result:\n",
      "{'params': {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'}, 'average_metric': np.float64(0.8393333333333335)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, ParameterGrid  # Using scikit-learn, but just  for cross-validation and grid search SETUP (split or combination generation).\n",
    "from joblib import Parallel, delayed  # For running grid search evaluations in parallel.\n",
    "\n",
    "from lib.data_loader import get_monks_dataset  # Custom function to load one of the Monks datasets.\n",
    "\n",
    "# ============================\n",
    "# Activation functions and their derivatives\n",
    "# ============================\n",
    "# These functions define common activation functions used in neural networks,\n",
    "# along with their derivatives needed for backpropagation.\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Compute the sigmoid activation: 1 / (1 + exp(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(z, a):\n",
    "    # Compute derivative of sigmoid using the already computed activation value.\n",
    "    # Note: z (the pre-activation) is not used here; it is kept to maintain a uniform signature.\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    # Compute the Rectified Linear Unit (ReLU): max(0, x)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(z, a):\n",
    "    # Derivative of ReLU: 1 if z > 0, else 0.\n",
    "    # Using z (the pre-activation) here to decide where the gradient flows.\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def linear(x):\n",
    "    # Linear activation (identity function): useful for regression output layers.\n",
    "    return x\n",
    "\n",
    "def linear_derivative(z, a):\n",
    "    # Derivative of a linear function is constant 1.\n",
    "    return np.ones_like(a)\n",
    "\n",
    "# Dictionaries to map activation function names to their implementations.\n",
    "activation_functions = {\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"relu\": relu,\n",
    "    \"linear\": linear\n",
    "}\n",
    "\n",
    "# Similarly, mapping names to derivative functions.\n",
    "activation_derivatives = {\n",
    "    \"sigmoid\": lambda z, a: sigmoid_derivative(z, a),\n",
    "    \"relu\": lambda z, a: relu_derivative(z, a),\n",
    "    \"linear\": lambda z, a: linear_derivative(z, a)\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Loss functions and their derivatives\n",
    "# ============================\n",
    "# Here we define loss functions for different tasks (binary classification and regression)\n",
    "# along with their derivatives for backpropagation.\n",
    "\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute binary crossentropy loss for binary classification.\n",
    "    Clipping is used to avoid taking the log of zero.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_crossentropy_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the binary crossentropy loss with respect to predictions.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return - (y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error (MSE) loss, typically used for regression tasks.\n",
    "    \"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the Mean Squared Error (MSE) loss.\n",
    "    \"\"\"\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "def mee_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute Mean Euclidean Error (MEE) loss:\n",
    "    For each sample, calculates the Euclidean distance between y_true and y_pred.\n",
    "    \"\"\"\n",
    "    diff = y_true - y_pred  # Difference between true and predicted values.\n",
    "    # Compute Euclidean distance for each sample.\n",
    "    dist = np.sqrt(np.sum(diff ** 2, axis=1))\n",
    "    return np.mean(dist)\n",
    "\n",
    "def mee_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute derivative of the Mean Euclidean Error (MEE) loss.\n",
    "    For each sample, the derivative is (1/N) * ((y_pred - y_true) / ||y_pred - y_true||).\n",
    "    An epsilon is used to avoid division by zero.\n",
    "    \"\"\"\n",
    "    diff = y_pred - y_true\n",
    "    dist = np.sqrt(np.sum(diff ** 2, axis=1, keepdims=True))\n",
    "    epsilon = 1e-8  # Small constant to avoid division by zero.\n",
    "    dist_safe = np.where(dist == 0, epsilon, dist)\n",
    "    N = y_true.shape[0]\n",
    "    derivative = diff / dist_safe / N\n",
    "    return derivative\n",
    "\n",
    "# Dictionaries to map loss function names to their implementations.\n",
    "loss_functions = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_loss,\n",
    "    \"mse\": mse_loss,\n",
    "    \"mee\": mee_loss,  \n",
    "}\n",
    "\n",
    "loss_derivatives = {\n",
    "    \"binary_crossentropy\": binary_crossentropy_derivative,\n",
    "    \"mse\": mse_derivative,\n",
    "    \"mee\": mee_derivative, \n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Regularization functions (modular)\n",
    "# ============================\n",
    "# Functions to compute regularization loss and gradients (L1 and L2) to avoid overfitting.\n",
    "\n",
    "def compute_reg_gradient(W, lambda_reg, reg_type, m):\n",
    "    # Compute regularization gradient for a weight matrix W.\n",
    "    if reg_type == \"l2\":\n",
    "        return lambda_reg * W / m\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * np.sign(W) / m\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_reg_loss(W_list, lambda_reg, reg_type):\n",
    "    # Compute the total regularization loss over a list of weight matrices.\n",
    "    if reg_type == \"l2\":\n",
    "        return (lambda_reg / 2) * sum(np.sum(W ** 2) for W in W_list)\n",
    "    elif reg_type == \"l1\":\n",
    "        return lambda_reg * sum(np.sum(np.abs(W)) for W in W_list)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# ============================\n",
    "# Neural Network Class with Learning Rate Decay, Momentum, Custom Weight Initialization, and Early Stopping\n",
    "# ============================\n",
    "# This class implements a feed-forward neural network with several advanced features.\n",
    "# I designed it to be flexible and modular so I can experiment with different training strategies.\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.01, lambda_reg=0.001, reg_type=\"l2\",\n",
    "                 loss_function_name=None,\n",
    "                 activation_function_name=\"relu\",\n",
    "                 output_activation_function_name=None,\n",
    "                 activation_function_names=None,\n",
    "                 task=\"classification\",\n",
    "                 lr_decay_type=\"none\",  # Options: \"none\", \"exponential\", \"linear\"\n",
    "                 decay_rate=0.0,\n",
    "                 weight_init=\"base\",  # \"base\" (fan-in scaling) or \"glorot\"\n",
    "                 momentum_type=\"none\",  # Options: \"none\", \"momentum\", \"nesterov momentum\"\n",
    "                 momentum_alpha=0.9):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with architecture and training hyperparameters.\n",
    "        \n",
    "        Parameters:\n",
    "            layers: List with the number of neurons per layer (input, hidden, output).\n",
    "            learning_rate: Starting learning rate.\n",
    "            lambda_reg: Regularization coefficient.\n",
    "            reg_type: Regularization type (\"l2\", \"l1\", or none).\n",
    "            loss_function_name: Loss function to use (set based on task if None).\n",
    "            activation_function_name: Default activation for hidden layers.\n",
    "            output_activation_function_name: Activation for the output layer (set based on task if None).\n",
    "            activation_function_names: List of activations per layer (overrides defaults if provided).\n",
    "            task: \"classification\" or \"regression\".\n",
    "            lr_decay_type: Learning rate decay strategy.\n",
    "            decay_rate: Decay rate factor.\n",
    "            weight_init: Weight initialization strategy.\n",
    "            momentum_type: Momentum strategy.\n",
    "            momentum_alpha: Momentum coefficient.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.initial_learning_rate = learning_rate  # Save the initial learning rate for decay computations.\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.reg_type = reg_type\n",
    "        self.task = task\n",
    "        self.lr_decay_type = lr_decay_type\n",
    "        self.decay_rate = decay_rate\n",
    "        self.weight_init = weight_init\n",
    "        \n",
    "        # Set momentum parameters; validate the input.\n",
    "        if momentum_type not in {\"none\", \"momentum\", \"nesterov momentum\"}:\n",
    "            raise ValueError(\"momentum_type must be 'none', 'momentum', or 'nesterov momentum'.\")\n",
    "        self.momentum_type = momentum_type\n",
    "        self.momentum_alpha = momentum_alpha if momentum_type != \"none\" else 0.0\n",
    "        \n",
    "        # Set default loss and output activation based on the task.\n",
    "        if self.task == \"regression\":\n",
    "            self.loss_function_name = loss_function_name or \"mse\"\n",
    "            output_activation_function_name = output_activation_function_name or \"linear\"\n",
    "        else:\n",
    "            # For classification tasks.\n",
    "            self.loss_function_name = loss_function_name or \"binary_crossentropy\"\n",
    "            output_activation_function_name = output_activation_function_name or \"sigmoid\"\n",
    "        \n",
    "        # Set activation functions for each layer.\n",
    "        if activation_function_names is None:\n",
    "            # Use a default activation for all hidden layers and a specific one for the output layer.\n",
    "            self.activation_function_names = [activation_function_name] * (len(layers) - 1)\n",
    "            self.activation_function_names[-1] = output_activation_function_name\n",
    "        else:\n",
    "            if len(activation_function_names) != len(layers) - 1:\n",
    "                raise ValueError(\"activation_function_names must have length equal to len(layers)-1.\")\n",
    "            self.activation_function_names = activation_function_names\n",
    "        \n",
    "        # Initialize weights and biases for the network.\n",
    "        self._initialize_parameters()\n",
    "        # Initialize momentum accumulators (even if not used, for consistency).\n",
    "        self.vW = [np.zeros_like(W) for W in self.W]\n",
    "        self.vb = [np.zeros_like(b) for b in self.b]\n",
    "        \n",
    "        # Loss history lists (these will be reinitialized in train()).\n",
    "        self.train_loss_history = []\n",
    "        self.val_loss_history = None\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize the weights and biases for each layer using either base (fan-in) or Glorot initialization.\n",
    "        \"\"\"\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        np.random.seed(42)  # For reproducibility.\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            fan_in = self.layers[i]\n",
    "            fan_out = self.layers[i + 1]\n",
    "            if self.weight_init == \"base\":\n",
    "                std = np.sqrt(1.0 / fan_in)\n",
    "            elif self.weight_init == \"glorot\":\n",
    "                std = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported weight initialization strategy. Use 'base' or 'glorot'.\")\n",
    "            weight = np.random.randn(fan_in, fan_out) * std  # Initialize weights with Gaussian noise.\n",
    "            self.W.append(weight)\n",
    "            self.b.append(np.zeros((1, fan_out)))  # Biases are initialized to zero.\n",
    "    \n",
    "    def _apply_activation(self, x, func_name):\n",
    "        \"\"\"\n",
    "        Helper to apply an activation function based on its name.\n",
    "        \"\"\"\n",
    "        if func_name not in activation_functions:\n",
    "            raise ValueError(f\"Unsupported activation: {func_name}\")\n",
    "        return activation_functions[func_name](x)\n",
    "    \n",
    "    def _apply_activation_derivative(self, z, a, func_name):\n",
    "        \"\"\"\n",
    "        Helper to apply the derivative of an activation function based on its name.\n",
    "        \"\"\"\n",
    "        if func_name not in activation_derivatives:\n",
    "            raise ValueError(f\"Unsupported activation derivative: {func_name}\")\n",
    "        return activation_derivatives[func_name](z, a)\n",
    "    \n",
    "    def _forward(self, X, weights=None, biases=None):\n",
    "        \"\"\"\n",
    "        Forward propagation through the network.\n",
    "        If custom weights and biases are provided (e.g., for Nesterov momentum lookahead), use them.\n",
    "        Returns:\n",
    "            Z: List of pre-activation values for each layer.\n",
    "            A: List of activations for each layer (including input and output).\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.W\n",
    "        if biases is None:\n",
    "            biases = self.b\n",
    "            \n",
    "        A = [X]  # Start with the input layer.\n",
    "        Z = []\n",
    "        # Propagate through hidden layers.\n",
    "        for i in range(len(weights) - 1):\n",
    "            z_curr = np.dot(A[-1], weights[i]) + biases[i]\n",
    "            Z.append(z_curr)\n",
    "            a_curr = self._apply_activation(z_curr, self.activation_function_names[i])\n",
    "            A.append(a_curr)\n",
    "        # Process the output layer separately.\n",
    "        z_out = np.dot(A[-1], weights[-1]) + biases[-1]\n",
    "        Z.append(z_out)\n",
    "        a_out = self._apply_activation(z_out, self.activation_function_names[-1])\n",
    "        A.append(a_out)\n",
    "        return Z, A\n",
    "    \n",
    "    def _compute_gradients(self, X, y, Z, A, weights=None):\n",
    "        \"\"\"\n",
    "        Compute gradients for weights and biases via backpropagation.\n",
    "        \n",
    "        Parameters:\n",
    "            X: Input batch.\n",
    "            y: True labels/targets.\n",
    "            Z: Pre-activation values collected during forward pass.\n",
    "            A: Activations collected during forward pass.\n",
    "            weights: Optional custom weights (e.g., for lookahead in Nesterov momentum).\n",
    "        \n",
    "        Returns:\n",
    "            dW: List of gradients for weight matrices.\n",
    "            db: List of gradients for bias vectors.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.W\n",
    "        m = X.shape[0]\n",
    "        # Compute derivative of loss w.r.t. output activation.\n",
    "        dA = loss_derivatives[self.loss_function_name](y, A[-1])\n",
    "        # For the output layer.\n",
    "        dZ = dA * self._apply_activation_derivative(Z[-1], A[-1], self.activation_function_names[-1])\n",
    "        # Regularization term for the output layer.\n",
    "        reg_term = compute_reg_gradient(weights[-1], self.lambda_reg, self.reg_type, m)\n",
    "        dW = [np.dot(A[-2].T, dZ) / m + reg_term]\n",
    "        db = [np.sum(dZ, axis=0, keepdims=True) / m]\n",
    "        \n",
    "        # Backpropagate through hidden layers.\n",
    "        for i in range(len(weights) - 2, -1, -1):\n",
    "            dA = np.dot(dZ, weights[i + 1].T)\n",
    "            dZ = dA * self._apply_activation_derivative(Z[i], A[i + 1], self.activation_function_names[i])\n",
    "            reg_term = compute_reg_gradient(weights[i], self.lambda_reg, self.reg_type, m)\n",
    "            dW.insert(0, np.dot(A[i].T, dZ) / m + reg_term)\n",
    "            db.insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "            \n",
    "        return dW, db\n",
    "    \n",
    "    def train(self, X, y, epochs=300, batch_size=32, verbose=True,\n",
    "              early_stopping=False, validation_data=None, patience=10, min_delta=0.0):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch gradient descent.\n",
    "        \n",
    "        Features include:\n",
    "         - Learning rate decay (exponential or linear).\n",
    "         - Momentum (classic or Nesterov).\n",
    "         - Early stopping based on validation loss.\n",
    "        \n",
    "        The training and validation loss histories are stored in:\n",
    "            self.train_loss_history and self.val_loss_history.\n",
    "        \"\"\"\n",
    "        # Reinitialize loss histories at the start of training.\n",
    "        self.train_loss_history = []\n",
    "        if validation_data is not None:\n",
    "            self.val_loss_history = []\n",
    "        else:\n",
    "            self.val_loss_history = None\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        best_loss = np.inf\n",
    "        patience_counter = 0\n",
    "        best_weights = None\n",
    "        best_biases = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Update learning rate based on the selected decay strategy.\n",
    "            if self.lr_decay_type == \"exponential\":\n",
    "                self.learning_rate = self.initial_learning_rate * np.exp(-self.decay_rate * epoch)\n",
    "            elif self.lr_decay_type == \"linear\":\n",
    "                self.learning_rate = self.initial_learning_rate * max(0, 1 - self.decay_rate * epoch)\n",
    "            # Otherwise, if \"none\", the learning rate remains constant.\n",
    "            \n",
    "            # Shuffle training data to ensure randomness in mini-batches.\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            \n",
    "            # Process the data in mini-batches.\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                \n",
    "                if self.momentum_type == \"nesterov momentum\":\n",
    "                    # Lookahead step: adjust current parameters with momentum before computing gradients.\n",
    "                    weights_lookahead = [self.W[j] - self.momentum_alpha * self.vW[j] for j in range(len(self.W))]\n",
    "                    biases_lookahead = [self.b[j] - self.momentum_alpha * self.vb[j] for j in range(len(self.b))]\n",
    "                    Z, A = self._forward(X_batch, weights=weights_lookahead, biases=biases_lookahead)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A, weights=weights_lookahead)\n",
    "                    # Update momentum accumulators and then update parameters.\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.vW[j] = self.momentum_alpha * self.vW[j] + self.learning_rate * dW[j]\n",
    "                        self.vb[j] = self.momentum_alpha * self.vb[j] + self.learning_rate * db[j]\n",
    "                        self.W[j] -= self.vW[j]\n",
    "                        self.b[j] -= self.vb[j]\n",
    "                        \n",
    "                elif self.momentum_type == \"momentum\":\n",
    "                    # Standard momentum update.\n",
    "                    Z, A = self._forward(X_batch)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.vW[j] = self.momentum_alpha * self.vW[j] + self.learning_rate * dW[j]\n",
    "                        self.vb[j] = self.momentum_alpha * self.vb[j] + self.learning_rate * db[j]\n",
    "                        self.W[j] -= self.vW[j]\n",
    "                        self.b[j] -= self.vb[j]\n",
    "                        \n",
    "                else:  # No momentum.\n",
    "                    Z, A = self._forward(X_batch)\n",
    "                    dW, db = self._compute_gradients(X_batch, y_batch, Z, A)\n",
    "                    for j in range(len(self.W)):\n",
    "                        self.W[j] -= self.learning_rate * dW[j]\n",
    "                        self.b[j] -= self.learning_rate * db[j]\n",
    "            \n",
    "            # Compute training loss over the full training set.\n",
    "            _, A_full = self._forward(X)\n",
    "            train_loss = loss_functions[self.loss_function_name](y, A_full[-1])\n",
    "            reg_loss = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "            total_train_loss = train_loss + reg_loss\n",
    "            self.train_loss_history.append(total_train_loss)\n",
    "            \n",
    "            # If validation data is provided, compute the validation loss.\n",
    "            if validation_data is not None:\n",
    "                X_val, y_val = validation_data\n",
    "                _, A_val = self._forward(X_val)\n",
    "                val_loss = loss_functions[self.loss_function_name](y_val, A_val[-1])\n",
    "                reg_loss_val = compute_reg_loss(self.W, self.lambda_reg, self.reg_type)\n",
    "                total_val_loss = val_loss + reg_loss_val\n",
    "                self.val_loss_history.append(total_val_loss)\n",
    "            else:\n",
    "                total_val_loss = None\n",
    "\n",
    "            # Print progress information if verbosity is enabled.\n",
    "            if verbose:\n",
    "                if total_val_loss is not None:\n",
    "                    print(f\"Epoch {epoch:4d}, Training Loss: {total_train_loss:.4f}, \"\n",
    "                          f\"Validation Loss: {total_val_loss:.4f}, Learning Rate: {self.learning_rate:.6f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch:4d}, Training Loss: {total_train_loss:.4f}, \"\n",
    "                          f\"Learning Rate: {self.learning_rate:.6f}\")\n",
    "            \n",
    "            # Check for early stopping if enabled and using validation data.\n",
    "            if early_stopping and (validation_data is not None):\n",
    "                if total_val_loss < best_loss - min_delta:\n",
    "                    best_loss = total_val_loss\n",
    "                    patience_counter = 0\n",
    "                    # Save the best parameters so far.\n",
    "                    best_weights = [w.copy() for w in self.W]\n",
    "                    best_biases = [b.copy() for b in self.b]\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        if verbose:\n",
    "                            print(f\"Early stopping triggered at epoch {epoch}. Restoring best model parameters.\")\n",
    "                        if best_weights is not None:\n",
    "                            self.W = best_weights\n",
    "                            self.b = best_biases\n",
    "                        break\n",
    "\n",
    "    def plot_loss_history(self):\n",
    "        \"\"\"\n",
    "        Plot the loss history for both training and (if available) validation.\n",
    "        Useful for visualizing convergence.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.train_loss_history, label=\"Training Loss\")\n",
    "        if self.val_loss_history is not None and len(self.val_loss_history) > 0:\n",
    "            plt.plot(self.val_loss_history, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss History\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Generate predictions for given input X.\n",
    "        For classification tasks, threshold or argmax is used as needed.\n",
    "        \"\"\"\n",
    "        _, A = self._forward(X)\n",
    "        output = A[-1]\n",
    "        if self.task == \"classification\":\n",
    "            if output.shape[1] == 1:\n",
    "                return (output > 0.5).astype(int)  # Binary classification threshold.\n",
    "            else:\n",
    "                return np.argmax(output, axis=1)  # Multi-class classification.\n",
    "        else:\n",
    "            return output\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Evaluate the model performance.\n",
    "        For regression, returns MSE.\n",
    "        For classification, returns accuracy.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        if self.task == \"regression\":\n",
    "            return mse_loss(y, predictions)\n",
    "        else:\n",
    "            if y.ndim > 1 and y.shape[1] > 1:\n",
    "                y_true = np.argmax(y, axis=1)\n",
    "            else:\n",
    "                y_true = y\n",
    "            return np.mean(predictions == y_true)\n",
    "\n",
    "# ============================\n",
    "# K-fold Cross-Validation Function\n",
    "# ============================\n",
    "# This function implements k-fold cross-validation.\n",
    "# It splits the data, trains a new model for each fold, and aggregates the evaluation metrics.\n",
    "\n",
    "def k_fold_cross_validation(model_builder, X, y, k=5, epochs=1000, batch_size=32,\n",
    "                            verbose=True, early_stopping=False, patience=10, min_delta=1e-4):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross validation for a neural network.\n",
    "    \n",
    "    Parameters:\n",
    "        model_builder: A callable that returns a new instance of NeuralNetwork.\n",
    "        X: Input features.\n",
    "        y: Targets.\n",
    "        k: Number of folds.\n",
    "        epochs: Number of training epochs per fold.\n",
    "        batch_size: Batch size.\n",
    "        verbose: Verbosity flag.\n",
    "        early_stopping: Whether to use early stopping.\n",
    "        patience: Number of epochs with no improvement before stopping.\n",
    "        min_delta: Minimum change in loss to be considered an improvement.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary with per-fold metrics and the overall average metric.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold_metrics = []\n",
    "    fold = 1\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        print(f\"\\nFold {fold}/{k}\")\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "        \n",
    "        # Build a new model instance for this fold.\n",
    "        nn_model = model_builder()\n",
    "        \n",
    "        # Train the model using the current fold's training and validation data.\n",
    "        nn_model.train(\n",
    "            X_train_fold, y_train_fold,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=verbose,\n",
    "            early_stopping=early_stopping,\n",
    "            validation_data=(X_val_fold, y_val_fold),\n",
    "            patience=patience,\n",
    "            min_delta=min_delta\n",
    "        )\n",
    "        \n",
    "        # Evaluate performance on the validation fold.\n",
    "        metric = nn_model.evaluate(X_val_fold, y_val_fold)\n",
    "        print(f\"Fold {fold} Evaluation Metric: {metric:.4f}\")\n",
    "        fold_metrics.append(metric)\n",
    "        fold += 1\n",
    "    \n",
    "    avg_metric = np.mean(fold_metrics)\n",
    "    print(f\"\\nAverage Evaluation Metric over {k} folds: {avg_metric:.4f}\")\n",
    "    return {\"fold_metrics\": fold_metrics, \"average_metric\": avg_metric}\n",
    "\n",
    "# ============================\n",
    "# Grid Search Function with Parallelism\n",
    "# ============================\n",
    "# This function performs a grid search over a set of hyperparameters.\n",
    "# It uses k-fold cross-validation to evaluate each hyperparameter combination\n",
    "# and runs evaluations in parallel for speed.\n",
    "\n",
    "def grid_search(model_builder, param_grid, X, y, k=5, epochs=1000, batch_size=32,\n",
    "                early_stopping=False, patience=10, min_delta=1e-4, n_jobs=-1,\n",
    "                maximize=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Perform grid search over the given parameter grid using k-fold cross validation.\n",
    "    \n",
    "    Parameters:\n",
    "        model_builder: A callable that accepts hyperparameters as keyword arguments and returns a NeuralNetwork.\n",
    "        param_grid: Dictionary of hyperparameters to try.\n",
    "        X: Input features.\n",
    "        y: Targets.\n",
    "        k: Number of folds for cross validation.\n",
    "        epochs: Number of epochs per fold.\n",
    "        batch_size: Batch size.\n",
    "        early_stopping: Whether to use early stopping.\n",
    "        patience: Patience for early stopping.\n",
    "        min_delta: Minimum change in loss for early stopping.\n",
    "        n_jobs: Number of parallel jobs (-1 uses all available processors).\n",
    "        maximize: If True, a higher evaluation metric is better.\n",
    "        verbose: Verbosity flag.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (best_result, all_results) where best_result contains the best hyperparameters and metric.\n",
    "    \"\"\"\n",
    "    grid = list(ParameterGrid(param_grid))\n",
    "    results = []\n",
    "\n",
    "    def evaluate_params(params):\n",
    "        # Create a new model instance with the current hyperparameters.\n",
    "        def builder():\n",
    "            return model_builder(**params)\n",
    "        cv_result = k_fold_cross_validation(builder, X, y, k=k, epochs=epochs,\n",
    "                                              batch_size=batch_size,\n",
    "                                              early_stopping=early_stopping,\n",
    "                                              patience=patience,\n",
    "                                              min_delta=min_delta,\n",
    "                                              verbose=False)\n",
    "        metric = cv_result[\"average_metric\"]\n",
    "        if verbose:\n",
    "            print(f\"Params: {params} => Average Metric: {metric:.4f}\")\n",
    "        return (params, metric)\n",
    "\n",
    "    # Run evaluations in parallel.\n",
    "    evaluated_results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(evaluate_params)(params) for params in grid\n",
    "    )\n",
    "\n",
    "    for params, metric in evaluated_results:\n",
    "        results.append({\"params\": params, \"average_metric\": metric})\n",
    "\n",
    "    # Choose the best hyperparameters based on whether a higher metric is better.\n",
    "    if maximize:\n",
    "        best_result = max(results, key=lambda x: x[\"average_metric\"])\n",
    "    else:\n",
    "        best_result = min(results, key=lambda x: x[\"average_metric\"])\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nGrid Search Results:\")\n",
    "        for res in results:\n",
    "            print(f\"Params: {res['params']}, Average Metric: {res['average_metric']:.4f}\")\n",
    "        print(f\"\\nBest Params: {best_result['params']}, Best Average Metric: {best_result['average_metric']:.4f}\")\n",
    "\n",
    "    return best_result, results\n",
    "\n",
    "# ============================\n",
    "# Testing on a Monk's Dataset\n",
    "# ============================\n",
    "# Load the dataset, configure the network architecture, and perform training,\n",
    "# evaluation, cross-validation, and grid search.\n",
    "\n",
    "# Load Monk's dataset (with one-hot encoding for targets).\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(1, one_hot_encode=True)\n",
    "\n",
    "# Define the network architecture.\n",
    "input_size = X_train.shape[1]\n",
    "hidden_units = 10\n",
    "output_size = 1  # Binary classification problem.\n",
    "layers = [input_size, hidden_units, output_size]\n",
    "\n",
    "# Specify activation functions for the hidden and output layers.\n",
    "activation_funcs = [\"relu\", \"sigmoid\"]\n",
    "\n",
    "# Build a neural network classifier instance with the desired configuration.\n",
    "nn_clf = NeuralNetwork(\n",
    "    layers=layers,\n",
    "    learning_rate=0.2,\n",
    "    lambda_reg=0.001,\n",
    "    reg_type=\"l2\",\n",
    "    loss_function_name=\"mse\",       # Even for classification, trying MSE here.\n",
    "    activation_function_names=activation_funcs,\n",
    "    task=\"classification\",\n",
    "    lr_decay_type=\"linear\",    # Options: \"exponential\", \"linear\", or \"none\"\n",
    "    decay_rate=0.001,\n",
    "    weight_init=\"base\",        # Options: \"base\" or \"glorot\"\n",
    ")\n",
    "\n",
    "# Train the network.\n",
    "# Early stopping is used here with the test set as validation data.\n",
    "nn_clf.train(\n",
    "    X_train, y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    verbose=True,\n",
    "    early_stopping=True,\n",
    "    validation_data=(X_test, y_test),\n",
    "    patience=10,\n",
    "    min_delta=1e-4\n",
    ")\n",
    "\n",
    "# Evaluate the network's classification accuracy.\n",
    "accuracy = nn_clf.evaluate(X_test, y_test)\n",
    "print(f\"\\nNeural Network Classification Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot the training and validation loss history.\n",
    "nn_clf.plot_loss_history()\n",
    "\n",
    "# ============================\n",
    "# K-fold Cross-Validation Example\n",
    "# ============================\n",
    "def build_nn_model():\n",
    "    \"\"\"\n",
    "    Build and return a new instance of NeuralNetwork with the current configuration.\n",
    "    This function is used to generate a fresh model for each cross-validation fold.\n",
    "    \"\"\"\n",
    "    return NeuralNetwork(\n",
    "        layers=layers,\n",
    "        learning_rate=0.2,\n",
    "        lambda_reg=0.001,\n",
    "        reg_type=\"l2\",\n",
    "        loss_function_name=\"mse\",       \n",
    "        activation_function_names=activation_funcs,\n",
    "        task=\"classification\",\n",
    "        lr_decay_type=\"linear\",    # Options: \"exponential\", \"linear\", or \"none\"\n",
    "        decay_rate=0.001,\n",
    "        weight_init=\"base\",        # Options: \"base\" or \"glorot\"\n",
    "    )\n",
    "\n",
    "# Perform 5-fold cross validation on the training set.\n",
    "cv_results = k_fold_cross_validation(\n",
    "    model_builder=build_nn_model,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    k=5,\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    verbose=False,  # Set to True if you want detailed logging for each fold.\n",
    "    early_stopping=True,\n",
    "    patience=10,\n",
    "    min_delta=1e-4\n",
    ")\n",
    "\n",
    "print(\"\\nK-fold Cross-Validation Results:\")\n",
    "print(cv_results)\n",
    "\n",
    "# ============================\n",
    "# Grid Search Example\n",
    "# ============================\n",
    "# Define a model builder that accepts hyperparameters as keyword arguments.\n",
    "def build_nn_model_with_params(learning_rate=0.2, lambda_reg=0.001, reg_type=\"l2\",\n",
    "                               lr_decay_type=\"linear\", decay_rate=0.001, weight_init=\"base\"):\n",
    "    return NeuralNetwork(\n",
    "        layers=layers,\n",
    "        learning_rate=learning_rate,\n",
    "        lambda_reg=lambda_reg,\n",
    "        reg_type=reg_type,\n",
    "        loss_function_name=\"mse\",       \n",
    "        activation_function_names=activation_funcs,\n",
    "        task=\"classification\",\n",
    "        lr_decay_type=lr_decay_type,\n",
    "        decay_rate=decay_rate,\n",
    "        weight_init=weight_init\n",
    "    )\n",
    "\n",
    "# Define the hyperparameter grid to search over.\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.1, 0.2],\n",
    "    \"lambda_reg\": [0.001, 0.01],\n",
    "    \"lr_decay_type\": [\"linear\", \"none\"],\n",
    "    \"decay_rate\": [0.001, 0.0],\n",
    "    \"weight_init\": [\"base\", \"glorot\"],\n",
    "}\n",
    "\n",
    "# Perform grid search with 5-fold cross validation.\n",
    "# Note: You can reduce epochs for grid search to speed up computation.\n",
    "best_params, all_results = grid_search(\n",
    "    model_builder=build_nn_model_with_params,\n",
    "    param_grid=param_grid,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    k=5,\n",
    "    epochs=500,       # Reduced epochs for quicker grid search.\n",
    "    batch_size=32,\n",
    "    early_stopping=True,\n",
    "    patience=10,\n",
    "    min_delta=1e-4,\n",
    "    n_jobs=-1,        # Utilize all available processors.\n",
    "    maximize=True,    # For classification, higher accuracy is better.\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Grid Search Best Result:\")\n",
    "print(best_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
