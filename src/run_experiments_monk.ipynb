{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ../datasets/monks/monk-1-train.csv\n",
      "Using cached ../datasets/monks/monk-1-test.csv\n",
      "One-hot encoding MONK-1 dataset...\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 118. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 118. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 118. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 114. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 118. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 55. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 55. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 100. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8400Early stopping triggered at epoch 18. Restoring best model parameters.\n",
      "\n",
      "\n",
      "Fold 2/5Fold 3 Evaluation Metric: 0.6400\n",
      "\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 74. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 100. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 18. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 4/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 98. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 100. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 81. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 81. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 98. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 98. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 64. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "Early stopping triggered at epoch 118. Restoring best model parameters.\n",
      "\n",
      "Fold 3/5Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 66. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 118. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 58. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8223\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.8223\n",
      "Early stopping triggered at epoch 56. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8467\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.8467\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 66. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9600\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 81. Restoring best model parameters.\n",
      "Early stopping triggered at epoch 116. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "Fold 3 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 4/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 96. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "Early stopping triggered at epoch 96. Restoring best model parameters.\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7983\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.7983\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8063\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.8063\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 98. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 64. Restoring best model parameters.\n",
      "Early stopping triggered at epoch 55. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 4/5\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 18. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 50. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 55. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 56. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8547\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.8547\n",
      "Early stopping triggered at epoch 50. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 66. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 18. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 58. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8223\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.8223\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 66. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9600\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 50. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 46. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 39. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 118. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 50. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 56. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8547\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.8547\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 56. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8467\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.8467\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 40. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 46. Restoring best model parameters.\n",
      "Early stopping triggered at epoch 39. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 40. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 98. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 98. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "Early stopping triggered at epoch 96. Restoring best model parameters.\n",
      "\n",
      "Fold 5/5\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 40. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 40. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 66. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9600\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 118. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 58. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7980\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.7980\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 58. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8223\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.8223\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 55. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9600\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 55. Restoring best model parameters.\n",
      "Early stopping triggered at epoch 78. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9200\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7900\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.7900\n",
      "Fold 5/5\n",
      "\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 118. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 45. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8467\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.8467\n",
      "Early stopping triggered at epoch 118. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 74. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 56. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8387\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.8387\n",
      "Early stopping triggered at epoch 55. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9600\n",
      "\n",
      "Fold 5/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 18. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 96. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 45. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8307\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.8307\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 3/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 45. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8387\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.8387\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 100. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 118. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 100. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 74. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 58. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8303\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.8303\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 18. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 81. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 100. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.9200\n",
      "Early stopping triggered at epoch 100. Restoring best model parameters.\n",
      "\n",
      "Fold 3/5\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 116. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 98. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 64. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 118. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 55. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 81. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 18. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 66. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9600\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 98. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 116. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8400\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 98. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 81. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 64. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 66. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 56. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8547\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.8547\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 81. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7600\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 98. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 58. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8223\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.8223\n",
      "Early stopping triggered at epoch 66. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9600\n",
      "\n",
      "Fold 5/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 96. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7983\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.7983\n",
      "Early stopping triggered at epoch 55. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 96. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7983\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.7983\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 18. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.6400\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 56. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8467\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.8467\n",
      "\n",
      "Fold 1/5\n",
      "Early stopping triggered at epoch 98. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 56. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8547\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.8547\n",
      "Early stopping triggered at epoch 66. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 50. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 50. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 50. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 58. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7980\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.7980\n",
      "Early stopping triggered at epoch 39. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 46. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 50. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 96. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 98. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 56. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8467\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.8467\n",
      "Early stopping triggered at epoch 40. Restoring best model parameters.\n",
      "Early stopping triggered at epoch 58. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8223\n",
      "Early stopping triggered at epoch 46. Restoring best model parameters.\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.8223\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 118. Restoring best model parameters.\n",
      "Fold 1 Evaluation Metric: 0.8000\n",
      "\n",
      "Fold 2/5\n",
      "Early stopping triggered at epoch 40. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 39. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.8800\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 58. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8303\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.8303\n",
      "Early stopping triggered at epoch 58. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.7500\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.7980\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.7980\n",
      "Early stopping triggered at epoch 40. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 40. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 55. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9600\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 2 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 3/5\n",
      "Early stopping triggered at epoch 55. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 45. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8387\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'} => Average Metric: 0.8387\n",
      "Early stopping triggered at epoch 55. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 55. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9600\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 45. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8307\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'} => Average Metric: 0.8307\n",
      "Early stopping triggered at epoch 53. Restoring best model parameters.\n",
      "Fold 3 Evaluation Metric: 0.7200\n",
      "\n",
      "Fold 4/5\n",
      "Early stopping triggered at epoch 45. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8307\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'} => Average Metric: 0.8307\n",
      "Early stopping triggered at epoch 45. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.8333\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8387\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.8387\n",
      "Early stopping triggered at epoch 96. Restoring best model parameters.\n",
      "Fold 4 Evaluation Metric: 0.9200\n",
      "\n",
      "Fold 5/5\n",
      "Early stopping triggered at epoch 58. Restoring best model parameters.\n",
      "Fold 5 Evaluation Metric: 0.7917\n",
      "\n",
      "Average Evaluation Metric over 5 folds: 0.8303\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'} => Average Metric: 0.8303\n",
      "\n",
      "Grid Search Results:\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.8063\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.8223\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.7983\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.8223\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.8467\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.8547\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.8467\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.8547\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.7900\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.8223\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.7980\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.8303\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.8387\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.8467\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.8307\n",
      "Params: {'decay_rate': 0.001, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.8387\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.7983\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.8223\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.7983\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.8223\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.8467\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.8547\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.8467\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.8547\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.7980\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.8303\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.7980\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.1, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.8303\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'base'}, Average Metric: 0.8307\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Average Metric: 0.8387\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'base'}, Average Metric: 0.8307\n",
      "Params: {'decay_rate': 0.0, 'lambda_reg': 0.01, 'learning_rate': 0.2, 'lr_decay_type': 'none', 'weight_init': 'glorot'}, Average Metric: 0.8387\n",
      "\n",
      "Best Params: {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, Best Average Metric: 0.8547\n",
      "\n",
      "Final Grid Search Best Result:\n",
      "{'params': {'decay_rate': 0.001, 'lambda_reg': 0.001, 'learning_rate': 0.2, 'lr_decay_type': 'linear', 'weight_init': 'glorot'}, 'average_metric': np.float64(0.8546666666666667)}\n",
      "Using cached ../datasets/monks/monk-1-train.csv\n",
      "Using cached ../datasets/monks/monk-1-test.csv\n",
      "One-hot encoding MONK-1 dataset...\n",
      "Epoch    0, Training Loss: 0.2601, Validation Loss: 0.2607, Training Acc: 0.5152, Validation Acc: 0.4800, Learning Rate: 0.200000\n",
      "Epoch    1, Training Loss: 0.2568, Validation Loss: 0.2598, Training Acc: 0.5253, Validation Acc: 0.4800, Learning Rate: 0.199800\n",
      "Epoch    2, Training Loss: 0.2533, Validation Loss: 0.2594, Training Acc: 0.5657, Validation Acc: 0.4800, Learning Rate: 0.199600\n",
      "Epoch    3, Training Loss: 0.2501, Validation Loss: 0.2593, Training Acc: 0.5455, Validation Acc: 0.4800, Learning Rate: 0.199400\n",
      "Epoch    4, Training Loss: 0.2474, Validation Loss: 0.2594, Training Acc: 0.5455, Validation Acc: 0.4800, Learning Rate: 0.199200\n",
      "Epoch    5, Training Loss: 0.2448, Validation Loss: 0.2594, Training Acc: 0.5556, Validation Acc: 0.4800, Learning Rate: 0.199000\n",
      "Epoch    6, Training Loss: 0.2427, Validation Loss: 0.2594, Training Acc: 0.5758, Validation Acc: 0.4800, Learning Rate: 0.198800\n",
      "Epoch    7, Training Loss: 0.2406, Validation Loss: 0.2590, Training Acc: 0.5556, Validation Acc: 0.4800, Learning Rate: 0.198600\n",
      "Epoch    8, Training Loss: 0.2385, Validation Loss: 0.2590, Training Acc: 0.5758, Validation Acc: 0.4800, Learning Rate: 0.198400\n",
      "Epoch    9, Training Loss: 0.2364, Validation Loss: 0.2585, Training Acc: 0.5859, Validation Acc: 0.4800, Learning Rate: 0.198200\n",
      "Epoch   10, Training Loss: 0.2344, Validation Loss: 0.2579, Training Acc: 0.6162, Validation Acc: 0.5600, Learning Rate: 0.198000\n",
      "Epoch   11, Training Loss: 0.2324, Validation Loss: 0.2580, Training Acc: 0.6263, Validation Acc: 0.5600, Learning Rate: 0.197800\n",
      "Epoch   12, Training Loss: 0.2306, Validation Loss: 0.2566, Training Acc: 0.6263, Validation Acc: 0.5600, Learning Rate: 0.197600\n",
      "Epoch   13, Training Loss: 0.2286, Validation Loss: 0.2553, Training Acc: 0.6465, Validation Acc: 0.5600, Learning Rate: 0.197400\n",
      "Epoch   14, Training Loss: 0.2267, Validation Loss: 0.2547, Training Acc: 0.6465, Validation Acc: 0.5600, Learning Rate: 0.197200\n",
      "Epoch   15, Training Loss: 0.2247, Validation Loss: 0.2540, Training Acc: 0.6869, Validation Acc: 0.6000, Learning Rate: 0.197000\n",
      "Epoch   16, Training Loss: 0.2229, Validation Loss: 0.2524, Training Acc: 0.6869, Validation Acc: 0.6000, Learning Rate: 0.196800\n",
      "Epoch   17, Training Loss: 0.2212, Validation Loss: 0.2508, Training Acc: 0.6970, Validation Acc: 0.6000, Learning Rate: 0.196600\n",
      "Epoch   18, Training Loss: 0.2195, Validation Loss: 0.2498, Training Acc: 0.6970, Validation Acc: 0.6000, Learning Rate: 0.196400\n",
      "Epoch   19, Training Loss: 0.2177, Validation Loss: 0.2485, Training Acc: 0.7172, Validation Acc: 0.6000, Learning Rate: 0.196200\n",
      "Epoch   20, Training Loss: 0.2157, Validation Loss: 0.2472, Training Acc: 0.7273, Validation Acc: 0.6000, Learning Rate: 0.196000\n",
      "Epoch   21, Training Loss: 0.2138, Validation Loss: 0.2463, Training Acc: 0.7273, Validation Acc: 0.6000, Learning Rate: 0.195800\n",
      "Epoch   22, Training Loss: 0.2119, Validation Loss: 0.2456, Training Acc: 0.7475, Validation Acc: 0.6000, Learning Rate: 0.195600\n",
      "Epoch   23, Training Loss: 0.2099, Validation Loss: 0.2448, Training Acc: 0.7475, Validation Acc: 0.5600, Learning Rate: 0.195400\n",
      "Epoch   24, Training Loss: 0.2080, Validation Loss: 0.2437, Training Acc: 0.7576, Validation Acc: 0.6000, Learning Rate: 0.195200\n",
      "Epoch   25, Training Loss: 0.2063, Validation Loss: 0.2425, Training Acc: 0.7576, Validation Acc: 0.6000, Learning Rate: 0.195000\n",
      "Epoch   26, Training Loss: 0.2044, Validation Loss: 0.2411, Training Acc: 0.7576, Validation Acc: 0.6400, Learning Rate: 0.194800\n",
      "Epoch   27, Training Loss: 0.2024, Validation Loss: 0.2394, Training Acc: 0.7778, Validation Acc: 0.6400, Learning Rate: 0.194600\n",
      "Epoch   28, Training Loss: 0.2006, Validation Loss: 0.2379, Training Acc: 0.7778, Validation Acc: 0.6400, Learning Rate: 0.194400\n",
      "Epoch   29, Training Loss: 0.1987, Validation Loss: 0.2358, Training Acc: 0.7778, Validation Acc: 0.6400, Learning Rate: 0.194200\n",
      "Epoch   30, Training Loss: 0.1968, Validation Loss: 0.2350, Training Acc: 0.7879, Validation Acc: 0.6400, Learning Rate: 0.194000\n",
      "Epoch   31, Training Loss: 0.1949, Validation Loss: 0.2330, Training Acc: 0.7879, Validation Acc: 0.6400, Learning Rate: 0.193800\n",
      "Epoch   32, Training Loss: 0.1931, Validation Loss: 0.2311, Training Acc: 0.7879, Validation Acc: 0.6400, Learning Rate: 0.193600\n",
      "Epoch   33, Training Loss: 0.1915, Validation Loss: 0.2293, Training Acc: 0.7879, Validation Acc: 0.6400, Learning Rate: 0.193400\n",
      "Epoch   34, Training Loss: 0.1896, Validation Loss: 0.2281, Training Acc: 0.7980, Validation Acc: 0.6400, Learning Rate: 0.193200\n",
      "Epoch   35, Training Loss: 0.1877, Validation Loss: 0.2259, Training Acc: 0.7980, Validation Acc: 0.6400, Learning Rate: 0.193000\n",
      "Epoch   36, Training Loss: 0.1859, Validation Loss: 0.2235, Training Acc: 0.8081, Validation Acc: 0.6400, Learning Rate: 0.192800\n",
      "Epoch   37, Training Loss: 0.1840, Validation Loss: 0.2198, Training Acc: 0.7879, Validation Acc: 0.6400, Learning Rate: 0.192600\n",
      "Epoch   38, Training Loss: 0.1821, Validation Loss: 0.2168, Training Acc: 0.7980, Validation Acc: 0.6400, Learning Rate: 0.192400\n",
      "Epoch   39, Training Loss: 0.1801, Validation Loss: 0.2143, Training Acc: 0.8182, Validation Acc: 0.6400, Learning Rate: 0.192200\n",
      "Epoch   40, Training Loss: 0.1783, Validation Loss: 0.2113, Training Acc: 0.8081, Validation Acc: 0.6400, Learning Rate: 0.192000\n",
      "Epoch   41, Training Loss: 0.1766, Validation Loss: 0.2086, Training Acc: 0.8182, Validation Acc: 0.6400, Learning Rate: 0.191800\n",
      "Epoch   42, Training Loss: 0.1751, Validation Loss: 0.2061, Training Acc: 0.8182, Validation Acc: 0.6400, Learning Rate: 0.191600\n",
      "Epoch   43, Training Loss: 0.1736, Validation Loss: 0.2044, Training Acc: 0.8081, Validation Acc: 0.6400, Learning Rate: 0.191400\n",
      "Epoch   44, Training Loss: 0.1720, Validation Loss: 0.2014, Training Acc: 0.8182, Validation Acc: 0.6400, Learning Rate: 0.191200\n",
      "Epoch   45, Training Loss: 0.1703, Validation Loss: 0.1984, Training Acc: 0.8182, Validation Acc: 0.6800, Learning Rate: 0.191000\n",
      "Epoch   46, Training Loss: 0.1686, Validation Loss: 0.1953, Training Acc: 0.8283, Validation Acc: 0.6800, Learning Rate: 0.190800\n",
      "Epoch   47, Training Loss: 0.1670, Validation Loss: 0.1923, Training Acc: 0.8384, Validation Acc: 0.7200, Learning Rate: 0.190600\n",
      "Epoch   48, Training Loss: 0.1655, Validation Loss: 0.1907, Training Acc: 0.8384, Validation Acc: 0.7200, Learning Rate: 0.190400\n",
      "Epoch   49, Training Loss: 0.1640, Validation Loss: 0.1876, Training Acc: 0.8384, Validation Acc: 0.7200, Learning Rate: 0.190200\n",
      "Epoch   50, Training Loss: 0.1624, Validation Loss: 0.1854, Training Acc: 0.8384, Validation Acc: 0.7200, Learning Rate: 0.190000\n",
      "Epoch   51, Training Loss: 0.1607, Validation Loss: 0.1833, Training Acc: 0.8384, Validation Acc: 0.7200, Learning Rate: 0.189800\n",
      "Epoch   52, Training Loss: 0.1592, Validation Loss: 0.1807, Training Acc: 0.8384, Validation Acc: 0.7200, Learning Rate: 0.189600\n",
      "Epoch   53, Training Loss: 0.1574, Validation Loss: 0.1783, Training Acc: 0.8384, Validation Acc: 0.7600, Learning Rate: 0.189400\n",
      "Epoch   54, Training Loss: 0.1558, Validation Loss: 0.1772, Training Acc: 0.8384, Validation Acc: 0.7200, Learning Rate: 0.189200\n",
      "Epoch   55, Training Loss: 0.1541, Validation Loss: 0.1749, Training Acc: 0.8485, Validation Acc: 0.8000, Learning Rate: 0.189000\n",
      "Epoch   56, Training Loss: 0.1523, Validation Loss: 0.1730, Training Acc: 0.8586, Validation Acc: 0.8400, Learning Rate: 0.188800\n",
      "Epoch   57, Training Loss: 0.1507, Validation Loss: 0.1719, Training Acc: 0.8485, Validation Acc: 0.8000, Learning Rate: 0.188600\n",
      "Epoch   58, Training Loss: 0.1493, Validation Loss: 0.1712, Training Acc: 0.8485, Validation Acc: 0.8000, Learning Rate: 0.188400\n",
      "Epoch   59, Training Loss: 0.1477, Validation Loss: 0.1692, Training Acc: 0.8485, Validation Acc: 0.8000, Learning Rate: 0.188200\n",
      "Epoch   60, Training Loss: 0.1464, Validation Loss: 0.1676, Training Acc: 0.8586, Validation Acc: 0.8400, Learning Rate: 0.188000\n",
      "Epoch   61, Training Loss: 0.1452, Validation Loss: 0.1668, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.187800\n",
      "Epoch   62, Training Loss: 0.1442, Validation Loss: 0.1663, Training Acc: 0.8485, Validation Acc: 0.8000, Learning Rate: 0.187600\n",
      "Epoch   63, Training Loss: 0.1430, Validation Loss: 0.1652, Training Acc: 0.8485, Validation Acc: 0.8000, Learning Rate: 0.187400\n",
      "Epoch   64, Training Loss: 0.1417, Validation Loss: 0.1638, Training Acc: 0.8485, Validation Acc: 0.8000, Learning Rate: 0.187200\n",
      "Epoch   65, Training Loss: 0.1405, Validation Loss: 0.1626, Training Acc: 0.8485, Validation Acc: 0.8000, Learning Rate: 0.187000\n",
      "Epoch   66, Training Loss: 0.1394, Validation Loss: 0.1617, Training Acc: 0.8586, Validation Acc: 0.7600, Learning Rate: 0.186800\n",
      "Epoch   67, Training Loss: 0.1383, Validation Loss: 0.1609, Training Acc: 0.8586, Validation Acc: 0.7600, Learning Rate: 0.186600\n",
      "Epoch   68, Training Loss: 0.1374, Validation Loss: 0.1601, Training Acc: 0.8586, Validation Acc: 0.7600, Learning Rate: 0.186400\n",
      "Epoch   69, Training Loss: 0.1367, Validation Loss: 0.1595, Training Acc: 0.8485, Validation Acc: 0.8000, Learning Rate: 0.186200\n",
      "Epoch   70, Training Loss: 0.1357, Validation Loss: 0.1586, Training Acc: 0.8586, Validation Acc: 0.7600, Learning Rate: 0.186000\n",
      "Epoch   71, Training Loss: 0.1349, Validation Loss: 0.1579, Training Acc: 0.8586, Validation Acc: 0.7600, Learning Rate: 0.185800\n",
      "Epoch   72, Training Loss: 0.1343, Validation Loss: 0.1576, Training Acc: 0.8586, Validation Acc: 0.7600, Learning Rate: 0.185600\n",
      "Epoch   73, Training Loss: 0.1335, Validation Loss: 0.1570, Training Acc: 0.8586, Validation Acc: 0.7600, Learning Rate: 0.185400\n",
      "Epoch   74, Training Loss: 0.1328, Validation Loss: 0.1564, Training Acc: 0.8586, Validation Acc: 0.7600, Learning Rate: 0.185200\n",
      "Epoch   75, Training Loss: 0.1321, Validation Loss: 0.1562, Training Acc: 0.8586, Validation Acc: 0.7600, Learning Rate: 0.185000\n",
      "Epoch   76, Training Loss: 0.1314, Validation Loss: 0.1560, Training Acc: 0.8586, Validation Acc: 0.7600, Learning Rate: 0.184800\n",
      "Epoch   77, Training Loss: 0.1307, Validation Loss: 0.1558, Training Acc: 0.8586, Validation Acc: 0.7600, Learning Rate: 0.184600\n",
      "Epoch   78, Training Loss: 0.1301, Validation Loss: 0.1557, Training Acc: 0.8586, Validation Acc: 0.7600, Learning Rate: 0.184400\n",
      "Epoch   79, Training Loss: 0.1295, Validation Loss: 0.1556, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.184200\n",
      "Epoch   80, Training Loss: 0.1288, Validation Loss: 0.1549, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.184000\n",
      "Epoch   81, Training Loss: 0.1282, Validation Loss: 0.1544, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.183800\n",
      "Epoch   82, Training Loss: 0.1276, Validation Loss: 0.1540, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.183600\n",
      "Epoch   83, Training Loss: 0.1269, Validation Loss: 0.1538, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.183400\n",
      "Epoch   84, Training Loss: 0.1263, Validation Loss: 0.1535, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.183200\n",
      "Epoch   85, Training Loss: 0.1257, Validation Loss: 0.1528, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.183000\n",
      "Epoch   86, Training Loss: 0.1253, Validation Loss: 0.1527, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.182800\n",
      "Epoch   87, Training Loss: 0.1246, Validation Loss: 0.1518, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.182600\n",
      "Epoch   88, Training Loss: 0.1241, Validation Loss: 0.1509, Training Acc: 0.8485, Validation Acc: 0.8000, Learning Rate: 0.182400\n",
      "Epoch   89, Training Loss: 0.1235, Validation Loss: 0.1499, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.182200\n",
      "Epoch   90, Training Loss: 0.1229, Validation Loss: 0.1494, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.182000\n",
      "Epoch   91, Training Loss: 0.1223, Validation Loss: 0.1487, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.181800\n",
      "Epoch   92, Training Loss: 0.1217, Validation Loss: 0.1483, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.181600\n",
      "Epoch   93, Training Loss: 0.1213, Validation Loss: 0.1481, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.181400\n",
      "Epoch   94, Training Loss: 0.1206, Validation Loss: 0.1476, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.181200\n",
      "Epoch   95, Training Loss: 0.1200, Validation Loss: 0.1472, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.181000\n",
      "Epoch   96, Training Loss: 0.1194, Validation Loss: 0.1468, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.180800\n",
      "Epoch   97, Training Loss: 0.1189, Validation Loss: 0.1469, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.180600\n",
      "Epoch   98, Training Loss: 0.1183, Validation Loss: 0.1465, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.180400\n",
      "Epoch   99, Training Loss: 0.1178, Validation Loss: 0.1463, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.180200\n",
      "Epoch  100, Training Loss: 0.1171, Validation Loss: 0.1454, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.180000\n",
      "Epoch  101, Training Loss: 0.1166, Validation Loss: 0.1447, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.179800\n",
      "Epoch  102, Training Loss: 0.1161, Validation Loss: 0.1440, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.179600\n",
      "Epoch  103, Training Loss: 0.1156, Validation Loss: 0.1436, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.179400\n",
      "Epoch  104, Training Loss: 0.1151, Validation Loss: 0.1434, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.179200\n",
      "Epoch  105, Training Loss: 0.1145, Validation Loss: 0.1425, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.179000\n",
      "Epoch  106, Training Loss: 0.1140, Validation Loss: 0.1420, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.178800\n",
      "Epoch  107, Training Loss: 0.1135, Validation Loss: 0.1418, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.178600\n",
      "Epoch  108, Training Loss: 0.1130, Validation Loss: 0.1414, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.178400\n",
      "Epoch  109, Training Loss: 0.1126, Validation Loss: 0.1406, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.178200\n",
      "Epoch  110, Training Loss: 0.1121, Validation Loss: 0.1406, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.178000\n",
      "Epoch  111, Training Loss: 0.1117, Validation Loss: 0.1405, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.177800\n",
      "Epoch  112, Training Loss: 0.1112, Validation Loss: 0.1397, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.177600\n",
      "Epoch  113, Training Loss: 0.1107, Validation Loss: 0.1398, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.177400\n",
      "Epoch  114, Training Loss: 0.1101, Validation Loss: 0.1394, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.177200\n",
      "Epoch  115, Training Loss: 0.1098, Validation Loss: 0.1385, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.177000\n",
      "Epoch  116, Training Loss: 0.1093, Validation Loss: 0.1377, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.176800\n",
      "Epoch  117, Training Loss: 0.1090, Validation Loss: 0.1384, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.176600\n",
      "Epoch  118, Training Loss: 0.1083, Validation Loss: 0.1380, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.176400\n",
      "Epoch  119, Training Loss: 0.1079, Validation Loss: 0.1374, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.176200\n",
      "Epoch  120, Training Loss: 0.1074, Validation Loss: 0.1369, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.176000\n",
      "Epoch  121, Training Loss: 0.1069, Validation Loss: 0.1366, Training Acc: 0.8586, Validation Acc: 0.8000, Learning Rate: 0.175800\n",
      "Epoch  122, Training Loss: 0.1067, Validation Loss: 0.1362, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.175600\n",
      "Epoch  123, Training Loss: 0.1061, Validation Loss: 0.1360, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.175400\n",
      "Epoch  124, Training Loss: 0.1056, Validation Loss: 0.1362, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.175200\n",
      "Epoch  125, Training Loss: 0.1052, Validation Loss: 0.1357, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.175000\n",
      "Epoch  126, Training Loss: 0.1047, Validation Loss: 0.1355, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.174800\n",
      "Epoch  127, Training Loss: 0.1043, Validation Loss: 0.1354, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.174600\n",
      "Epoch  128, Training Loss: 0.1039, Validation Loss: 0.1350, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.174400\n",
      "Epoch  129, Training Loss: 0.1034, Validation Loss: 0.1347, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.174200\n",
      "Epoch  130, Training Loss: 0.1030, Validation Loss: 0.1345, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.174000\n",
      "Epoch  131, Training Loss: 0.1026, Validation Loss: 0.1343, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.173800\n",
      "Epoch  132, Training Loss: 0.1023, Validation Loss: 0.1334, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.173600\n",
      "Epoch  133, Training Loss: 0.1019, Validation Loss: 0.1333, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.173400\n",
      "Epoch  134, Training Loss: 0.1014, Validation Loss: 0.1323, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.173200\n",
      "Epoch  135, Training Loss: 0.1011, Validation Loss: 0.1318, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.173000\n",
      "Epoch  136, Training Loss: 0.1006, Validation Loss: 0.1314, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.172800\n",
      "Epoch  137, Training Loss: 0.1001, Validation Loss: 0.1315, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.172600\n",
      "Epoch  138, Training Loss: 0.0997, Validation Loss: 0.1313, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.172400\n",
      "Epoch  139, Training Loss: 0.0993, Validation Loss: 0.1306, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.172200\n",
      "Epoch  140, Training Loss: 0.0990, Validation Loss: 0.1298, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.172000\n",
      "Epoch  141, Training Loss: 0.0985, Validation Loss: 0.1296, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.171800\n",
      "Epoch  142, Training Loss: 0.0982, Validation Loss: 0.1289, Training Acc: 0.8687, Validation Acc: 0.8400, Learning Rate: 0.171600\n",
      "Epoch  143, Training Loss: 0.0978, Validation Loss: 0.1282, Training Acc: 0.8788, Validation Acc: 0.8000, Learning Rate: 0.171400\n",
      "Epoch  144, Training Loss: 0.0973, Validation Loss: 0.1289, Training Acc: 0.8687, Validation Acc: 0.8000, Learning Rate: 0.171200\n",
      "Epoch  145, Training Loss: 0.0970, Validation Loss: 0.1289, Training Acc: 0.8889, Validation Acc: 0.8000, Learning Rate: 0.171000\n",
      "Epoch  146, Training Loss: 0.0967, Validation Loss: 0.1292, Training Acc: 0.8889, Validation Acc: 0.8000, Learning Rate: 0.170800\n",
      "Epoch  147, Training Loss: 0.0963, Validation Loss: 0.1287, Training Acc: 0.8889, Validation Acc: 0.8400, Learning Rate: 0.170600\n",
      "Epoch  148, Training Loss: 0.0959, Validation Loss: 0.1286, Training Acc: 0.8889, Validation Acc: 0.8400, Learning Rate: 0.170400\n",
      "Epoch  149, Training Loss: 0.0955, Validation Loss: 0.1282, Training Acc: 0.8788, Validation Acc: 0.8400, Learning Rate: 0.170200\n",
      "Epoch  150, Training Loss: 0.0952, Validation Loss: 0.1277, Training Acc: 0.9091, Validation Acc: 0.8400, Learning Rate: 0.170000\n",
      "Epoch  151, Training Loss: 0.0948, Validation Loss: 0.1269, Training Acc: 0.8889, Validation Acc: 0.8400, Learning Rate: 0.169800\n",
      "Epoch  152, Training Loss: 0.0944, Validation Loss: 0.1268, Training Acc: 0.8990, Validation Acc: 0.8400, Learning Rate: 0.169600\n",
      "Epoch  153, Training Loss: 0.0943, Validation Loss: 0.1262, Training Acc: 0.9091, Validation Acc: 0.8400, Learning Rate: 0.169400\n",
      "Epoch  154, Training Loss: 0.0937, Validation Loss: 0.1253, Training Acc: 0.9192, Validation Acc: 0.8400, Learning Rate: 0.169200\n",
      "Epoch  155, Training Loss: 0.0933, Validation Loss: 0.1251, Training Acc: 0.9192, Validation Acc: 0.8400, Learning Rate: 0.169000\n",
      "Epoch  156, Training Loss: 0.0930, Validation Loss: 0.1249, Training Acc: 0.9192, Validation Acc: 0.8400, Learning Rate: 0.168800\n",
      "Epoch  157, Training Loss: 0.0926, Validation Loss: 0.1246, Training Acc: 0.9293, Validation Acc: 0.8400, Learning Rate: 0.168600\n",
      "Epoch  158, Training Loss: 0.0922, Validation Loss: 0.1241, Training Acc: 0.9293, Validation Acc: 0.8400, Learning Rate: 0.168400\n",
      "Epoch  159, Training Loss: 0.0919, Validation Loss: 0.1237, Training Acc: 0.9293, Validation Acc: 0.8400, Learning Rate: 0.168200\n",
      "Epoch  160, Training Loss: 0.0916, Validation Loss: 0.1237, Training Acc: 0.9293, Validation Acc: 0.8400, Learning Rate: 0.168000\n",
      "Epoch  161, Training Loss: 0.0912, Validation Loss: 0.1235, Training Acc: 0.9293, Validation Acc: 0.8400, Learning Rate: 0.167800\n",
      "Epoch  162, Training Loss: 0.0908, Validation Loss: 0.1229, Training Acc: 0.9293, Validation Acc: 0.8400, Learning Rate: 0.167600\n",
      "Epoch  163, Training Loss: 0.0906, Validation Loss: 0.1228, Training Acc: 0.9293, Validation Acc: 0.8400, Learning Rate: 0.167400\n",
      "Epoch  164, Training Loss: 0.0902, Validation Loss: 0.1228, Training Acc: 0.9293, Validation Acc: 0.8400, Learning Rate: 0.167200\n",
      "Epoch  165, Training Loss: 0.0898, Validation Loss: 0.1225, Training Acc: 0.9293, Validation Acc: 0.8400, Learning Rate: 0.167000\n",
      "Epoch  166, Training Loss: 0.0896, Validation Loss: 0.1224, Training Acc: 0.9293, Validation Acc: 0.8400, Learning Rate: 0.166800\n",
      "Epoch  167, Training Loss: 0.0891, Validation Loss: 0.1217, Training Acc: 0.9293, Validation Acc: 0.8400, Learning Rate: 0.166600\n",
      "Epoch  168, Training Loss: 0.0888, Validation Loss: 0.1213, Training Acc: 0.9293, Validation Acc: 0.8400, Learning Rate: 0.166400\n",
      "Epoch  169, Training Loss: 0.0885, Validation Loss: 0.1210, Training Acc: 0.9293, Validation Acc: 0.8400, Learning Rate: 0.166200\n",
      "Epoch  170, Training Loss: 0.0882, Validation Loss: 0.1204, Training Acc: 0.9293, Validation Acc: 0.8800, Learning Rate: 0.166000\n",
      "Epoch  171, Training Loss: 0.0878, Validation Loss: 0.1202, Training Acc: 0.9293, Validation Acc: 0.8800, Learning Rate: 0.165800\n",
      "Epoch  172, Training Loss: 0.0875, Validation Loss: 0.1201, Training Acc: 0.9293, Validation Acc: 0.8800, Learning Rate: 0.165600\n",
      "Epoch  173, Training Loss: 0.0873, Validation Loss: 0.1196, Training Acc: 0.9293, Validation Acc: 0.8800, Learning Rate: 0.165400\n",
      "Epoch  174, Training Loss: 0.0868, Validation Loss: 0.1196, Training Acc: 0.9394, Validation Acc: 0.8800, Learning Rate: 0.165200\n",
      "Epoch  175, Training Loss: 0.0865, Validation Loss: 0.1195, Training Acc: 0.9394, Validation Acc: 0.8800, Learning Rate: 0.165000\n",
      "Epoch  176, Training Loss: 0.0862, Validation Loss: 0.1190, Training Acc: 0.9293, Validation Acc: 0.8800, Learning Rate: 0.164800\n",
      "Epoch  177, Training Loss: 0.0859, Validation Loss: 0.1187, Training Acc: 0.9394, Validation Acc: 0.8800, Learning Rate: 0.164600\n",
      "Epoch  178, Training Loss: 0.0857, Validation Loss: 0.1182, Training Acc: 0.9394, Validation Acc: 0.8800, Learning Rate: 0.164400\n",
      "Epoch  179, Training Loss: 0.0853, Validation Loss: 0.1171, Training Acc: 0.9394, Validation Acc: 0.8800, Learning Rate: 0.164200\n",
      "Epoch  180, Training Loss: 0.0849, Validation Loss: 0.1168, Training Acc: 0.9394, Validation Acc: 0.8800, Learning Rate: 0.164000\n",
      "Epoch  181, Training Loss: 0.0846, Validation Loss: 0.1166, Training Acc: 0.9394, Validation Acc: 0.8800, Learning Rate: 0.163800\n",
      "Epoch  182, Training Loss: 0.0845, Validation Loss: 0.1163, Training Acc: 0.9394, Validation Acc: 0.8800, Learning Rate: 0.163600\n",
      "Epoch  183, Training Loss: 0.0841, Validation Loss: 0.1157, Training Acc: 0.9394, Validation Acc: 0.8800, Learning Rate: 0.163400\n",
      "Epoch  184, Training Loss: 0.0838, Validation Loss: 0.1156, Training Acc: 0.9394, Validation Acc: 0.8800, Learning Rate: 0.163200\n",
      "Epoch  185, Training Loss: 0.0835, Validation Loss: 0.1148, Training Acc: 0.9394, Validation Acc: 0.8800, Learning Rate: 0.163000\n",
      "Epoch  186, Training Loss: 0.0832, Validation Loss: 0.1146, Training Acc: 0.9394, Validation Acc: 0.9200, Learning Rate: 0.162800\n",
      "Epoch  187, Training Loss: 0.0829, Validation Loss: 0.1145, Training Acc: 0.9394, Validation Acc: 0.9200, Learning Rate: 0.162600\n",
      "Epoch  188, Training Loss: 0.0827, Validation Loss: 0.1148, Training Acc: 0.9394, Validation Acc: 0.8800, Learning Rate: 0.162400\n",
      "Epoch  189, Training Loss: 0.0824, Validation Loss: 0.1139, Training Acc: 0.9394, Validation Acc: 0.9200, Learning Rate: 0.162200\n",
      "Epoch  190, Training Loss: 0.0821, Validation Loss: 0.1142, Training Acc: 0.9394, Validation Acc: 0.9200, Learning Rate: 0.162000\n",
      "Epoch  191, Training Loss: 0.0818, Validation Loss: 0.1134, Training Acc: 0.9394, Validation Acc: 0.9200, Learning Rate: 0.161800\n",
      "Epoch  192, Training Loss: 0.0815, Validation Loss: 0.1131, Training Acc: 0.9394, Validation Acc: 0.9200, Learning Rate: 0.161600\n",
      "Epoch  193, Training Loss: 0.0812, Validation Loss: 0.1127, Training Acc: 0.9394, Validation Acc: 0.9200, Learning Rate: 0.161400\n",
      "Epoch  194, Training Loss: 0.0809, Validation Loss: 0.1118, Training Acc: 0.9394, Validation Acc: 0.9200, Learning Rate: 0.161200\n",
      "Epoch  195, Training Loss: 0.0808, Validation Loss: 0.1115, Training Acc: 0.9495, Validation Acc: 0.9200, Learning Rate: 0.161000\n",
      "Epoch  196, Training Loss: 0.0804, Validation Loss: 0.1108, Training Acc: 0.9495, Validation Acc: 0.9200, Learning Rate: 0.160800\n",
      "Epoch  197, Training Loss: 0.0801, Validation Loss: 0.1104, Training Acc: 0.9495, Validation Acc: 0.9200, Learning Rate: 0.160600\n",
      "Epoch  198, Training Loss: 0.0797, Validation Loss: 0.1098, Training Acc: 0.9495, Validation Acc: 0.9200, Learning Rate: 0.160400\n",
      "Epoch  199, Training Loss: 0.0794, Validation Loss: 0.1091, Training Acc: 0.9495, Validation Acc: 0.9200, Learning Rate: 0.160200\n",
      "Epoch  200, Training Loss: 0.0792, Validation Loss: 0.1093, Training Acc: 0.9495, Validation Acc: 0.9200, Learning Rate: 0.160000\n",
      "Epoch  201, Training Loss: 0.0788, Validation Loss: 0.1086, Training Acc: 0.9495, Validation Acc: 0.9200, Learning Rate: 0.159800\n",
      "Epoch  202, Training Loss: 0.0785, Validation Loss: 0.1074, Training Acc: 0.9495, Validation Acc: 0.9200, Learning Rate: 0.159600\n",
      "Epoch  203, Training Loss: 0.0782, Validation Loss: 0.1070, Training Acc: 0.9596, Validation Acc: 0.9200, Learning Rate: 0.159400\n",
      "Epoch  204, Training Loss: 0.0778, Validation Loss: 0.1061, Training Acc: 0.9596, Validation Acc: 0.9200, Learning Rate: 0.159200\n",
      "Epoch  205, Training Loss: 0.0774, Validation Loss: 0.1062, Training Acc: 0.9495, Validation Acc: 0.9200, Learning Rate: 0.159000\n",
      "Epoch  206, Training Loss: 0.0771, Validation Loss: 0.1063, Training Acc: 0.9495, Validation Acc: 0.9200, Learning Rate: 0.158800\n",
      "Epoch  207, Training Loss: 0.0767, Validation Loss: 0.1060, Training Acc: 0.9495, Validation Acc: 0.9200, Learning Rate: 0.158600\n",
      "Epoch  208, Training Loss: 0.0764, Validation Loss: 0.1063, Training Acc: 0.9495, Validation Acc: 0.9200, Learning Rate: 0.158400\n",
      "Epoch  209, Training Loss: 0.0762, Validation Loss: 0.1058, Training Acc: 0.9495, Validation Acc: 0.9200, Learning Rate: 0.158200\n",
      "Epoch  210, Training Loss: 0.0759, Validation Loss: 0.1051, Training Acc: 0.9596, Validation Acc: 0.9200, Learning Rate: 0.158000\n",
      "Epoch  211, Training Loss: 0.0755, Validation Loss: 0.1045, Training Acc: 0.9596, Validation Acc: 0.9200, Learning Rate: 0.157800\n",
      "Epoch  212, Training Loss: 0.0752, Validation Loss: 0.1040, Training Acc: 0.9495, Validation Acc: 0.9200, Learning Rate: 0.157600\n",
      "Epoch  213, Training Loss: 0.0750, Validation Loss: 0.1038, Training Acc: 0.9596, Validation Acc: 0.9200, Learning Rate: 0.157400\n",
      "Epoch  214, Training Loss: 0.0747, Validation Loss: 0.1033, Training Acc: 0.9596, Validation Acc: 0.9200, Learning Rate: 0.157200\n",
      "Epoch  215, Training Loss: 0.0744, Validation Loss: 0.1031, Training Acc: 0.9596, Validation Acc: 0.9200, Learning Rate: 0.157000\n",
      "Epoch  216, Training Loss: 0.0740, Validation Loss: 0.1028, Training Acc: 0.9596, Validation Acc: 0.9200, Learning Rate: 0.156800\n",
      "Epoch  217, Training Loss: 0.0738, Validation Loss: 0.1020, Training Acc: 0.9596, Validation Acc: 0.9200, Learning Rate: 0.156600\n",
      "Epoch  218, Training Loss: 0.0735, Validation Loss: 0.1021, Training Acc: 0.9596, Validation Acc: 0.9200, Learning Rate: 0.156400\n",
      "Epoch  219, Training Loss: 0.0732, Validation Loss: 0.1010, Training Acc: 0.9596, Validation Acc: 0.9200, Learning Rate: 0.156200\n",
      "Epoch  220, Training Loss: 0.0729, Validation Loss: 0.1003, Training Acc: 0.9697, Validation Acc: 0.9200, Learning Rate: 0.156000\n",
      "Epoch  221, Training Loss: 0.0727, Validation Loss: 0.1001, Training Acc: 0.9697, Validation Acc: 0.9200, Learning Rate: 0.155800\n",
      "Epoch  222, Training Loss: 0.0725, Validation Loss: 0.1000, Training Acc: 0.9495, Validation Acc: 0.9200, Learning Rate: 0.155600\n",
      "Epoch  223, Training Loss: 0.0721, Validation Loss: 0.0989, Training Acc: 0.9596, Validation Acc: 0.9200, Learning Rate: 0.155400\n",
      "Epoch  224, Training Loss: 0.0718, Validation Loss: 0.0987, Training Acc: 0.9596, Validation Acc: 0.9200, Learning Rate: 0.155200\n",
      "Epoch  225, Training Loss: 0.0715, Validation Loss: 0.0988, Training Acc: 0.9596, Validation Acc: 0.9200, Learning Rate: 0.155000\n",
      "Epoch  226, Training Loss: 0.0712, Validation Loss: 0.0988, Training Acc: 0.9697, Validation Acc: 0.9200, Learning Rate: 0.154800\n",
      "Epoch  227, Training Loss: 0.0710, Validation Loss: 0.0982, Training Acc: 0.9697, Validation Acc: 0.9200, Learning Rate: 0.154600\n",
      "Epoch  228, Training Loss: 0.0707, Validation Loss: 0.0983, Training Acc: 0.9697, Validation Acc: 0.9200, Learning Rate: 0.154400\n",
      "Epoch  229, Training Loss: 0.0704, Validation Loss: 0.0981, Training Acc: 0.9697, Validation Acc: 0.9200, Learning Rate: 0.154200\n",
      "Epoch  230, Training Loss: 0.0702, Validation Loss: 0.0977, Training Acc: 0.9798, Validation Acc: 0.9200, Learning Rate: 0.154000\n",
      "Epoch  231, Training Loss: 0.0699, Validation Loss: 0.0972, Training Acc: 0.9798, Validation Acc: 0.9200, Learning Rate: 0.153800\n",
      "Epoch  232, Training Loss: 0.0696, Validation Loss: 0.0972, Training Acc: 0.9798, Validation Acc: 0.9200, Learning Rate: 0.153600\n",
      "Epoch  233, Training Loss: 0.0694, Validation Loss: 0.0970, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.153400\n",
      "Epoch  234, Training Loss: 0.0692, Validation Loss: 0.0966, Training Acc: 0.9798, Validation Acc: 0.9200, Learning Rate: 0.153200\n",
      "Epoch  235, Training Loss: 0.0688, Validation Loss: 0.0959, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.153000\n",
      "Epoch  236, Training Loss: 0.0686, Validation Loss: 0.0953, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.152800\n",
      "Epoch  237, Training Loss: 0.0684, Validation Loss: 0.0950, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.152600\n",
      "Epoch  238, Training Loss: 0.0681, Validation Loss: 0.0950, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.152400\n",
      "Epoch  239, Training Loss: 0.0679, Validation Loss: 0.0949, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.152200\n",
      "Epoch  240, Training Loss: 0.0676, Validation Loss: 0.0947, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.152000\n",
      "Epoch  241, Training Loss: 0.0674, Validation Loss: 0.0948, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.151800\n",
      "Epoch  242, Training Loss: 0.0671, Validation Loss: 0.0949, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.151600\n",
      "Epoch  243, Training Loss: 0.0669, Validation Loss: 0.0948, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.151400\n",
      "Epoch  244, Training Loss: 0.0667, Validation Loss: 0.0945, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.151200\n",
      "Epoch  245, Training Loss: 0.0665, Validation Loss: 0.0941, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.151000\n",
      "Epoch  246, Training Loss: 0.0663, Validation Loss: 0.0943, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.150800\n",
      "Epoch  247, Training Loss: 0.0660, Validation Loss: 0.0937, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.150600\n",
      "Epoch  248, Training Loss: 0.0658, Validation Loss: 0.0937, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.150400\n",
      "Epoch  249, Training Loss: 0.0655, Validation Loss: 0.0930, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.150200\n",
      "Epoch  250, Training Loss: 0.0653, Validation Loss: 0.0933, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.150000\n",
      "Epoch  251, Training Loss: 0.0651, Validation Loss: 0.0928, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.149800\n",
      "Epoch  252, Training Loss: 0.0649, Validation Loss: 0.0925, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.149600\n",
      "Epoch  253, Training Loss: 0.0646, Validation Loss: 0.0918, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.149400\n",
      "Epoch  254, Training Loss: 0.0644, Validation Loss: 0.0912, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.149200\n",
      "Epoch  255, Training Loss: 0.0641, Validation Loss: 0.0911, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.149000\n",
      "Epoch  256, Training Loss: 0.0639, Validation Loss: 0.0903, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.148800\n",
      "Epoch  257, Training Loss: 0.0637, Validation Loss: 0.0901, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.148600\n",
      "Epoch  258, Training Loss: 0.0635, Validation Loss: 0.0899, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.148400\n",
      "Epoch  259, Training Loss: 0.0633, Validation Loss: 0.0900, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.148200\n",
      "Epoch  260, Training Loss: 0.0630, Validation Loss: 0.0900, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.148000\n",
      "Epoch  261, Training Loss: 0.0628, Validation Loss: 0.0893, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.147800\n",
      "Epoch  262, Training Loss: 0.0626, Validation Loss: 0.0889, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.147600\n",
      "Epoch  263, Training Loss: 0.0624, Validation Loss: 0.0884, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.147400\n",
      "Epoch  264, Training Loss: 0.0622, Validation Loss: 0.0884, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.147200\n",
      "Epoch  265, Training Loss: 0.0619, Validation Loss: 0.0883, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.147000\n",
      "Epoch  266, Training Loss: 0.0618, Validation Loss: 0.0882, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.146800\n",
      "Epoch  267, Training Loss: 0.0617, Validation Loss: 0.0881, Training Acc: 0.9899, Validation Acc: 0.9200, Learning Rate: 0.146600\n",
      "Epoch  268, Training Loss: 0.0614, Validation Loss: 0.0875, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.146400\n",
      "Epoch  269, Training Loss: 0.0612, Validation Loss: 0.0871, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.146200\n",
      "Epoch  270, Training Loss: 0.0610, Validation Loss: 0.0867, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.146000\n",
      "Epoch  271, Training Loss: 0.0608, Validation Loss: 0.0864, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.145800\n",
      "Epoch  272, Training Loss: 0.0606, Validation Loss: 0.0861, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.145600\n",
      "Epoch  273, Training Loss: 0.0605, Validation Loss: 0.0861, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.145400\n",
      "Epoch  274, Training Loss: 0.0602, Validation Loss: 0.0857, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.145200\n",
      "Epoch  275, Training Loss: 0.0600, Validation Loss: 0.0860, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.145000\n",
      "Epoch  276, Training Loss: 0.0598, Validation Loss: 0.0854, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.144800\n",
      "Epoch  277, Training Loss: 0.0599, Validation Loss: 0.0851, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.144600\n",
      "Epoch  278, Training Loss: 0.0594, Validation Loss: 0.0851, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.144400\n",
      "Epoch  279, Training Loss: 0.0592, Validation Loss: 0.0844, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.144200\n",
      "Epoch  280, Training Loss: 0.0590, Validation Loss: 0.0844, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.144000\n",
      "Epoch  281, Training Loss: 0.0588, Validation Loss: 0.0841, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.143800\n",
      "Epoch  282, Training Loss: 0.0587, Validation Loss: 0.0840, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.143600\n",
      "Epoch  283, Training Loss: 0.0584, Validation Loss: 0.0833, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.143400\n",
      "Epoch  284, Training Loss: 0.0583, Validation Loss: 0.0828, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.143200\n",
      "Epoch  285, Training Loss: 0.0581, Validation Loss: 0.0821, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.143000\n",
      "Epoch  286, Training Loss: 0.0579, Validation Loss: 0.0819, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.142800\n",
      "Epoch  287, Training Loss: 0.0578, Validation Loss: 0.0814, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.142600\n",
      "Epoch  288, Training Loss: 0.0577, Validation Loss: 0.0812, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.142400\n",
      "Epoch  289, Training Loss: 0.0574, Validation Loss: 0.0814, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.142200\n",
      "Epoch  290, Training Loss: 0.0573, Validation Loss: 0.0813, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.142000\n",
      "Epoch  291, Training Loss: 0.0570, Validation Loss: 0.0807, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.141800\n",
      "Epoch  292, Training Loss: 0.0569, Validation Loss: 0.0809, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.141600\n",
      "Epoch  293, Training Loss: 0.0569, Validation Loss: 0.0813, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.141400\n",
      "Epoch  294, Training Loss: 0.0566, Validation Loss: 0.0814, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.141200\n",
      "Epoch  295, Training Loss: 0.0564, Validation Loss: 0.0815, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.141000\n",
      "Epoch  296, Training Loss: 0.0563, Validation Loss: 0.0814, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.140800\n",
      "Epoch  297, Training Loss: 0.0560, Validation Loss: 0.0810, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.140600\n",
      "Epoch  298, Training Loss: 0.0559, Validation Loss: 0.0806, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.140400\n",
      "Epoch  299, Training Loss: 0.0557, Validation Loss: 0.0805, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.140200\n",
      "Epoch  300, Training Loss: 0.0556, Validation Loss: 0.0803, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.140000\n",
      "Epoch  301, Training Loss: 0.0554, Validation Loss: 0.0801, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.139800\n",
      "Epoch  302, Training Loss: 0.0552, Validation Loss: 0.0801, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.139600\n",
      "Epoch  303, Training Loss: 0.0551, Validation Loss: 0.0805, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.139400\n",
      "Epoch  304, Training Loss: 0.0549, Validation Loss: 0.0806, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.139200\n",
      "Epoch  305, Training Loss: 0.0547, Validation Loss: 0.0800, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.139000\n",
      "Epoch  306, Training Loss: 0.0547, Validation Loss: 0.0799, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.138800\n",
      "Epoch  307, Training Loss: 0.0544, Validation Loss: 0.0794, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.138600\n",
      "Epoch  308, Training Loss: 0.0543, Validation Loss: 0.0796, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.138400\n",
      "Epoch  309, Training Loss: 0.0541, Validation Loss: 0.0793, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.138200\n",
      "Epoch  310, Training Loss: 0.0540, Validation Loss: 0.0791, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.138000\n",
      "Epoch  311, Training Loss: 0.0538, Validation Loss: 0.0789, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.137800\n",
      "Epoch  312, Training Loss: 0.0537, Validation Loss: 0.0789, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.137600\n",
      "Epoch  313, Training Loss: 0.0536, Validation Loss: 0.0784, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.137400\n",
      "Epoch  314, Training Loss: 0.0534, Validation Loss: 0.0777, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.137200\n",
      "Epoch  315, Training Loss: 0.0532, Validation Loss: 0.0774, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.137000\n",
      "Epoch  316, Training Loss: 0.0531, Validation Loss: 0.0771, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.136800\n",
      "Epoch  317, Training Loss: 0.0530, Validation Loss: 0.0768, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.136600\n",
      "Epoch  318, Training Loss: 0.0528, Validation Loss: 0.0770, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.136400\n",
      "Epoch  319, Training Loss: 0.0527, Validation Loss: 0.0769, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.136200\n",
      "Epoch  320, Training Loss: 0.0525, Validation Loss: 0.0764, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.136000\n",
      "Epoch  321, Training Loss: 0.0524, Validation Loss: 0.0763, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.135800\n",
      "Epoch  322, Training Loss: 0.0523, Validation Loss: 0.0761, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.135600\n",
      "Epoch  323, Training Loss: 0.0522, Validation Loss: 0.0765, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.135400\n",
      "Epoch  324, Training Loss: 0.0520, Validation Loss: 0.0758, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.135200\n",
      "Epoch  325, Training Loss: 0.0519, Validation Loss: 0.0756, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.135000\n",
      "Epoch  326, Training Loss: 0.0517, Validation Loss: 0.0751, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.134800\n",
      "Epoch  327, Training Loss: 0.0516, Validation Loss: 0.0750, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.134600\n",
      "Epoch  328, Training Loss: 0.0514, Validation Loss: 0.0747, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.134400\n",
      "Epoch  329, Training Loss: 0.0513, Validation Loss: 0.0748, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.134200\n",
      "Epoch  330, Training Loss: 0.0512, Validation Loss: 0.0748, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.134000\n",
      "Epoch  331, Training Loss: 0.0511, Validation Loss: 0.0750, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.133800\n",
      "Epoch  332, Training Loss: 0.0509, Validation Loss: 0.0745, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.133600\n",
      "Epoch  333, Training Loss: 0.0508, Validation Loss: 0.0742, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.133400\n",
      "Epoch  334, Training Loss: 0.0507, Validation Loss: 0.0740, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.133200\n",
      "Epoch  335, Training Loss: 0.0505, Validation Loss: 0.0739, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.133000\n",
      "Epoch  336, Training Loss: 0.0504, Validation Loss: 0.0741, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.132800\n",
      "Epoch  337, Training Loss: 0.0503, Validation Loss: 0.0737, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.132600\n",
      "Epoch  338, Training Loss: 0.0502, Validation Loss: 0.0739, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.132400\n",
      "Epoch  339, Training Loss: 0.0501, Validation Loss: 0.0738, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.132200\n",
      "Epoch  340, Training Loss: 0.0500, Validation Loss: 0.0738, Training Acc: 1.0000, Validation Acc: 0.9200, Learning Rate: 0.132000\n",
      "Epoch  341, Training Loss: 0.0498, Validation Loss: 0.0734, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.131800\n",
      "Epoch  342, Training Loss: 0.0497, Validation Loss: 0.0734, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.131600\n",
      "Epoch  343, Training Loss: 0.0496, Validation Loss: 0.0731, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.131400\n",
      "Epoch  344, Training Loss: 0.0495, Validation Loss: 0.0730, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.131200\n",
      "Epoch  345, Training Loss: 0.0493, Validation Loss: 0.0725, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.131000\n",
      "Epoch  346, Training Loss: 0.0492, Validation Loss: 0.0720, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.130800\n",
      "Epoch  347, Training Loss: 0.0491, Validation Loss: 0.0719, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.130600\n",
      "Epoch  348, Training Loss: 0.0490, Validation Loss: 0.0716, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.130400\n",
      "Epoch  349, Training Loss: 0.0489, Validation Loss: 0.0714, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.130200\n",
      "Epoch  350, Training Loss: 0.0488, Validation Loss: 0.0710, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.130000\n",
      "Epoch  351, Training Loss: 0.0487, Validation Loss: 0.0710, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.129800\n",
      "Epoch  352, Training Loss: 0.0485, Validation Loss: 0.0709, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.129600\n",
      "Epoch  353, Training Loss: 0.0485, Validation Loss: 0.0710, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.129400\n",
      "Epoch  354, Training Loss: 0.0484, Validation Loss: 0.0707, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.129200\n",
      "Epoch  355, Training Loss: 0.0482, Validation Loss: 0.0705, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.129000\n",
      "Epoch  356, Training Loss: 0.0481, Validation Loss: 0.0707, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.128800\n",
      "Epoch  357, Training Loss: 0.0480, Validation Loss: 0.0702, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.128600\n",
      "Epoch  358, Training Loss: 0.0480, Validation Loss: 0.0698, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.128400\n",
      "Epoch  359, Training Loss: 0.0478, Validation Loss: 0.0698, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.128200\n",
      "Epoch  360, Training Loss: 0.0477, Validation Loss: 0.0698, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.128000\n",
      "Epoch  361, Training Loss: 0.0476, Validation Loss: 0.0695, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.127800\n",
      "Epoch  362, Training Loss: 0.0475, Validation Loss: 0.0698, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.127600\n",
      "Epoch  363, Training Loss: 0.0474, Validation Loss: 0.0692, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.127400\n",
      "Epoch  364, Training Loss: 0.0473, Validation Loss: 0.0689, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.127200\n",
      "Epoch  365, Training Loss: 0.0472, Validation Loss: 0.0686, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.127000\n",
      "Epoch  366, Training Loss: 0.0471, Validation Loss: 0.0687, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.126800\n",
      "Epoch  367, Training Loss: 0.0470, Validation Loss: 0.0686, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.126600\n",
      "Epoch  368, Training Loss: 0.0469, Validation Loss: 0.0686, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.126400\n",
      "Epoch  369, Training Loss: 0.0469, Validation Loss: 0.0688, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.126200\n",
      "Epoch  370, Training Loss: 0.0467, Validation Loss: 0.0689, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.126000\n",
      "Epoch  371, Training Loss: 0.0467, Validation Loss: 0.0692, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.125800\n",
      "Epoch  372, Training Loss: 0.0465, Validation Loss: 0.0688, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.125600\n",
      "Epoch  373, Training Loss: 0.0465, Validation Loss: 0.0689, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.125400\n",
      "Epoch  374, Training Loss: 0.0464, Validation Loss: 0.0687, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.125200\n",
      "Epoch  375, Training Loss: 0.0463, Validation Loss: 0.0686, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.125000\n",
      "Epoch  376, Training Loss: 0.0462, Validation Loss: 0.0684, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.124800\n",
      "Epoch  377, Training Loss: 0.0461, Validation Loss: 0.0683, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.124600\n",
      "Epoch  378, Training Loss: 0.0460, Validation Loss: 0.0679, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.124400\n",
      "Epoch  379, Training Loss: 0.0459, Validation Loss: 0.0680, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.124200\n",
      "Epoch  380, Training Loss: 0.0458, Validation Loss: 0.0680, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.124000\n",
      "Epoch  381, Training Loss: 0.0457, Validation Loss: 0.0678, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.123800\n",
      "Epoch  382, Training Loss: 0.0457, Validation Loss: 0.0678, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.123600\n",
      "Epoch  383, Training Loss: 0.0456, Validation Loss: 0.0675, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.123400\n",
      "Epoch  384, Training Loss: 0.0455, Validation Loss: 0.0673, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.123200\n",
      "Epoch  385, Training Loss: 0.0454, Validation Loss: 0.0670, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.123000\n",
      "Epoch  386, Training Loss: 0.0453, Validation Loss: 0.0667, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.122800\n",
      "Epoch  387, Training Loss: 0.0452, Validation Loss: 0.0666, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.122600\n",
      "Epoch  388, Training Loss: 0.0451, Validation Loss: 0.0665, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.122400\n",
      "Epoch  389, Training Loss: 0.0451, Validation Loss: 0.0664, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.122200\n",
      "Epoch  390, Training Loss: 0.0450, Validation Loss: 0.0665, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.122000\n",
      "Epoch  391, Training Loss: 0.0449, Validation Loss: 0.0658, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.121800\n",
      "Epoch  392, Training Loss: 0.0448, Validation Loss: 0.0658, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.121600\n",
      "Epoch  393, Training Loss: 0.0447, Validation Loss: 0.0656, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.121400\n",
      "Epoch  394, Training Loss: 0.0447, Validation Loss: 0.0659, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.121200\n",
      "Epoch  395, Training Loss: 0.0446, Validation Loss: 0.0659, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.121000\n",
      "Epoch  396, Training Loss: 0.0445, Validation Loss: 0.0657, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.120800\n",
      "Epoch  397, Training Loss: 0.0444, Validation Loss: 0.0655, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.120600\n",
      "Epoch  398, Training Loss: 0.0444, Validation Loss: 0.0654, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.120400\n",
      "Epoch  399, Training Loss: 0.0443, Validation Loss: 0.0650, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.120200\n",
      "Epoch  400, Training Loss: 0.0442, Validation Loss: 0.0646, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.120000\n",
      "Epoch  401, Training Loss: 0.0441, Validation Loss: 0.0646, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.119800\n",
      "Epoch  402, Training Loss: 0.0440, Validation Loss: 0.0643, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.119600\n",
      "Epoch  403, Training Loss: 0.0440, Validation Loss: 0.0642, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.119400\n",
      "Epoch  404, Training Loss: 0.0439, Validation Loss: 0.0638, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.119200\n",
      "Epoch  405, Training Loss: 0.0438, Validation Loss: 0.0638, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.119000\n",
      "Epoch  406, Training Loss: 0.0437, Validation Loss: 0.0638, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.118800\n",
      "Epoch  407, Training Loss: 0.0437, Validation Loss: 0.0641, Training Acc: 1.0000, Validation Acc: 0.9600, Learning Rate: 0.118600\n",
      "Epoch  408, Training Loss: 0.0436, Validation Loss: 0.0639, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.118400\n",
      "Epoch  409, Training Loss: 0.0436, Validation Loss: 0.0636, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.118200\n",
      "Epoch  410, Training Loss: 0.0435, Validation Loss: 0.0635, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.118000\n",
      "Epoch  411, Training Loss: 0.0434, Validation Loss: 0.0635, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.117800\n",
      "Epoch  412, Training Loss: 0.0433, Validation Loss: 0.0633, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.117600\n",
      "Epoch  413, Training Loss: 0.0432, Validation Loss: 0.0634, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.117400\n",
      "Epoch  414, Training Loss: 0.0432, Validation Loss: 0.0631, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.117200\n",
      "Epoch  415, Training Loss: 0.0431, Validation Loss: 0.0630, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.117000\n",
      "Epoch  416, Training Loss: 0.0430, Validation Loss: 0.0630, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.116800\n",
      "Epoch  417, Training Loss: 0.0430, Validation Loss: 0.0630, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.116600\n",
      "Epoch  418, Training Loss: 0.0429, Validation Loss: 0.0626, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.116400\n",
      "Epoch  419, Training Loss: 0.0429, Validation Loss: 0.0624, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.116200\n",
      "Epoch  420, Training Loss: 0.0428, Validation Loss: 0.0622, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.116000\n",
      "Epoch  421, Training Loss: 0.0427, Validation Loss: 0.0620, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.115800\n",
      "Epoch  422, Training Loss: 0.0427, Validation Loss: 0.0621, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.115600\n",
      "Epoch  423, Training Loss: 0.0426, Validation Loss: 0.0622, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.115400\n",
      "Epoch  424, Training Loss: 0.0425, Validation Loss: 0.0619, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.115200\n",
      "Epoch  425, Training Loss: 0.0425, Validation Loss: 0.0617, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.115000\n",
      "Epoch  426, Training Loss: 0.0424, Validation Loss: 0.0618, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.114800\n",
      "Epoch  427, Training Loss: 0.0424, Validation Loss: 0.0618, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.114600\n",
      "Epoch  428, Training Loss: 0.0423, Validation Loss: 0.0618, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.114400\n",
      "Epoch  429, Training Loss: 0.0422, Validation Loss: 0.0617, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.114200\n",
      "Epoch  430, Training Loss: 0.0421, Validation Loss: 0.0618, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.114000\n",
      "Epoch  431, Training Loss: 0.0421, Validation Loss: 0.0621, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.113800\n",
      "Epoch  432, Training Loss: 0.0421, Validation Loss: 0.0619, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.113600\n",
      "Epoch  433, Training Loss: 0.0420, Validation Loss: 0.0615, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.113400\n",
      "Epoch  434, Training Loss: 0.0419, Validation Loss: 0.0614, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.113200\n",
      "Epoch  435, Training Loss: 0.0419, Validation Loss: 0.0613, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.113000\n",
      "Epoch  436, Training Loss: 0.0418, Validation Loss: 0.0614, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.112800\n",
      "Epoch  437, Training Loss: 0.0418, Validation Loss: 0.0615, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.112600\n",
      "Epoch  438, Training Loss: 0.0417, Validation Loss: 0.0610, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.112400\n",
      "Epoch  439, Training Loss: 0.0417, Validation Loss: 0.0609, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.112200\n",
      "Epoch  440, Training Loss: 0.0416, Validation Loss: 0.0609, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.112000\n",
      "Epoch  441, Training Loss: 0.0415, Validation Loss: 0.0611, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.111800\n",
      "Epoch  442, Training Loss: 0.0415, Validation Loss: 0.0608, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.111600\n",
      "Epoch  443, Training Loss: 0.0414, Validation Loss: 0.0608, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.111400\n",
      "Epoch  444, Training Loss: 0.0414, Validation Loss: 0.0608, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.111200\n",
      "Epoch  445, Training Loss: 0.0413, Validation Loss: 0.0608, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.111000\n",
      "Epoch  446, Training Loss: 0.0413, Validation Loss: 0.0608, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.110800\n",
      "Epoch  447, Training Loss: 0.0412, Validation Loss: 0.0604, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.110600\n",
      "Epoch  448, Training Loss: 0.0412, Validation Loss: 0.0607, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.110400\n",
      "Epoch  449, Training Loss: 0.0411, Validation Loss: 0.0605, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.110200\n",
      "Epoch  450, Training Loss: 0.0410, Validation Loss: 0.0605, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.110000\n",
      "Epoch  451, Training Loss: 0.0410, Validation Loss: 0.0603, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.109800\n",
      "Epoch  452, Training Loss: 0.0410, Validation Loss: 0.0603, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.109600\n",
      "Epoch  453, Training Loss: 0.0409, Validation Loss: 0.0607, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.109400\n",
      "Epoch  454, Training Loss: 0.0409, Validation Loss: 0.0604, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.109200\n",
      "Epoch  455, Training Loss: 0.0408, Validation Loss: 0.0604, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.109000\n",
      "Epoch  456, Training Loss: 0.0408, Validation Loss: 0.0600, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.108800\n",
      "Epoch  457, Training Loss: 0.0407, Validation Loss: 0.0599, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.108600\n",
      "Epoch  458, Training Loss: 0.0406, Validation Loss: 0.0598, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.108400\n",
      "Epoch  459, Training Loss: 0.0406, Validation Loss: 0.0598, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.108200\n",
      "Epoch  460, Training Loss: 0.0405, Validation Loss: 0.0599, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.108000\n",
      "Epoch  461, Training Loss: 0.0405, Validation Loss: 0.0599, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.107800\n",
      "Epoch  462, Training Loss: 0.0405, Validation Loss: 0.0600, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.107600\n",
      "Epoch  463, Training Loss: 0.0404, Validation Loss: 0.0600, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.107400\n",
      "Epoch  464, Training Loss: 0.0404, Validation Loss: 0.0599, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.107200\n",
      "Epoch  465, Training Loss: 0.0403, Validation Loss: 0.0598, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.107000\n",
      "Epoch  466, Training Loss: 0.0403, Validation Loss: 0.0595, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.106800\n",
      "Epoch  467, Training Loss: 0.0402, Validation Loss: 0.0597, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.106600\n",
      "Epoch  468, Training Loss: 0.0403, Validation Loss: 0.0599, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.106400\n",
      "Epoch  469, Training Loss: 0.0402, Validation Loss: 0.0598, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.106200\n",
      "Epoch  470, Training Loss: 0.0401, Validation Loss: 0.0598, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.106000\n",
      "Epoch  471, Training Loss: 0.0401, Validation Loss: 0.0596, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.105800\n",
      "Epoch  472, Training Loss: 0.0401, Validation Loss: 0.0597, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.105600\n",
      "Epoch  473, Training Loss: 0.0400, Validation Loss: 0.0596, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.105400\n",
      "Epoch  474, Training Loss: 0.0400, Validation Loss: 0.0596, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.105200\n",
      "Epoch  475, Training Loss: 0.0399, Validation Loss: 0.0597, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.105000\n",
      "Epoch  476, Training Loss: 0.0399, Validation Loss: 0.0596, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.104800\n",
      "Epoch  477, Training Loss: 0.0398, Validation Loss: 0.0593, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.104600\n",
      "Epoch  478, Training Loss: 0.0398, Validation Loss: 0.0590, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.104400\n",
      "Epoch  479, Training Loss: 0.0398, Validation Loss: 0.0592, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.104200\n",
      "Epoch  480, Training Loss: 0.0397, Validation Loss: 0.0592, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.104000\n",
      "Epoch  481, Training Loss: 0.0397, Validation Loss: 0.0592, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.103800\n",
      "Epoch  482, Training Loss: 0.0396, Validation Loss: 0.0590, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.103600\n",
      "Epoch  483, Training Loss: 0.0397, Validation Loss: 0.0591, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.103400\n",
      "Epoch  484, Training Loss: 0.0396, Validation Loss: 0.0587, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.103200\n",
      "Epoch  485, Training Loss: 0.0395, Validation Loss: 0.0586, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.103000\n",
      "Epoch  486, Training Loss: 0.0395, Validation Loss: 0.0583, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.102800\n",
      "Epoch  487, Training Loss: 0.0395, Validation Loss: 0.0585, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.102600\n",
      "Epoch  488, Training Loss: 0.0394, Validation Loss: 0.0580, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.102400\n",
      "Epoch  489, Training Loss: 0.0393, Validation Loss: 0.0578, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.102200\n",
      "Epoch  490, Training Loss: 0.0393, Validation Loss: 0.0577, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.102000\n",
      "Epoch  491, Training Loss: 0.0393, Validation Loss: 0.0576, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.101800\n",
      "Epoch  492, Training Loss: 0.0392, Validation Loss: 0.0577, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.101600\n",
      "Epoch  493, Training Loss: 0.0392, Validation Loss: 0.0575, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.101400\n",
      "Epoch  494, Training Loss: 0.0392, Validation Loss: 0.0572, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.101200\n",
      "Epoch  495, Training Loss: 0.0391, Validation Loss: 0.0572, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.101000\n",
      "Epoch  496, Training Loss: 0.0391, Validation Loss: 0.0573, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.100800\n",
      "Epoch  497, Training Loss: 0.0391, Validation Loss: 0.0570, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.100600\n",
      "Epoch  498, Training Loss: 0.0390, Validation Loss: 0.0570, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.100400\n",
      "Epoch  499, Training Loss: 0.0390, Validation Loss: 0.0566, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.100200\n",
      "Epoch  500, Training Loss: 0.0389, Validation Loss: 0.0569, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.100000\n",
      "Epoch  501, Training Loss: 0.0389, Validation Loss: 0.0566, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.099800\n",
      "Epoch  502, Training Loss: 0.0389, Validation Loss: 0.0566, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.099600\n",
      "Epoch  503, Training Loss: 0.0389, Validation Loss: 0.0567, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.099400\n",
      "Epoch  504, Training Loss: 0.0388, Validation Loss: 0.0566, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.099200\n",
      "Epoch  505, Training Loss: 0.0388, Validation Loss: 0.0567, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.099000\n",
      "Epoch  506, Training Loss: 0.0388, Validation Loss: 0.0564, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.098800\n",
      "Epoch  507, Training Loss: 0.0387, Validation Loss: 0.0563, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.098600\n",
      "Epoch  508, Training Loss: 0.0387, Validation Loss: 0.0564, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.098400\n",
      "Epoch  509, Training Loss: 0.0386, Validation Loss: 0.0564, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.098200\n",
      "Epoch  510, Training Loss: 0.0386, Validation Loss: 0.0564, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.098000\n",
      "Epoch  511, Training Loss: 0.0386, Validation Loss: 0.0562, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.097800\n",
      "Epoch  512, Training Loss: 0.0385, Validation Loss: 0.0562, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.097600\n",
      "Epoch  513, Training Loss: 0.0385, Validation Loss: 0.0562, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.097400\n",
      "Epoch  514, Training Loss: 0.0385, Validation Loss: 0.0561, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.097200\n",
      "Epoch  515, Training Loss: 0.0384, Validation Loss: 0.0561, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.097000\n",
      "Epoch  516, Training Loss: 0.0384, Validation Loss: 0.0559, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.096800\n",
      "Epoch  517, Training Loss: 0.0384, Validation Loss: 0.0558, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.096600\n",
      "Epoch  518, Training Loss: 0.0384, Validation Loss: 0.0558, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.096400\n",
      "Epoch  519, Training Loss: 0.0384, Validation Loss: 0.0557, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.096200\n",
      "Epoch  520, Training Loss: 0.0383, Validation Loss: 0.0559, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.096000\n",
      "Epoch  521, Training Loss: 0.0383, Validation Loss: 0.0558, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.095800\n",
      "Epoch  522, Training Loss: 0.0382, Validation Loss: 0.0560, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.095600\n",
      "Epoch  523, Training Loss: 0.0382, Validation Loss: 0.0556, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.095400\n",
      "Epoch  524, Training Loss: 0.0382, Validation Loss: 0.0558, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.095200\n",
      "Epoch  525, Training Loss: 0.0381, Validation Loss: 0.0558, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.095000\n",
      "Epoch  526, Training Loss: 0.0381, Validation Loss: 0.0557, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.094800\n",
      "Epoch  527, Training Loss: 0.0381, Validation Loss: 0.0556, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.094600\n",
      "Epoch  528, Training Loss: 0.0381, Validation Loss: 0.0554, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.094400\n",
      "Epoch  529, Training Loss: 0.0380, Validation Loss: 0.0556, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.094200\n",
      "Epoch  530, Training Loss: 0.0380, Validation Loss: 0.0556, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.094000\n",
      "Epoch  531, Training Loss: 0.0380, Validation Loss: 0.0553, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.093800\n",
      "Epoch  532, Training Loss: 0.0379, Validation Loss: 0.0554, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.093600\n",
      "Epoch  533, Training Loss: 0.0379, Validation Loss: 0.0557, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.093400\n",
      "Epoch  534, Training Loss: 0.0379, Validation Loss: 0.0552, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.093200\n",
      "Epoch  535, Training Loss: 0.0379, Validation Loss: 0.0554, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.093000\n",
      "Epoch  536, Training Loss: 0.0378, Validation Loss: 0.0553, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.092800\n",
      "Epoch  537, Training Loss: 0.0378, Validation Loss: 0.0549, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.092600\n",
      "Epoch  538, Training Loss: 0.0378, Validation Loss: 0.0548, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.092400\n",
      "Epoch  539, Training Loss: 0.0377, Validation Loss: 0.0549, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.092200\n",
      "Epoch  540, Training Loss: 0.0377, Validation Loss: 0.0548, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.092000\n",
      "Epoch  541, Training Loss: 0.0377, Validation Loss: 0.0549, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.091800\n",
      "Epoch  542, Training Loss: 0.0377, Validation Loss: 0.0551, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.091600\n",
      "Epoch  543, Training Loss: 0.0377, Validation Loss: 0.0547, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.091400\n",
      "Epoch  544, Training Loss: 0.0376, Validation Loss: 0.0548, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.091200\n",
      "Epoch  545, Training Loss: 0.0376, Validation Loss: 0.0547, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.091000\n",
      "Epoch  546, Training Loss: 0.0376, Validation Loss: 0.0547, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.090800\n",
      "Epoch  547, Training Loss: 0.0376, Validation Loss: 0.0549, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.090600\n",
      "Epoch  548, Training Loss: 0.0375, Validation Loss: 0.0548, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.090400\n",
      "Epoch  549, Training Loss: 0.0375, Validation Loss: 0.0548, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.090200\n",
      "Epoch  550, Training Loss: 0.0375, Validation Loss: 0.0548, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.090000\n",
      "Epoch  551, Training Loss: 0.0375, Validation Loss: 0.0544, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.089800\n",
      "Epoch  552, Training Loss: 0.0374, Validation Loss: 0.0546, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.089600\n",
      "Epoch  553, Training Loss: 0.0374, Validation Loss: 0.0545, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.089400\n",
      "Epoch  554, Training Loss: 0.0374, Validation Loss: 0.0544, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.089200\n",
      "Epoch  555, Training Loss: 0.0374, Validation Loss: 0.0543, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.089000\n",
      "Epoch  556, Training Loss: 0.0373, Validation Loss: 0.0543, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.088800\n",
      "Epoch  557, Training Loss: 0.0374, Validation Loss: 0.0542, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.088600\n",
      "Epoch  558, Training Loss: 0.0373, Validation Loss: 0.0541, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.088400\n",
      "Epoch  559, Training Loss: 0.0373, Validation Loss: 0.0541, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.088200\n",
      "Epoch  560, Training Loss: 0.0372, Validation Loss: 0.0542, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.088000\n",
      "Epoch  561, Training Loss: 0.0372, Validation Loss: 0.0541, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.087800\n",
      "Epoch  562, Training Loss: 0.0372, Validation Loss: 0.0541, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.087600\n",
      "Epoch  563, Training Loss: 0.0372, Validation Loss: 0.0540, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.087400\n",
      "Epoch  564, Training Loss: 0.0372, Validation Loss: 0.0540, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.087200\n",
      "Epoch  565, Training Loss: 0.0371, Validation Loss: 0.0540, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.087000\n",
      "Epoch  566, Training Loss: 0.0371, Validation Loss: 0.0540, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.086800\n",
      "Epoch  567, Training Loss: 0.0371, Validation Loss: 0.0538, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.086600\n",
      "Epoch  568, Training Loss: 0.0371, Validation Loss: 0.0538, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.086400\n",
      "Epoch  569, Training Loss: 0.0371, Validation Loss: 0.0539, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.086200\n",
      "Epoch  570, Training Loss: 0.0370, Validation Loss: 0.0538, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.086000\n",
      "Epoch  571, Training Loss: 0.0370, Validation Loss: 0.0539, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.085800\n",
      "Epoch  572, Training Loss: 0.0370, Validation Loss: 0.0540, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.085600\n",
      "Epoch  573, Training Loss: 0.0370, Validation Loss: 0.0538, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.085400\n",
      "Epoch  574, Training Loss: 0.0369, Validation Loss: 0.0537, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.085200\n",
      "Epoch  575, Training Loss: 0.0369, Validation Loss: 0.0535, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.085000\n",
      "Epoch  576, Training Loss: 0.0369, Validation Loss: 0.0535, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.084800\n",
      "Epoch  577, Training Loss: 0.0369, Validation Loss: 0.0535, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.084600\n",
      "Epoch  578, Training Loss: 0.0369, Validation Loss: 0.0535, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.084400\n",
      "Epoch  579, Training Loss: 0.0369, Validation Loss: 0.0535, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.084200\n",
      "Epoch  580, Training Loss: 0.0368, Validation Loss: 0.0534, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.084000\n",
      "Epoch  581, Training Loss: 0.0368, Validation Loss: 0.0533, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.083800\n",
      "Epoch  582, Training Loss: 0.0368, Validation Loss: 0.0534, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.083600\n",
      "Epoch  583, Training Loss: 0.0368, Validation Loss: 0.0532, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.083400\n",
      "Epoch  584, Training Loss: 0.0368, Validation Loss: 0.0530, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.083200\n",
      "Epoch  585, Training Loss: 0.0368, Validation Loss: 0.0531, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.083000\n",
      "Epoch  586, Training Loss: 0.0367, Validation Loss: 0.0532, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.082800\n",
      "Epoch  587, Training Loss: 0.0367, Validation Loss: 0.0531, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.082600\n",
      "Epoch  588, Training Loss: 0.0367, Validation Loss: 0.0531, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.082400\n",
      "Epoch  589, Training Loss: 0.0367, Validation Loss: 0.0529, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.082200\n",
      "Epoch  590, Training Loss: 0.0366, Validation Loss: 0.0529, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.082000\n",
      "Epoch  591, Training Loss: 0.0366, Validation Loss: 0.0528, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.081800\n",
      "Epoch  592, Training Loss: 0.0366, Validation Loss: 0.0527, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.081600\n",
      "Epoch  593, Training Loss: 0.0366, Validation Loss: 0.0527, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.081400\n",
      "Epoch  594, Training Loss: 0.0366, Validation Loss: 0.0527, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.081200\n",
      "Epoch  595, Training Loss: 0.0366, Validation Loss: 0.0527, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.081000\n",
      "Epoch  596, Training Loss: 0.0365, Validation Loss: 0.0527, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.080800\n",
      "Epoch  597, Training Loss: 0.0365, Validation Loss: 0.0526, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.080600\n",
      "Epoch  598, Training Loss: 0.0365, Validation Loss: 0.0528, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.080400\n",
      "Epoch  599, Training Loss: 0.0365, Validation Loss: 0.0526, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.080200\n",
      "Epoch  600, Training Loss: 0.0365, Validation Loss: 0.0526, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.080000\n",
      "Epoch  601, Training Loss: 0.0365, Validation Loss: 0.0527, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.079800\n",
      "Epoch  602, Training Loss: 0.0364, Validation Loss: 0.0526, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.079600\n",
      "Epoch  603, Training Loss: 0.0364, Validation Loss: 0.0526, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.079400\n",
      "Epoch  604, Training Loss: 0.0364, Validation Loss: 0.0523, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.079200\n",
      "Epoch  605, Training Loss: 0.0364, Validation Loss: 0.0524, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.079000\n",
      "Epoch  606, Training Loss: 0.0364, Validation Loss: 0.0524, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.078800\n",
      "Epoch  607, Training Loss: 0.0364, Validation Loss: 0.0523, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.078600\n",
      "Epoch  608, Training Loss: 0.0363, Validation Loss: 0.0523, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.078400\n",
      "Epoch  609, Training Loss: 0.0363, Validation Loss: 0.0521, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.078200\n",
      "Epoch  610, Training Loss: 0.0363, Validation Loss: 0.0524, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.078000\n",
      "Epoch  611, Training Loss: 0.0363, Validation Loss: 0.0524, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.077800\n",
      "Epoch  612, Training Loss: 0.0363, Validation Loss: 0.0523, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.077600\n",
      "Epoch  613, Training Loss: 0.0363, Validation Loss: 0.0522, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.077400\n",
      "Epoch  614, Training Loss: 0.0362, Validation Loss: 0.0522, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.077200\n",
      "Epoch  615, Training Loss: 0.0362, Validation Loss: 0.0521, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.077000\n",
      "Epoch  616, Training Loss: 0.0362, Validation Loss: 0.0522, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.076800\n",
      "Epoch  617, Training Loss: 0.0362, Validation Loss: 0.0522, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.076600\n",
      "Epoch  618, Training Loss: 0.0362, Validation Loss: 0.0520, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.076400\n",
      "Epoch  619, Training Loss: 0.0362, Validation Loss: 0.0522, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.076200\n",
      "Epoch  620, Training Loss: 0.0361, Validation Loss: 0.0522, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.076000\n",
      "Epoch  621, Training Loss: 0.0361, Validation Loss: 0.0520, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.075800\n",
      "Epoch  622, Training Loss: 0.0361, Validation Loss: 0.0520, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.075600\n",
      "Epoch  623, Training Loss: 0.0361, Validation Loss: 0.0518, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.075400\n",
      "Epoch  624, Training Loss: 0.0361, Validation Loss: 0.0519, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.075200\n",
      "Epoch  625, Training Loss: 0.0361, Validation Loss: 0.0521, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.075000\n",
      "Epoch  626, Training Loss: 0.0360, Validation Loss: 0.0520, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.074800\n",
      "Epoch  627, Training Loss: 0.0360, Validation Loss: 0.0519, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.074600\n",
      "Epoch  628, Training Loss: 0.0360, Validation Loss: 0.0518, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.074400\n",
      "Epoch  629, Training Loss: 0.0360, Validation Loss: 0.0519, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.074200\n",
      "Epoch  630, Training Loss: 0.0360, Validation Loss: 0.0516, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.074000\n",
      "Epoch  631, Training Loss: 0.0360, Validation Loss: 0.0516, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.073800\n",
      "Epoch  632, Training Loss: 0.0360, Validation Loss: 0.0517, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.073600\n",
      "Epoch  633, Training Loss: 0.0360, Validation Loss: 0.0518, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.073400\n",
      "Epoch  634, Training Loss: 0.0360, Validation Loss: 0.0519, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.073200\n",
      "Epoch  635, Training Loss: 0.0359, Validation Loss: 0.0517, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.073000\n",
      "Epoch  636, Training Loss: 0.0359, Validation Loss: 0.0517, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.072800\n",
      "Epoch  637, Training Loss: 0.0359, Validation Loss: 0.0517, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.072600\n",
      "Epoch  638, Training Loss: 0.0359, Validation Loss: 0.0517, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.072400\n",
      "Epoch  639, Training Loss: 0.0359, Validation Loss: 0.0517, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.072200\n",
      "Epoch  640, Training Loss: 0.0359, Validation Loss: 0.0517, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.072000\n",
      "Epoch  641, Training Loss: 0.0359, Validation Loss: 0.0516, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.071800\n",
      "Epoch  642, Training Loss: 0.0358, Validation Loss: 0.0517, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.071600\n",
      "Epoch  643, Training Loss: 0.0358, Validation Loss: 0.0517, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.071400\n",
      "Epoch  644, Training Loss: 0.0358, Validation Loss: 0.0516, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.071200\n",
      "Epoch  645, Training Loss: 0.0358, Validation Loss: 0.0515, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.071000\n",
      "Epoch  646, Training Loss: 0.0358, Validation Loss: 0.0516, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.070800\n",
      "Epoch  647, Training Loss: 0.0358, Validation Loss: 0.0516, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.070600\n",
      "Epoch  648, Training Loss: 0.0358, Validation Loss: 0.0515, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.070400\n",
      "Epoch  649, Training Loss: 0.0358, Validation Loss: 0.0514, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.070200\n",
      "Epoch  650, Training Loss: 0.0357, Validation Loss: 0.0514, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.070000\n",
      "Epoch  651, Training Loss: 0.0357, Validation Loss: 0.0513, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.069800\n",
      "Epoch  652, Training Loss: 0.0357, Validation Loss: 0.0512, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.069600\n",
      "Epoch  653, Training Loss: 0.0357, Validation Loss: 0.0512, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.069400\n",
      "Epoch  654, Training Loss: 0.0357, Validation Loss: 0.0511, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.069200\n",
      "Epoch  655, Training Loss: 0.0357, Validation Loss: 0.0513, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.069000\n",
      "Epoch  656, Training Loss: 0.0357, Validation Loss: 0.0513, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.068800\n",
      "Epoch  657, Training Loss: 0.0356, Validation Loss: 0.0513, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.068600\n",
      "Epoch  658, Training Loss: 0.0356, Validation Loss: 0.0512, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.068400\n",
      "Epoch  659, Training Loss: 0.0356, Validation Loss: 0.0513, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.068200\n",
      "Epoch  660, Training Loss: 0.0356, Validation Loss: 0.0511, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.068000\n",
      "Epoch  661, Training Loss: 0.0356, Validation Loss: 0.0511, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.067800\n",
      "Epoch  662, Training Loss: 0.0356, Validation Loss: 0.0511, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.067600\n",
      "Epoch  663, Training Loss: 0.0356, Validation Loss: 0.0512, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.067400\n",
      "Epoch  664, Training Loss: 0.0356, Validation Loss: 0.0511, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.067200\n",
      "Epoch  665, Training Loss: 0.0355, Validation Loss: 0.0511, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.067000\n",
      "Epoch  666, Training Loss: 0.0356, Validation Loss: 0.0511, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.066800\n",
      "Epoch  667, Training Loss: 0.0355, Validation Loss: 0.0512, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.066600\n",
      "Epoch  668, Training Loss: 0.0355, Validation Loss: 0.0512, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.066400\n",
      "Epoch  669, Training Loss: 0.0355, Validation Loss: 0.0511, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.066200\n",
      "Epoch  670, Training Loss: 0.0355, Validation Loss: 0.0511, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.066000\n",
      "Epoch  671, Training Loss: 0.0355, Validation Loss: 0.0511, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.065800\n",
      "Epoch  672, Training Loss: 0.0355, Validation Loss: 0.0510, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.065600\n",
      "Epoch  673, Training Loss: 0.0355, Validation Loss: 0.0510, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.065400\n",
      "Epoch  674, Training Loss: 0.0355, Validation Loss: 0.0511, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.065200\n",
      "Epoch  675, Training Loss: 0.0354, Validation Loss: 0.0509, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.065000\n",
      "Epoch  676, Training Loss: 0.0354, Validation Loss: 0.0510, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.064800\n",
      "Epoch  677, Training Loss: 0.0354, Validation Loss: 0.0510, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.064600\n",
      "Epoch  678, Training Loss: 0.0354, Validation Loss: 0.0511, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.064400\n",
      "Epoch  679, Training Loss: 0.0354, Validation Loss: 0.0509, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.064200\n",
      "Epoch  680, Training Loss: 0.0354, Validation Loss: 0.0510, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.064000\n",
      "Epoch  681, Training Loss: 0.0354, Validation Loss: 0.0510, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.063800\n",
      "Epoch  682, Training Loss: 0.0354, Validation Loss: 0.0509, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.063600\n",
      "Epoch  683, Training Loss: 0.0354, Validation Loss: 0.0508, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.063400\n",
      "Epoch  684, Training Loss: 0.0354, Validation Loss: 0.0507, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.063200\n",
      "Epoch  685, Training Loss: 0.0353, Validation Loss: 0.0507, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.063000\n",
      "Epoch  686, Training Loss: 0.0353, Validation Loss: 0.0507, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.062800\n",
      "Epoch  687, Training Loss: 0.0353, Validation Loss: 0.0507, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.062600\n",
      "Epoch  688, Training Loss: 0.0353, Validation Loss: 0.0507, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.062400\n",
      "Epoch  689, Training Loss: 0.0353, Validation Loss: 0.0507, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.062200\n",
      "Epoch  690, Training Loss: 0.0353, Validation Loss: 0.0506, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.062000\n",
      "Epoch  691, Training Loss: 0.0353, Validation Loss: 0.0507, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.061800\n",
      "Epoch  692, Training Loss: 0.0353, Validation Loss: 0.0505, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.061600\n",
      "Epoch  693, Training Loss: 0.0353, Validation Loss: 0.0505, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.061400\n",
      "Epoch  694, Training Loss: 0.0353, Validation Loss: 0.0505, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.061200\n",
      "Epoch  695, Training Loss: 0.0352, Validation Loss: 0.0505, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.061000\n",
      "Epoch  696, Training Loss: 0.0352, Validation Loss: 0.0505, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.060800\n",
      "Epoch  697, Training Loss: 0.0352, Validation Loss: 0.0504, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.060600\n",
      "Epoch  698, Training Loss: 0.0352, Validation Loss: 0.0504, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.060400\n",
      "Epoch  699, Training Loss: 0.0352, Validation Loss: 0.0504, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.060200\n",
      "Epoch  700, Training Loss: 0.0352, Validation Loss: 0.0505, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.060000\n",
      "Epoch  701, Training Loss: 0.0352, Validation Loss: 0.0504, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.059800\n",
      "Epoch  702, Training Loss: 0.0352, Validation Loss: 0.0504, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.059600\n",
      "Epoch  703, Training Loss: 0.0352, Validation Loss: 0.0504, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.059400\n",
      "Epoch  704, Training Loss: 0.0352, Validation Loss: 0.0504, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.059200\n",
      "Epoch  705, Training Loss: 0.0352, Validation Loss: 0.0503, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.059000\n",
      "Epoch  706, Training Loss: 0.0351, Validation Loss: 0.0504, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.058800\n",
      "Epoch  707, Training Loss: 0.0351, Validation Loss: 0.0503, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.058600\n",
      "Epoch  708, Training Loss: 0.0351, Validation Loss: 0.0503, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.058400\n",
      "Epoch  709, Training Loss: 0.0351, Validation Loss: 0.0503, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.058200\n",
      "Epoch  710, Training Loss: 0.0351, Validation Loss: 0.0503, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.058000\n",
      "Epoch  711, Training Loss: 0.0351, Validation Loss: 0.0504, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.057800\n",
      "Epoch  712, Training Loss: 0.0351, Validation Loss: 0.0503, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.057600\n",
      "Epoch  713, Training Loss: 0.0351, Validation Loss: 0.0504, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.057400\n",
      "Epoch  714, Training Loss: 0.0351, Validation Loss: 0.0503, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.057200\n",
      "Epoch  715, Training Loss: 0.0351, Validation Loss: 0.0503, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.057000\n",
      "Epoch  716, Training Loss: 0.0351, Validation Loss: 0.0503, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.056800\n",
      "Epoch  717, Training Loss: 0.0351, Validation Loss: 0.0503, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.056600\n",
      "Epoch  718, Training Loss: 0.0350, Validation Loss: 0.0501, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.056400\n",
      "Epoch  719, Training Loss: 0.0350, Validation Loss: 0.0501, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.056200\n",
      "Epoch  720, Training Loss: 0.0350, Validation Loss: 0.0501, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.056000\n",
      "Epoch  721, Training Loss: 0.0350, Validation Loss: 0.0502, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.055800\n",
      "Epoch  722, Training Loss: 0.0350, Validation Loss: 0.0501, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.055600\n",
      "Epoch  723, Training Loss: 0.0350, Validation Loss: 0.0501, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.055400\n",
      "Epoch  724, Training Loss: 0.0350, Validation Loss: 0.0500, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.055200\n",
      "Epoch  725, Training Loss: 0.0350, Validation Loss: 0.0499, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.055000\n",
      "Epoch  726, Training Loss: 0.0350, Validation Loss: 0.0500, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.054800\n",
      "Epoch  727, Training Loss: 0.0350, Validation Loss: 0.0500, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.054600\n",
      "Epoch  728, Training Loss: 0.0350, Validation Loss: 0.0498, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.054400\n",
      "Epoch  729, Training Loss: 0.0350, Validation Loss: 0.0498, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.054200\n",
      "Epoch  730, Training Loss: 0.0350, Validation Loss: 0.0498, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.054000\n",
      "Epoch  731, Training Loss: 0.0349, Validation Loss: 0.0498, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.053800\n",
      "Epoch  732, Training Loss: 0.0349, Validation Loss: 0.0498, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.053600\n",
      "Epoch  733, Training Loss: 0.0349, Validation Loss: 0.0498, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.053400\n",
      "Epoch  734, Training Loss: 0.0349, Validation Loss: 0.0499, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.053200\n",
      "Epoch  735, Training Loss: 0.0349, Validation Loss: 0.0497, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.053000\n",
      "Epoch  736, Training Loss: 0.0349, Validation Loss: 0.0498, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.052800\n",
      "Epoch  737, Training Loss: 0.0349, Validation Loss: 0.0498, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.052600\n",
      "Epoch  738, Training Loss: 0.0349, Validation Loss: 0.0498, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.052400\n",
      "Epoch  739, Training Loss: 0.0349, Validation Loss: 0.0497, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.052200\n",
      "Epoch  740, Training Loss: 0.0349, Validation Loss: 0.0498, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.052000\n",
      "Epoch  741, Training Loss: 0.0349, Validation Loss: 0.0498, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.051800\n",
      "Epoch  742, Training Loss: 0.0349, Validation Loss: 0.0498, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.051600\n",
      "Epoch  743, Training Loss: 0.0349, Validation Loss: 0.0498, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.051400\n",
      "Epoch  744, Training Loss: 0.0349, Validation Loss: 0.0497, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.051200\n",
      "Epoch  745, Training Loss: 0.0349, Validation Loss: 0.0498, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.051000\n",
      "Epoch  746, Training Loss: 0.0348, Validation Loss: 0.0497, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.050800\n",
      "Epoch  747, Training Loss: 0.0348, Validation Loss: 0.0497, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.050600\n",
      "Epoch  748, Training Loss: 0.0348, Validation Loss: 0.0497, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.050400\n",
      "Epoch  749, Training Loss: 0.0348, Validation Loss: 0.0498, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.050200\n",
      "Epoch  750, Training Loss: 0.0348, Validation Loss: 0.0498, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.050000\n",
      "Epoch  751, Training Loss: 0.0348, Validation Loss: 0.0498, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.049800\n",
      "Epoch  752, Training Loss: 0.0348, Validation Loss: 0.0497, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.049600\n",
      "Epoch  753, Training Loss: 0.0348, Validation Loss: 0.0497, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.049400\n",
      "Epoch  754, Training Loss: 0.0348, Validation Loss: 0.0498, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.049200\n",
      "Epoch  755, Training Loss: 0.0348, Validation Loss: 0.0496, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.049000\n",
      "Epoch  756, Training Loss: 0.0348, Validation Loss: 0.0496, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.048800\n",
      "Epoch  757, Training Loss: 0.0348, Validation Loss: 0.0497, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.048600\n",
      "Epoch  758, Training Loss: 0.0348, Validation Loss: 0.0497, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.048400\n",
      "Epoch  759, Training Loss: 0.0348, Validation Loss: 0.0497, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.048200\n",
      "Epoch  760, Training Loss: 0.0348, Validation Loss: 0.0497, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.048000\n",
      "Epoch  761, Training Loss: 0.0348, Validation Loss: 0.0497, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.047800\n",
      "Epoch  762, Training Loss: 0.0347, Validation Loss: 0.0496, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.047600\n",
      "Epoch  763, Training Loss: 0.0347, Validation Loss: 0.0497, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.047400\n",
      "Epoch  764, Training Loss: 0.0347, Validation Loss: 0.0496, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.047200\n",
      "Epoch  765, Training Loss: 0.0347, Validation Loss: 0.0496, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.047000\n",
      "Epoch  766, Training Loss: 0.0347, Validation Loss: 0.0495, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.046800\n",
      "Epoch  767, Training Loss: 0.0347, Validation Loss: 0.0495, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.046600\n",
      "Epoch  768, Training Loss: 0.0347, Validation Loss: 0.0495, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.046400\n",
      "Epoch  769, Training Loss: 0.0347, Validation Loss: 0.0494, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.046200\n",
      "Epoch  770, Training Loss: 0.0347, Validation Loss: 0.0493, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.046000\n",
      "Epoch  771, Training Loss: 0.0347, Validation Loss: 0.0494, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.045800\n",
      "Epoch  772, Training Loss: 0.0347, Validation Loss: 0.0494, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.045600\n",
      "Epoch  773, Training Loss: 0.0347, Validation Loss: 0.0494, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.045400\n",
      "Epoch  774, Training Loss: 0.0347, Validation Loss: 0.0493, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.045200\n",
      "Epoch  775, Training Loss: 0.0347, Validation Loss: 0.0494, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.045000\n",
      "Epoch  776, Training Loss: 0.0347, Validation Loss: 0.0494, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.044800\n",
      "Epoch  777, Training Loss: 0.0347, Validation Loss: 0.0494, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.044600\n",
      "Epoch  778, Training Loss: 0.0347, Validation Loss: 0.0494, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.044400\n",
      "Epoch  779, Training Loss: 0.0346, Validation Loss: 0.0494, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.044200\n",
      "Epoch  780, Training Loss: 0.0346, Validation Loss: 0.0494, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.044000\n",
      "Epoch  781, Training Loss: 0.0346, Validation Loss: 0.0493, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.043800\n",
      "Epoch  782, Training Loss: 0.0346, Validation Loss: 0.0493, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.043600\n",
      "Epoch  783, Training Loss: 0.0346, Validation Loss: 0.0493, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.043400\n",
      "Epoch  784, Training Loss: 0.0346, Validation Loss: 0.0493, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.043200\n",
      "Epoch  785, Training Loss: 0.0346, Validation Loss: 0.0494, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.043000\n",
      "Epoch  786, Training Loss: 0.0346, Validation Loss: 0.0492, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.042800\n",
      "Epoch  787, Training Loss: 0.0346, Validation Loss: 0.0492, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.042600\n",
      "Epoch  788, Training Loss: 0.0346, Validation Loss: 0.0492, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.042400\n",
      "Epoch  789, Training Loss: 0.0346, Validation Loss: 0.0492, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.042200\n",
      "Epoch  790, Training Loss: 0.0346, Validation Loss: 0.0492, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.042000\n",
      "Epoch  791, Training Loss: 0.0346, Validation Loss: 0.0492, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.041800\n",
      "Epoch  792, Training Loss: 0.0346, Validation Loss: 0.0492, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.041600\n",
      "Epoch  793, Training Loss: 0.0346, Validation Loss: 0.0492, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.041400\n",
      "Epoch  794, Training Loss: 0.0346, Validation Loss: 0.0492, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.041200\n",
      "Epoch  795, Training Loss: 0.0346, Validation Loss: 0.0492, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.041000\n",
      "Epoch  796, Training Loss: 0.0346, Validation Loss: 0.0492, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.040800\n",
      "Epoch  797, Training Loss: 0.0346, Validation Loss: 0.0492, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.040600\n",
      "Epoch  798, Training Loss: 0.0346, Validation Loss: 0.0492, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.040400\n",
      "Epoch  799, Training Loss: 0.0346, Validation Loss: 0.0492, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.040200\n",
      "Epoch  800, Training Loss: 0.0345, Validation Loss: 0.0492, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.040000\n",
      "Epoch  801, Training Loss: 0.0345, Validation Loss: 0.0491, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.039800\n",
      "Epoch  802, Training Loss: 0.0345, Validation Loss: 0.0492, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.039600\n",
      "Epoch  803, Training Loss: 0.0345, Validation Loss: 0.0491, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.039400\n",
      "Epoch  804, Training Loss: 0.0345, Validation Loss: 0.0491, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.039200\n",
      "Epoch  805, Training Loss: 0.0345, Validation Loss: 0.0491, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.039000\n",
      "Epoch  806, Training Loss: 0.0345, Validation Loss: 0.0491, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.038800\n",
      "Epoch  807, Training Loss: 0.0345, Validation Loss: 0.0491, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.038600\n",
      "Epoch  808, Training Loss: 0.0345, Validation Loss: 0.0490, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.038400\n",
      "Epoch  809, Training Loss: 0.0345, Validation Loss: 0.0489, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.038200\n",
      "Epoch  810, Training Loss: 0.0345, Validation Loss: 0.0489, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.038000\n",
      "Epoch  811, Training Loss: 0.0345, Validation Loss: 0.0489, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.037800\n",
      "Epoch  812, Training Loss: 0.0345, Validation Loss: 0.0489, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.037600\n",
      "Epoch  813, Training Loss: 0.0345, Validation Loss: 0.0489, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.037400\n",
      "Epoch  814, Training Loss: 0.0345, Validation Loss: 0.0489, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.037200\n",
      "Epoch  815, Training Loss: 0.0345, Validation Loss: 0.0489, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.037000\n",
      "Epoch  816, Training Loss: 0.0345, Validation Loss: 0.0489, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.036800\n",
      "Epoch  817, Training Loss: 0.0345, Validation Loss: 0.0489, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.036600\n",
      "Epoch  818, Training Loss: 0.0345, Validation Loss: 0.0488, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.036400\n",
      "Epoch  819, Training Loss: 0.0345, Validation Loss: 0.0488, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.036200\n",
      "Epoch  820, Training Loss: 0.0345, Validation Loss: 0.0489, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.036000\n",
      "Epoch  821, Training Loss: 0.0345, Validation Loss: 0.0488, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.035800\n",
      "Epoch  822, Training Loss: 0.0345, Validation Loss: 0.0487, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.035600\n",
      "Epoch  823, Training Loss: 0.0344, Validation Loss: 0.0488, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.035400\n",
      "Epoch  824, Training Loss: 0.0344, Validation Loss: 0.0487, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.035200\n",
      "Epoch  825, Training Loss: 0.0344, Validation Loss: 0.0487, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.035000\n",
      "Epoch  826, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.034800\n",
      "Epoch  827, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.034600\n",
      "Epoch  828, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.034400\n",
      "Epoch  829, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.034200\n",
      "Epoch  830, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.034000\n",
      "Epoch  831, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.033800\n",
      "Epoch  832, Training Loss: 0.0344, Validation Loss: 0.0487, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.033600\n",
      "Epoch  833, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.033400\n",
      "Epoch  834, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.033200\n",
      "Epoch  835, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.033000\n",
      "Epoch  836, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.032800\n",
      "Epoch  837, Training Loss: 0.0344, Validation Loss: 0.0485, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.032600\n",
      "Epoch  838, Training Loss: 0.0344, Validation Loss: 0.0485, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.032400\n",
      "Epoch  839, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.032200\n",
      "Epoch  840, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.032000\n",
      "Epoch  841, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.031800\n",
      "Epoch  842, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.031600\n",
      "Epoch  843, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.031400\n",
      "Epoch  844, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.031200\n",
      "Epoch  845, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.031000\n",
      "Epoch  846, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.030800\n",
      "Epoch  847, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.030600\n",
      "Epoch  848, Training Loss: 0.0344, Validation Loss: 0.0487, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.030400\n",
      "Epoch  849, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.030200\n",
      "Epoch  850, Training Loss: 0.0344, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.030000\n",
      "Epoch  851, Training Loss: 0.0343, Validation Loss: 0.0486, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.029800\n",
      "Epoch  852, Training Loss: 0.0343, Validation Loss: 0.0485, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.029600\n",
      "Epoch  853, Training Loss: 0.0343, Validation Loss: 0.0485, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.029400\n",
      "Epoch  854, Training Loss: 0.0343, Validation Loss: 0.0485, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.029200\n",
      "Epoch  855, Training Loss: 0.0343, Validation Loss: 0.0485, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.029000\n",
      "Epoch  856, Training Loss: 0.0343, Validation Loss: 0.0485, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.028800\n",
      "Epoch  857, Training Loss: 0.0343, Validation Loss: 0.0485, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.028600\n",
      "Epoch  858, Training Loss: 0.0343, Validation Loss: 0.0484, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.028400\n",
      "Epoch  859, Training Loss: 0.0343, Validation Loss: 0.0485, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.028200\n",
      "Epoch  860, Training Loss: 0.0343, Validation Loss: 0.0485, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.028000\n",
      "Epoch  861, Training Loss: 0.0343, Validation Loss: 0.0485, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.027800\n",
      "Epoch  862, Training Loss: 0.0343, Validation Loss: 0.0485, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.027600\n",
      "Epoch  863, Training Loss: 0.0343, Validation Loss: 0.0485, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.027400\n",
      "Epoch  864, Training Loss: 0.0343, Validation Loss: 0.0484, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.027200\n",
      "Epoch  865, Training Loss: 0.0343, Validation Loss: 0.0484, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.027000\n",
      "Epoch  866, Training Loss: 0.0343, Validation Loss: 0.0485, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.026800\n",
      "Epoch  867, Training Loss: 0.0343, Validation Loss: 0.0484, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.026600\n",
      "Epoch  868, Training Loss: 0.0343, Validation Loss: 0.0485, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.026400\n",
      "Epoch  869, Training Loss: 0.0343, Validation Loss: 0.0484, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.026200\n",
      "Epoch  870, Training Loss: 0.0343, Validation Loss: 0.0485, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.026000\n",
      "Epoch  871, Training Loss: 0.0343, Validation Loss: 0.0484, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.025800\n",
      "Epoch  872, Training Loss: 0.0343, Validation Loss: 0.0484, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.025600\n",
      "Epoch  873, Training Loss: 0.0343, Validation Loss: 0.0484, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.025400\n",
      "Epoch  874, Training Loss: 0.0343, Validation Loss: 0.0484, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.025200\n",
      "Epoch  875, Training Loss: 0.0343, Validation Loss: 0.0484, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.025000\n",
      "Epoch  876, Training Loss: 0.0343, Validation Loss: 0.0484, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.024800\n",
      "Epoch  877, Training Loss: 0.0343, Validation Loss: 0.0484, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.024600\n",
      "Epoch  878, Training Loss: 0.0343, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.024400\n",
      "Epoch  879, Training Loss: 0.0343, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.024200\n",
      "Epoch  880, Training Loss: 0.0343, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.024000\n",
      "Epoch  881, Training Loss: 0.0343, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.023800\n",
      "Epoch  882, Training Loss: 0.0343, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.023600\n",
      "Epoch  883, Training Loss: 0.0343, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.023400\n",
      "Epoch  884, Training Loss: 0.0343, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.023200\n",
      "Epoch  885, Training Loss: 0.0343, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.023000\n",
      "Epoch  886, Training Loss: 0.0343, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.022800\n",
      "Epoch  887, Training Loss: 0.0343, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.022600\n",
      "Epoch  888, Training Loss: 0.0342, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.022400\n",
      "Epoch  889, Training Loss: 0.0342, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.022200\n",
      "Epoch  890, Training Loss: 0.0342, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.022000\n",
      "Epoch  891, Training Loss: 0.0342, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.021800\n",
      "Epoch  892, Training Loss: 0.0342, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.021600\n",
      "Epoch  893, Training Loss: 0.0342, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.021400\n",
      "Epoch  894, Training Loss: 0.0342, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.021200\n",
      "Epoch  895, Training Loss: 0.0342, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.021000\n",
      "Epoch  896, Training Loss: 0.0342, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.020800\n",
      "Epoch  897, Training Loss: 0.0342, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.020600\n",
      "Epoch  898, Training Loss: 0.0342, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.020400\n",
      "Epoch  899, Training Loss: 0.0342, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.020200\n",
      "Epoch  900, Training Loss: 0.0342, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.020000\n",
      "Epoch  901, Training Loss: 0.0342, Validation Loss: 0.0482, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.019800\n",
      "Epoch  902, Training Loss: 0.0342, Validation Loss: 0.0482, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.019600\n",
      "Epoch  903, Training Loss: 0.0342, Validation Loss: 0.0482, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.019400\n",
      "Epoch  904, Training Loss: 0.0342, Validation Loss: 0.0482, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.019200\n",
      "Epoch  905, Training Loss: 0.0342, Validation Loss: 0.0482, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.019000\n",
      "Epoch  906, Training Loss: 0.0342, Validation Loss: 0.0483, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.018800\n",
      "Epoch  907, Training Loss: 0.0342, Validation Loss: 0.0482, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.018600\n",
      "Epoch  908, Training Loss: 0.0342, Validation Loss: 0.0482, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.018400\n",
      "Epoch  909, Training Loss: 0.0342, Validation Loss: 0.0482, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.018200\n",
      "Epoch  910, Training Loss: 0.0342, Validation Loss: 0.0482, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.018000\n",
      "Epoch  911, Training Loss: 0.0342, Validation Loss: 0.0482, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.017800\n",
      "Epoch  912, Training Loss: 0.0342, Validation Loss: 0.0482, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.017600\n",
      "Epoch  913, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.017400\n",
      "Epoch  914, Training Loss: 0.0342, Validation Loss: 0.0482, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.017200\n",
      "Epoch  915, Training Loss: 0.0342, Validation Loss: 0.0482, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.017000\n",
      "Epoch  916, Training Loss: 0.0342, Validation Loss: 0.0482, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.016800\n",
      "Epoch  917, Training Loss: 0.0342, Validation Loss: 0.0482, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.016600\n",
      "Epoch  918, Training Loss: 0.0342, Validation Loss: 0.0482, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.016400\n",
      "Epoch  919, Training Loss: 0.0342, Validation Loss: 0.0482, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.016200\n",
      "Epoch  920, Training Loss: 0.0342, Validation Loss: 0.0482, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.016000\n",
      "Epoch  921, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.015800\n",
      "Epoch  922, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.015600\n",
      "Epoch  923, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.015400\n",
      "Epoch  924, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.015200\n",
      "Epoch  925, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.015000\n",
      "Epoch  926, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.014800\n",
      "Epoch  927, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.014600\n",
      "Epoch  928, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.014400\n",
      "Epoch  929, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.014200\n",
      "Epoch  930, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.014000\n",
      "Epoch  931, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.013800\n",
      "Epoch  932, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.013600\n",
      "Epoch  933, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.013400\n",
      "Epoch  934, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.013200\n",
      "Epoch  935, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.013000\n",
      "Epoch  936, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.012800\n",
      "Epoch  937, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.012600\n",
      "Epoch  938, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.012400\n",
      "Epoch  939, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.012200\n",
      "Epoch  940, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.012000\n",
      "Epoch  941, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.011800\n",
      "Epoch  942, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.011600\n",
      "Epoch  943, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.011400\n",
      "Epoch  944, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.011200\n",
      "Epoch  945, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.011000\n",
      "Epoch  946, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.010800\n",
      "Epoch  947, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.010600\n",
      "Epoch  948, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.010400\n",
      "Epoch  949, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.010200\n",
      "Epoch  950, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.010000\n",
      "Epoch  951, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.009800\n",
      "Epoch  952, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.009600\n",
      "Epoch  953, Training Loss: 0.0342, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.009400\n",
      "Epoch  954, Training Loss: 0.0341, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.009200\n",
      "Epoch  955, Training Loss: 0.0341, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.009000\n",
      "Epoch  956, Training Loss: 0.0341, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.008800\n",
      "Epoch  957, Training Loss: 0.0341, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.008600\n",
      "Epoch  958, Training Loss: 0.0341, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.008400\n",
      "Epoch  959, Training Loss: 0.0341, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.008200\n",
      "Epoch  960, Training Loss: 0.0341, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.008000\n",
      "Epoch  961, Training Loss: 0.0341, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.007800\n",
      "Epoch  962, Training Loss: 0.0341, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.007600\n",
      "Epoch  963, Training Loss: 0.0341, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.007400\n",
      "Epoch  964, Training Loss: 0.0341, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.007200\n",
      "Epoch  965, Training Loss: 0.0341, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.007000\n",
      "Epoch  966, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.006800\n",
      "Epoch  967, Training Loss: 0.0341, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.006600\n",
      "Epoch  968, Training Loss: 0.0341, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.006400\n",
      "Epoch  969, Training Loss: 0.0341, Validation Loss: 0.0481, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.006200\n",
      "Epoch  970, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.006000\n",
      "Epoch  971, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.005800\n",
      "Epoch  972, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.005600\n",
      "Epoch  973, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.005400\n",
      "Epoch  974, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.005200\n",
      "Epoch  975, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.005000\n",
      "Epoch  976, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.004800\n",
      "Epoch  977, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.004600\n",
      "Epoch  978, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.004400\n",
      "Epoch  979, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.004200\n",
      "Epoch  980, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.004000\n",
      "Epoch  981, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.003800\n",
      "Epoch  982, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.003600\n",
      "Epoch  983, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.003400\n",
      "Epoch  984, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.003200\n",
      "Epoch  985, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.003000\n",
      "Epoch  986, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.002800\n",
      "Epoch  987, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.002600\n",
      "Epoch  988, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.002400\n",
      "Epoch  989, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.002200\n",
      "Epoch  990, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.002000\n",
      "Epoch  991, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.001800\n",
      "Epoch  992, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.001600\n",
      "Epoch  993, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.001400\n",
      "Epoch  994, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.001200\n",
      "Epoch  995, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.001000\n",
      "Epoch  996, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.000800\n",
      "Epoch  997, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.000600\n",
      "Epoch  998, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.000400\n",
      "Epoch  999, Training Loss: 0.0341, Validation Loss: 0.0480, Training Acc: 1.0000, Validation Acc: 1.0000, Learning Rate: 0.000200\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0, Training Loss: 0.2577, Training Acc: 0.5161, Learning Rate: 0.200000\n",
      "Epoch    1, Training Loss: 0.2520, Training Acc: 0.5323, Learning Rate: 0.199800\n",
      "Epoch    2, Training Loss: 0.2468, Training Acc: 0.5403, Learning Rate: 0.199600\n",
      "Epoch    3, Training Loss: 0.2420, Training Acc: 0.5645, Learning Rate: 0.199400\n",
      "Epoch    4, Training Loss: 0.2378, Training Acc: 0.5806, Learning Rate: 0.199200\n",
      "Epoch    5, Training Loss: 0.2334, Training Acc: 0.6290, Learning Rate: 0.199000\n",
      "Epoch    6, Training Loss: 0.2291, Training Acc: 0.6371, Learning Rate: 0.198800\n",
      "Epoch    7, Training Loss: 0.2249, Training Acc: 0.6613, Learning Rate: 0.198600\n",
      "Epoch    8, Training Loss: 0.2209, Training Acc: 0.6774, Learning Rate: 0.198400\n",
      "Epoch    9, Training Loss: 0.2162, Training Acc: 0.6774, Learning Rate: 0.198200\n",
      "Epoch   10, Training Loss: 0.2101, Training Acc: 0.7258, Learning Rate: 0.198000\n",
      "Epoch   11, Training Loss: 0.2047, Training Acc: 0.7581, Learning Rate: 0.197800\n",
      "Epoch   12, Training Loss: 0.1998, Training Acc: 0.7661, Learning Rate: 0.197600\n",
      "Epoch   13, Training Loss: 0.1946, Training Acc: 0.7903, Learning Rate: 0.197400\n",
      "Epoch   14, Training Loss: 0.1890, Training Acc: 0.7903, Learning Rate: 0.197200\n",
      "Epoch   15, Training Loss: 0.1830, Training Acc: 0.7903, Learning Rate: 0.197000\n",
      "Epoch   16, Training Loss: 0.1776, Training Acc: 0.7984, Learning Rate: 0.196800\n",
      "Epoch   17, Training Loss: 0.1732, Training Acc: 0.8065, Learning Rate: 0.196600\n",
      "Epoch   18, Training Loss: 0.1693, Training Acc: 0.8226, Learning Rate: 0.196400\n",
      "Epoch   19, Training Loss: 0.1657, Training Acc: 0.8306, Learning Rate: 0.196200\n",
      "Epoch   20, Training Loss: 0.1625, Training Acc: 0.8387, Learning Rate: 0.196000\n",
      "Epoch   21, Training Loss: 0.1592, Training Acc: 0.8387, Learning Rate: 0.195800\n",
      "Epoch   22, Training Loss: 0.1564, Training Acc: 0.8387, Learning Rate: 0.195600\n",
      "Epoch   23, Training Loss: 0.1536, Training Acc: 0.8387, Learning Rate: 0.195400\n",
      "Epoch   24, Training Loss: 0.1511, Training Acc: 0.8468, Learning Rate: 0.195200\n",
      "Epoch   25, Training Loss: 0.1487, Training Acc: 0.8629, Learning Rate: 0.195000\n",
      "Epoch   26, Training Loss: 0.1466, Training Acc: 0.8629, Learning Rate: 0.194800\n",
      "Epoch   27, Training Loss: 0.1446, Training Acc: 0.8629, Learning Rate: 0.194600\n",
      "Epoch   28, Training Loss: 0.1426, Training Acc: 0.8629, Learning Rate: 0.194400\n",
      "Epoch   29, Training Loss: 0.1409, Training Acc: 0.8629, Learning Rate: 0.194200\n",
      "Epoch   30, Training Loss: 0.1392, Training Acc: 0.8629, Learning Rate: 0.194000\n",
      "Epoch   31, Training Loss: 0.1379, Training Acc: 0.8710, Learning Rate: 0.193800\n",
      "Epoch   32, Training Loss: 0.1364, Training Acc: 0.8710, Learning Rate: 0.193600\n",
      "Epoch   33, Training Loss: 0.1347, Training Acc: 0.8710, Learning Rate: 0.193400\n",
      "Epoch   34, Training Loss: 0.1334, Training Acc: 0.8790, Learning Rate: 0.193200\n",
      "Epoch   35, Training Loss: 0.1318, Training Acc: 0.8710, Learning Rate: 0.193000\n",
      "Epoch   36, Training Loss: 0.1305, Training Acc: 0.8710, Learning Rate: 0.192800\n",
      "Epoch   37, Training Loss: 0.1292, Training Acc: 0.8629, Learning Rate: 0.192600\n",
      "Epoch   38, Training Loss: 0.1280, Training Acc: 0.8629, Learning Rate: 0.192400\n",
      "Epoch   39, Training Loss: 0.1266, Training Acc: 0.8629, Learning Rate: 0.192200\n",
      "Epoch   40, Training Loss: 0.1253, Training Acc: 0.8790, Learning Rate: 0.192000\n",
      "Epoch   41, Training Loss: 0.1243, Training Acc: 0.8871, Learning Rate: 0.191800\n",
      "Epoch   42, Training Loss: 0.1229, Training Acc: 0.8790, Learning Rate: 0.191600\n",
      "Epoch   43, Training Loss: 0.1217, Training Acc: 0.8790, Learning Rate: 0.191400\n",
      "Epoch   44, Training Loss: 0.1207, Training Acc: 0.8871, Learning Rate: 0.191200\n",
      "Epoch   45, Training Loss: 0.1196, Training Acc: 0.8871, Learning Rate: 0.191000\n",
      "Epoch   46, Training Loss: 0.1184, Training Acc: 0.8790, Learning Rate: 0.190800\n",
      "Epoch   47, Training Loss: 0.1174, Training Acc: 0.8790, Learning Rate: 0.190600\n",
      "Epoch   48, Training Loss: 0.1164, Training Acc: 0.8871, Learning Rate: 0.190400\n",
      "Epoch   49, Training Loss: 0.1154, Training Acc: 0.8952, Learning Rate: 0.190200\n",
      "Epoch   50, Training Loss: 0.1144, Training Acc: 0.9032, Learning Rate: 0.190000\n",
      "Epoch   51, Training Loss: 0.1134, Training Acc: 0.9032, Learning Rate: 0.189800\n",
      "Epoch   52, Training Loss: 0.1125, Training Acc: 0.8952, Learning Rate: 0.189600\n",
      "Epoch   53, Training Loss: 0.1115, Training Acc: 0.9032, Learning Rate: 0.189400\n",
      "Epoch   54, Training Loss: 0.1105, Training Acc: 0.9113, Learning Rate: 0.189200\n",
      "Epoch   55, Training Loss: 0.1096, Training Acc: 0.9113, Learning Rate: 0.189000\n",
      "Epoch   56, Training Loss: 0.1088, Training Acc: 0.9113, Learning Rate: 0.188800\n",
      "Epoch   57, Training Loss: 0.1078, Training Acc: 0.9113, Learning Rate: 0.188600\n",
      "Epoch   58, Training Loss: 0.1070, Training Acc: 0.9113, Learning Rate: 0.188400\n",
      "Epoch   59, Training Loss: 0.1062, Training Acc: 0.9113, Learning Rate: 0.188200\n",
      "Epoch   60, Training Loss: 0.1054, Training Acc: 0.9113, Learning Rate: 0.188000\n",
      "Epoch   61, Training Loss: 0.1045, Training Acc: 0.9113, Learning Rate: 0.187800\n",
      "Epoch   62, Training Loss: 0.1038, Training Acc: 0.9113, Learning Rate: 0.187600\n",
      "Epoch   63, Training Loss: 0.1033, Training Acc: 0.9113, Learning Rate: 0.187400\n",
      "Epoch   64, Training Loss: 0.1020, Training Acc: 0.9113, Learning Rate: 0.187200\n",
      "Epoch   65, Training Loss: 0.1012, Training Acc: 0.9113, Learning Rate: 0.187000\n",
      "Epoch   66, Training Loss: 0.1004, Training Acc: 0.9113, Learning Rate: 0.186800\n",
      "Epoch   67, Training Loss: 0.0996, Training Acc: 0.9113, Learning Rate: 0.186600\n",
      "Epoch   68, Training Loss: 0.0989, Training Acc: 0.9194, Learning Rate: 0.186400\n",
      "Epoch   69, Training Loss: 0.0981, Training Acc: 0.9113, Learning Rate: 0.186200\n",
      "Epoch   70, Training Loss: 0.0975, Training Acc: 0.9274, Learning Rate: 0.186000\n",
      "Epoch   71, Training Loss: 0.0967, Training Acc: 0.9274, Learning Rate: 0.185800\n",
      "Epoch   72, Training Loss: 0.0959, Training Acc: 0.9274, Learning Rate: 0.185600\n",
      "Epoch   73, Training Loss: 0.0955, Training Acc: 0.9274, Learning Rate: 0.185400\n",
      "Epoch   74, Training Loss: 0.0945, Training Acc: 0.9274, Learning Rate: 0.185200\n",
      "Epoch   75, Training Loss: 0.0938, Training Acc: 0.9274, Learning Rate: 0.185000\n",
      "Epoch   76, Training Loss: 0.0930, Training Acc: 0.9274, Learning Rate: 0.184800\n",
      "Epoch   77, Training Loss: 0.0923, Training Acc: 0.9274, Learning Rate: 0.184600\n",
      "Epoch   78, Training Loss: 0.0916, Training Acc: 0.9274, Learning Rate: 0.184400\n",
      "Epoch   79, Training Loss: 0.0911, Training Acc: 0.9274, Learning Rate: 0.184200\n",
      "Epoch   80, Training Loss: 0.0903, Training Acc: 0.9274, Learning Rate: 0.184000\n",
      "Epoch   81, Training Loss: 0.0898, Training Acc: 0.9274, Learning Rate: 0.183800\n",
      "Epoch   82, Training Loss: 0.0891, Training Acc: 0.9274, Learning Rate: 0.183600\n",
      "Epoch   83, Training Loss: 0.0886, Training Acc: 0.9355, Learning Rate: 0.183400\n",
      "Epoch   84, Training Loss: 0.0877, Training Acc: 0.9355, Learning Rate: 0.183200\n",
      "Epoch   85, Training Loss: 0.0874, Training Acc: 0.9435, Learning Rate: 0.183000\n",
      "Epoch   86, Training Loss: 0.0867, Training Acc: 0.9435, Learning Rate: 0.182800\n",
      "Epoch   87, Training Loss: 0.0859, Training Acc: 0.9435, Learning Rate: 0.182600\n",
      "Epoch   88, Training Loss: 0.0856, Training Acc: 0.9516, Learning Rate: 0.182400\n",
      "Epoch   89, Training Loss: 0.0848, Training Acc: 0.9435, Learning Rate: 0.182200\n",
      "Epoch   90, Training Loss: 0.0841, Training Acc: 0.9516, Learning Rate: 0.182000\n",
      "Epoch   91, Training Loss: 0.0838, Training Acc: 0.9516, Learning Rate: 0.181800\n",
      "Epoch   92, Training Loss: 0.0831, Training Acc: 0.9516, Learning Rate: 0.181600\n",
      "Epoch   93, Training Loss: 0.0825, Training Acc: 0.9516, Learning Rate: 0.181400\n",
      "Epoch   94, Training Loss: 0.0819, Training Acc: 0.9597, Learning Rate: 0.181200\n",
      "Epoch   95, Training Loss: 0.0815, Training Acc: 0.9597, Learning Rate: 0.181000\n",
      "Epoch   96, Training Loss: 0.0808, Training Acc: 0.9677, Learning Rate: 0.180800\n",
      "Epoch   97, Training Loss: 0.0803, Training Acc: 0.9677, Learning Rate: 0.180600\n",
      "Epoch   98, Training Loss: 0.0802, Training Acc: 0.9758, Learning Rate: 0.180400\n",
      "Epoch   99, Training Loss: 0.0793, Training Acc: 0.9677, Learning Rate: 0.180200\n",
      "Epoch  100, Training Loss: 0.0787, Training Acc: 0.9677, Learning Rate: 0.180000\n",
      "Epoch  101, Training Loss: 0.0782, Training Acc: 0.9677, Learning Rate: 0.179800\n",
      "Epoch  102, Training Loss: 0.0777, Training Acc: 0.9677, Learning Rate: 0.179600\n",
      "Epoch  103, Training Loss: 0.0772, Training Acc: 0.9758, Learning Rate: 0.179400\n",
      "Epoch  104, Training Loss: 0.0769, Training Acc: 0.9758, Learning Rate: 0.179200\n",
      "Epoch  105, Training Loss: 0.0764, Training Acc: 0.9758, Learning Rate: 0.179000\n",
      "Epoch  106, Training Loss: 0.0758, Training Acc: 0.9677, Learning Rate: 0.178800\n",
      "Epoch  107, Training Loss: 0.0754, Training Acc: 0.9758, Learning Rate: 0.178600\n",
      "Epoch  108, Training Loss: 0.0748, Training Acc: 0.9758, Learning Rate: 0.178400\n",
      "Epoch  109, Training Loss: 0.0747, Training Acc: 0.9758, Learning Rate: 0.178200\n",
      "Epoch  110, Training Loss: 0.0740, Training Acc: 0.9758, Learning Rate: 0.178000\n",
      "Epoch  111, Training Loss: 0.0736, Training Acc: 0.9758, Learning Rate: 0.177800\n",
      "Epoch  112, Training Loss: 0.0730, Training Acc: 0.9758, Learning Rate: 0.177600\n",
      "Epoch  113, Training Loss: 0.0726, Training Acc: 0.9758, Learning Rate: 0.177400\n",
      "Epoch  114, Training Loss: 0.0722, Training Acc: 0.9677, Learning Rate: 0.177200\n",
      "Epoch  115, Training Loss: 0.0716, Training Acc: 0.9758, Learning Rate: 0.177000\n",
      "Epoch  116, Training Loss: 0.0716, Training Acc: 0.9677, Learning Rate: 0.176800\n",
      "Epoch  117, Training Loss: 0.0708, Training Acc: 0.9758, Learning Rate: 0.176600\n",
      "Epoch  118, Training Loss: 0.0704, Training Acc: 0.9758, Learning Rate: 0.176400\n",
      "Epoch  119, Training Loss: 0.0701, Training Acc: 0.9839, Learning Rate: 0.176200\n",
      "Epoch  120, Training Loss: 0.0695, Training Acc: 0.9839, Learning Rate: 0.176000\n",
      "Epoch  121, Training Loss: 0.0691, Training Acc: 0.9758, Learning Rate: 0.175800\n",
      "Epoch  122, Training Loss: 0.0687, Training Acc: 0.9758, Learning Rate: 0.175600\n",
      "Epoch  123, Training Loss: 0.0688, Training Acc: 0.9758, Learning Rate: 0.175400\n",
      "Epoch  124, Training Loss: 0.0680, Training Acc: 0.9758, Learning Rate: 0.175200\n",
      "Epoch  125, Training Loss: 0.0676, Training Acc: 0.9839, Learning Rate: 0.175000\n",
      "Epoch  126, Training Loss: 0.0673, Training Acc: 0.9839, Learning Rate: 0.174800\n",
      "Epoch  127, Training Loss: 0.0675, Training Acc: 0.9758, Learning Rate: 0.174600\n",
      "Epoch  128, Training Loss: 0.0664, Training Acc: 0.9839, Learning Rate: 0.174400\n",
      "Epoch  129, Training Loss: 0.0661, Training Acc: 0.9839, Learning Rate: 0.174200\n",
      "Epoch  130, Training Loss: 0.0659, Training Acc: 0.9758, Learning Rate: 0.174000\n",
      "Epoch  131, Training Loss: 0.0653, Training Acc: 0.9839, Learning Rate: 0.173800\n",
      "Epoch  132, Training Loss: 0.0649, Training Acc: 0.9839, Learning Rate: 0.173600\n",
      "Epoch  133, Training Loss: 0.0646, Training Acc: 0.9839, Learning Rate: 0.173400\n",
      "Epoch  134, Training Loss: 0.0642, Training Acc: 0.9839, Learning Rate: 0.173200\n",
      "Epoch  135, Training Loss: 0.0638, Training Acc: 0.9839, Learning Rate: 0.173000\n",
      "Epoch  136, Training Loss: 0.0640, Training Acc: 0.9919, Learning Rate: 0.172800\n",
      "Epoch  137, Training Loss: 0.0631, Training Acc: 0.9839, Learning Rate: 0.172600\n",
      "Epoch  138, Training Loss: 0.0626, Training Acc: 0.9839, Learning Rate: 0.172400\n",
      "Epoch  139, Training Loss: 0.0623, Training Acc: 0.9919, Learning Rate: 0.172200\n",
      "Epoch  140, Training Loss: 0.0621, Training Acc: 0.9919, Learning Rate: 0.172000\n",
      "Epoch  141, Training Loss: 0.0617, Training Acc: 0.9839, Learning Rate: 0.171800\n",
      "Epoch  142, Training Loss: 0.0612, Training Acc: 0.9919, Learning Rate: 0.171600\n",
      "Epoch  143, Training Loss: 0.0615, Training Acc: 0.9839, Learning Rate: 0.171400\n",
      "Epoch  144, Training Loss: 0.0607, Training Acc: 0.9839, Learning Rate: 0.171200\n",
      "Epoch  145, Training Loss: 0.0602, Training Acc: 0.9919, Learning Rate: 0.171000\n",
      "Epoch  146, Training Loss: 0.0604, Training Acc: 0.9839, Learning Rate: 0.170800\n",
      "Epoch  147, Training Loss: 0.0595, Training Acc: 0.9919, Learning Rate: 0.170600\n",
      "Epoch  148, Training Loss: 0.0593, Training Acc: 1.0000, Learning Rate: 0.170400\n",
      "Epoch  149, Training Loss: 0.0593, Training Acc: 0.9919, Learning Rate: 0.170200\n",
      "Epoch  150, Training Loss: 0.0587, Training Acc: 1.0000, Learning Rate: 0.170000\n",
      "Epoch  151, Training Loss: 0.0582, Training Acc: 0.9919, Learning Rate: 0.169800\n",
      "Epoch  152, Training Loss: 0.0580, Training Acc: 0.9919, Learning Rate: 0.169600\n",
      "Epoch  153, Training Loss: 0.0577, Training Acc: 0.9919, Learning Rate: 0.169400\n",
      "Epoch  154, Training Loss: 0.0576, Training Acc: 0.9919, Learning Rate: 0.169200\n",
      "Epoch  155, Training Loss: 0.0571, Training Acc: 0.9919, Learning Rate: 0.169000\n",
      "Epoch  156, Training Loss: 0.0567, Training Acc: 0.9919, Learning Rate: 0.168800\n",
      "Epoch  157, Training Loss: 0.0564, Training Acc: 1.0000, Learning Rate: 0.168600\n",
      "Epoch  158, Training Loss: 0.0561, Training Acc: 1.0000, Learning Rate: 0.168400\n",
      "Epoch  159, Training Loss: 0.0558, Training Acc: 1.0000, Learning Rate: 0.168200\n",
      "Epoch  160, Training Loss: 0.0555, Training Acc: 1.0000, Learning Rate: 0.168000\n",
      "Epoch  161, Training Loss: 0.0553, Training Acc: 1.0000, Learning Rate: 0.167800\n",
      "Epoch  162, Training Loss: 0.0550, Training Acc: 1.0000, Learning Rate: 0.167600\n",
      "Epoch  163, Training Loss: 0.0548, Training Acc: 1.0000, Learning Rate: 0.167400\n",
      "Epoch  164, Training Loss: 0.0544, Training Acc: 1.0000, Learning Rate: 0.167200\n",
      "Epoch  165, Training Loss: 0.0544, Training Acc: 1.0000, Learning Rate: 0.167000\n",
      "Epoch  166, Training Loss: 0.0539, Training Acc: 1.0000, Learning Rate: 0.166800\n",
      "Epoch  167, Training Loss: 0.0536, Training Acc: 1.0000, Learning Rate: 0.166600\n",
      "Epoch  168, Training Loss: 0.0535, Training Acc: 1.0000, Learning Rate: 0.166400\n",
      "Epoch  169, Training Loss: 0.0532, Training Acc: 1.0000, Learning Rate: 0.166200\n",
      "Epoch  170, Training Loss: 0.0529, Training Acc: 1.0000, Learning Rate: 0.166000\n",
      "Epoch  171, Training Loss: 0.0528, Training Acc: 1.0000, Learning Rate: 0.165800\n",
      "Epoch  172, Training Loss: 0.0524, Training Acc: 1.0000, Learning Rate: 0.165600\n",
      "Epoch  173, Training Loss: 0.0521, Training Acc: 1.0000, Learning Rate: 0.165400\n",
      "Epoch  174, Training Loss: 0.0520, Training Acc: 1.0000, Learning Rate: 0.165200\n",
      "Epoch  175, Training Loss: 0.0518, Training Acc: 1.0000, Learning Rate: 0.165000\n",
      "Epoch  176, Training Loss: 0.0514, Training Acc: 1.0000, Learning Rate: 0.164800\n",
      "Epoch  177, Training Loss: 0.0514, Training Acc: 1.0000, Learning Rate: 0.164600\n",
      "Epoch  178, Training Loss: 0.0511, Training Acc: 1.0000, Learning Rate: 0.164400\n",
      "Epoch  179, Training Loss: 0.0508, Training Acc: 1.0000, Learning Rate: 0.164200\n",
      "Epoch  180, Training Loss: 0.0506, Training Acc: 1.0000, Learning Rate: 0.164000\n",
      "Epoch  181, Training Loss: 0.0503, Training Acc: 1.0000, Learning Rate: 0.163800\n",
      "Epoch  182, Training Loss: 0.0501, Training Acc: 1.0000, Learning Rate: 0.163600\n",
      "Epoch  183, Training Loss: 0.0499, Training Acc: 1.0000, Learning Rate: 0.163400\n",
      "Epoch  184, Training Loss: 0.0497, Training Acc: 1.0000, Learning Rate: 0.163200\n",
      "Epoch  185, Training Loss: 0.0495, Training Acc: 1.0000, Learning Rate: 0.163000\n",
      "Epoch  186, Training Loss: 0.0493, Training Acc: 1.0000, Learning Rate: 0.162800\n",
      "Epoch  187, Training Loss: 0.0490, Training Acc: 1.0000, Learning Rate: 0.162600\n",
      "Epoch  188, Training Loss: 0.0489, Training Acc: 1.0000, Learning Rate: 0.162400\n",
      "Epoch  189, Training Loss: 0.0487, Training Acc: 1.0000, Learning Rate: 0.162200\n",
      "Epoch  190, Training Loss: 0.0484, Training Acc: 1.0000, Learning Rate: 0.162000\n",
      "Epoch  191, Training Loss: 0.0484, Training Acc: 1.0000, Learning Rate: 0.161800\n",
      "Epoch  192, Training Loss: 0.0482, Training Acc: 1.0000, Learning Rate: 0.161600\n",
      "Epoch  193, Training Loss: 0.0480, Training Acc: 1.0000, Learning Rate: 0.161400\n",
      "Epoch  194, Training Loss: 0.0477, Training Acc: 1.0000, Learning Rate: 0.161200\n",
      "Epoch  195, Training Loss: 0.0476, Training Acc: 1.0000, Learning Rate: 0.161000\n",
      "Epoch  196, Training Loss: 0.0473, Training Acc: 1.0000, Learning Rate: 0.160800\n",
      "Epoch  197, Training Loss: 0.0471, Training Acc: 1.0000, Learning Rate: 0.160600\n",
      "Epoch  198, Training Loss: 0.0473, Training Acc: 1.0000, Learning Rate: 0.160400\n",
      "Epoch  199, Training Loss: 0.0468, Training Acc: 1.0000, Learning Rate: 0.160200\n",
      "Epoch  200, Training Loss: 0.0466, Training Acc: 1.0000, Learning Rate: 0.160000\n",
      "Epoch  201, Training Loss: 0.0466, Training Acc: 1.0000, Learning Rate: 0.159800\n",
      "Epoch  202, Training Loss: 0.0463, Training Acc: 1.0000, Learning Rate: 0.159600\n",
      "Epoch  203, Training Loss: 0.0461, Training Acc: 1.0000, Learning Rate: 0.159400\n",
      "Epoch  204, Training Loss: 0.0459, Training Acc: 1.0000, Learning Rate: 0.159200\n",
      "Epoch  205, Training Loss: 0.0458, Training Acc: 1.0000, Learning Rate: 0.159000\n",
      "Epoch  206, Training Loss: 0.0457, Training Acc: 1.0000, Learning Rate: 0.158800\n",
      "Epoch  207, Training Loss: 0.0456, Training Acc: 1.0000, Learning Rate: 0.158600\n",
      "Epoch  208, Training Loss: 0.0453, Training Acc: 1.0000, Learning Rate: 0.158400\n",
      "Epoch  209, Training Loss: 0.0452, Training Acc: 1.0000, Learning Rate: 0.158200\n",
      "Epoch  210, Training Loss: 0.0450, Training Acc: 1.0000, Learning Rate: 0.158000\n",
      "Epoch  211, Training Loss: 0.0449, Training Acc: 1.0000, Learning Rate: 0.157800\n",
      "Epoch  212, Training Loss: 0.0447, Training Acc: 1.0000, Learning Rate: 0.157600\n",
      "Epoch  213, Training Loss: 0.0446, Training Acc: 1.0000, Learning Rate: 0.157400\n",
      "Epoch  214, Training Loss: 0.0445, Training Acc: 1.0000, Learning Rate: 0.157200\n",
      "Epoch  215, Training Loss: 0.0443, Training Acc: 1.0000, Learning Rate: 0.157000\n",
      "Epoch  216, Training Loss: 0.0441, Training Acc: 1.0000, Learning Rate: 0.156800\n",
      "Epoch  217, Training Loss: 0.0440, Training Acc: 1.0000, Learning Rate: 0.156600\n",
      "Epoch  218, Training Loss: 0.0439, Training Acc: 1.0000, Learning Rate: 0.156400\n",
      "Epoch  219, Training Loss: 0.0437, Training Acc: 1.0000, Learning Rate: 0.156200\n",
      "Epoch  220, Training Loss: 0.0437, Training Acc: 1.0000, Learning Rate: 0.156000\n",
      "Epoch  221, Training Loss: 0.0436, Training Acc: 1.0000, Learning Rate: 0.155800\n",
      "Epoch  222, Training Loss: 0.0433, Training Acc: 1.0000, Learning Rate: 0.155600\n",
      "Epoch  223, Training Loss: 0.0433, Training Acc: 1.0000, Learning Rate: 0.155400\n",
      "Epoch  224, Training Loss: 0.0431, Training Acc: 1.0000, Learning Rate: 0.155200\n",
      "Epoch  225, Training Loss: 0.0430, Training Acc: 1.0000, Learning Rate: 0.155000\n",
      "Epoch  226, Training Loss: 0.0428, Training Acc: 1.0000, Learning Rate: 0.154800\n",
      "Epoch  227, Training Loss: 0.0428, Training Acc: 1.0000, Learning Rate: 0.154600\n",
      "Epoch  228, Training Loss: 0.0426, Training Acc: 1.0000, Learning Rate: 0.154400\n",
      "Epoch  229, Training Loss: 0.0425, Training Acc: 1.0000, Learning Rate: 0.154200\n",
      "Epoch  230, Training Loss: 0.0424, Training Acc: 1.0000, Learning Rate: 0.154000\n",
      "Epoch  231, Training Loss: 0.0422, Training Acc: 1.0000, Learning Rate: 0.153800\n",
      "Epoch  232, Training Loss: 0.0425, Training Acc: 1.0000, Learning Rate: 0.153600\n",
      "Epoch  233, Training Loss: 0.0420, Training Acc: 1.0000, Learning Rate: 0.153400\n",
      "Epoch  234, Training Loss: 0.0419, Training Acc: 1.0000, Learning Rate: 0.153200\n",
      "Epoch  235, Training Loss: 0.0418, Training Acc: 1.0000, Learning Rate: 0.153000\n",
      "Epoch  236, Training Loss: 0.0417, Training Acc: 1.0000, Learning Rate: 0.152800\n",
      "Epoch  237, Training Loss: 0.0416, Training Acc: 1.0000, Learning Rate: 0.152600\n",
      "Epoch  238, Training Loss: 0.0415, Training Acc: 1.0000, Learning Rate: 0.152400\n",
      "Epoch  239, Training Loss: 0.0414, Training Acc: 1.0000, Learning Rate: 0.152200\n",
      "Epoch  240, Training Loss: 0.0413, Training Acc: 1.0000, Learning Rate: 0.152000\n",
      "Epoch  241, Training Loss: 0.0412, Training Acc: 1.0000, Learning Rate: 0.151800\n",
      "Epoch  242, Training Loss: 0.0411, Training Acc: 1.0000, Learning Rate: 0.151600\n",
      "Epoch  243, Training Loss: 0.0410, Training Acc: 1.0000, Learning Rate: 0.151400\n",
      "Epoch  244, Training Loss: 0.0409, Training Acc: 1.0000, Learning Rate: 0.151200\n",
      "Epoch  245, Training Loss: 0.0408, Training Acc: 1.0000, Learning Rate: 0.151000\n",
      "Epoch  246, Training Loss: 0.0408, Training Acc: 1.0000, Learning Rate: 0.150800\n",
      "Epoch  247, Training Loss: 0.0406, Training Acc: 1.0000, Learning Rate: 0.150600\n",
      "Epoch  248, Training Loss: 0.0405, Training Acc: 1.0000, Learning Rate: 0.150400\n",
      "Epoch  249, Training Loss: 0.0405, Training Acc: 1.0000, Learning Rate: 0.150200\n",
      "Epoch  250, Training Loss: 0.0405, Training Acc: 1.0000, Learning Rate: 0.150000\n",
      "Epoch  251, Training Loss: 0.0403, Training Acc: 1.0000, Learning Rate: 0.149800\n",
      "Epoch  252, Training Loss: 0.0401, Training Acc: 1.0000, Learning Rate: 0.149600\n",
      "Epoch  253, Training Loss: 0.0401, Training Acc: 1.0000, Learning Rate: 0.149400\n",
      "Epoch  254, Training Loss: 0.0400, Training Acc: 1.0000, Learning Rate: 0.149200\n",
      "Epoch  255, Training Loss: 0.0399, Training Acc: 1.0000, Learning Rate: 0.149000\n",
      "Epoch  256, Training Loss: 0.0398, Training Acc: 1.0000, Learning Rate: 0.148800\n",
      "Epoch  257, Training Loss: 0.0397, Training Acc: 1.0000, Learning Rate: 0.148600\n",
      "Epoch  258, Training Loss: 0.0397, Training Acc: 1.0000, Learning Rate: 0.148400\n",
      "Epoch  259, Training Loss: 0.0396, Training Acc: 1.0000, Learning Rate: 0.148200\n",
      "Epoch  260, Training Loss: 0.0395, Training Acc: 1.0000, Learning Rate: 0.148000\n",
      "Epoch  261, Training Loss: 0.0395, Training Acc: 1.0000, Learning Rate: 0.147800\n",
      "Epoch  262, Training Loss: 0.0394, Training Acc: 1.0000, Learning Rate: 0.147600\n",
      "Epoch  263, Training Loss: 0.0394, Training Acc: 1.0000, Learning Rate: 0.147400\n",
      "Epoch  264, Training Loss: 0.0392, Training Acc: 1.0000, Learning Rate: 0.147200\n",
      "Epoch  265, Training Loss: 0.0392, Training Acc: 1.0000, Learning Rate: 0.147000\n",
      "Epoch  266, Training Loss: 0.0391, Training Acc: 1.0000, Learning Rate: 0.146800\n",
      "Epoch  267, Training Loss: 0.0390, Training Acc: 1.0000, Learning Rate: 0.146600\n",
      "Epoch  268, Training Loss: 0.0389, Training Acc: 1.0000, Learning Rate: 0.146400\n",
      "Epoch  269, Training Loss: 0.0390, Training Acc: 1.0000, Learning Rate: 0.146200\n",
      "Epoch  270, Training Loss: 0.0388, Training Acc: 1.0000, Learning Rate: 0.146000\n",
      "Epoch  271, Training Loss: 0.0387, Training Acc: 1.0000, Learning Rate: 0.145800\n",
      "Epoch  272, Training Loss: 0.0387, Training Acc: 1.0000, Learning Rate: 0.145600\n",
      "Epoch  273, Training Loss: 0.0386, Training Acc: 1.0000, Learning Rate: 0.145400\n",
      "Epoch  274, Training Loss: 0.0386, Training Acc: 1.0000, Learning Rate: 0.145200\n",
      "Epoch  275, Training Loss: 0.0385, Training Acc: 1.0000, Learning Rate: 0.145000\n",
      "Epoch  276, Training Loss: 0.0385, Training Acc: 1.0000, Learning Rate: 0.144800\n",
      "Epoch  277, Training Loss: 0.0383, Training Acc: 1.0000, Learning Rate: 0.144600\n",
      "Epoch  278, Training Loss: 0.0384, Training Acc: 1.0000, Learning Rate: 0.144400\n",
      "Epoch  279, Training Loss: 0.0382, Training Acc: 1.0000, Learning Rate: 0.144200\n",
      "Epoch  280, Training Loss: 0.0382, Training Acc: 1.0000, Learning Rate: 0.144000\n",
      "Epoch  281, Training Loss: 0.0381, Training Acc: 1.0000, Learning Rate: 0.143800\n",
      "Epoch  282, Training Loss: 0.0381, Training Acc: 1.0000, Learning Rate: 0.143600\n",
      "Epoch  283, Training Loss: 0.0380, Training Acc: 1.0000, Learning Rate: 0.143400\n",
      "Epoch  284, Training Loss: 0.0380, Training Acc: 1.0000, Learning Rate: 0.143200\n",
      "Epoch  285, Training Loss: 0.0379, Training Acc: 1.0000, Learning Rate: 0.143000\n",
      "Epoch  286, Training Loss: 0.0379, Training Acc: 1.0000, Learning Rate: 0.142800\n",
      "Epoch  287, Training Loss: 0.0378, Training Acc: 1.0000, Learning Rate: 0.142600\n",
      "Epoch  288, Training Loss: 0.0377, Training Acc: 1.0000, Learning Rate: 0.142400\n",
      "Epoch  289, Training Loss: 0.0377, Training Acc: 1.0000, Learning Rate: 0.142200\n",
      "Epoch  290, Training Loss: 0.0377, Training Acc: 1.0000, Learning Rate: 0.142000\n",
      "Epoch  291, Training Loss: 0.0376, Training Acc: 1.0000, Learning Rate: 0.141800\n",
      "Epoch  292, Training Loss: 0.0376, Training Acc: 1.0000, Learning Rate: 0.141600\n",
      "Epoch  293, Training Loss: 0.0375, Training Acc: 1.0000, Learning Rate: 0.141400\n",
      "Epoch  294, Training Loss: 0.0375, Training Acc: 1.0000, Learning Rate: 0.141200\n",
      "Epoch  295, Training Loss: 0.0374, Training Acc: 1.0000, Learning Rate: 0.141000\n",
      "Epoch  296, Training Loss: 0.0374, Training Acc: 1.0000, Learning Rate: 0.140800\n",
      "Epoch  297, Training Loss: 0.0373, Training Acc: 1.0000, Learning Rate: 0.140600\n",
      "Epoch  298, Training Loss: 0.0373, Training Acc: 1.0000, Learning Rate: 0.140400\n",
      "Epoch  299, Training Loss: 0.0372, Training Acc: 1.0000, Learning Rate: 0.140200\n",
      "Epoch  300, Training Loss: 0.0372, Training Acc: 1.0000, Learning Rate: 0.140000\n",
      "Epoch  301, Training Loss: 0.0372, Training Acc: 1.0000, Learning Rate: 0.139800\n",
      "Epoch  302, Training Loss: 0.0371, Training Acc: 1.0000, Learning Rate: 0.139600\n",
      "Epoch  303, Training Loss: 0.0371, Training Acc: 1.0000, Learning Rate: 0.139400\n",
      "Epoch  304, Training Loss: 0.0370, Training Acc: 1.0000, Learning Rate: 0.139200\n",
      "Epoch  305, Training Loss: 0.0370, Training Acc: 1.0000, Learning Rate: 0.139000\n",
      "Epoch  306, Training Loss: 0.0369, Training Acc: 1.0000, Learning Rate: 0.138800\n",
      "Epoch  307, Training Loss: 0.0369, Training Acc: 1.0000, Learning Rate: 0.138600\n",
      "Epoch  308, Training Loss: 0.0369, Training Acc: 1.0000, Learning Rate: 0.138400\n",
      "Epoch  309, Training Loss: 0.0368, Training Acc: 1.0000, Learning Rate: 0.138200\n",
      "Epoch  310, Training Loss: 0.0368, Training Acc: 1.0000, Learning Rate: 0.138000\n",
      "Epoch  311, Training Loss: 0.0368, Training Acc: 1.0000, Learning Rate: 0.137800\n",
      "Epoch  312, Training Loss: 0.0367, Training Acc: 1.0000, Learning Rate: 0.137600\n",
      "Epoch  313, Training Loss: 0.0367, Training Acc: 1.0000, Learning Rate: 0.137400\n",
      "Epoch  314, Training Loss: 0.0367, Training Acc: 1.0000, Learning Rate: 0.137200\n",
      "Epoch  315, Training Loss: 0.0367, Training Acc: 1.0000, Learning Rate: 0.137000\n",
      "Epoch  316, Training Loss: 0.0366, Training Acc: 1.0000, Learning Rate: 0.136800\n",
      "Epoch  317, Training Loss: 0.0366, Training Acc: 1.0000, Learning Rate: 0.136600\n",
      "Epoch  318, Training Loss: 0.0365, Training Acc: 1.0000, Learning Rate: 0.136400\n",
      "Epoch  319, Training Loss: 0.0365, Training Acc: 1.0000, Learning Rate: 0.136200\n",
      "Epoch  320, Training Loss: 0.0365, Training Acc: 1.0000, Learning Rate: 0.136000\n",
      "Epoch  321, Training Loss: 0.0364, Training Acc: 1.0000, Learning Rate: 0.135800\n",
      "Epoch  322, Training Loss: 0.0364, Training Acc: 1.0000, Learning Rate: 0.135600\n",
      "Epoch  323, Training Loss: 0.0364, Training Acc: 1.0000, Learning Rate: 0.135400\n",
      "Epoch  324, Training Loss: 0.0363, Training Acc: 1.0000, Learning Rate: 0.135200\n",
      "Epoch  325, Training Loss: 0.0363, Training Acc: 1.0000, Learning Rate: 0.135000\n",
      "Epoch  326, Training Loss: 0.0363, Training Acc: 1.0000, Learning Rate: 0.134800\n",
      "Epoch  327, Training Loss: 0.0362, Training Acc: 1.0000, Learning Rate: 0.134600\n",
      "Epoch  328, Training Loss: 0.0362, Training Acc: 1.0000, Learning Rate: 0.134400\n",
      "Epoch  329, Training Loss: 0.0362, Training Acc: 1.0000, Learning Rate: 0.134200\n",
      "Epoch  330, Training Loss: 0.0362, Training Acc: 1.0000, Learning Rate: 0.134000\n",
      "Epoch  331, Training Loss: 0.0361, Training Acc: 1.0000, Learning Rate: 0.133800\n",
      "Epoch  332, Training Loss: 0.0361, Training Acc: 1.0000, Learning Rate: 0.133600\n",
      "Epoch  333, Training Loss: 0.0361, Training Acc: 1.0000, Learning Rate: 0.133400\n",
      "Epoch  334, Training Loss: 0.0361, Training Acc: 1.0000, Learning Rate: 0.133200\n",
      "Epoch  335, Training Loss: 0.0361, Training Acc: 1.0000, Learning Rate: 0.133000\n",
      "Epoch  336, Training Loss: 0.0360, Training Acc: 1.0000, Learning Rate: 0.132800\n",
      "Epoch  337, Training Loss: 0.0360, Training Acc: 1.0000, Learning Rate: 0.132600\n",
      "Epoch  338, Training Loss: 0.0360, Training Acc: 1.0000, Learning Rate: 0.132400\n",
      "Epoch  339, Training Loss: 0.0359, Training Acc: 1.0000, Learning Rate: 0.132200\n",
      "Epoch  340, Training Loss: 0.0359, Training Acc: 1.0000, Learning Rate: 0.132000\n",
      "Epoch  341, Training Loss: 0.0359, Training Acc: 1.0000, Learning Rate: 0.131800\n",
      "Epoch  342, Training Loss: 0.0359, Training Acc: 1.0000, Learning Rate: 0.131600\n",
      "Epoch  343, Training Loss: 0.0358, Training Acc: 1.0000, Learning Rate: 0.131400\n",
      "Epoch  344, Training Loss: 0.0358, Training Acc: 1.0000, Learning Rate: 0.131200\n",
      "Epoch  345, Training Loss: 0.0358, Training Acc: 1.0000, Learning Rate: 0.131000\n",
      "Epoch  346, Training Loss: 0.0358, Training Acc: 1.0000, Learning Rate: 0.130800\n",
      "Epoch  347, Training Loss: 0.0358, Training Acc: 1.0000, Learning Rate: 0.130600\n",
      "Epoch  348, Training Loss: 0.0357, Training Acc: 1.0000, Learning Rate: 0.130400\n",
      "Epoch  349, Training Loss: 0.0357, Training Acc: 1.0000, Learning Rate: 0.130200\n",
      "Epoch  350, Training Loss: 0.0357, Training Acc: 1.0000, Learning Rate: 0.130000\n",
      "Epoch  351, Training Loss: 0.0357, Training Acc: 1.0000, Learning Rate: 0.129800\n",
      "Epoch  352, Training Loss: 0.0357, Training Acc: 1.0000, Learning Rate: 0.129600\n",
      "Epoch  353, Training Loss: 0.0356, Training Acc: 1.0000, Learning Rate: 0.129400\n",
      "Epoch  354, Training Loss: 0.0357, Training Acc: 1.0000, Learning Rate: 0.129200\n",
      "Epoch  355, Training Loss: 0.0356, Training Acc: 1.0000, Learning Rate: 0.129000\n",
      "Epoch  356, Training Loss: 0.0356, Training Acc: 1.0000, Learning Rate: 0.128800\n",
      "Epoch  357, Training Loss: 0.0356, Training Acc: 1.0000, Learning Rate: 0.128600\n",
      "Epoch  358, Training Loss: 0.0356, Training Acc: 1.0000, Learning Rate: 0.128400\n",
      "Epoch  359, Training Loss: 0.0355, Training Acc: 1.0000, Learning Rate: 0.128200\n",
      "Epoch  360, Training Loss: 0.0355, Training Acc: 1.0000, Learning Rate: 0.128000\n",
      "Epoch  361, Training Loss: 0.0355, Training Acc: 1.0000, Learning Rate: 0.127800\n",
      "Epoch  362, Training Loss: 0.0355, Training Acc: 1.0000, Learning Rate: 0.127600\n",
      "Epoch  363, Training Loss: 0.0355, Training Acc: 1.0000, Learning Rate: 0.127400\n",
      "Epoch  364, Training Loss: 0.0355, Training Acc: 1.0000, Learning Rate: 0.127200\n",
      "Epoch  365, Training Loss: 0.0354, Training Acc: 1.0000, Learning Rate: 0.127000\n",
      "Epoch  366, Training Loss: 0.0354, Training Acc: 1.0000, Learning Rate: 0.126800\n",
      "Epoch  367, Training Loss: 0.0354, Training Acc: 1.0000, Learning Rate: 0.126600\n",
      "Epoch  368, Training Loss: 0.0354, Training Acc: 1.0000, Learning Rate: 0.126400\n",
      "Epoch  369, Training Loss: 0.0354, Training Acc: 1.0000, Learning Rate: 0.126200\n",
      "Epoch  370, Training Loss: 0.0354, Training Acc: 1.0000, Learning Rate: 0.126000\n",
      "Epoch  371, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.125800\n",
      "Epoch  372, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.125600\n",
      "Epoch  373, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.125400\n",
      "Epoch  374, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.125200\n",
      "Epoch  375, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.125000\n",
      "Epoch  376, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.124800\n",
      "Epoch  377, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.124600\n",
      "Epoch  378, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.124400\n",
      "Epoch  379, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.124200\n",
      "Epoch  380, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.124000\n",
      "Epoch  381, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.123800\n",
      "Epoch  382, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.123600\n",
      "Epoch  383, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.123400\n",
      "Epoch  384, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.123200\n",
      "Epoch  385, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.123000\n",
      "Epoch  386, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.122800\n",
      "Epoch  387, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.122600\n",
      "Epoch  388, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.122400\n",
      "Epoch  389, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.122200\n",
      "Epoch  390, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.122000\n",
      "Epoch  391, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.121800\n",
      "Epoch  392, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.121600\n",
      "Epoch  393, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.121400\n",
      "Epoch  394, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.121200\n",
      "Epoch  395, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.121000\n",
      "Epoch  396, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.120800\n",
      "Epoch  397, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.120600\n",
      "Epoch  398, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.120400\n",
      "Epoch  399, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.120200\n",
      "Epoch  400, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.120000\n",
      "Epoch  401, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.119800\n",
      "Epoch  402, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.119600\n",
      "Epoch  403, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.119400\n",
      "Epoch  404, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.119200\n",
      "Epoch  405, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.119000\n",
      "Epoch  406, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.118800\n",
      "Epoch  407, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.118600\n",
      "Epoch  408, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.118400\n",
      "Epoch  409, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.118200\n",
      "Epoch  410, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.118000\n",
      "Epoch  411, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.117800\n",
      "Epoch  412, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.117600\n",
      "Epoch  413, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.117400\n",
      "Epoch  414, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.117200\n",
      "Epoch  415, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.117000\n",
      "Epoch  416, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.116800\n",
      "Epoch  417, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.116600\n",
      "Epoch  418, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.116400\n",
      "Epoch  419, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.116200\n",
      "Epoch  420, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.116000\n",
      "Epoch  421, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.115800\n",
      "Epoch  422, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.115600\n",
      "Epoch  423, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.115400\n",
      "Epoch  424, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.115200\n",
      "Epoch  425, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.115000\n",
      "Epoch  426, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.114800\n",
      "Epoch  427, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.114600\n",
      "Epoch  428, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.114400\n",
      "Epoch  429, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.114200\n",
      "Epoch  430, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.114000\n",
      "Epoch  431, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.113800\n",
      "Epoch  432, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.113600\n",
      "Epoch  433, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.113400\n",
      "Epoch  434, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.113200\n",
      "Epoch  435, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.113000\n",
      "Epoch  436, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.112800\n",
      "Epoch  437, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.112600\n",
      "Epoch  438, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.112400\n",
      "Epoch  439, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.112200\n",
      "Epoch  440, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.112000\n",
      "Epoch  441, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.111800\n",
      "Epoch  442, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.111600\n",
      "Epoch  443, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.111400\n",
      "Epoch  444, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.111200\n",
      "Epoch  445, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.111000\n",
      "Epoch  446, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.110800\n",
      "Epoch  447, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.110600\n",
      "Epoch  448, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.110400\n",
      "Epoch  449, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.110200\n",
      "Epoch  450, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.110000\n",
      "Epoch  451, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.109800\n",
      "Epoch  452, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.109600\n",
      "Epoch  453, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.109400\n",
      "Epoch  454, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.109200\n",
      "Epoch  455, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.109000\n",
      "Epoch  456, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.108800\n",
      "Epoch  457, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.108600\n",
      "Epoch  458, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.108400\n",
      "Epoch  459, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.108200\n",
      "Epoch  460, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.108000\n",
      "Epoch  461, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.107800\n",
      "Epoch  462, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.107600\n",
      "Epoch  463, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.107400\n",
      "Epoch  464, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.107200\n",
      "Epoch  465, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.107000\n",
      "Epoch  466, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.106800\n",
      "Epoch  467, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.106600\n",
      "Epoch  468, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.106400\n",
      "Epoch  469, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.106200\n",
      "Epoch  470, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.106000\n",
      "Epoch  471, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.105800\n",
      "Epoch  472, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.105600\n",
      "Epoch  473, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.105400\n",
      "Epoch  474, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.105200\n",
      "Epoch  475, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.105000\n",
      "Epoch  476, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.104800\n",
      "Epoch  477, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.104600\n",
      "Epoch  478, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.104400\n",
      "Epoch  479, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.104200\n",
      "Epoch  480, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.104000\n",
      "Epoch  481, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.103800\n",
      "Epoch  482, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.103600\n",
      "Epoch  483, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.103400\n",
      "Epoch  484, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.103200\n",
      "Epoch  485, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.103000\n",
      "Epoch  486, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.102800\n",
      "Epoch  487, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.102600\n",
      "Epoch  488, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.102400\n",
      "Epoch  489, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.102200\n",
      "Epoch  490, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.102000\n",
      "Epoch  491, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.101800\n",
      "Epoch  492, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.101600\n",
      "Epoch  493, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.101400\n",
      "Epoch  494, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.101200\n",
      "Epoch  495, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.101000\n",
      "Epoch  496, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.100800\n",
      "Epoch  497, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.100600\n",
      "Epoch  498, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.100400\n",
      "Epoch  499, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.100200\n",
      "Epoch  500, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.100000\n",
      "Epoch  501, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.099800\n",
      "Epoch  502, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.099600\n",
      "Epoch  503, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.099400\n",
      "Epoch  504, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.099200\n",
      "Epoch  505, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.099000\n",
      "Epoch  506, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.098800\n",
      "Epoch  507, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.098600\n",
      "Epoch  508, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.098400\n",
      "Epoch  509, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.098200\n",
      "Epoch  510, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.098000\n",
      "Epoch  511, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.097800\n",
      "Epoch  512, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.097600\n",
      "Epoch  513, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.097400\n",
      "Epoch  514, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.097200\n",
      "Epoch  515, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.097000\n",
      "Epoch  516, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.096800\n",
      "Epoch  517, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.096600\n",
      "Epoch  518, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.096400\n",
      "Epoch  519, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.096200\n",
      "Epoch  520, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.096000\n",
      "Epoch  521, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.095800\n",
      "Epoch  522, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.095600\n",
      "Epoch  523, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.095400\n",
      "Epoch  524, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.095200\n",
      "Epoch  525, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.095000\n",
      "Epoch  526, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.094800\n",
      "Epoch  527, Training Loss: 0.0346, Training Acc: 1.0000, Learning Rate: 0.094600\n",
      "Epoch  528, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.094400\n",
      "Epoch  529, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.094200\n",
      "Epoch  530, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.094000\n",
      "Epoch  531, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.093800\n",
      "Epoch  532, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.093600\n",
      "Epoch  533, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.093400\n",
      "Epoch  534, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.093200\n",
      "Epoch  535, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.093000\n",
      "Epoch  536, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.092800\n",
      "Epoch  537, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.092600\n",
      "Epoch  538, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.092400\n",
      "Epoch  539, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.092200\n",
      "Epoch  540, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.092000\n",
      "Epoch  541, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.091800\n",
      "Epoch  542, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.091600\n",
      "Epoch  543, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.091400\n",
      "Epoch  544, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.091200\n",
      "Epoch  545, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.091000\n",
      "Epoch  546, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.090800\n",
      "Epoch  547, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.090600\n",
      "Epoch  548, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.090400\n",
      "Epoch  549, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.090200\n",
      "Epoch  550, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.090000\n",
      "Epoch  551, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.089800\n",
      "Epoch  552, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.089600\n",
      "Epoch  553, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.089400\n",
      "Epoch  554, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.089200\n",
      "Epoch  555, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.089000\n",
      "Epoch  556, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.088800\n",
      "Epoch  557, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.088600\n",
      "Epoch  558, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.088400\n",
      "Epoch  559, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.088200\n",
      "Epoch  560, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.088000\n",
      "Epoch  561, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.087800\n",
      "Epoch  562, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.087600\n",
      "Epoch  563, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.087400\n",
      "Epoch  564, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.087200\n",
      "Epoch  565, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.087000\n",
      "Epoch  566, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.086800\n",
      "Epoch  567, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.086600\n",
      "Epoch  568, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.086400\n",
      "Epoch  569, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.086200\n",
      "Epoch  570, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.086000\n",
      "Epoch  571, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.085800\n",
      "Epoch  572, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.085600\n",
      "Epoch  573, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.085400\n",
      "Epoch  574, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.085200\n",
      "Epoch  575, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.085000\n",
      "Epoch  576, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.084800\n",
      "Epoch  577, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.084600\n",
      "Epoch  578, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.084400\n",
      "Epoch  579, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.084200\n",
      "Epoch  580, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.084000\n",
      "Epoch  581, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.083800\n",
      "Epoch  582, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.083600\n",
      "Epoch  583, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.083400\n",
      "Epoch  584, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.083200\n",
      "Epoch  585, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.083000\n",
      "Epoch  586, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.082800\n",
      "Epoch  587, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.082600\n",
      "Epoch  588, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.082400\n",
      "Epoch  589, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.082200\n",
      "Epoch  590, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.082000\n",
      "Epoch  591, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.081800\n",
      "Epoch  592, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.081600\n",
      "Epoch  593, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.081400\n",
      "Epoch  594, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.081200\n",
      "Epoch  595, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.081000\n",
      "Epoch  596, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.080800\n",
      "Epoch  597, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.080600\n",
      "Epoch  598, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.080400\n",
      "Epoch  599, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.080200\n",
      "Epoch  600, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.080000\n",
      "Epoch  601, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.079800\n",
      "Epoch  602, Training Loss: 0.0347, Training Acc: 1.0000, Learning Rate: 0.079600\n",
      "Epoch  603, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.079400\n",
      "Epoch  604, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.079200\n",
      "Epoch  605, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.079000\n",
      "Epoch  606, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.078800\n",
      "Epoch  607, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.078600\n",
      "Epoch  608, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.078400\n",
      "Epoch  609, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.078200\n",
      "Epoch  610, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.078000\n",
      "Epoch  611, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.077800\n",
      "Epoch  612, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.077600\n",
      "Epoch  613, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.077400\n",
      "Epoch  614, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.077200\n",
      "Epoch  615, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.077000\n",
      "Epoch  616, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.076800\n",
      "Epoch  617, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.076600\n",
      "Epoch  618, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.076400\n",
      "Epoch  619, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.076200\n",
      "Epoch  620, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.076000\n",
      "Epoch  621, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.075800\n",
      "Epoch  622, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.075600\n",
      "Epoch  623, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.075400\n",
      "Epoch  624, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.075200\n",
      "Epoch  625, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.075000\n",
      "Epoch  626, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.074800\n",
      "Epoch  627, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.074600\n",
      "Epoch  628, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.074400\n",
      "Epoch  629, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.074200\n",
      "Epoch  630, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.074000\n",
      "Epoch  631, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.073800\n",
      "Epoch  632, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.073600\n",
      "Epoch  633, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.073400\n",
      "Epoch  634, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.073200\n",
      "Epoch  635, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.073000\n",
      "Epoch  636, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.072800\n",
      "Epoch  637, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.072600\n",
      "Epoch  638, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.072400\n",
      "Epoch  639, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.072200\n",
      "Epoch  640, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.072000\n",
      "Epoch  641, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.071800\n",
      "Epoch  642, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.071600\n",
      "Epoch  643, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.071400\n",
      "Epoch  644, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.071200\n",
      "Epoch  645, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.071000\n",
      "Epoch  646, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.070800\n",
      "Epoch  647, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.070600\n",
      "Epoch  648, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.070400\n",
      "Epoch  649, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.070200\n",
      "Epoch  650, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.070000\n",
      "Epoch  651, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.069800\n",
      "Epoch  652, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.069600\n",
      "Epoch  653, Training Loss: 0.0348, Training Acc: 1.0000, Learning Rate: 0.069400\n",
      "Epoch  654, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.069200\n",
      "Epoch  655, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.069000\n",
      "Epoch  656, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.068800\n",
      "Epoch  657, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.068600\n",
      "Epoch  658, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.068400\n",
      "Epoch  659, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.068200\n",
      "Epoch  660, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.068000\n",
      "Epoch  661, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.067800\n",
      "Epoch  662, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.067600\n",
      "Epoch  663, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.067400\n",
      "Epoch  664, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.067200\n",
      "Epoch  665, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.067000\n",
      "Epoch  666, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.066800\n",
      "Epoch  667, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.066600\n",
      "Epoch  668, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.066400\n",
      "Epoch  669, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.066200\n",
      "Epoch  670, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.066000\n",
      "Epoch  671, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.065800\n",
      "Epoch  672, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.065600\n",
      "Epoch  673, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.065400\n",
      "Epoch  674, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.065200\n",
      "Epoch  675, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.065000\n",
      "Epoch  676, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.064800\n",
      "Epoch  677, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.064600\n",
      "Epoch  678, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.064400\n",
      "Epoch  679, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.064200\n",
      "Epoch  680, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.064000\n",
      "Epoch  681, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.063800\n",
      "Epoch  682, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.063600\n",
      "Epoch  683, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.063400\n",
      "Epoch  684, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.063200\n",
      "Epoch  685, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.063000\n",
      "Epoch  686, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.062800\n",
      "Epoch  687, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.062600\n",
      "Epoch  688, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.062400\n",
      "Epoch  689, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.062200\n",
      "Epoch  690, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.062000\n",
      "Epoch  691, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.061800\n",
      "Epoch  692, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.061600\n",
      "Epoch  693, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.061400\n",
      "Epoch  694, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.061200\n",
      "Epoch  695, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.061000\n",
      "Epoch  696, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.060800\n",
      "Epoch  697, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.060600\n",
      "Epoch  698, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.060400\n",
      "Epoch  699, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.060200\n",
      "Epoch  700, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.060000\n",
      "Epoch  701, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.059800\n",
      "Epoch  702, Training Loss: 0.0349, Training Acc: 1.0000, Learning Rate: 0.059600\n",
      "Epoch  703, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.059400\n",
      "Epoch  704, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.059200\n",
      "Epoch  705, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.059000\n",
      "Epoch  706, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.058800\n",
      "Epoch  707, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.058600\n",
      "Epoch  708, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.058400\n",
      "Epoch  709, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.058200\n",
      "Epoch  710, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.058000\n",
      "Epoch  711, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.057800\n",
      "Epoch  712, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.057600\n",
      "Epoch  713, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.057400\n",
      "Epoch  714, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.057200\n",
      "Epoch  715, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.057000\n",
      "Epoch  716, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.056800\n",
      "Epoch  717, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.056600\n",
      "Epoch  718, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.056400\n",
      "Epoch  719, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.056200\n",
      "Epoch  720, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.056000\n",
      "Epoch  721, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.055800\n",
      "Epoch  722, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.055600\n",
      "Epoch  723, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.055400\n",
      "Epoch  724, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.055200\n",
      "Epoch  725, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.055000\n",
      "Epoch  726, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.054800\n",
      "Epoch  727, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.054600\n",
      "Epoch  728, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.054400\n",
      "Epoch  729, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.054200\n",
      "Epoch  730, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.054000\n",
      "Epoch  731, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.053800\n",
      "Epoch  732, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.053600\n",
      "Epoch  733, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.053400\n",
      "Epoch  734, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.053200\n",
      "Epoch  735, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.053000\n",
      "Epoch  736, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.052800\n",
      "Epoch  737, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.052600\n",
      "Epoch  738, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.052400\n",
      "Epoch  739, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.052200\n",
      "Epoch  740, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.052000\n",
      "Epoch  741, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.051800\n",
      "Epoch  742, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.051600\n",
      "Epoch  743, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.051400\n",
      "Epoch  744, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.051200\n",
      "Epoch  745, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.051000\n",
      "Epoch  746, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.050800\n",
      "Epoch  747, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.050600\n",
      "Epoch  748, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.050400\n",
      "Epoch  749, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.050200\n",
      "Epoch  750, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.050000\n",
      "Epoch  751, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.049800\n",
      "Epoch  752, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.049600\n",
      "Epoch  753, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.049400\n",
      "Epoch  754, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.049200\n",
      "Epoch  755, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.049000\n",
      "Epoch  756, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.048800\n",
      "Epoch  757, Training Loss: 0.0350, Training Acc: 1.0000, Learning Rate: 0.048600\n",
      "Epoch  758, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.048400\n",
      "Epoch  759, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.048200\n",
      "Epoch  760, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.048000\n",
      "Epoch  761, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.047800\n",
      "Epoch  762, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.047600\n",
      "Epoch  763, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.047400\n",
      "Epoch  764, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.047200\n",
      "Epoch  765, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.047000\n",
      "Epoch  766, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.046800\n",
      "Epoch  767, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.046600\n",
      "Epoch  768, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.046400\n",
      "Epoch  769, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.046200\n",
      "Epoch  770, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.046000\n",
      "Epoch  771, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.045800\n",
      "Epoch  772, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.045600\n",
      "Epoch  773, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.045400\n",
      "Epoch  774, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.045200\n",
      "Epoch  775, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.045000\n",
      "Epoch  776, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.044800\n",
      "Epoch  777, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.044600\n",
      "Epoch  778, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.044400\n",
      "Epoch  779, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.044200\n",
      "Epoch  780, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.044000\n",
      "Epoch  781, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.043800\n",
      "Epoch  782, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.043600\n",
      "Epoch  783, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.043400\n",
      "Epoch  784, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.043200\n",
      "Epoch  785, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.043000\n",
      "Epoch  786, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.042800\n",
      "Epoch  787, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.042600\n",
      "Epoch  788, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.042400\n",
      "Epoch  789, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.042200\n",
      "Epoch  790, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.042000\n",
      "Epoch  791, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.041800\n",
      "Epoch  792, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.041600\n",
      "Epoch  793, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.041400\n",
      "Epoch  794, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.041200\n",
      "Epoch  795, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.041000\n",
      "Epoch  796, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.040800\n",
      "Epoch  797, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.040600\n",
      "Epoch  798, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.040400\n",
      "Epoch  799, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.040200\n",
      "Epoch  800, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.040000\n",
      "Epoch  801, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.039800\n",
      "Epoch  802, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.039600\n",
      "Epoch  803, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.039400\n",
      "Epoch  804, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.039200\n",
      "Epoch  805, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.039000\n",
      "Epoch  806, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.038800\n",
      "Epoch  807, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.038600\n",
      "Epoch  808, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.038400\n",
      "Epoch  809, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.038200\n",
      "Epoch  810, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.038000\n",
      "Epoch  811, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.037800\n",
      "Epoch  812, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.037600\n",
      "Epoch  813, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.037400\n",
      "Epoch  814, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.037200\n",
      "Epoch  815, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.037000\n",
      "Epoch  816, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.036800\n",
      "Epoch  817, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.036600\n",
      "Epoch  818, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.036400\n",
      "Epoch  819, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.036200\n",
      "Epoch  820, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.036000\n",
      "Epoch  821, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.035800\n",
      "Epoch  822, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.035600\n",
      "Epoch  823, Training Loss: 0.0351, Training Acc: 1.0000, Learning Rate: 0.035400\n",
      "Epoch  824, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.035200\n",
      "Epoch  825, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.035000\n",
      "Epoch  826, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.034800\n",
      "Epoch  827, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.034600\n",
      "Epoch  828, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.034400\n",
      "Epoch  829, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.034200\n",
      "Epoch  830, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.034000\n",
      "Epoch  831, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.033800\n",
      "Epoch  832, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.033600\n",
      "Epoch  833, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.033400\n",
      "Epoch  834, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.033200\n",
      "Epoch  835, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.033000\n",
      "Epoch  836, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.032800\n",
      "Epoch  837, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.032600\n",
      "Epoch  838, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.032400\n",
      "Epoch  839, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.032200\n",
      "Epoch  840, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.032000\n",
      "Epoch  841, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.031800\n",
      "Epoch  842, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.031600\n",
      "Epoch  843, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.031400\n",
      "Epoch  844, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.031200\n",
      "Epoch  845, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.031000\n",
      "Epoch  846, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.030800\n",
      "Epoch  847, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.030600\n",
      "Epoch  848, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.030400\n",
      "Epoch  849, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.030200\n",
      "Epoch  850, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.030000\n",
      "Epoch  851, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.029800\n",
      "Epoch  852, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.029600\n",
      "Epoch  853, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.029400\n",
      "Epoch  854, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.029200\n",
      "Epoch  855, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.029000\n",
      "Epoch  856, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.028800\n",
      "Epoch  857, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.028600\n",
      "Epoch  858, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.028400\n",
      "Epoch  859, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.028200\n",
      "Epoch  860, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.028000\n",
      "Epoch  861, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.027800\n",
      "Epoch  862, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.027600\n",
      "Epoch  863, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.027400\n",
      "Epoch  864, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.027200\n",
      "Epoch  865, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.027000\n",
      "Epoch  866, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.026800\n",
      "Epoch  867, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.026600\n",
      "Epoch  868, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.026400\n",
      "Epoch  869, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.026200\n",
      "Epoch  870, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.026000\n",
      "Epoch  871, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.025800\n",
      "Epoch  872, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.025600\n",
      "Epoch  873, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.025400\n",
      "Epoch  874, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.025200\n",
      "Epoch  875, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.025000\n",
      "Epoch  876, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.024800\n",
      "Epoch  877, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.024600\n",
      "Epoch  878, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.024400\n",
      "Epoch  879, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.024200\n",
      "Epoch  880, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.024000\n",
      "Epoch  881, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.023800\n",
      "Epoch  882, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.023600\n",
      "Epoch  883, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.023400\n",
      "Epoch  884, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.023200\n",
      "Epoch  885, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.023000\n",
      "Epoch  886, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.022800\n",
      "Epoch  887, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.022600\n",
      "Epoch  888, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.022400\n",
      "Epoch  889, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.022200\n",
      "Epoch  890, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.022000\n",
      "Epoch  891, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.021800\n",
      "Epoch  892, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.021600\n",
      "Epoch  893, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.021400\n",
      "Epoch  894, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.021200\n",
      "Epoch  895, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.021000\n",
      "Epoch  896, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.020800\n",
      "Epoch  897, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.020600\n",
      "Epoch  898, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.020400\n",
      "Epoch  899, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.020200\n",
      "Epoch  900, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.020000\n",
      "Epoch  901, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.019800\n",
      "Epoch  902, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.019600\n",
      "Epoch  903, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.019400\n",
      "Epoch  904, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.019200\n",
      "Epoch  905, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.019000\n",
      "Epoch  906, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.018800\n",
      "Epoch  907, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.018600\n",
      "Epoch  908, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.018400\n",
      "Epoch  909, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.018200\n",
      "Epoch  910, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.018000\n",
      "Epoch  911, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.017800\n",
      "Epoch  912, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.017600\n",
      "Epoch  913, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.017400\n",
      "Epoch  914, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.017200\n",
      "Epoch  915, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.017000\n",
      "Epoch  916, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.016800\n",
      "Epoch  917, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.016600\n",
      "Epoch  918, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.016400\n",
      "Epoch  919, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.016200\n",
      "Epoch  920, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.016000\n",
      "Epoch  921, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.015800\n",
      "Epoch  922, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.015600\n",
      "Epoch  923, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.015400\n",
      "Epoch  924, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.015200\n",
      "Epoch  925, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.015000\n",
      "Epoch  926, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.014800\n",
      "Epoch  927, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.014600\n",
      "Epoch  928, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.014400\n",
      "Epoch  929, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.014200\n",
      "Epoch  930, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.014000\n",
      "Epoch  931, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.013800\n",
      "Epoch  932, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.013600\n",
      "Epoch  933, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.013400\n",
      "Epoch  934, Training Loss: 0.0352, Training Acc: 1.0000, Learning Rate: 0.013200\n",
      "Epoch  935, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.013000\n",
      "Epoch  936, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.012800\n",
      "Epoch  937, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.012600\n",
      "Epoch  938, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.012400\n",
      "Epoch  939, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.012200\n",
      "Epoch  940, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.012000\n",
      "Epoch  941, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.011800\n",
      "Epoch  942, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.011600\n",
      "Epoch  943, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.011400\n",
      "Epoch  944, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.011200\n",
      "Epoch  945, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.011000\n",
      "Epoch  946, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.010800\n",
      "Epoch  947, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.010600\n",
      "Epoch  948, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.010400\n",
      "Epoch  949, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.010200\n",
      "Epoch  950, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.010000\n",
      "Epoch  951, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.009800\n",
      "Epoch  952, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.009600\n",
      "Epoch  953, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.009400\n",
      "Epoch  954, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.009200\n",
      "Epoch  955, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.009000\n",
      "Epoch  956, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.008800\n",
      "Epoch  957, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.008600\n",
      "Epoch  958, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.008400\n",
      "Epoch  959, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.008200\n",
      "Epoch  960, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.008000\n",
      "Epoch  961, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.007800\n",
      "Epoch  962, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.007600\n",
      "Epoch  963, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.007400\n",
      "Epoch  964, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.007200\n",
      "Epoch  965, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.007000\n",
      "Epoch  966, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.006800\n",
      "Epoch  967, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.006600\n",
      "Epoch  968, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.006400\n",
      "Epoch  969, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.006200\n",
      "Epoch  970, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.006000\n",
      "Epoch  971, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.005800\n",
      "Epoch  972, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.005600\n",
      "Epoch  973, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.005400\n",
      "Epoch  974, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.005200\n",
      "Epoch  975, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.005000\n",
      "Epoch  976, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.004800\n",
      "Epoch  977, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.004600\n",
      "Epoch  978, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.004400\n",
      "Epoch  979, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.004200\n",
      "Epoch  980, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.004000\n",
      "Epoch  981, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.003800\n",
      "Epoch  982, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.003600\n",
      "Epoch  983, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.003400\n",
      "Epoch  984, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.003200\n",
      "Epoch  985, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.003000\n",
      "Epoch  986, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.002800\n",
      "Epoch  987, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.002600\n",
      "Epoch  988, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.002400\n",
      "Epoch  989, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.002200\n",
      "Epoch  990, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.002000\n",
      "Epoch  991, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.001800\n",
      "Epoch  992, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.001600\n",
      "Epoch  993, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.001400\n",
      "Epoch  994, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.001200\n",
      "Epoch  995, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.001000\n",
      "Epoch  996, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.000800\n",
      "Epoch  997, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.000600\n",
      "Epoch  998, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.000400\n",
      "Epoch  999, Training Loss: 0.0353, Training Acc: 1.0000, Learning Rate: 0.000200\n",
      "\n",
      "Best Model Test Accuracy: 0.9954\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lib.data_loader import get_monks_dataset\n",
    "from lib.neural_network import NeuralNetwork\n",
    "from lib.grid_search import grid_search\n",
    "\n",
    "\n",
    "MONK_DATASET_ID = 1\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_monks_dataset(MONK_DATASET_ID, one_hot_encode=True)\n",
    "input_size = X_train.shape[1]\n",
    "hidden_layers = [10]\n",
    "output_size = 1\n",
    "layers = [input_size] + hidden_layers + [output_size]\n",
    "activation_funcs = [\"relu\", \"sigmoid\"]\n",
    "\n",
    "### fin qui dovrebbe essere tutto chiaro...\n",
    "\n",
    "def build_nn_model_with_params(learning_rate=0.2, lambda_reg=0.001, reg_type=\"l2\",\n",
    "                                   lr_decay_type=\"linear\", decay_rate=0.001, weight_init=\"base\"):\n",
    "    \n",
    "    ### mi serve una funzione helper da passare alla grid search, cosìcche possa costruire un modello per ogni combinazione di parametri\n",
    "        \n",
    "    return NeuralNetwork(\n",
    "        layers=layers,\n",
    "        learning_rate=learning_rate,\n",
    "        lambda_reg=lambda_reg,\n",
    "        reg_type=reg_type,\n",
    "        loss_function_name=\"mse\",\n",
    "        activation_function_names=activation_funcs,\n",
    "        task=\"classification\",\n",
    "        lr_decay_type=lr_decay_type,\n",
    "        decay_rate=decay_rate,\n",
    "        weight_init=weight_init\n",
    "    )\n",
    "\n",
    "param_grid = { # la griglia dei parametri da testare\n",
    "        \"learning_rate\": [0.1, 0.2],\n",
    "        \"lambda_reg\": [0.001, 0.01],\n",
    "        \"lr_decay_type\": [\"linear\", \"none\"],\n",
    "        \"decay_rate\": [0.001, 0.0],\n",
    "        \"weight_init\": [\"base\", \"glorot\"],\n",
    "}\n",
    "\n",
    "best_params, all_results = grid_search(\n",
    "        # la grid search farà la seguente cosa. \n",
    "        # per ogni combinazione di parametri:\n",
    "            # 1. crea un modello con quei parametri\n",
    "            # 2. fa il k-fold cross validation, quindi divide il dataset in k parti, e per ogni parte:\n",
    "            #   1. addestra il modello sulle altre k-1 parti\n",
    "            #   2. valuta il modello sulla parte rimanente\n",
    "            #   3. calcola le performance ottenute\n",
    "            # 3. calcola la media delle performance ottenute su tutte le k parti\n",
    "            # 4. restituisce la media delle performance ottenute su tutte le k parti\n",
    "        # dopo di che, restituisce i parametri che hanno ottenuto la migliore media delle performance\n",
    "    \n",
    "        model_builder=build_nn_model_with_params,\n",
    "        param_grid=param_grid,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        k=5,\n",
    "        epochs=500,       \n",
    "        batch_size=32,\n",
    "        early_stopping=True, # nella grid search, l'early stopping può essere indicato True anche senza un validation set, perché viene usato il k-fold cross validation. Guarda sotto, penultimo train()\n",
    "        patience=10,\n",
    "        min_delta=1e-4,\n",
    "        n_jobs=-1, # usa tutti i core disponibili\n",
    "        maximize=True, # la media va massimizzata, dato il fatto che viene usata l'accuracy [:questo è specificato nel metodo evaluate() della rete!]\n",
    "        verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Grid Search Best Result:\")\n",
    "print(best_params)\n",
    "best_hyperparams = best_params[\"params\"] \n",
    "\n",
    "# mi rebuildo la rete con i migliori params\n",
    "nn_best = build_nn_model_with_params(\n",
    "    learning_rate=best_hyperparams['learning_rate'],\n",
    "    lr_decay_type=best_hyperparams['lr_decay_type'],\n",
    "    lambda_reg=best_hyperparams['lambda_reg'],\n",
    "    weight_init=best_hyperparams['weight_init']\n",
    ")\n",
    "\n",
    "# splitto il monk su dev set e test set, così da poter poi splittare il dev set in train e validation, e fare i plot sia sul train che sul validation\n",
    "X_dev, y_dev, X_test, y_test = get_monks_dataset(1, one_hot_encode=True)\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_dev, y_dev, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "nn_best.train(\n",
    "    X_train_split, y_train_split,\n",
    "    epochs=1000,\n",
    "    batch_size=64,\n",
    "    verbose=True,\n",
    "    validation_data=(X_val_split, y_val_split) # passo il validation set, così da poter fare i plot. \n",
    "                                               # Nota anche il fatto che il validation set è NECESSARIO se si intende fare early stopping (non questo caso)\n",
    ")\n",
    "    \n",
    "nn_best.plot_loss_history() # plotto la loss sia sul train che sul validation\n",
    "nn_best.plot_accuracy_history() # plotto l'accuracy sia sul train che sul validation\n",
    "\n",
    "# OK, quindi abbiamo i nostri plot su training e validation, ora retraino sull'intero dev set, per testare sul test set\n",
    "\n",
    "nn_best = build_nn_model_with_params(\n",
    "    learning_rate=best_hyperparams['learning_rate'],\n",
    "    lr_decay_type=best_hyperparams['lr_decay_type'],\n",
    "    lambda_reg=best_hyperparams['lambda_reg'],\n",
    "    weight_init=best_hyperparams['weight_init']\n",
    ")\n",
    "\n",
    "nn_best.train(\n",
    "    X_dev, y_dev,\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "test_accuracy = nn_best.evaluate(X_test, y_test)\n",
    "print(f\"\\nBest Model Test Accuracy: {test_accuracy:.4f}\")\n",
    "nn_best.plot_loss_history() # questi plot non conterranno il validation set, ma solo i dati relativi al training set (che è il dev set)\n",
    "nn_best.plot_accuracy_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
